{\rtf1\ansi\ansicpg1252\cocoartf2708
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww19440\viewh35880\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 The Case Against Reality by Amanda Gefter and Quanta Magazine\
\
\
A professor of cognitive science argues that the world is nothing like the one we experience through ur senses.\
\
As we go about our daily lives, we tend to assume that our perceptions\'97 sights, sounds, textures, tastes\'97are an accurate portrayal of the real world. Sure, when we stop and think about it\'97or when we find ourselves fooled by a perceptual illusion\'97we realize with a jolt that what we perceive is never the world directly, but rather our brain\'92s best guess at what that world is like, a kind of internal simulation of an external reality. Still, we bank on the fact that our simulation is a reasonably decent one. If it wasn\'92t, wouldn\'92t evolution have weeded us out by now? The true reality might be forever beyond our reach, but surely our senses give us at least an inkling of what it\'92s really like.\
\
Not so, says Donald D. Hoffman, a professor of cognitive science at the University of California, Irvine. Hoffman has spent the past three decades studying perception, artificial intelligence, evolutionary game theory and the brain, and his conclusion is a dramatic one: The world presented to us by our perceptions is nothing like reality. What\'92s more, he says, we have evolution itself to thank for this magnificent illusion, as it maximizes evolutionary fitness by driving truth to extinction.\
\
Getting at questions about the nature of reality, and disentangling the observer from the observed, is an endeavor that straddles the boundaries of neuroscience and fundamental physics. On one side you\'92ll find researchers scratching their chins raw trying to understand how a three-pound lump of gray matter obeying nothing more than the ordinary laws of physics can give rise to first-person conscious experience. This is the aptly named \'93hard problem.\'94\
\
On the other side are quantum physicists, marveling at the strange fact that quantum systems don\'92t seem to be definite objects localized in space until we come along to observe them. Experiment after experiment has shown\'97 defying common sense\'97that if we assume that the particles that make up ordinary objects have an objective, observer-independent existence, we get the wrong answers. The central lesson of quantum physics is clear: There are no public objects sitting out there in some preexisting space. As the physicist John Wheeler put it, \'93Useful as it is under ordinary circumstances to say that the world exists \'91out there\'92 independent of us, that view can no longer be upheld.\'94\
\
So while neuroscientists struggle to understand how there can be such a thing as a first-person reality, quantum physicists have to grapple with the mystery of how there can be anything but a first-person reality. In short, all roads lead back to the observer. And that\'92s where you can find Hoffman\'97straddling the boundaries, attempting a mathematical model of the observer, trying to get at the reality behind the illusion. Quanta Magazine caught up with him to find out more.\
\
Gefter: People often use Darwinian evolution as an argument that our perceptions accurately reflect reality. They say, \'93Obviously we must be latching onto reality in some way because otherwise we would have been wiped out a long time ago. If I think I\'92m seeing a palm tree but it\'92s really a tiger, I\'92m in trouble.\'94\
\
Hoffman: Right. The classic argument is that those of our ancestors who saw more accurately had a competitive advantage over those who saw less accurately and thus were more likely to pass on their genes that coded for those more accurate perceptions, so after thousands of generations we can be quite confident that we\'92re the offspring of those who saw accurately, and so we see accurately. That sounds very plausible. But I think it is utterly false. It misunderstands the fundamental fact about evolution, which is that it\'92s about fitness functions\'97mathematical functions that describe how well a given strategy achieves the goals of survival and reproduction. The mathematical physicist Chetan Prakash proved a theorem that I devised that says: According to evolution by natural selection, an organism that sees reality as it is will never be more fit than an organism of equal complexity that sees none of reality but is just tuned to fitness. Never.\
\
Gefter: You\'92ve done computer simulations to show this. Can you give an example?\
\
Hoffman: Suppose in reality there\'92s a resource, like water, and you can quantify how much of it there is in an objective order\'97very little water, medium amount of water, a lot of water. Now suppose your fitness function is linear, so a little water gives you a little fitness, medium water gives you medium fitness, and lots of water gives you lots of fitness\'97in that case, the organism that sees the truth about the water in the world can win, but only because the fitness function happens to align with the true structure in reality. Generically, in the real world, that will never be the case. Something much more natural is a bell curve\'97say, too little water you die of thirst, but too much water you drown, and only somewhere in between is good for survival. Now the fitness function doesn\'92t match the structure in the real world. And that\'92s enough to send truth to extinction. For example, an organism tuned to fitness might see small and large quantities of some resource as, say, red, to indicate low fitness, whereas they might see intermediate quantities as green, to indicate high fitness. Its perceptions will be tuned to fitness, but not to truth. It won\'92t see any distinction between small and large\'97it only sees red\'97 even though such a distinction exists in reality.\
\
Gefter: But how can seeing a false reality be beneficial to an organism\'92s survival?\
\
Hoffman: There\'92s a metaphor that\'92s only been available to us in the past 30 or 40 years, and that\'92s the desktop interface. Suppose there\'92s a blue rectangular icon on the lower right corner of your computer\'92s desktop \'97 does that mean that the file itself is blue and rectangular and lives in the lower right corner of your computer? Of course not. But those are the only things that can be asserted about anything on the desktop \'97 it has color, position, and shape. Those are the only categories available to you, and yet none of them are true about the file itself or anything in the computer. They couldn\'92t possibly be true. That\'92s an interesting thing. You could not form a true description of the innards of the computer if your entire view of reality was confined to the desktop. And yet the desktop is useful. That blue rectangular icon guides my behavior, and it hides a complex reality that I don\'92t need to know. That\'92s the key idea. Evolution has shaped us with perceptions that allow us to survive. They guide adaptive behaviors. But part of that involves hiding from us the stuff we don\'92t need to know. And that\'92s pretty much all of reality, whatever reality might be. If you had to spend all that time figuring it out, the tiger would eat you.\
\
Gefter: So everything we see is one big illusion?\
\
Hoffman: We\'92ve been shaped to have perceptions that keep us alive, so we have to take them seriously. If I see something that I think of as a snake, I don\'92t pick it up. If I see a train, I don\'92t step in front of it. I\'92ve evolved these symbols to keep me alive, so I have to take them seriously. But it\'92s a logical flaw to think that if we have to take it seriously, we also have to take it literally.\
\
Gefter: If snakes aren\'92t snakes and trains aren\'92t trains, what are they?\
\
Hoffman: Snakes and trains, like the particles of physics, have no objective, observer-independent features. The snake I see is a description created by my sensory system to inform me of the fitness consequences of my actions. Evolution shapes acceptable solutions, not optimal ones. A snake is an acceptable solution to the problem of telling me how to act in a situation. My snakes and trains are my mental representations; your snakes and trains are your mental representations.\
\
Gefter: How did you first become interested in these ideas?\
\
Hoffman: As a teenager, I was very interested in the question \'93Are we machines?\'94 My reading of the science suggested that we are. But my dad was a minister, and at church they were saying we\'92re not. So I decided I needed to figure it out for myself. It\'92s sort of an important personal question\'97if I\'92m a machine, I would like to find that out! And if I\'92m not, I\'92d like to know, what is that special magic beyond the machine? So eventually in the 1980s I went to the artificial-intelligence lab at MIT and worked on machine perception. The field of vision research was enjoying a newfound success in developing mathematical models for specific visual abilities. I noticed that they seemed to share a common mathematical structure, so I thought it might be possible to write down a formal structure for observation that encompassed all of them, perhaps all possible modes of observation. I was inspired in part by Alan Turing. When he invented the Turing machine, he was trying to come up with a notion of computation, and instead of putting bells and whistles on it, he said, Let\'92s get the simplest, most pared down mathematical description that could possibly work. And that simple formalism is the foundation for the science of computation. So I wondered, could I provide a similarly simple formal foundation for the science of observation?\
\
Gefter: A mathematical model of consciousness.\
\
Hoffman: That\'92s right. My intuition was, there are conscious experiences. I have pains, tastes, smells, all my sensory experiences, moods, emotions and so forth. So I\'92m just going to say: One part of this consciousness structure is a set of all possible experiences. When I\'92m having an experience, based on that experience I may want to change what I\'92m doing. So I need to have a collection of possible actions I can take and a decision strategy that, given my experiences, allows me to change how I\'92m acting. That\'92s the basic idea of the whole thing. I have a space X of experiences, a space G of actions, and an algorithm D that lets me choose a new action given my experiences. Then I posited a W for a world, which is also a probability space. Somehow the world affects my perceptions, so there\'92s a perception map P from the world to my experiences, and when I act, I change the world, so there\'92s a map A from the space of actions to the world. That\'92s the entire structure. Six elements. The claim is: this is the structure of consciousness. I put that out there so people have something to shoot at.\
\
Gefter: But if there\'92s a W, are you saying there is an external world?\
\
Hoffman: Here\'92s the striking thing about that. I can pull the W out of the model and stick a conscious agent in its place and get a circuit of conscious agents. In fact, you can have whole networks of arbitrary complexity. And that\'92s the world.\
\
Gefter: The world is just other conscious agents?\
\
Hoffman: I call it conscious realism: Objective reality is just conscious agents, just points of view. Interestingly, I can take two conscious agents and have them interact, and the mathematical structure of that interaction also satisfies the definition of a conscious agent. This mathematics is telling me something. I can take two minds, and they can generate a new, unified single mind. Here\'92s a concrete example. We have two hemispheres in our brain. But when you do a split-brain operation, a complete transection of the corpus callosum, you get clear evidence of two separate consciousnesses. Before that slicing happened, it seemed there was a single unified consciousness. So it\'92s not implausible that there is a single conscious agent. And yet it\'92s also the case that there are two conscious agents there, and you can see that when they\'92re split. I didn\'92t expect that, the mathematics forced me to recognize this. It suggests that I can take separate observers, put them together and create new observers, and keep doing this ad infinitum. It\'92s conscious agents all the way down.\
\
Gefter: If it\'92s conscious agents all the way down, all first-person points of view, what happens to science? Science has always been a third-person description of the world.\
\
Hoffman: The idea that what we\'92re doing is measuring publicly accessible objects, the idea that objectivity results from the fact that you and I can measure the same object in the exact same situation and get the same results \'97 it\'92s very clear from quantum mechanics that that idea has to go. Physics tells us that there are no public physical objects. So what\'92s going on? Here\'92s how I think about it. I can talk to you about my headache and believe that I am communicating effectively with you, because you\'92ve had your own headaches. The same thing is true as apples and the moon and the sun and the universe. Just like you have your own headache, you have your own moon. But I assume it\'92s relevantly similar to mine. That\'92s an assumption that could be false, but that\'92s the source of my communication, and that\'92s the best we can do in terms of public physical objects and objective science.\
\
Gefter: It doesn\'92t seem like many people in neuroscience or philosophy of mind are thinking about fundamental physics. Do you think that\'92s been a stumbling block for those trying to understand consciousness?\
\
Hoffman: I think it has been. Not only are they ignoring the progress in fundamental physics, they are often explicit about it. They\'92ll say openly that quantum physics is not relevant to the aspects of brain function that are causally involved in consciousness. They are certain that it\'92s got to be classical properties of neural activity, which exist independent of any observers\'97 spiking rates, connection strengths at synapses, perhaps dynamical properties as well. These are all very classical notions under Newtonian physics, where time is absolute and objects exist absolutely. And then [neuroscientists] are mystified as to why they don\'92t make progress. They don\'92t avail themselves of the incredible insights and breakthroughs that physics has made. Those insights are out there for us to use, and yet my field says, \'93We\'92ll stick with Newton, thank you. We\'92ll stay 300 years behind in our physics.\'94\
\
Gefter: I suspect they\'92re reacting to things like Roger Penrose and Stuart Hameroff\'92s model, where you still have a physical brain, it\'92s still sitting in space, but supposedly it\'92s performing some quantum feat. In contrast, you\'92re saying, \'93Look, quantum mechanics is telling us that we have to question the very notions of \'91physical things\'92 sitting in \'91space.\'92\'94\
\
Hoffman: I think that\'92s absolutely true. The neuroscientists are saying, \'93We don\'92t need to invoke those kind of quantum processes, we don\'92t need quantum wave functions collapsing inside neurons, we can just use classical physics to describe processes in the brain.\'94 I\'92m emphasizing the larger lesson of quantum mechanics: Neurons, brains, space \'85 these are just symbols we use, they\'92re not real. It\'92s not that there\'92s a classical brain that does some quantum magic. It\'92s that there\'92s no brain! Quantum mechanics says that classical objects\'97including brains\'97don\'92t exist. So this is a far more radical claim about the nature of reality and does not involve the brain pulling off some tricky quantum computation. So even Penrose hasn\'92t taken it far enough. But most of us, you know, we\'92re born realists. We\'92re born physicalists. This is a really, really hard one to let go of.\
\
Gefter: To return to the question you started with as a teenager, are we machines?\
\
Hoffman: The formal theory of conscious agents I\'92ve been developing is computationally universal\'97in that sense, it\'92s a machine theory. And it\'92s because the theory is computationally universal that I can get all of cognitive science and neural networks back out of it. Nevertheless, for now I don\'92t think we are machines\'97in part because I distinguish between the mathematical representation and the thing being represented. As a conscious realist, I am postulating conscious experiences as ontological primitives, the most basic ingredients of the world. I\'92m claiming that experiences are the real coin of the realm. The experiences of everyday life\'97my real feeling of a headache, my real taste of chocolate\'97that really is the ultimate nature of reality.\
\
\
Pause Giant AI Experiments: An Open Letter\
\
\
AI systems with human-competitive intelligence can pose profound risks to society and humanity, as shown by extensive research and acknowledged by top AI labs. As stated in the widely-endorsed Asilomar AI Principles, Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources. Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one \'96 not even their creators \'96 can understand, predict, or reliably control.\
\
Contemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable. This confidence must be well justified and increase with the magnitude of a system's potential effects. OpenAI's recent statement regarding artificial general intelligence, states that "At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models." We agree. That point is now.\
\
Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT@4. This pause should be public and verifiable, and include all key actors. If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.\
\
AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts. These protocols should ensure that systems adhering to them are safe beyond a reasonable doubt. This does not mean a pause on AI development in general, merely a stepping back from the dangerous race to ever-larger unpredictable black-box models with emergent capabilities.\
\
AI research and development should be refocused on making today's powerful, state-of-the-art systems more accurate, safe, interpretable, transparent, robust, aligned, trustworthy, and loyal.\
\
\pard\pardeftab720\partightenfactor0
\cf0 In parallel, AI developers must work with policymakers to dramatically accelerate development of robust AI governance systems. These should at a minimum include: new and capable regulatory authorities dedicated to AI; oversight and tracking of highly capable AI systems and large pools of computational capability; provenance and watermarking systems to help distinguish real from synthetic and to track model leaks; a robust auditing and certification ecosystem; liability for AI-caused harm; robust public funding for technical AI safety research; and well-resourced institutions for coping with the dramatic economic and political disruptions (especially to democracy) that AI will cause.\
\
Humanity can enjoy a flourishing future with AI. Having succeeded in creating powerful AI systems, we can now enjoy an "AI summer" in which we reap the rewards, engineer these systems for the clear benefit of all, and give society a chance to adapt. Society has hit pause on other technologies with potentially catastrophic effects on society. We can do so here. Let's enjoy a long AI summer, not rush unprepared into a fall.\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 What\'92s the Future for A.I.? Where we\'92re heading tomorrow, next year and beyond by Cade Metz\
\
\
In today\'92s A.I. newsletter, the last in our five-part series, I look at where artificial intelligence may be headed in the years to come.\
\
\pard\pardeftab720\partightenfactor0
\cf0 In early March, I visited OpenAI\'92s San Francisco offices for an early look at GPT-4, a new version of the technology that underpins its ChatGPT chatbot. The most eye-popping moment arrived when Greg Brockman, OpenAI\'92s president and co-founder, showed off a feature that is still unavailable to the public: He gave the bot a photograph from the Hubble Space Telescope and asked it to describe the image \'93in painstaking detail.\'94\
\
The description was completely accurate, right down to the strange white line created by a satellite streaking across the heavens. This is one look at the future of chatbots and other A.I. technologies: A new wave of multimodal systems will juggle images, sounds and videos as well as text.\
\
Yesterday, my colleague Kevin Roose told you about what A.I. can do now. I\'92m going to focus on the opportunities and upheavals to come as it gains abilities and skills.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
AI. In the near term\
\
Generative A.I.s can already answer questions, write poetry, generate computer code and carry on conversations. As \'93chatbot\'94 suggests, they are first being rolled out in conversational formats like ChatGPT and Bing.\
\
But that\'92s not going to last long. Microsoft and Google have already announced plans to incorporate these A.I. technologies into their products. You\'92ll be able to use them to write a rough draft of an email, automatically summarize a meeting and pull off many other cool tricks.\
\
OpenAI also offers an A.P.I., or application programming interface, that other tech companies can use to plug GPT-4 into their apps and products. And it has created a series of plug-ins from companies like Instacart, Expedia and Wolfram Alpha that expand ChatGPT\'92s abilities.\
\
A.I. in the medium term\
\
Many experts believe A.I. will make some workers, including doctors, lawyers and computer programmers, more productive than ever. They also believe some workers will be replaced.\
\
\'93This will affect tasks that are more repetitive, more formulaic, more generic,\'94 said Zachary Lipton, a professor at Carnegie Mellon who specializes in artificial intelligence and its impact on society. \'93This can liberate some people who are not good at repetitive tasks. At the same time, there is a threat to people who specialize in the repetitive part.\'94\
\
Human-performed jobs could disappear from audio-to-text transcription and translation. In the legal field, GPT-4 is already proficient enough to ace the bar exam, and the accounting firm PricewaterhouseCoopers plans to roll out an OpenAI-powered legal chatbot to its staff.\
\
At the same time, companies like OpenAI, Google and Meta are building systems that let you instantly generate images and videos simply by describing what you want to see.\
\
Other companies are building bots that can actually use websites and software applications as a human does. In the next stage of the technology, A.I. systems could shop online for your Christmas presents, hire people to do small jobs around the house and track your monthly expenses.\
\
All that is a lot to think about. But the biggest issue may be this: Before we have a chance to grasp how these systems will affect the world, they will get even more powerful.\
\
A.I. in the long term\
\
For companies like OpenAI and DeepMind, a lab that\'92s owned by Google\'92s parent company, the plan is to push this technology as far as it will go. They hope to eventually build what researchers call artificial general intelligence, or A.G.I. \'97 a machine that can do anything the human brain can do.\
\
As Sam Altman, OpenAI\'92s chief executive, told me three years ago: \'93My goal is to build broadly beneficial A.G.I. I also understand this sounds ridiculous.\'94 Today, it sounds less ridiculous. But it is still easier said than done.\
\
For an A.I. to become an A.G.I., it will require an understanding of the physical world writ large. And it is not clear whether systems can learn to mimic the length and breadth of human reasoning and common sense using the methods that have produced technologies like GPT-4. New breakthroughs will probably be necessary.\
\
The question is, do we really want artificial intelligence to become that powerful? A very important related question: Is there any way to stop it from happening?\
\
The risks of A.I.\
\
Many A.I. executives believe the technologies they are creating will improve our lives. But some have been warning for decades about a darker scenario, where our creations don\'92t always do what we want them to do, or they follow our instructions in unpredictable ways, with potentially dire consequences.\
\
A.I. experts talk about \'93alignment\'94 \'97 that is, making sure A.I. systems are in line with human values and goals.\
\
Before GPT-4 was released, OpenAI handed it over to an outside group to imagine and test dangerous uses of the chatbot.\
\
The group found that the system was able to hire a human online to defeat a Captcha test. When the human asked if it was \'93a robot,\'94 the system, unprompted by the testers, lied and said it was a person with a visual impairment.\
\
Testers also showed that the system could be coaxed into suggesting how to buy illegal firearms online and into describing ways to make dangerous substances from household items. After changes by OpenAI, the system no longer does these things.\
\
But it\'92s impossible to eliminate all potential misuses. As a system like this learns from data, it develops skills that its creators never expected. It is hard to know how things might go wrong after millions of people start using it.\
\
\'93Every time we make a new A.I. system, we are unable to fully characterize all its capabilities and all of its safety problems \'97 and this problem is getting worse over time rather than better,\'94 said Jack Clark, a founder and the head of policy of Anthropic, a San Francisco start-up building this same kind of technology.\
\
And OpenAI and giants like Google are hardly the only ones exploring this technology. The basic methods used to build these systems are widely understood, and other companies, countries, research labs and bad actors may be less careful.\
\
The remedies for A.I. \
\
Ultimately, keeping a lid on dangerous A.I. technology will require far-reaching oversight. But experts are not optimistic.\
\
\'93We need a regulatory system that is international,\'94 said Aviv Ovadya, a researcher at the Berkman Klein Center for Internet & Society at Harvard who helped test GPT-4 before its release. \'93But I do not see our existing government institutions being about to navigate this at the rate that is necessary.\'94\
\
As we told you earlier this week, more than 1,000 technology leaders and researchers, including Elon Musk, have urged artificial intelligence labs to pause development of the most advanced systems, warning in an open letter that A.I. tools present \'93profound risks to society and humanity.\'94\
\
A.I. developers are \'93locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one \'97 not even their creators \'97 can understand, predict or reliably control,\'94 according to the letter.\
\
Some experts are mostly concerned about near-term dangers, including the spread of disinformation and the risk that people would rely on these systems for inaccurate or harmful medical and emotional advice.\
\
But other critics are part of a vast and influential online community called rationalists or effective altruists, who believe that A.I could eventually destroy humanity. This mind-set is reflected in the letter.\
\
\
An early guide to policymaking on generative AI: How lawmakers are thinking about the risks of the latest tech revolution by Tate Ryan-Mosley.\
\
\
Earlier this week, I was chatting with a policy professor in Washington, DC, who told me that students and colleagues alike are asking about GPT-4 and generative AI: What should they be reading? How much attention should they be paying?\
\
She wanted to know if I had any suggestions, and asked what I thought all the new advances meant for lawmakers. I\'92ve spent a few days thinking, reading, and chatting with the experts about this, and my answer morphed into this newsletter. So here goes!\
\
Though GPT-4 is the standard bearer, it\'92s just one of many high-profile generative AI releases in the past few months: Google, Nvidia, Adobe, and Baidu have all announced their own projects. In short, generative AI is the thing that everyone is talking about. And though the tech is not new, its policy implications are months if not years from being understood.\
\
GPT-4, released by OpenAI last week, is a multimodal large language model that uses deep learning to predict words in a sentence. It generates remarkably fluent text, and it can respond to images as well as word-based prompts. For paying customers, GPT-4 will now power ChatGPT, which has already been incorporated into commercial applications.\
\
The newest iteration has made a major splash, and Bill Gates called it \'93revolutionary\'94 in a letter this week. However, OpenAI has also been criticized for a lack of transparency about how the model was trained and evaluated for bias.\
\
Despite all the excitement, generative AI comes with significant risks. The models are trained on the toxic repository that is the internet, which means they often produce racist and sexist output. They also regularly make things up and state them with convincing confidence. That could be a nightmare from a misinformation standpoint and could make scams more persuasive and prolific.\
\
Generative AI tools are also potential threats to people\'92s security and privacy, and they have little regard for copyright laws. Companies using generative AI that has stolen the work of others are already being sued.\
\
Alex Engler, a fellow in governance studies at the Brookings Institution, has considered how policymakers should be thinking about this and sees two main types of risks: harms from malicious use and harms from commercial use. Malicious uses of the technology, like disinformation, automated hate speech, and scamming, \'93have a lot in common with content moderation,\'94 Engler said in an email to me, \'93and the best way to tackle these risks is likely platform governance.\'94 (If you want to learn more about this, I\'92d recommend listening to this week\'92s Sunday Show from Tech Policy Press, where Justin Hendrix, an editor and a lecturer on tech, media, and democracy, talks with a panel of experts about whether generative AI systems should be regulated similarly to search and recommendation algorithms. Hint: Section 230.)\
\
Policy discussions about generative AI have so far focused on that second category: risks from commercial use of the technology, like coding or advertising. So far, the US government has taken small but notable actions, primarily through the Federal Trade Commission (FTC). The FTC issued a warning statement to companies last month urging them not to make claims about technical capabilities that they can\'92t substantiate, such as overstating what AI can do. This week, on its business blog, it used even stronger language about risks companies should consider when using generative AI.\
\
\'93If you develop or offer a synthetic media or generative AI product, consider at the design stage and thereafter the reasonably foreseeable\'97and often obvious\'97ways it could be misused for fraud or cause other harm. Then ask yourself whether such risks are high enough that you shouldn\'92t offer the product at all,\'94 the blog post reads.\
\
The US Copyright Office also launched a new initiative intended to deal with the thorny policy questions around AI, attribution, and intellectual property.\
\
The EU, meanwhile, is sticking true to its reputation as the world leader in tech policy. At the start of this year my colleague Melissa Heikkil\'e4 wrote about the EU\'92s efforts to try to pass the AI Act. It\'92s a set of rules that would prevent companies from releasing models into the wild without disclosing their inner workings, which is precisely what some critics are accusing OpenAI of with the GPT-4 release.\
\
The EU intends to separate high-risk uses of AI, like hiring, legal, or financial applications, from lower-risk uses like video games and spam filters, and require more transparency around the more sensitive uses. OpenAI has acknowledged some of the concerns about the speed of adoption. In fact, its own CEO, Sam Altman, told ABC News he shares many of the same fears. However, the company is still not disclosing key data about GPT-4.\
\
For policy folks in Washington, Brussels, London, and offices everywhere else in the world, it\'92s important to understand that generative AI is here to stay. Yes, there\'92s significant hype, but the recent advances in AI are as real and important as the risks that they pose.\
\
Yesterday, the United States Congress called Shou Zi Chew, the CEO of TikTok, to a hearing about privacy and security concerns raised by the popular social media app. His appearance came after the Biden administration threatened a national ban if its parent company, ByteDance, didn\'92t sell off the majority of its shares.\
\
There were lots of headlines, most using a temporal pun, and the hearing laid bare the depths of the new technological cold war between the US and China. For many watching, the hearing was both important and disappointing, with some legislators displaying poor technical understanding and hypocrisy about how Chinese companies handle privacy when American companies collect and trade data in much the same ways.\
\
It also revealed how deeply American lawmakers distrust Chinese tech. Here are some of the spicier takes and helpful articles to get up to speed:\
\
Key takeaways from TikTok hearing in Congress \'96 and the uncertain road ahead - Kari Paul and Johana Bhuiyan, The Guardian\
What to Know About the TikTok Security Concerns - Billy Perrigo, Time\
America\'92s online privacy problems are much bigger than TikTok - Will Oremus, Washington Post\
There\'92s a Problem With Banning TikTok. It\'92s Called the First\
Amendment - Jameel Jaffer (Executive Director of the Knight First Amendment Institute), NYT Opinion\
\
AI is able to persuade people to change their minds about hot-button political issues like an assault weapon ban and paid parental leave, according to a study by a team at Stanford\'92s Polarization and Social Change Lab. The researchers compared people\'92s political opinions on a topic before and after reading an AI-generated argument, and found that these arguments can be as effective as human-written ones in persuading the readers: \'93AI ranked consistently as more factual and logical, less angry, and less reliant upon storytelling as a persuasive technique.\'94\
\
The teams point to concerns about the use of generative AI in a political context, such as in lobbying or online discourse. (For more on the use of generative AI in politics, do please read this recent piece by Nathan Sanders and Bruce Schneier.)\
\
\
A celebration of connectionism by Geoffrey North \
\
\
New developments in neural network theory have excited both psychologists and neurobiologists. Practitioners of the new art displayed their wares last week.\
\
WHEN David Rumelhart, Geoffrey Hinton and Ronald Williams described for neural networks a powerful new learning procedure called back-propagation (Nature 323, 533; 1986), they noted that theirs was not a plausible model of how brains learn. Yet the generality of their approach, and the several intriguing features of network learning by backpropagation which have come to light, have stimulated a resurgence of interest in neural network models among neuroscientists, theoreticians and experimentalists alike. Last week, at a meeting organized by the Society of Experimental Psychology, a packed audience heard Geoffrey Hinton describe a new learning algorithm that seems a better model of biological learning than is back-propagation by parallel networks which nevertheless seems to retain much of the power of its predecessor.\
\
In parallel distributed processing, a network can be thought of as embodying a mathematical function mapping vectors in 'input space' to vectors in 'output space', much as matrices effect linear transformations between vector spaces. A vector in the neural context is simply the pattern of excitation of some set of units taken to be the input or the output of the network. The processing is thus distributed in the pattern of the connections between units of the network and their strengths. Corresponding to the real physiological task of, say, pattern recognition, will be some kind of network function mapping inputs (patterns) onto outputs (interpretations). The all-important question is what kind of network is needed for a particular task.\
\
Simple networks developed in the 1960s, known as perceptrons, involved only two layers of units, an input and an output, with direct connections between them. Such networks are very limited in the range of tasks they can carry out. The versatility of a network can be greatly increased by the introduction of intermediate layers of 'hidden' units, but this raises the problem of how it can be trained.\
\
Back-propagation provides an elegant way of training a multi-layered network. During learning, the output vectors generated by the network for a given input are compared with the desired output, giving an error calculated from the difference between the two. Back-propagation calculates the dependence of this error on all the connection weights, simply by using the chain-rule for differentiation, and the weights are adjusted to reduce the error, so that the network converges by gradient descent on the required structure.\
\
During learning, the network comes to capture certain general features which are characteristic of its task. The hidden units, in particular, develop features that seem especially significant to neuroscientists who record the properties of single neurons in brains. For example, in some cases they are reminiscent of the way in which some neurons in the brain are found to be specific for different aspects of the representation of the visual field.\
\
Even so, this system of learning by back-propagation has not seemed very biologically realistic. Hinton (CarnegieMellon University) and his colleagues have been looking for a more plausible system oflearning.\
\
The new development is known as a 'recirculation' algorithm, and works as follows. In a network learning by backpropagation, there is a linear flow of activity (via the hidden intermediate units) from the input units to the output units.In the new system, the hidden units connect back to the single layer of 'visible' input units. Activity thus recirculates through the network; during training, the connection weights are adjusted to minimize the rate of change of activity in each unit. Thus, when trained, the network is set up so as to stabilize on certain states, and so can work as a kind of 'content addressable memory' with the property that degraded or incomplete forms of the training inputs can regenerate the correct version.\
\
It has been shown that, under certain conditions, the new algorithm is equivalent to gradient descent, and it has been found empirically that the system still works when these conditions are relaxed.\
\
A number of interesting applications of back-propagation were reported at the meeting. Hinton described a network for recognizing one-dimensional shapes on a one-dimensional retina independently of position: the hidden units learn to respond to shapes in different positions. Hinton also described a speech-recognition network which learns to recognize spoken consonants given very noisy corrupted data. It appears to perform almost as well as people, and better than the previous best system of automated speech recognition.\
\
Several speakers described analogies between the behaviour of their networks during training or after 'damage' and what is known of human learning and cognitive disorders. For example, J. L. McClelland (Carnegie-Mellon University) described how a network for learning a balancebeam task progressed through stages of competence similar to those of children given the same task. The problem is to decide which way a balance-beam will tip, depending on the position and size of weights on either side of the fulcrum. With an appropriately biased learning environment, such as children might well experience, the network, like children, initially bases its decisions purely on weight information; gradually the network learns to use the position of the weights. M.S. Seidenberg (McGill University) described a network for word recognition and pronunciation that, when 'damaged' by the removal of hidden units, displayed behaviour reminiscent of some human disorders, such as dyslexia.\
\
Parallel distributed processing is not without its critics, and S. Pinker (MIT) reported that a linguistic analysis of Rumelhart and McClelland's network for changing the tense of verbs in sentences shows that the system is not 'descriptively adequate' as a model for human language, in that it abandons certain symbolic rules and principles that linguistic studies suggest are crucial in human language.\
\
David Willshaw (Edinburgh) asked whether parallel distributed processing networks might, like perceptrons, similarly cease to make significant progress and fade in interest after a period of development and excitement. McClelland's riposte was that work on perceptrons was severely limited by the available computer power and circumstances are now sufficiently different to justify optimism.\
\
The biological relevance of parallel distributed processing remains an open question. Independently of relevance, however, work on network systems may be of interest at a purely theoretical level. The present work is a kind of experimental mathematics, and in that respect is rather similar to that of Mandlebrot on fractals, also made possible and inspired by computers. The hope is that, in future, deductive proofs will give a more rigorous basis to work on networks.\
\
Many interesting problems remain. On what set of functions will a given network topology converge? How can the optimal network for a given task be predicted, and how long will training take? And so on.\
\
}