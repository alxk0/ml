/ 000. Turing, Alan M. "Computing Machinery and Intelligence," 1950.


1. The Imitation Game

I propose to consider the question, "Can machines think?" This should begin with definitions of the meaning of the terms "machine" and "think." The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words "machine" and "think" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, "Can machines think?" is to be sought in a statistical survey such as a Gallup poll. But this is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.

The new form of the problem can be described in terms of a game which we call the 'imitation game." It is played with three people, a man (A), a woman (B), and an interrogator (C) who may be of either sex. The interrogator stays in a room apart front the other two. The object of the game for the interrogator is to determine which of the other two is the man and which is the woman. He knows them by labels X and Y, and at the end of the game he says either "X is A and Y is B" or "X is B and Y is A." The interrogator is allowed to put questions to A and B thus:

C: Will X please tell me the length of his or her hair?

Now suppose X is actually A, then A must answer. It is A's object in the game to try and cause C to make the wrong identification. His answer might therefore be:

"My hair is shingled, and the longest strands are about nine inches long."

In order that tones of voice may not help the interrogator the answers should be written, or better still, typewritten. The ideal arrangement is to have a teleprinter communicating between the two rooms. Alternatively the question and answers can be repeated by an intermediary. The object of the game for the third player (B) is to help the interrogator. The best strategy for her is probably to give truthful answers. She can add such things as "I am the woman, don't listen to him!" to her answers, but it will avail nothing as the man can make similar remarks.

We now ask the question, "What will happen when a machine takes the part of A in this game?" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, "Can machines think?"

2. Critique of the New Problem
As well as asking, "What is the answer to this new form of the question," one may ask, "Is this new question a worthy one to investigate?" This latter question we investigate without further ado, thereby cutting short an infinite regress.
The new problem has the advantage of drawing a fairly sharp line between the physical and the intellectual capacities of a man. No engineer or chemist claims to be able to produce a material which is indistinguishable from the human skin. It is possible that at some time this might be done, but even supposing this invention available we should feel there was little point in trying to make a "thinking machine" more human by dressing it up in such artificial flesh. The form in which we have set the problem reflects this fact in the condition which prevents the interrogator from seeing or touching the other competitors, or hearing -their voices. Some other advantages of the proposed criterion may be shown up by specimen questions and answers. Thus:
Q: Please write me a sonnet on the subject of the Forth Bridge.
A : Count me out on this one. I never could write poetry.
Q: Add 34957 to 70764.
A: (Pause about 30 seconds and then give as answer) 105621.
Q: Do you play chess?
A: Yes.
Q: I have K at my K1, and no other pieces. You have only K at K6 and R at R1. It is your move. What do you play?
A: (After a pause of 15 seconds) R-R8 mate.
The question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include. We do not wish to penalise the machine for its inability to shine in beauty competitions, nor to penalise a man for losing in a race against an aeroplane. The conditions of our game make these disabilities irrelevant. The "witnesses" can brag, if they consider it advisable, as much as they please about their charms, strength or heroism, but the interrogator cannot demand practical demonstrations.
The game may perhaps be criticised on the ground that the odds are weighted too heavily against the machine. If the man were to try and pretend to be the machine he would clearly make a very poor showing. He would be given away at once by slowness and inaccuracy in arithmetic. May not machines carry out something which ought to be described as thinking but which is very different from what a man does? This objection is a very strong one, but at least we can say that if, nevertheless, a machine can be constructed to play the imitation game satisfactorily, we need not be troubled by this objection.

It might be urged that when playing the "imitation game" the best strategy for the machine may possibly be something other than imitation of the behaviour of a man. This may be, but I think it is unlikely that there is any great effect of this kind. In any case there is no intention to investigate here the theory of the game, and it will be assumed that the best strategy is to try to provide answers that would naturally be given by a man.
3. The Machines Concerned in the GameThe question which we put in 1 will not be quite definite until we have specified what we mean by the word "machine." It is natural that we should wish to permit every kind of engineering technique to be used in our machines. We also wish to allow the possibility than an engineer or team of engineers may construct a machine which works, but whose manner of operation cannot be satisfactorily described by its constructors because they have applied a method which is largely experimental. Finally, we wish to exclude from the machines men born in the usual manner. It is difficult to frame the definitions so as to satisfy these three conditions. One might for instance insist that the team of engineers should be all of one sex, but this would not really be satisfactory, for it is probably possible to rear a complete individual from a single cell of the skin (say) of a man. To do so would be a feat of biological technique deserving of the very highest praise, but we would not be inclined to regard it as a case of "constructing a thinking machine." This prompts us to abandon the requirement that every kind of technique should be permitted. We are the more ready to do so in view of the fact that the present interest in "thinking machines" has been aroused by a particular kind of machine, usually called an "electronic computer" or "digital computer." Following this suggestion we only permit digital computers to take part in our game.
This restriction appears at first sight to be a very drastic one. I shall attempt to show that it is not so in reality. To do this necessitates a short account of the nature and properties of these computers.
It may also be said that this identification of machines with digital computers, like our criterion for "thinking," will only be unsatisfactory if (contrary to my belief), it turns out that digital computers are unable to give a good showing in the game.
There are already a number of digital computers in working order, and it may be asked, "Why not try the experiment straight away? It would be easy to satisfy the conditions of the game. A number of interrogators could be used, and statistics compiled to show how often the right identification was given." The short answer is that we are not asking whether all digital computers would do well in the game nor whether the computers at present available would do well, but whether there are imaginable computers which would do well. But this is only the short answer. We shall see this question in a different light later.

4. Digital Computers
The idea behind digital computers may be explained by saying that these machines are intended to carry out any operations which could be done by a human computer. The human computer is supposed to be following fixed rules; he has no authority to deviate from them in any detail. We may suppose that these rules are supplied in a book, which is altered whenever he is put on to a new job. He has also an unlimited supply of paper on which he does his calculations. He may also do his multiplications and additions on a "desk machine," but this is not important.
If we use the above explanation as a definition we shall be in danger of circularity of argument. We avoid this by giving an outline. of the means by which the desired effect is achieved. A digital computer can usually be regarded as consisting of three parts:
(i) Store.
(ii) Executive unit.
(iii) Control.
The store is a store of information, and corresponds to the human computer's paper, whether this is the paper on which he does his calculations or that on which his book of rules is printed. In so far as the human computer does calculations in his bead a part of the store will correspond to his memory.
The executive unit is the part which carries out the various individual operations involved in a calculation. What these individual operations are will vary from machine to machine. Usually fairly lengthy operations can be done such as "Multiply 3540675445 by 7076345687" but in some machines only very simple ones such as "Write down 0" are possible.
We have mentioned that the "book of rules" supplied to the computer is replaced in the machine by a part of the store. It is then called the "table of instructions." It is the duty of the control to see that these instructions are obeyed correctly and in the right order. The control is so constructed that this necessarily happens.
The information in the store is usually broken up into packets of moderately small size. In one machine, for instance, a packet might consist of ten decimal digits. Numbers are assigned to the parts of the store in which the various packets of information are stored, in some systematic manner. A typical instruction might say-
"Add the number stored in position 6809 to that in 4302 and put the result back into the latter storage position."
Needless to say it would not occur in the machine expressed in English. It would more likely be coded in a form such as 6809430217. Here 17 says which of various possible operations is to be performed on the two numbers. In this case the)e operation is that described above, viz., "Add the number. . . ." It will be noticed that the instruction takes up 10 digits and so forms one packet of information, very conveniently. The control will normally take the instructions to be obeyed in the order of the positions in which they are stored, but occasionally an instruction such as"Now obey the instruction stored in position 5606, and continue from there"may be encountered, or again"If position 4505 contains 0 obey next the instruction stored in 6707, otherwise continue straight on."Instructions of these latter types are very important because they make it possible for a sequence of operations to be replaced over and over again until some condition is fulfilled, but in doing so to obey, not fresh instructions on each repetition, but the same ones over and over again. To take a domestic analogy. Suppose Mother wants Tommy to call at the cobbler's every morning on his way to school to see if her shoes are done, she can ask him afresh every morning. Alternatively she can stick up a notice once and for all in the hall which he will see when he leaves for school and which tells him to call for the shoes, and also to destroy the notice when he comes back if he has the shoes with him.
The reader must accept it as a fact that digital computers can be constructed, and indeed have been constructed, according to the principles we have described, and that they can in fact mimic the actions of a human computer very closely.
The book of rules which we have described our human computer as using is of course a convenient fiction. Actual human computers really remember what they have got to do. If one wants to make a machine mimic the behaviour of the human computer in some complex operation one has to ask him how it is done, and then translate the answer into the form of an instruction table. Constructing instruction tables is usually described as "programming." To "programme a machine to carry out the operation A" means to put the appropriate instruction table into the machine so that it will do A.
An interesting variant on the idea of a digital computer is a "digital computer with a random element." These have instructions involving the throwing of a die or some equivalent electronic process; one such instruction might for instance be, "Throw the die and put the-resulting number into store 1000." Sometimes such a machine is described as having free will (though I would not use this phrase myself), It is not normally possible to determine from observing a machine whether it has a random element, for a similar effect can be produced by such devices as making the choices depend on the digits of the decimal for .
Most actual digital computers have only a finite store. There is no theoretical difficulty in the idea of a computer with an unlimited store. Of course only a finite part can have been used at any one time. Likewise only a finite amount can have been constructed, but we can imagine more and more being added as required. Such computers have special theoretical interest and will be called infinitive capacity computers.
The idea of a digital computer is an old one. Charles Babbage, Lucasian Professor of Mathematics at Cambridge from 1828 to 1839, planned such a machine, called the Analytical Engine, but it was never completed. Although Babbage had all the essential ideas, his machine was not at that time such a very attractive prospect. The speed which would have been available would be definitely faster than a human computer but something like I 00 times slower than the Manchester machine, itself one of the slower of the modern machines, The storage was to be purely mechanical, using wheels and cards.
The fact that Babbage's Analytical Engine was to be entirely mechanical will help us to rid ourselves of a superstition. Importance is often attached to the fact that modern digital computers are electrical, and that the nervous system also is electrical. Since Babbage's machine was not electrical, and since all digital computers are in a sense equivalent, we see that this use of electricity cannot be of theoretical importance. Of course electricity usually comes in where fast signalling is concerned, so that it is not surprising that we find it in both these connections. In the nervous system chemical phenomena are at least as important as electrical. In certain computers the storage system is mainly acoustic. The feature of using electricity is thus seen to be only a very superficial similarity. If we wish to find such similarities we should took rather for mathematical analogies of function.
5. Universality of Digital ComputersThe digital computers considered in the last section may be classified amongst the "discrete-state machines." These are the machines which move by sudden jumps or clicks from one quite definite state to another. These states are sufficiently different for the possibility of confusion between them to be ignored. Strictly speaking there, are no such machines. Everything really moves continuously. But there are many kinds of machine which can profitably be thought of as being discrete-state machines. For instance in considering the switches for a lighting system it is a convenient fiction that each switch must be definitely on or definitely off. There must be intermediate positions, but for most purposes we can forget about them. As an example of a discrete-state machine we might consider a wheel which clicks round through 120 once a second, but may be stopped by a ]ever which can be operated from outside; in addition a lamp is to light in one of the positions of the wheel. This machine could be described abstractly as follows. The internal state of the machine (which is described by the position of the wheel) may be q1, q2 or q3. There is an input signal i0. or i1 (position of ]ever). The internal state at any moment is determined by the last state and input signal according to the table
(TABLE DELETED)The output signals, the only externally visible indication of the internal state (the light) are described by the table

State q1 q2 q3
output o0 o0 o1This example is typical of discrete-state machines. They can be described by such tables provided they have only a finite number of possible states.It will seem that given the initial state of the machine and the input signals it is always possible to predict all future states, This is reminiscent of Laplace's view that from the complete state of the universe at one moment of time, as described by the positions and velocities of all particles, it should be possible to predict all future states. The prediction which we are considering is, however, rather nearer to practicability than that considered by Laplace. The system of the "universe as a whole" is such that quite small errors in the initial conditions can have an overwhelming effect at a later time. The displacement of a single electron by a billionth of a centimetre at one moment might make the difference between a man being killed by an avalanche a year later, or escaping. It is an essential property of the mechanical systems which we have called "discrete-state machines" that this phenomenon does not occur. Even when we consider the actual physical machines instead of the idealised machines, reasonably accurate knowledge of the state at one moment yields reasonably accurate knowledge any number of steps later.
As we have mentioned, digital computers fall within the class of discrete-state machines. But the number of states of which such a machine is capable is usually enormously large. For instance, the number for the machine now working at Manchester is about 2 165,000, i.e., about 10 50,000. Compare this with our example of the clicking wheel described above, which had three states. It is not difficult to see why the number of states should be so immense. The computer includes a store corresponding to the paper used by a human computer. It must be possible to write into the store any one of the combinations of symbols which might have been written on the paper. For simplicity suppose that only digits from 0 to 9 are used as symbols. Variations in handwriting are ignored. Suppose the computer is allowed 100 sheets of paper each containing 50 lines each with room for 30 digits. Then the number of states is 10 100x50x30 i.e., 10 150,000 . This is about the number of states of three Manchester machines put together. The logarithm to the base two of the number of states is usually called the "storage capacity" of the machine. Thus the Manchester machine has a storage capacity of about 165,000 and the wheel machine of our example about 1.6. If two machines are put together their capacities must be added to obtain the capacity of the resultant machine. This leads to the possibility of statements such as "The Manchester machine contains 64 magnetic tracks each with a capacity of 2560, eight electronic tubes with a capacity of 1280. Miscellaneous storage amounts to about 300 making a total of 174,380."
Given the table corresponding to a discrete-state machine it is possible to predict what it will do. There is no reason why this calculation should not be carried out by means of a digital computer. Provided it could be carried out sufficiently quickly the digital computer could mimic the behavior of any discrete-state machine. The imitation game could then be played with the machine in question (as B) and the mimicking digital computer (as A) and the interrogator would be unable to distinguish them. Of course the digital computer must have an adequate storage capacity as well as working sufficiently fast. Moreover, it must be programmed afresh for each new machine which it is desired to mimic.
This special property of digital computers, that they can mimic any discrete-state machine, is described by saying that they are universal machines. The existence of machines with this property has the important consequence that, considerations of speed apart, it is unnecessary to design various new machines to do various computing processes. They can all be done with one digital computer, suitably programmed for each case. It 'ill be seen that as a consequence of this all digital computers are in a sense equivalent.
We may now consider again the point raised at the end of §3. It was suggested tentatively that the question, "Can machines think?" should be replaced by "Are there imaginable digital computers which would do well in the imitation game?" If we wish we can make this superficially more general and ask "Are there discrete-state machines which would do well?" But in view of the universality property we see that either of these questions is equivalent to this, "Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?"
6. Contrary Views on the Main Question
We may now consider the ground to have been cleared and we are ready to proceed to the debate on our question, "Can machines think?" and the variant of it quoted at the end of the last section. We cannot altogether abandon the original form of the problem, for opinions will differ as to the appropriateness of the substitution and we must at least listen to what has to be said in this connexion.
It will simplify matters for the reader if I explain first my own beliefs in the matter. Consider first the more accurate form of the question. I believe that in about fifty years' time it will be possible, to programme computers, with a storage capacity of about 109, to make them play the imitation game so well that an average interrogator will not have more than 70 per cent chance of making the right identification after five minutes of questioning. The original question, "Can machines think?" I believe to be too meaningless to deserve discussion. Nevertheless I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted. I believe further that no useful purpose is served by concealing these beliefs. The popular view that scientists proceed inexorably from well-established fact to well-established fact, never being influenced by any improved conjecture, is quite mistaken. Provided it is made clear which are proved facts and which are conjectures, no harm can result. Conjectures are of great importance since they suggest useful lines of research.

I now proceed to consider opinions opposed to my own.
(1) The Theological ObjectionThinking is a function of man's immortal soul. God has given an immortal soul to every man and woman, but not to any other animal or to machines. Hence no animal or machine can think.I am unable to accept any part of this, but will attempt to reply in theological terms. I should find the argument more convincing if animals were classed with men, for there is a greater difference, to my mind, between the typical animate and the inanimate than there is between man and the other animals. The arbitrary character of the orthodox view becomes clearer if we consider how it might appear to a member of some other religious community. How do Christians regard the Moslem view that women have no souls? But let us leave this point aside and return to the main argument. It appears to me that the argument quoted above implies a serious restriction of the omnipotence of the Almighty. It is admitted that there are certain things that He cannot do such as making one equal to two, but should we not believe that He has freedom to confer a soul on an elephant if He sees fit? We might expect that He would only exercise this power in conjunction with a mutation which provided the elephant with an appropriately improved brain to minister to the needs of this sort[. An argument of exactly similar form may be made for the case of machines. It may seem different because it is more difficult to "swallow." But this really only means that we think it would be less likely that He would consider the circumstances suitable for conferring a soul. The circumstances in question are discussed in the rest of this paper. In attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing .mansions for the souls that He creates.
However, this is mere speculation. I am not very impressed with theological arguments whatever they may be used to support. Such arguments have often been found unsatisfactory in the past. In the time of Galileo it was argued that the texts, "And the sun stood still . . . and hasted not to go down about a whole day" (Joshua x. 13) and "He laid the foundations of the earth, that it should not move at any time" (Psalm cv. 5) were an adequate refutation of the Copernican theory. With our present knowledge such an argument appears futile. When that knowledge was not available it made a quite different impression.
(2) The "Heads in the Sand" ObjectionThe consequences of machines thinking would be too dreadful. Let us hope and believe that they cannot do so."This argument is seldom expressed quite so openly as in the form above. But it affects most of us who think about it at all. We like to believe that Man is in some subtle way superior to the rest of creation. It is best if he can be shown to be necessarily superior, for then there is no danger of him losing his commanding position. The popularity of the theological argument is clearly connected with this feeling. It is likely to be quite strong in intellectual people, since they value the power of thinking more highly than others, and are more inclined to base their belief in the superiority of Man on this power.I do not think that this argument is sufficiently substantial to require refutation. Consolation would be more appropriate: perhaps this should be sought in the transmigration of souls.
(3) The Mathematical Objection
There are a number of results of mathematical logic which can be used to show that there are limitations to the powers of discrete-state machines. The best known of these results is known as Godel's theorem ( 1931 ) and shows that in any sufficiently powerful logical system statements can be formulated which can neither be proved nor disproved within the system, unless possibly the system itself is inconsistent. There are other, in some respects similar, results due to Church (1936), Kleene (1935), Rosser, and Turing (1937). The latter result is the most convenient to consider, since it refers directly to machines, whereas the others can only be used in a comparatively indirect argument: for instance if Godel's theorem is to be used we need in addition to have some means of describing logical systems in terms of machines, and machines in terms of logical systems. The result in question refers to a type of machine which is essentially a digital computer with an infinite capacity. It states that there are certain things that such a machine cannot do. If it is rigged up to give answers to questions as in the imitation game, there will be some questions to which it will either give a wrong answer, or fail to give an answer at all however much time is allowed for a reply. There may, of course, be many such questions, and questions which cannot be answered by one machine may be satisfactorily answered by another. We are of course supposing for the present that the questions are of the kind to which an answer "Yes" or "No" is appropriate, rather than questions such as "What do you think of Picasso?" The questions that we know the machines must fail on are of this type, "Consider the machine specified as follows. . . . Will this machine ever answer 'Yes' to any question?" The dots are to be replaced by a description of some machine in a standard form, which could be something like that used in §5. When the machine described bears a certain comparatively simple relation to the machine which is under interrogation, it can be shown that the answer is either wrong or not forthcoming. This is the mathematical result: it is argued that it proves a disability of machines to which the human intellect is not subject.
The short answer to this argument is that although it is established that there are limitations to the Powers If any particular machine, it has only been stated, without any sort of proof, that no such limitations apply to the human intellect. But I do not think this view can be dismissed quite so lightly. Whenever one of these machines is asked the appropriate critical question, and gives a definite answer, we know that this answer must be wrong, and this gives us a certain feeling of superiority. Is this feeling illusory? It is no doubt quite genuine, but I do not think too much importance should be attached to it. We too often give wrong answers to questions ourselves to be justified in being very pleased at such evidence of fallibility on the part of the machines. Further, our superiority can only be felt on such an occasion in relation to the one machine over which we have scored our petty triumph. There would be no question of triumphing simultaneously over all machines. In short, then, there might be men cleverer than any given machine, but then again there might be other machines cleverer again, and so on.
Those who hold to the mathematical argument would, I think, mostly he willing to accept the imitation game as a basis for discussion, Those who believe in the two previous objections would probably not be interested in any criteria.
(4) The Argument from Consciousness
This argument is very, well expressed in Professor Jefferson's Lister Oration for 1949, from which I quote. "Not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain-that is, not only write it but know that it had written it. No mechanism could feel (and not merely artificially signal, an easy contrivance) pleasure at its successes, grief when its valves fuse, be warmed by flattery, be made miserable by its mistakes, be charmed by sex, be angry or depressed when it cannot get what it wants."
This argument appears to be a denial of the validity of our test. According to the most extreme form of this view the only way by which one could be sure that machine thinks is to be the machine and to feel oneself thinking. One could then describe these feelings to the world, but of course no one would be justified in taking any notice. Likewise according to this view the only way to know that a man thinks is to be that particular man. It is in fact the solipsist point of view. It may be the most logical view to hold but it makes communication of ideas difficult. A is liable to believe "A thinks but B does not" whilst B believes "B thinks but A does not." instead of arguing continually over this point it is usual to have the polite convention that everyone thinks.
I am sure that Professor Jefferson does not wish to adopt the extreme and solipsist point of view. Probably he would be quite willing to accept the imitation game as a test. The game (with the player B omitted) is frequently used in practice under the name of viva voce to discover whether some one really understands something or has "learnt it parrot fashion." Let us listen in to a part of such a viva voce:
Interrogator: In the first line of your sonnet which reads "Shall I compare thee to a summer's day," would not "a spring day" do as well or better?
Witness: It wouldn't scan.
Interrogator: How about "a winter's day," That would scan all right.
Witness: Yes, but nobody wants to be compared to a winter's day.

Interrogator: Would you say Mr. Pickwick reminded you of Christmas?
Witness: In a way.
Interrogator: Yet Christmas is a winter's day, and I do not think Mr. Pickwick would mind the comparison.
Witness: I don't think you're serious. By a winter's day one means a typical winter's day, rather than a special one like Christmas.
And so on, What would Professor Jefferson say if the sonnet-writing machine was able to answer like this in the viva voce? I do not know whether he would regard the machine as "merely artificially signalling" these answers, but if the answers were as satisfactory and sustained as in the above passage I do not think he would describe it as "an easy contrivance." This phrase is, I think, intended to cover such devices as the inclusion in the machine of a record of someone reading a sonnet, with appropriate switching to turn it on from time to time.
In short then, I think that most of those who support the argument from consciousness could be persuaded to abandon it rather than be forced into the solipsist position. They will then probably be willing to accept our test.
I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.
(5) Arguments from Various Disabilities
These arguments take the form, "I grant you that you can make machines do all the things you have mentioned but you will never be able to make one to do X." Numerous features X are suggested in this connexion I offer a selection:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humour, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make some one fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.
No support is usually offered for these statements. I believe they are mostly founded on the principle of scientific induction. A man has seen thousands of machines in his lifetime. From what he sees of them he draws a number of general conclusions. They are ugly, each is designed for a very limited purpose, when required for a minutely different purpose they are useless, the variety of behaviour of any one of them is very small, etc., etc. Naturally he concludes that these are necessary properties of machines in general. Many of these limitations are associated with the very small storage capacity of most machines. (I am assuming that the idea of storage capacity is extended in some way to cover machines other than discrete-state machines. The exact definition does not matter as no mathematical accuracy is claimed in the present discussion,) A few years ago, when very little had been heard of digital computers, it was possible to elicit much incredulity concerning them, if one mentioned their properties without describing their construction. That was presumably due to a similar application of the principle of scientific induction. These applications of the principle are of course largely unconscious. When a burnt child fears the fire and shows that he fears it by avoiding it, f should say that he was applying scientific induction. (I could of course also describe his behaviour in many other ways.) The works and customs of mankind do not seem to be very suitable material to which to apply scientific induction. A very large part of space-time must be investigated, if reliable results are to be obtained. Otherwise we may (as most English 'Children do) decide that everybody speaks English, and that it is silly to learn French.
There are, however, special remarks to be made about many of the disabilities that have been mentioned. The inability to enjoy strawberries and cream may have struck the reader as frivolous. Possibly a machine might be made to enjoy this delicious dish, but any attempt to make one do so would be idiotic.
The claim that "machines cannot make mistakes" seems a curious one. One is tempted to retort, "Are they any the worse for that?" But let us adopt a more sympathetic attitude, and try to see what is really meant. I think this criticism can be explained in terms of the imitation game. It is claimed that the interrogator could distinguish the machine from the man simply by setting them a number of problems in arithmetic. The machine would be unmasked because of its deadly accuracy. The reply to this is simple. The machine (programmed for playing the game) would not attempt to give the right answers to the arithmetic problems. It would deliberately introduce mistakes in a manner calculated to confuse the interrogator. A mechanical fault would probably show itself through an unsuitable decision as to what sort of a mistake to make in the arithmetic. Even this interpretation of the criticism is not sufficiently sympathetic. But we cannot afford the space to go into it much further. It seems to me that this criticism depends on a confusion between two kinds of mistake, We may call them "errors of functioning" and "errors of conclusion." Errors of functioning are due to some mechanical or electrical fault which causes the machine to behave otherwise than it was designed to do. In philosophical discussions one likes to ignore the possibility of such errors; one is therefore discussing "abstract machines." These abstract machines are mathematical fictions rather than physical objects. By definition they are incapable of errors of functioning. In this sense we can truly say that "machines can never make mistakes." Errors of conclusion can only arise when some meaning is attached to the output signals from the machine. The machine might, for instance, type out mathematical equations, or sentences in English. When a false proposition is typed we say that the machine has committed an error of conclusion. There is clearly no reason at all for saying that a machine cannot make this kind of mistake. It might do nothing but type out repeatedly "O = I." To take a less perverse example, it might have some method for drawing conclusions by scientific induction. We must expect such a method to lead occasionally to erroneous results.
The claim that a machine cannot be the subject of its own thought can of course only be answered if it can be shown that the machine has some thought with some subject matter. Nevertheless, "the subject matter of a machine's operations" does seem to mean something, at least to the people who deal with it. If, for instance, the machine was trying to find a solution of the equation x2 - 40x - 11 = 0 one would be tempted to describe this equation as part of the machine's subject matter at that moment. In this sort of sense a machine undoubtedly can be its own subject matter. It may be used to help in making up its own programmes, or to predict the effect of alterations in its own structure. By observing the results of its own behaviour it can modify its own programmes so as to achieve some purpose more effectively. These are possibilities of the near future, rather than Utopian dreams.
The criticism that a machine cannot have much diversity of behaviour is just a way of saying that it cannot have much storage capacity. Until fairly recently a storage capacity of even a thousand digits was very rare.
The criticisms that we are considering here are often disguised forms of the argument from consciousness, Usually if one maintains that a machine can do one of these things, and describes the kind of method that the machine could use, one will not make much of an impression. It is thought that tile method (whatever it may be, for it must be mechanical) is really rather base. Compare the parentheses in Jefferson's statement quoted on page 22.
(6) Lady Lovelace's ObjectionOur most detailed information of Babbage's Analytical Engine comes from a memoir by Lady Lovelace ( 1842). In it she states, "The Analytical Engine has no pretensions to originate anything. It can do whatever we know how to order it to perform" (her italics). This statement is quoted by Hartree ( 1949) who adds: "This does not imply that it may not be possible to construct electronic equipment which will 'think for itself,' or in which, in biological terms, one could set up a conditioned reflex, which would serve as a basis for 'learning.' Whether this is possible in principle or not is a stimulating and exciting question, suggested by some of these recent developments But it did not seem that the machines constructed or projected at the time had this property."
I am in thorough agreement with Hartree over this. It will be noticed that he does not assert that the machines in question had not got the property, but rather that the evidence available to Lady Lovelace did not encourage her to believe that they had it. It is quite possible that the machines in question had in a sense got this property. For suppose that some discrete-state machine has the property. The Analytical Engine was a universal digital computer, so that, if its storage capacity and speed were adequate, it could by suitable programming be made to mimic the machine in question. Probably this argument did not occur to the Countess or to Babbage. In any case there was no obligation on them to claim all that could be claimed.
This whole question will be considered again under the heading of learning machines.
A variant of Lady Lovelace's objection states that a machine can "never do anything really new." This may be parried for a moment with the saw, "There is nothing new under the sun." Who can be certain that "original work" that he has done was not simply the growth of the seed planted in him by teaching, or the effect of following well-known general principles. A better variant of the objection says that a machine can never "take us by surprise." This statement is a more direct challenge and can be met directly. Machines take me by surprise with great frequency. This is largely because I do not do sufficient calculation to decide what to expect them to do, or rather because, although I do a calculation, I do it in a hurried, slipshod fashion, taking risks. Perhaps I say to myself, "I suppose the Voltage here ought to he the same as there: anyway let's assume it is." Naturally I am often wrong, and the result is a surprise for me for by the time the experiment is done these assumptions have been forgotten. These admissions lay me open to lectures on the subject of my vicious ways, but do not throw any doubt on my credibility when I testify to the surprises I experience.
I do not expect this reply to silence my critic. He will probably say that h surprises are due to some creative mental act on my part, and reflect no credit on the machine. This leads us back to the argument from consciousness, and far from the idea of surprise. It is a line of argument we must consider closed, but it is perhaps worth remarking that the appreciation of something as surprising requires as much of a "creative mental act" whether the surprising event originates from a man, a book, a machine or anything else.
The view that machines cannot give rise to surprises is due, I believe, to a fallacy to which philosophers and mathematicians are particularly subject. This is the assumption that as soon as a fact is presented to a mind all consequences of that fact spring into the mind simultaneously with it. It is a very useful assumption under many circumstances, but one too easily forgets that it is false. A natural consequence of doing so is that one then assumes that there is no virtue in the mere working out of consequences from data and general principles.
(7) Argument from Continuity in the Nervous System
The nervous system is certainly not a discrete-state machine. A small error in the information about the size of a nervous impulse impinging on a neuron, may make a large difference to the size of the outgoing impulse. It may be argued that, this being so, one cannot expect to be able to mimic the behaviour of the nervous system with a discrete-state system.
It is true that a discrete-state machine must be different from a continuous machine. But if we adhere to the conditions of the imitation game, the interrogator will not be able to take any advantage of this difference. The situation can be made clearer if we consider sonic other simpler continuous machine. A differential analyser will do very well. (A differential analyser is a certain kind of machine not of the discrete-state type used for some kinds of calculation.) Some of these provide their answers in a typed form, and so are suitable for taking part in the game. It would not be possible for a digital computer to predict exactly what answers the differential analyser would give to a problem, but it would be quite capable of giving the right sort of answer. For instance, if asked to give the value of (actually about 3.1416) it would be reasonable to choose at random between the values 3.12, 3.13, 3.14, 3.15, 3.16 with the probabilities of 0.05, 0.15, 0.55, 0.19, 0.06 (say). Under these circumstances it would be very difficult for the interrogator to distinguish the differential analyser from the digital computer.
(8) The Argument from Informality of BehaviourIt is not possible to produce a set of rules purporting to describe what a man should do in every conceivable set of circumstances. One might for instance have a rule that one is to stop when one sees a red traffic light, and to go if one sees a green one, but what if by some fault both appear together? One may perhaps decide that it is safest to stop. But some further difficulty may well arise from this decision later. To attempt to provide rules of conduct to cover every eventuality, even those arising from traffic lights, appears to be impossible. With all this I agree.
From this it is argued that we cannot be machines. I shall try to reproduce the argument, but I fear I shall hardly do it justice. It seems to run something like this. "if each man had a definite set of rules of conduct by which he regulated his life he would be no better than a machine. But there are no such rules, so men cannot be machines." The undistributed middle is glaring. I do not think the argument is ever put quite like this, but I believe this is the argument used nevertheless. There may however be a certain confusion between "rules of conduct" and "laws of behaviour" to cloud the issue. By "rules of conduct" I mean precepts such as "Stop if you see red lights," on which one can act, and of which one can be conscious. By "laws of behaviour" I mean laws of nature as applied to a man's body such as "if you pinch him he will squeak." If we substitute "laws of behaviour which regulate his life" for "laws of conduct by which he regulates his life" in the argument quoted the undistributed middle is no longer insuperable. For we believe that it is not only true that being regulated by laws of behaviour implies being some sort of machine (though not necessarily a discrete-state machine), but that conversely being such a machine implies being regulated by such laws. However, we cannot so easily convince ourselves of the absence of complete laws of behaviour as of complete rules of conduct. The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, "We have searched enough. There are no such laws."
We can demonstrate more forcibly that any such statement would be unjustified. For suppose we could be sure of finding such laws if they existed. Then given a discrete-state machine it should certainly be possible to discover by observation sufficient about it to predict its future behaviour, and this within a reasonable time, say a thousand years. But this does not seem to be the case. I have set up on the Manchester computer a small programme using only 1,000 units of storage, whereby the machine supplied with one sixteen-figure number replies with another within two seconds. I would defy anyone to learn from these replies sufficient about the programme to be able to predict any replies to untried values.
(9) The Argument from Extrasensory PerceptionI assume that the reader is familiar with the idea of extrasensory perception, and the meaning of the four items of it, viz., telepathy, clairvoyance, precognition and psychokinesis. These disturbing phenomena seem to deny all our usual scientific ideas. How we should like to discredit them! Unfortunately the statistical evidence, at least for telepathy, is overwhelming. It is very difficult to rearrange one's ideas so as to fit these new facts in. Once one has accepted them it does not seem a very big step to believe in ghosts and bogies. The idea that our bodies move simply according to the known laws of physics, together with some others not yet discovered but somewhat similar, would be one of the first to go.
This argument is to my mind quite a strong one. One can say in reply that many scientific theories seem to remain workable in practice, in spite of clashing with ESP; that in fact one can get along very nicely if one forgets about it. This is rather cold comfort, and one fears that thinking is just the kind of phenomenon where ESP may be especially relevant.A more specific argument based on ESP might run as follows: "Let us play the imitation game, using as witnesses a man who is good as a telepathic receiver, and a digital computer. The interrogator can ask such questions as 'What suit does the card in my right hand belong to?' The man by telepathy or clairvoyance gives the right answer 130 times out of 400 cards. The machine can only guess at random, and perhaps gets 104 right, so the interrogator makes the right identification." There is an interesting possibility which opens here. Suppose the digital computer contains a random number generator. Then it will be natural to use this to decide what answer to give. But then the random number generator will be subject to the psychokinetic powers of the interrogator. Perhaps this psychokinesis might cause the machine to guess right more often than would be expected on a probability calculation, so that the interrogator might still be unable to make the right identification. On the other hand, he might be able to guess right without any questioning, by clairvoyance. With ESP anything may happen.
If telepathy is admitted it will be necessary to tighten our test up. The situation could be regarded as analogous to that which would occur if the interrogator were talking to himself and one of the competitors was listening with his ear to the wall. To put the competitors into a "telepathy-proof room" would satisfy all requirements.
7. Learning MachinesThe reader will have anticipated that I have no very convincing arguments of a positive nature to support my views. If I had I should not have taken such pains to point out the fallacies in contrary views. Such evidence as I have I shall now give.

Let us return for a moment to Lady Lovelace's objection, which stated that the machine can only do what we tell it to do. One could say that a man can "inject" an idea into the machine, and that it will respond to a certain extent and then drop into quiescence, like a piano string struck by a hammer. Another simile would be an atomic pile of less than critical size: an injected idea is to correspond to a neutron entering the pile from without. Each such neutron will cause a certain disturbance which eventually dies away. If, however, the size of the pile is sufficiently increased, tire disturbance caused by such an incoming neutron will very likely go on and on increasing until the whole pile is destroyed. Is there a corresponding phenomenon for minds, and is there one for machines? There does seem to be one for the human mind. The majority of them seem to be "subcritical," i.e., to correspond in this analogy to piles of subcritical size. An idea presented to such a mind will on average give rise to less than one idea in reply. A smallish proportion are supercritical. An idea presented to such a mind that may give rise to a whole "theory" consisting of secondary, tertiary and more remote ideas. Animals minds seem to be very definitely subcritical. Adhering to this analogy we ask, "Can a machine be made to be supercritical?"
The "skin-of-an-onion" analogy is also helpful. In considering the functions of the mind or the brain we find certain operations which we can explain in purely mechanical terms. This we say does not correspond to the real mind: it is a sort of skin which we must strip off if we are to find the real mind. But then in what remains we find a further skin to be stripped off, and so on. Proceeding in this way do we ever come to the "real" mind, or do we eventually come to the skin which has nothing in it? In the latter case the whole mind is mechanical. (It would not be a discrete-state machine however. We have discussed this.)
These last two paragraphs do not claim to be convincing arguments. They should rather be described as "recitations tending to produce belief."
The only really satisfactory support that can be given for the view expressed at the beginning of §6, will be that provided by waiting for the end of the century and then doing the experiment described. But what can we say in the meantime? What steps should be taken now if the experiment is to be successful?
As I have explained, the problem is mainly one of programming. Advances in engineering will have to be made too, but it seems unlikely that these will not be adequate for the requirements. Estimates of the storage capacity of the brain vary from 1010 to 1015 binary digits. I incline to the lower values and believe that only a very small fraction is used for the higher types of thinking. Most of it is probably used for the retention of visual impressions, I should be surprised if more than 109 was required for satisfactory playing of the imitation game, at any rate against a blind man. (Note: The capacity of the Encyclopaedia Britannica, 11th edition, is 2 X 109) A storage capacity of 107, would be a very practicable possibility even by present techniques. It is probably not necessary to increase the speed of operations of the machines at all. Parts of modern machines which can be regarded as analogs of nerve cells work about a thousand times faster than the latter. This should provide a "margin of safety" which could cover losses of speed arising in many ways, Our problem then is to find out how to programme these machines to play the game. At my present rate of working I produce about a thousand digits of progratiirne a day, so that about sixty workers, working steadily through the fifty years might accomplish the job, if nothing went into the wastepaper basket. Some more expeditious method seems desirable.
In the process of trying to imitate an adult human mind we are bound to think a good deal about the process which has brought it to the state that it is in. We may notice three components.
(a) The initial state of the mind, say at birth,
(b) The education to which it has been subjected,
(c) Other experience, not to be described as education, to which it has been subjected.
Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain. Presumably the child brain is something like a notebook as one buys it from the stationer's. Rather little mechanism, and lots of blank sheets. (Mechanism and writing are from our point of view almost synonymous.) Our hope is that there is so little mechanism in the child brain that something like it can be easily programmed. The amount of work in the education we can assume, as a first approximation, to be much the same as for the human child.
We have thus divided our problem into two parts. The child programme and the education process. These two remain very closely connected. We cannot expect to find a good child machine at the first attempt. One must experiment with teaching one such machine and see how well it learns. One can then try another and see if it is better or worse. There is an obvious connection between this process and evolution, by the identifications
Structure of the child machine = hereditary material
Changes of the child machine = mutation,
Natural selection = judgment of the experimenter
One may hope, however, that this process will be more expeditious than evolution. The survival of the fittest is a slow method for measuring advantages. The experimenter, by the exercise of intelligence, should he able to speed it up. Equally important is the fact that he is not restricted to random mutations. If he can trace a cause for some weakness he can probably think of the kind of mutation which will improve it.
It will not be possible to apply exactly the same teaching process to the machine as to a normal child. It will not, for instance, be provided with legs, so that it could not be asked to go out and fill the coal scuttle. Possibly it might not have eyes. But however well these deficiencies might be overcome by clever engineering, one could not send the creature to school without the other children making excessive fun of it. It must be given some tuition. We need not be too concerned about the legs, eyes, etc. The example of Miss Helen Keller shows that education can take place provided that communication in both directions between teacher and pupil can take place by some means or other.
We normally associate punishments and rewards with the teaching process. Some simple child machines can be constructed or programmed on this sort of principle. The machine has to be so constructed that events which shortly preceded the occurrence of a punishment signal are unlikely to be repeated, whereas a reward signal increased the probability of repetition of the events which led up to it. These definitions do not presuppose any feelings on the part of the machine, I have done some experiments with one such child machine, and succeeded in teaching it a few things, but the teaching method was too unorthodox for the experiment to be considered really successful.The use of punishments and rewards can at best be a part of the teaching process. Roughly speaking, if the teacher has no other means of communicating to the pupil, the amount of information which can reach him does not exceed the total number of rewards and punishments applied. By the time a child has learnt to repeat "Casabianca" he would probably feel very sore indeed, if the text could only be discovered by a "Twenty Questions" technique, every "NO" taking the form of a blow. It is necessary therefore to have some other "unemotional" channels of communication. If these are available it is possible to teach a machine by punishments and rewards to obey orders given in some language, e.g., a symbolic language. These orders are to be transmitted through the "unemotional" channels. The use of this language will diminish greatly the number of punishments and rewards required.
Opinions may vary as to the complexity which is suitable in the child machine. One might try to make it as simple as possible consistently with the general principles. Alternatively one might have a complete system of logical inference "built in."' In the latter case the store would be largely occupied with definitions and propositions. The propositions would have various kinds of status, e.g., well-established facts, conjectures, mathematically proved theorems, statements given by an authority, expressions having the logical form of proposition but not belief-value. Certain propositions may be described as "imperatives." The machine should be so constructed that as soon as an imperative is classed as "well established" the appropriate action automatically takes place. To illustrate this, suppose the teacher says to the machine, "Do your homework now." This may cause "Teacher says 'Do your homework now' " to be included amongst the well-established facts. Another such fact might be, "Everything that teacher says is true." Combining these may eventually lead to the imperative, "Do your homework now," being included amongst the well-established facts, and this, by the construction of the machine, will mean that the homework actually gets started, but the effect is very satisfactory. The processes of inference used by the machine need not be such as would satisfy the most exacting logicians. There might for instance be no hierarchy of types. But this need not mean that type fallacies will occur, any more than we are bound to fall over unfenced cliffs. Suitable imperatives (expressed within the systems, not forming part of the rules of the system) such as "Do not use a class unless it is a subclass of one which has been mentioned by teacher" can have a similar effect to "Do not go too near the edge."
The imperatives that can be obeyed by a machine that has no limbs are bound to be of a rather intellectual character, as in the example (doing homework) given above. important amongst such imperatives will be ones which regulate the order in which the rules of the logical system concerned are to be applied, For at each stage when one is using a logical system, there is a very large number of alternative steps, any of which one is permitted to apply, so far as obedience to the rules of the logical system is concerned. These choices make the difference between a brilliant and a footling reasoner, not the difference between a sound and a fallacious one. Propositions leading to imperatives of this kind might be "When Socrates is mentioned, use the syllogism in Barbara" or "If one method has been proved to be quicker than another, do not use the slower method." Some of these may be "given by authority," but others may be produced by the machine itself, e.g. by scientific induction.
The idea of a learning machine may appear paradoxical to some readers. How can the rules of operation of the machine change? They should describe completely how the machine will react whatever its history might be, whatever changes it might undergo. The rules are thus quite time-invariant. This is quite true. The explanation of the paradox is that the rules which get changed in the learning process are of a rather less pretentious kind, claiming only an ephemeral validity. The reader may draw a parallel with the Constitution of the United States.
An important feature of a learning machine is that its teacher will often be very largely ignorant of quite what is going on inside, although he may still be able to some extent to predict his pupil's behavior. This should apply most strongly to the later education of a machine arising from a child machine of well-tried design (or programme). This is in clear contrast with normal procedure when using a machine to do computations one's object is then to have a clear mental picture of the state of the machine at each moment in the computation. This object can only be achieved with a struggle. The view that "the machine can only do what we know how to order it to do,"' appears strange in face of this. Most of the programmes which we can put into the machine will result in its doing something that we cannot make sense (if at all, or which we regard as completely random behaviour. Intelligent behaviour presumably consists in a departure from the completely disciplined behaviour involved in computation, but a rather slight one, which does not give rise to random behaviour, or to pointless repetitive loops. Another important result of preparing our machine for its part in the imitation game by a process of teaching and learning is that "human fallibility" is likely to be omitted in a rather natural way, i.e., without special "coaching." (The reader should reconcile this with the point of view on pages 23 and 24.) Processes that are learnt do not produce a hundred per cent certainty of result; if they did they could not be unlearnt.


/ 001. Benanav, Aaron. "Automation and the Future of Work," 2019.


From the New Left Review 119 September October 2019.

The world is abuzz with talk of automation. Rapid advances in artificial intelligence, machine learning and robotics seem set to transform the world of work. In the most advanced factories, companies like Tesla have been aiming for ‘lights-out’ production, in which fully automated work processes, no longer needing human hands, can run in the dark. Meanwhile, in the illuminated halls of robotics conventions, machines are on display that can play ping-pong, cook food, have sex and even hold conversations. Computers are not only developing new strategies for playing Go, but are said to be writing symphonies that bring audiences to tears. Dressed in white lab coats or donning virtual suits, computers are learning to identify cancers and will soon be developing legal strategies. Trucks are already barrelling across the us without drivers; robotic dogs are carrying military-grade weapons across desolate plains. Are we living in the last days of human toil? Is what Edward Bellamy once called the ‘edict of Eden’ about to be revoked, as ‘men’—or at least, the wealthiest among them—become like gods?

There are many reasons to doubt the hype. For one thing, machines remain comically incapable of opening doors or, alas, folding laundry. Robotic security guards are toppling into mall fountains. Computerized digital assistants can answer questions and translate documents, but not well enough to do the job without human intervention; the same is true of self-driving cars. In the midst of the American ‘Fight for Fifteen’ movement, billboards went up in San Francisco threatening to replace fast-food workers with touchscreens if a law raising the minimum wage were passed. The Wall Street Journal dubbed the bill the ‘robot employment act’. Yet many fast-food workers in Europe already work alongside touchscreens and often earn better pay than in the us. Is the talk of automation overdone?

1. THE AUTOMATION DISCOURSE

In the pages of newspapers and popular magazines, scare stories about automation may remain just idle chatter. However, over the past decade, this talk has crystalized into an influential social theory, which purports not only to analyse current technologies and predict their future, but also to explore the consequences of technological change for society at large. This automation discourse rests on four main propositions. First, workers are already being displaced by ever-more advanced machines, resulting in rising levels of ‘technological unemployment’. Second, this displacement is a sign that we are on the verge of achieving a largely automated society, in which nearly all work will be performed by self-moving machines and intelligent computers. Third: automation should entail humanity’s collective liberation from toil, but because we live in a society where most people must work in order to live, this dream may well turn out to be a nightmare. Fourth, therefore, the only way to prevent a mass-unemployment catastrophe is to provide a universal basic income (ubi), breaking the connection between the incomes people earn and the work they do, as a way to inaugurate a new society.

This argument has been put forward by a number of self-described futurists. In the widely read Second Machine Age (2014), Erik Brynjolfsson and Andrew McAfee argue that we find ourselves ‘at an inflection point—a bend in the curve where many technologies that used to be found only in science fiction are becoming everyday reality.’ New technologies promise an enormous ‘bounty’, but Brynjolfsson and McAfee caution that ‘there is no economic law that says that all workers, or even a majority of workers, will benefit from these advances.’ On the contrary: as the demand for labour falls with the adoption of more advanced technologies, wages are stagnating; a rising share of annual income is therefore being captured by capital rather than by labour. The result is growing inequality, which could ‘slow our journey’ into what they call ‘the second machine age’ by generating a ‘failure mode of capitalism’ in which rentier extraction crowds out technological innovation. In Rise of the Robots (2015), Martin Ford similarly claims that we are pushing ‘towards a tipping point’ that is poised to ‘make the entire economy less labour-intensive.’ Again, ‘the most frightening long-term scenario of all might be if the global economic system eventually manages to adapt to the new reality’, leading to the creation of an ‘automated feudalism’ in which the ‘peasants would be largely superfluous’ and the elite impervious to economic demands. For these authors, education and retraining will not be enough to stabilize the demand for labour in an automated economy; some form of guaranteed non-wage income, such as a negative income tax, must be put in place.

The automation discourse has been enthusiastically adopted by the jeans-wearing elite of Silicon Valley. Bill Gates is advocating for a tax on robots. Mark Zuckerberg told Harvard undergraduate inductees that they should ‘explore ideas like universal basic income’, a policy Elon Musk also thinks will become increasingly ‘necessary’ over time, as robots outcompete humans across a growing range of jobs. Musk has been naming his SpaceX drone vessels after spaceships from Iain M. Banks’s Culture Series, a set of ambiguously utopian science-fiction novels depicting a post-scarcity world in which human beings live fulfilling lives alongside intelligent robots, called ‘minds’, without the need for markets or states.

Politicians and their advisors have equally identified with the automation discourse, which has become one of the leading perspectives on our ‘digital future’. In his farewell presidential address, Obama suggested that the ‘next wave of economic dislocations’ will come not from overseas trade, but rather from ‘the relentless pace of automation that makes a lot of good, middle-class jobs obsolete.’ Robert Reich, former Labour Secretary under Bill Clinton, expressed similar fears: we will soon reach a point ‘where technology is displacing so many jobs, not just menial jobs but also professional jobs, that we’re going to have to take seriously the notion of a universal basic income.’ Clinton’s former Treasury Secretary, Lawrence Summers, made the same admission: once-‘stupid’ ideas about technological unemployment now seem increasingly smart, he said, as workers’ wages stagnate and economic inequality rises. The discourse has become the basis of a long-shot presidential campaign for 2020: Andrew Yang, Obama’s former ‘Ambassador of Global Entrepreneurship’, has penned his own tome on automation, The War on Normal People, and is now running a futuristic campaign on a ‘Humanity First’, ubi platform. Among Yang’s vocal supporters is Andy Stern, former head of the seiu, whose Raising the Floor is yet another example of the discourse.10

Yang and Stern—like all of the other writers named so far—take pains to assure readers that some variant of capitalism is here to stay, even if it must jettison its labour markets; however, they admit to the influence of figures on the far left who offer a more radical version of the automation discourse. In Inventing the Future, Nick Srnicek and Alex Williams argue that the ‘most recent wave of automation is poised’ to transform the labour market ‘drastically, as it comes to encompass every aspect of the economy’.11 They claim that only a socialist government would actually be able to fulfil the promise of full automation by creating a post-work or post-scarcity society. In Four Futures, Peter Frase thoughtfully explores the alternative outcomes for such a post-scarcity society, depending on whether it still had private property and still suffered from resource scarcity, which could persist even if labour scarcity were overcome. Like the liberal proponents of the automation discourse, these left-wing writers stress that, even if the coming of advanced robotics is inevitable, ‘there is no necessary progression into a post-work world’. Srnicek, Williams and Frase are all proponents of ubi, but in a left-wing variant. For them, ubi serves as a bridge to ‘fully automated luxury communism’, a term originally coined in 2014 by Aaron Bastani to name a possible goal of socialist politics, and which flourished for five years as a meme on the internet before his book—outlining an automated future in which artificial intelligence, solar power, gene-editing, asteroid mining and lab-grown meat generate a world of limitless leisure and self-invention—finally appeared.

Recurrent fears

These futurist visions, from all points of the political spectrum, depend upon a common prediction of the trajectory of technological change. Have they got this right? To answer this question, it is helpful to have a couple of working definitions. Automation may be distinguished as a specific form of labour-saving technical innovation: automation technologies fully substitute for human labour, rather than merely augmenting human-productive capacities. With labour-augmenting technologies, a given job category will continue to exist, but each worker in that category will be more productive. For example, adding new machines to an assembly-line producing cars may make line workers more productive without abolishing line work as such. However, fewer workers will be needed in total to produce any given number of automobiles. Whether that results in fewer jobs will then depend on how much output—the total number of cars—also increases.

By contrast, automation may be defined as what Kurt Vonnegut describes in Player Piano: it takes place whenever an entire ‘job classification has been eliminated. Poof.’ No matter how much production might increase, another telephone-switchboard operator or hand-manipulator of rolled steel will never be hired. In these cases, machines have fully substituted for human labour. Much of the debate around the future of workplace automation turns on an evaluation of the degree to which present or near-future technologies are labour-substituting or labour-augmenting in character. Distinguishing between these two types of technical change turns out to be incredibly difficult in practice. One famous study from the Oxford Martin School suggested that 47 per cent of jobs in the us are at high risk of automation; a more recent study from the oecd predicts that 14 per cent of oecd jobs are at high risk, with another 32 per cent at risk of significant change in the way they are carried out (due to labour-augmenting rather than substituting innovations).

It is unclear, however, whether even the highest of these estimates suggests that a qualitative break with the past has taken place. By one count, ‘57 per cent of the jobs workers did in the 1960s no longer exist today’.16 Automation, in fact, turns out to be a constant feature of the history of capitalism. By contrast, the discourse around automation, which extrapolates from instances of technological change to a broader social theory, is not constant; it periodically recurs in modern history. Excitement about a coming age of automation can be traced back to at least the mid-19th century. Charles Babbage published On the Economy of Machinery and Manufactures in 1832; John Adolphus Etzler’s The Paradise Within the Reach of All Men, Without Labour appeared in 1833, Andrew Ure’s The Philosophy of Manufactures in 1835. These books presaged the imminent emergence of largely or fully automated factories, run with minimal or merely supervisory human labour. This vision was a major influence on Marx, whose Capital, Volume One argued that a complex world of interacting machines was in the process of displacing labour at the centre of economic life.

Visions of automated factories then appeared again in the 1930s, 1950s and 1980s, before their re-emergence in the 2010s. Each time, they were accompanied or shortly followed by predictions of a coming age of ‘catastrophic unemployment and social breakdown’, which could be prevented only if society were reorganized.17 To point out the periodicity of this discourse is not to say that its accompanying social visions should be dismissed. For one thing, the technological breakthroughs presaged by automation discourse could still be achieved at any time: just because they were wrong in the past does not necessarily mean that they will always be wrong in the future. More than that, these visions of automation have clearly been generative in social terms: they point to certain utopian possibilities latent within modern capitalist societies. The error in their approach is merely to suppose that, via ongoing technological shifts, these utopian possibilities will imminently be revealed via a catastrophe of mass unemployment.

The basic insight on which automation theory relies was described, most succinctly, by the Harvard economist Wassily Leontief. He pointed out that the ‘effective operation of the automatic price mechanism’ at the core of capitalist societies ‘depends critically’ on a peculiar feature of modern technology, namely that in spite of bringing about ‘an unprecedented rise in total output’, it nevertheless ‘strengthened the dominant role of human labour in most kinds of productive processes’.18 At any time, a breakthrough could destroy this fragile pin, annihilating the social preconditions of functioning market economies. Drawing on this insight—and adding only that such a technological breakthrough now exists—the automation prognosticators often argue that capitalism must be a transitory mode of production, which will eventually give way to a new form of life that does not organize itself around work for wages and monetary exchange.

Taking its periodicity into account, automation theory may be described as a spontaneous discourse of capitalist societies, which, for a mixture of structural and contingent reasons, reappears in those societies time and again as a way of thinking through their limits. What summons the automation discourse periodically into being is a deep anxiety about the functioning of the labour market: there are simply too few jobs for too many people. Proponents of the automation discourse consistently explain the problem of a low demand for labour in terms of runaway technological change.

Declining labour demand
If automation discourse appeals so widely again today, it is because, whatever their causes, the ascribed consequences of automation are all around us: global capitalism clearly is failing to provide jobs for many of the people who need them. There is, in other words, a persistently low demand for labour, reflected not only in higher spikes of unemployment and increasingly jobless recoveries—both frequently cited by automation theorists—but also in a phenomenon with more generic consequences: declining labour shares of income. Many studies have now confirmed that the labour share, whose steadiness was held to be a stylized fact of economic growth, has been falling for decades (Figure 1).
These shifts signal a radical decline in workers’ bargaining power. Realities for the typical worker are worse than these statistics suggest, since wage growth has become increasingly skewed towards the highest earners: the infamous top one per cent. A growing gap has opened up not only between the growth of labour productivity and average wage-incomes, but also between the growth of average wages and that of median wages, with the result that many workers see a vanishingly thin slice of economic growth (Figure 2).20 Under these conditions, rising inequality is contained only by the strength of redistributive programmes. Even critics of automation discourse such as David Autor and Robert Gordon are disturbed by these trends: something has gone wrong with the economy, leading to a low demand for labour.

Is automation the cause of the low demand for labour? I will join the critics of automation discourse in arguing that it is not. However, along the way, I will also criticize the critics—both for producing explanations of low labour demand that only apply in high-income countries and for failing to produce anything like a radical vision of social change that is adequate to the scale of the problems we now confront. Indeed, it should be said from the outset that I am more sympathetic to the left automation theorists than to their critics.

Even if the explanation they offer turns out to be inadequate, the automation theorists have at least focused the world’s attention on the problem of a persistently low demand for labour. They have also excelled in actually trying to imagine solutions to this problem that are broadly emancipatory in character. In Jameson’s terms, the automation theorists are our late capitalist utopians. In a world reeling from the ‘perfect storm’ of climate change, rising inequality, recalcitrant neoliberalism and resurgent ethno-nationalism, the automation theorists are the ones pushing through the catastrophe with a vision of an emancipated future, in which humanity advances to the next stage in our history, whatever that might mean (or whatever we want to make it mean), and technology helps to free us all to discover and follow our passions. That is true in spite of the fact that—like many of the utopians of the past—the actual visions these latest utopians offer need to be freed from their largely technocratic fantasies of how social change to a better future might take place.

Major shifts in the forms of government intervention in the economy are adopted only under massive social pressure, such as, in the course of the 20th century, the threat of communism or of civilizational collapse. Today, policy reforms could emerge in response to pressure coming from a new mass movement, aiming to change the basic makeup of the social order. Instead of fearing that movement, we should see ourselves as part of it, helping articulate its goals and paths forward. If that movement is defeated, maybe the best we will get is basic income, but that should not be our goal. We should be reaching towards a post-scarcity world, which advanced technologies will certainly help us realize, even if full automation is not achievable—or even desirable.The return of automation discourse is a symptom of our era, as it was in times past: it arises when the global economy’s failure to create enough jobs causes people to question its fundamental viability. The breakdown of this market mechanism today is more extreme than at any time in the past. This is because a greater share of the world’s population than ever before depends on selling its labour or the simple products of its labour to survive, in the context of weakening global economic growth. Our present reality is better described by near-future science-fiction dystopias than by standard economic analysis; ours is a hot planet, with micro-drones flying over the heads of the street hawkers and rickshaw pullers, where the rich live in guarded, climate-controlled communities while the rest of us wile away our time in dead-end jobs, playing video games on smartphones. We need to slip out of this timeline and into another.
Reaching towards a post-scarcity world—in which all individuals are guaranteed access to whatever they need to make a life, without exception—can become the basis on which humanity mounts a battle against climate change. It can also be the foundation on which we remake the world, creating the conditions in which, as James Boggs once put it, ‘for the first time in human history, great masses of people will be free to explore and reflect, to question and to create, to learn and to teach, unhampered by the fear of where the next meal is coming from’. Finding our way forward requires a break between work and income, as the automation theorists recognize, but also between profit and income, as many do not.
In responding to the automation discourse, then, I will argue that the decline in the demand for labour is due not to an unprecedented leap in technological innovation, but to ongoing technical change in an environment of deepening economic stagnation. In the second part of this contribution, to be published in nlr 120, I contend that this fall in labour demand manifests not as mass unemployment, but rather as mass under-employment, not necessarily a problem for the elites. On this basis, I mount a critique of technocratic solutions, like basic income. I offer a thought-experiment of how we might imagine a post-scarcity society that centres on humans, not machines, and project a path of how we might get there through social struggle, rather than administrative intervention. But first, in Part One, I provide a diagnosis of the underlying causes of the decline in demand for labour. This involves a detour to consider the fortunes of the global manufacturing sector and the competitive dynamics at work in labour’s ‘deindustrialization’.
2. LABOUR'S GLOBAL DEINDUSTRALIZATION
Automation-discourse theorists recognize that, if technologically induced job-destruction is to have widespread social ramifications, it will have to eliminate employment in the vast and variegated service sector, which absorbs 74 per cent of workers in high-income countries and 52 per cent worldwide. They therefore focus on ‘new forms of service-sector automation’ in retail, transportation and food services, where ‘robotization’ is said to be ‘gathering steam’ with a growing army of machines that take orders, stock shelves, drive cars and flip burgers. Many more service-sector jobs, including some that require years of education and training, will supposedly be rendered obsolete in the coming years due to advances in artificial intelligence. Of course, these claims are mostly predictions about the effects that technologies will have on future patterns of employment. Such predictions can go wrong—as for example when Eatsa, an automated fast-food company which employed neither cashiers nor waiters, was forced to close most of its stores in 2017.
In making their case, automation theorists often point to the manufacturing sector as the precedent for what they imagine is beginning to happen in services—for in manufacturing, the employment-apocalypse has already taken place. To evaluate the theorists’ claims, it therefore makes sense to begin by looking at what role automation has played in that sector’s fate. After all, manufacturing is the area most amenable to automation, since on the shop floor it is possible to ‘radically simplify the environment in which machines work, to enable autonomous operation’. Industrial robotics has been around for a long time: the first robot, the ‘unimate’, was installed in a General Motors plant in 1961. Still, until the 1960s, scholars studying this sector were able to dismiss Luddite fears of long-term technological unemployment out of hand. Manufacturing employment in fact grew most rapidly in those lines where technical innovation was happening at the fastest pace, because it was in those lines that prices fell the fastest, stoking the growth of demand for the products.
Industrialization has long since given way to deindustrialization, and not just in any one line but across the manufacturing sectors of most countries. The share of workers employed in manufacturing fell first across the high-income world: manufacturing employed 22 per cent of all workers in the us in 1970; that share declined to just 8 per cent in 2017. Over the same period, manufacturing employment shares fell from 23 per cent to 9 per cent in France, and from 30 per cent to 8 per cent in the uk. Japan, Germany and Italy have experienced smaller but still substantial declines: in Japan from 25 per cent to 15 per cent, in Germany from 29 per cent to 17 per cent, and in Italy from 25 per cent to 15 per cent. In all cases, the declines were eventually associated with substantial falls in the total number of people employed in manufacturing. In the us, Germany, Italy and Japan, the overall number of manufacturing jobs fell by approximately a third from postwar peaks; in France, by 50 per cent and in the uk, by 67 per cent.

It is commonly assumed that deindustrialization must be the result of production facilities moving offshore. Yet in none of the countries named above has manufacturing job loss been associated with declines in manufacturing output. Real value added in manufacturing more than doubled in the us, France, Germany, Japan and Italy between 1970 and 2017. Even the uk, whose manufacturing sector fared worst of all among this group, saw a 25 per cent increase in manufacturing real value added over this period. To be sure, low- and middle-income countries are producing more and more goods for import into high-income countries; however, deindustrialization in the latter cannot simply be the result of productive capacity moving to the former. In the scholarly literature, deindustrialization is therefore ‘most commonly defined as a decline in the share of manufacturing in total employment’, regardless of corresponding trends in levels of manufactured output. This definition moves in step with automation theorists’ core expectations: more goods are being produced but by fewer workers.

It is on this basis that commentators typically cite rapidly rising labour productivity, rather than an influx of low-cost imports from abroad, as the primary cause of industrial-job loss in advanced economies. On closer inspection, however, this explanation turns out to be inadequate: no upward leap has taken place in manufacturing productivity levels. On the contrary, manufacturing productivity has been growing at a sluggish pace for decades, leading Robert Solow to quip, ‘We see the computer age everywhere, except in the productivity statistics.’ Automation theorists discuss this ‘productivity paradox’ as a problem for their account—explaining it in terms of weak demand for products, or the persistent availability of low-wage workers—but they understate its true significance. This is partly due to the appearance of steady labour-productivity growth in us manufacturing, at an average rate of around 3 per cent per year since 1950. On that basis, Brynjolfsson and McAfee suggest, automation could show up in the compounding effects of exponential growth, rather than an uptick in the growth rate.

However, official us manufacturing growth-rate statistics are overinflated, for example in logging the production of computers with higher processing speeds as equivalent to the production of more computers. On that basis, government statistics claim that productivity levels in the computers and electronics sub-sector rose at an average rate of over 10 per cent per year between 1987 and 2011, even as productivity growth rates outside of that sub-sector fell to around 2 per cent per year over the same period. Since 2011, trends across the manufacturing sector have worsened: real output per hour in the sector as a whole was lower in 2017 than at its peak in 2010. Productivity growth rates in manufacturing collapsed precisely when they were supposed to be rising rapidly due to industrial automation.

Correcting manufacturing-productivity statistics in the us brings them more into line with trends visible in the statistics of other countries. In Germany and Japan, manufacturing-productivity growth rates have fallen dramatically since their postwar peaks. In Germany, for example, manufacturing productivity grew at an average annual rate of 6.3 per cent per year in the 1950s and 60s, falling to 2.4 per cent since 2000. This downward trend is to some extent an expected result of the end of an era of rapid, catch-up growth. However, it should still be surprising to the automation theorists, since Germany and Japan have raced ahead of the us in the field of industrial robotics. Indeed, the robots used in Tesla’s largely automated car factory in California were made by a German robotics company. German and Japanese firms deploy about 60 per cent more industrial robots per 10,000 manufacturing workers, compared to the US.

Yet deindustrialization continues to take place in all these countries, despite lacklustre manufacturing-productivity growth rates: that is, it is taking place as the automation theorists expect, but not for the reasons they offer. To explore the causes of deindustrialization in more detail, I use the following accounting identity. For any given industry, the rate of growth of output (ΔO) minus the rate of growth of labour productivity (ΔP) equals the rate of growth of employment (ΔE). Thus, ΔO – ΔP = ΔE. So, for example, if the output of automobiles grows by 3 per cent per year, and productivity in the automobile industry grows by 2 per cent per year, then employment in that industry must necessarily rise by one per cent per year (3 – 2 = 1). Contrariwise, if output grows by 3 per cent per year and productivity grows by 4 per cent per year, employment will contract by 1 per cent per year (3 – 4 = -1).

Disaggregating manufacturing-output growth rates in France provides us with a sense of the typical pattern playing out across the high-income countries (Figure 3). During the so-called Golden Age of postwar capitalism, productivity growth rates in French manufacturing were much higher than they are today—5.2 per cent per year, on average, between 1950 and 1973—but output growth rates were even higher than that—5.9 per cent per year—as a result of a steady increase in employment of 0.7 per cent per year. Since 1973, both output and productivity rates have declined, but output rates fell much more sharply than productivity rates. By the early years of the 21st century, productivity growth rates—although much slower, at 2.7 per cent per year—were now faster than their corresponding output growth rates—at 0.9 per cent—as manufacturing employment contracted rapidly, by 1.7 per cent per year.

This disaggregation helps explain why automation theorists falsely perceive productivity to be growing at a rapid pace in manufacturing: in fact, productivity growth has been rapid only relative to extremely sluggish output growth. The same pattern can be seen in the statistics of other countries: no absolute decline in levels of manufacturing production has taken place, but there has been a decline in the output growth rate, with the result that output is growing more slowly than productivity (Table 1, overleaf). The simultaneity of limited technological dynamism and worsening economic stagnation combines to generate a progressive decline in industrial employment levels.

As such, ‘output-led’ deindustrialization is impossible to explain in purely technological terms. In searching for alternative perspectives, economists have mostly preferred to describe it as a harmless evolutionary feature of advanced economies. However, that perspective is itself at a loss in explaining extreme variations in the gdp per capita levels at which this supposedly evolutionary economic shift has taken place. Deindustrialization unfolded first in high-income countries in the late 1960s and early 1970s, at the tail-end of a period in which levels of income per person had converged across the us, Europe and Japan. In the decades that followed, deindustrialization then spread ‘prematurely’ to middle- and low-income countries, with larger variations in incomes per capita (Figure 4). In the late 1970s, deindustrialization arrived in southern Europe; much of Latin America, parts of East and Southeast Asia, and southern Africa followed in the 1980s and 1990s. Peak industrialization levels in many poorer countries were so low that it may be more accurate to say that they never industrialized in the first place.

By the end of the 20th century, it was possible to describe deindustrialization as a kind of global epidemic: worldwide manufacturing employment rose in absolute terms by 0.4 per cent per year between 1991 and 2016, but that was much slower than the overall growth of the global labour force, with the result that the manufacturing share of total employment declined by 3 percentage points over the same period. China is a key exception, but only a partial one (Figure 5, overleaf). In the mid 1990s, Chinese state-owned enterprises shed large numbers of workers, sending manufacturing-employment shares on a steady downward trajectory. China re-industrialized, starting in the early 2000s, but then began to deindustrialize once again in the mid 2010s: its manufacturing-employment share has since dropped from 19.3 per cent in 2013 to 17.5 per cent in 2017, with further falls likely. If deindustrialization cannot be explained by either automation or the internal evolution of advanced economies, what could be its source?

3. BLIGHT OF MANUFACTURING OVERCAPACITY

What the economists’ accounts fail to register in explaining deindustrialization is also what is missing from the automation theorists’ accounts. The truth is that rates of output growth in manufacturing have tended to decline, not only in this or that country, but worldwide (Figure 6). In the 1950s and 60s, global manufacturing production expanded at an average annual rate of 7.1 per cent per year, in real terms. That rate fell progressively to 4.8 per cent in the 1970s, and to 3.0 per cent between 1980 and 2007. Since the 2008 crisis and up to 2014, manufacturing output expanded at just 1.6 per cent per year, on a world scale—that is, at less than a quarter of the pace achieved during the so-called postwar Golden Age. It is worth noting that these figures include the dramatic expansion of manufacturing productive capacity in China. Again, it is the incredible degree of slowdown or even stagnation in manufacturing-output growth, visible on the world scale, that explains why manufacturing-productivity growth appears to be advancing at a rapid clip, even though it is actually much slower than before. More and more is produced with fewer workers, as the automation theorists claim, but not because technological change is giving rise to high rates of productivity growth. On the contrary, productivity growth in manufacturing appears rapid today only because the yardstick of output growth, against which it is measured, is shrinking.Seen from this perspective, the global wave of deindustrialization can be said to find its origins not in runaway technical change but rather in worsening overcapacity in world markets for manufactured goods. The rise in overcapacity developed stepwise after World War Two. In the immediate postwar period, the us hosted the most dynamic economy in the world, with the most advanced technologies. Under the threat of communist expansion within Europe, as well as in East and Southeast Asia, the us proved willing to share its technological largesse with its former imperial competitors Germany and Japan, as well as other ‘frontline’ countries, in order to bring them all under the us security umbrella. In the first few decades of the post-wwii era, these technology transfers were a major boost to economic growth in Europe and Japan, opening up opportunities for export-led expansion. This strategy was also supported by the devaluation of European and Japanese currencies against the dollar. However, as Robert Brenner has argued, rising manufacturing capacity across the globe quickly generated overcapacity, issuing in a ‘long downturn’ in manufacturing output growth rates.

What mattered here was not only the later building out of manufacturing capacity in the global South, but the earlier creation of such capacity in countries like Germany, Italy and Japan, which hosted the first low-cost producers in the postwar era who succeeded in taking shares in global markets for industrial goods, and then invading the previously impenetrable us domestic market. That competition caused rates of industrial-output growth in the us to decline in the late 1960s, issuing in deindustrialization in employment terms. As the us responded to heightened import penetration in the 1970s by breaking up the Bretton Woods order and devaluing the dollar, these same problems spread from the highest wage countries in North America and northern Europe to Japan and the rest of Europe. Thereafter, as more and more countries built up manufacturing capacity, adopted export-led growth strategies and entered global markets for manufactured goods, falling rates of manufacturing-output growth and consequent labour deindustrialization also spread to Latin America, the Middle East, Asia and Africa, as well as to the global economy taken as a whole.

Deindustrialization is not only a matter of technological advance, but also of a global redundancy of technological capacities, creating more crowded markets in which rapid rates of industrial-output expansion become more difficult to achieve. The mechanism transmitting this problem across the globe was severely depressed prices in global markets for manufactured goods. That led to falling income-per-unit capital ratios, then to falling rates of profit, then to lower rates of investment, and hence lower rates of output growth. In this environment, firms have faced heightened competition for market share: as overall growth rates slow, the only way to grow quickly is to steal market shares from other firms. Each firm has to do everything it can to keep up with its competitors. Overcapacity explains why, since the early 1970s, productivity-growth rates have fallen less severely than output-growth rates: firms have continued to raise their productivity levels as best they can despite falling rates of output growth (or else have gone under, disappearing from statistical averages). As manufacturing-output growth rates slipped below productivity-growth rates in one country after another, deindustrialization spread worldwide.

Driving globalization

Explaining global waves of deindustrialization in terms of global overcapacity rather than industrial automation allows us to understand a number of features of this phenomenon that otherwise appear paradoxical. For example, rising overcapacity explains why deindustrialization has been accompanied not only by ongoing efforts to develop new labour-saving technologies, but also by the building out of gigantic, labour-using supply chains—usually with a more damaging environmental impact. A key turning point in that story came in the 1960s, when low-cost Japanese and German products invaded the us domestic market, sending the us industrial-import penetration ratio soaring from less than 7 per cent in the mid-60s to 16 per cent in the early 1970s. From that point forward, it became clear that high levels of labour productivity would no longer serve as a shield against competition from lower-wage countries. The us firms that did best in this context were the ones that responded by globalizing production. Facing competition on prices, us multinational firms built international supply chains, shifting the more labour-intensive components of their production processes abroad and playing suppliers off against one another to achieve the best prices. In the mid-60s the first export-processing zones opened in Taiwan and South Korea. Even Silicon Valley, which formerly produced its computer chips locally in the San Jose area, shifted its production to low-wage areas, using lower grades of technology (and also benefitting from laxer laws around pollution and workers’ safety). mncs in Germany and Japan adopted similar strategies, which were everywhere supported by new infrastructures of transportation and communication technologies.

The globalization of production allowed the world’s wealthiest economies to retain manufacturing capacity, but it did not reverse the overall trend towards labour deindustrialization. As supply chains were built out across the world, firms in more and more countries were pulled into the swirl of world-market competition. In some countries, this move was accompanied by shifts in the location of new plants: rustbelts oriented towards production for domestic markets went into decline, while sunbelts integrated into global supply networks expanded dramatically. Chattanooga grew at the expense of Detroit, Ciudad Juárez at the expense of Mexico City, Guangdong at the expense of Dongbei. Yet given the overall slowdown in rates of world manufacturing-market expansion, this re-orientation towards the world market could only result in lacklustre outcomes: the rise of sunbelts failed to balance out the decline of rustbelts, resulting in global deindustrialization.

At the same time, global manufacturing overcapacity explains why the countries that have succeeded in attaining a high degree of robotization are not those that have seen the worst degree of deindustrialization. In the context of intense global competition, high degrees of robotization have given firms competitive advantages, allowing them to take market share from firms in other countries. Thus Germany, Japan and South Korea have some of the highest levels of robotization; they also have the largest trade surpluses in the world. Workers in European and East Asian firms know that automation helps preserve their jobs. China is also a top-four country in terms of trade surpluses, providing its manufacturing sector with a gigantic boost in terms of both output and employment growth. China has advanced on this front not due to high levels of robotization, but rather due to a mix of low wages, moderate to advanced technologies, and strong infrastructural capacities. Yet the result was the same: in spite of system-wide overcapacity and slow growth rates, the prc has industrialized rapidly because Chinese firms have been able to take market share away from other firms—not only in the us, but also in countries like Mexico and Brazil—which lost market share as Chinese firms expanded. It could not have been otherwise, since in an environment where average growth rates are low, firms can only achieve high rates of growth by taking market share from their competitors. Whether China will be able to retain its competitive position as its wage levels rise remains an open question; Chinese firms are now racing to robotize in order to head off this possibility.

4. BEYOND MANUFACTURING

The evidence I have cited so far to explain job loss in the manufacturing sector through worsening overcapacity may appear to have little purchase on the larger, economy-wide patterns—of stagnant wages, falling labour shares of income, declining labour-force participation rates and jobless recoveries after recessions—that the automation theorists have sought to explain by growing technological dynamism. Automation may therefore still seem a good explanation for the decline in demand for labour across the service sectors of each country’s economy, and so across the world economy as a whole. Yet this broader problem of declining labour demand also turns out to be better explained by the worsening industrial stagnation I have described than by widespread technological dynamism.

This is because, as rates of manufacturing-output growth stagnated in one country after another from the 1970s onward, no other sector appeared on the scene to replace industry as a major economic-growth engine. Instead, the slowdown in manufacturing-output growth rates was accompanied by a slowdown in overall growth rates. This trend is visible in the economic statistics of high-income countries. France is again a striking example (Figure 7). In France, real manufacturing value added (mva) rose at 5.9 per cent per year between 1950 and 1973, while real value added in the total economy (gdp) rose at 5.1 per cent per year. Since 1973, both growth measures have declined significantly: by the 2001–17 period, mva was rising at only 0.9 per cent per year, while gdp was rising at a faster but still sluggish pace of 1.2 per cent per year. Note that during the 1950s and 60s, mva growth generally led the overall economy: manufacturing served as the major engine of overall growth. Since 1973, mva growth rates have trailed overall economic growth. Similar patterns can be seen in other high-income countries (Table 2, overleaf).Their export-led growth engines sputtered and slowed to a crawl; and as they did so, overall rates of economic growth slowed considerably.

Economists studying deindustrialization often point out that while manufacturing has declined as a share of nominal gdp, it has maintained, until recently, a more or less steady share of real gdp, which is to say that, between 1973 and 2000, real mva grew at approximately the same pace as real gdp. What that has meant in practice is that, as manufacturing has become less dynamic, so has the overall economy. There was no significant shift in demand from industry to services. Instead, as capital accumulation slowed down in manufacturing, the expansion of aggregate output also slowed significantly across the economy as a whole.

This tendency to economy-wide stagnation, associated with the decline in manufacturing dynamism, then explains the system-wide decline in the demand for labour, and so also the problems that the automation theorists cite: stagnant real wages, falling labour shares of income and so on. This economy-wide pattern of declining labour demand is not the result of rising productivity-growth rates, associated with automation in the service sector. On the contrary, productivity is growing even more slowly outside of the manufacturing sector than inside of it: in France, for example, while productivity in the manufacturing sector was rising at an average annual rate of 2.7 per cent per year between 2001–17, productivity in the service sector was rising at just 0.6 per cent per year. Similar gaps exist in other countries. Once again, the mistake of the automation theorists is to focus on rising productivity growth rather than falling output growth. The environment of slower economic growth explains the low demand for labour all by itself. Workers, and especially workers who are not protected by powerful unions or labour laws, find it difficult to pressure employers to raise their wages when there is so much slack in the labour market.

These trends are as visible in the world economy—including China—as they are in the high-income countries (Figure 8, overleaf). In the 1950s and 60s, global mva growth and gdp growth were expanding at rapid clips of 7.1 per cent and 5.0 per cent respectively, with mva growth leading gdp growth by a significant margin. From the 1970s onward, as global mva growth slowed, so did global gdp growth. In most of the decades that followed, global mva growth continued to lead gdp growth but by a much smaller margin. Since 2008, both rates have been growing at the exceptionally slow pace of 1.6 per cent per year. Again, the implication is that, as manufacturing growth rates declined, nothing emerged to replace industry as a growth engine. Not all regions of the world economy are experiencing this slowdown in the same way or to the same extent, but even countries like China that have grown quickly have to contend with this global slowdown and its consequences. Since the 2008 crisis, China’s economic growth rate has slowed considerably; its economy is deindustrializing.

The clear conclusion is that manufacturing turned out to be a unique engine of overall economic growth. Industrial production tends to be amenable to incremental increases in productivity, achieved via technologies that can be repurposed across numerous lines. Industry also benefits from static and dynamic economies of scale. Meanwhile, there is no necessary boundary to industrial expansion: industry consists of all economic activities that are capable of being rendered via an industrial process. The reallocation of workers from low-productivity jobs in agriculture, domestic industry and domestic services to high-productivity jobs in factories raises levels of income per worker and hence overall economic growth rates. The countries that have caught up with the West in terms of income—such as Japan, South Korea and Taiwan—mostly did so by industrializing: they exploited opportunities to produce for the world market, at increasing scale and using advanced technologies, allowing them to grow at speeds that would have been unachievable had they depended on domestic-market demand alone.

When the growth engine of industrialization sputters—due to the replication of technical capacities, international redundancy and fierce competition for markets—there has been no replacement for it as a source of rapid growth. Instead of workers reallocating from low-productivity jobs to high-productivity ones, the reverse of this process takes place, as workers pool increasingly in low-productivity jobs in the service sector. As countries have deindustrialized, they have also seen a massive build-up of financialized capital, chasing returns to the ownership of relatively liquid assets, rather than investment in new fixed capital. In spite of the high degree of overcapacity in industry, there is nowhere more profitable in the real economy for capital to invest itself. Indeed, if there had been, we would have evidence of it in higher rates of investment and hence higher gdp growth rates. This helps explain why firms have reacted to over-accumulation by trying to make their existing manufacturing capacity more flexible and efficient, rather than ceding territory to lower-cost, higher-productivity firms from other countries.

The lack of an alternative growth engine also explains why governments in poorer countries have encouraged domestic producers to try to break into already oversupplied international markets for manufactures. Nothing has replaced those markets as a major source of globally accessible demand. Overcapacity exists in agriculture, too, and is even worse there than in industry; meanwhile services, which are mostly non-tradable, make up only a tiny share of global exports. If countries are to retain any dependable link to the international market under these conditions, they must find some way to insert themselves into industrial lines, however oversupplied. System-wide overcapacity and the generalized slowdown in economic growth have therefore been devastating for most poorer countries: the amount of foreign exchange they have captured through liberalization has been pitiful; so, too, has been the number of jobs created.

Indeed, global economic downshifts have been particularly devastating for low- and middle-income countries, not only because they are poorer, but also because those downshifts have taken place in an era of rapid labour-force expansion: between 1980 and the present, the world’s waged workforce grew by about 75 per cent, adding more than 1.5 billion people to the world’s labour markets. These labour market entrants, living mostly in poorer countries, had the misfortune of growing up and looking for work at a time when global industrial overcapacity began to shape patterns of economic growth in post-colonial countries: declining rates of manufactured export growth into the us and Europe in the late 1970s and early 1980s ignited the 1982 debt crisis, followed by imf-led structural adjustment, which pushed countries to deepen their imbrications in global markets at a time of ever slower global growth and rising competition from China. In spite of shocks to the demand for labour generated by slowing global growth rates and rising economic turmoil, huge numbers of workers were still forced to seek employment in order to live.

Some may respond that the present low rates of global growth are in fact nothing out of the ordinary, if only we shift our baseline from the exceptional postwar ‘Golden Age’ to previous periods, such as the pre-WWI era. But a global perspective on the decline in the demand for labour provides the answer to this objection. It is true that, during the Belle Epoque, average rates of economic growth were more comparable to growth rates today. However, in that period, large sections of the population still lived in the countryside and produced much of what they needed to live. European empires still overran the globe, not only limiting the diffusion of new manufacturing technologies to a few regions, but also actively deindustrializing the rest of the world economy. Yet in spite of the much more limited sphere in which labour markets were active—and in which industrialization took place—the pre-wwi era, as also the inter-war period, was marked by a persistently low demand for labour, making for employment insecurity, rising inequality and tumultuous social movements aimed at transforming economic relations. In this respect, the world of today does look like the world of the Belle Epoque. The difference is that today, a much larger share of the world’s population depends on finding work in labour markets in order to live.What automation theorists describe as the result of rising technological dynamism is actually the consequence of worsening economic stagnation: productivity-growth rates appear to rise when, in reality, output-growth rates are falling. This mistake is not without reason. The demand for labour is determined by the gap between productivity and output growth rates. Reading the shrinking of this gap the wrong way around—that is, as due to rising productivity rather than falling output rates—is what generates the upside-down world of the automation discourse. Proponents of this discourse then search for the technological evidence that supports their view of the causes for the declining demand for labour. In making this leap, the automation theorists miss the true story of overcrowded markets and economic slowdown that actually explains the decline in labour demand.

Yet even if automation is not itself the primary cause of a low demand for labour, it is nevertheless the case that, in a slow-growing world economy, technological changes within a near-future horizon may still threaten large numbers of jobs with destruction, in a context of economic stagnation and slower rates of job creation. Technological change then acts as a secondary cause of a low labour demand, operating within the context of the first. The concluding section of this essay in nlr 120 will address these technological dynamics, as well as the socio-political problems—and opportunities—generated by a persistently low demand for labour in late-capitalist societies.


/ 002. Purcell, Conor. "Beyond Human Intelligence," Farsight, 2023.


Anyone with personal experience taking LSD, psilocybin, ayahuasca, or any other mindaltering psychedelic will truly recognise the world’s interconnected nature. Fractalising into white diamonds, the air shatters, trees breathe, and animals speak your language. Some suggest these drugs dismantle an evolved human filter, revealing nature for what it truly is: a connected intelligence.

James Bridle’s new book Ways of Being: Beyond Human Intelligence is an exploration of different forms of intelligence, both biological and artificial. It’s also, as the author says, a call for us humans to start forming new relationships with non-human intelligence. Throughout the book Bridle argues that our common future demands less industrial hubris, and more cooperation with existing and deeply knowledgeable biological systems.

A writer and artist known for coining the term ‘New Aesthetic’ – used to refer to the increasing appearance of the visual language of digital technology in the physical world, and the combination of the virtual and physical – James Bridle advocates for a future characterised by human, animal, and plant reconnectivity for the sake of achieving a better planetary balance.

Our regular contributor, Conor Purcell, PhD, had the opportunity to interview Bridle for FARSIGHT, speaking by video call between Purcell’s home in County Donegal, Ireland, and the interviewee’s in Greece.

What inspired you to write this book?

I studied artificial intelligence almost twenty years ago when it was kind of fading from the curriculum because it wasn’t going anywhere. Since then, there haven’t been any kind of major discoveries. But what has happened is that vast amounts of data have become available, which have been harvested largely by social media giants and governments. At the same time, processing power has massively increased. We’re now seeing how AI is revealing itself to be something not quite human in that it thinks and approaches the world in a very different way than we do. We’re also starting to realise, thanks to decades of research, that intelligence is something much more interesting and greater than our very narrow human idea of it.

With the book, I wanted to understand how we can better accommodate ourselves with everything else that we share the planet with. For me, this question is central to achieving environmental justice and progress. I now see an opportunity with AI for reimagining, firstly, what intelligence is, and secondly, how we impact other forms of intelligence beyond the human.

How do you think people have become so disconnected from these other ways of being, specifically the intelligences of animals and plants in nature?

A good example to demonstrate this is how in medieval times there were cases when animals were accused of committing a crime and tried in courtrooms. There were lawyers there, and the animals were presented to juries. This wasn’t pantomime, but a deeply serious undertaking because non-humans were part of the community. That meant they had rights and responsibilities.

Over time, and especially with industrialisation and urbanisation, attitudes towards nonhuman life in all its forms changed. We started to view them essentially as machines – unfeeling automatons who didn’t have the kind of inner life or higher importance which we ascribe to humans. This became the dominant mode of thought within Western, postenlightenment societies. That’s when the abattoirs began. And now the environmental mess that we find ourselves in is all related to how we’re out of balance with our deeply entangled and interdependent relationships with all other species.

What do we now know about the intelligent behavior of plants?

Recent research has shown several surprising behavioural qualities in plants. I write in the book about scientists who subjected certain plants to repeated shocks and found that quite quickly they learned essentially to ignore the shock and move away from its source. What’s more is that they remembered patterns and continued to avoid the source of the shock in the future.

This is an extraordinary finding that completely changes our understanding of plant behaviour. Even the idea that plants have a thing that we might call behaviour is astonishing because the traditional kind of botanical approach mostly involves cutting them up into small pieces and studying them as if they were machines. What’s interesting too is that these researchers write about working with plant spirits, and their work is informed by both the knowledge that has come from the plants themselves and by treating the plant as already having its own personhood.

This is real science published in legitimate scientific journals. It’s peer reviewed. It’s reproducible. It conforms to all the structures of the scientific method. What that tells me is that there are multiple ways of approaching these intelligences and to do that via a kind of synthesis of these different ways of knowing is incredibly powerful. We can explore the world by observing and connecting with these behaviours, as long as our goal is to truly understand. Ultimately, it all depends on admitting the possibility in the first place that these kinds of alternative intelligences are real.

In a chapter called "Non-Binary Machines" you talk about the fields of cybernetics- which has a long history dating back to the mid-20th century- and how this shows a future alternative to what you call 'corporate artificial intelligence'. Can you explain what you mean by this?

It has to do with thinking of intelligence as a process, rather than as a machine that thinks like a kind of brain in a box. Particularly in Britain, cybernetic researchers – those involved in the science of communication and automation in machines and other living beings – envisioned a kind of intelligence that is active in the world, which is connected to the world around it, which is learning, and which is defined by what it does, rather than what it is. This is different to the corporate artificial intelligence of today which is currently being developed to increase profits.

Cybernetic research continues in various ways. There is very interesting new research around soft robotics, which essentially tries to make robotic systems more adaptive to the world around them. Programmes like the Unconventional Computing Lab at the University of the West of England is a good example. One of the things they study is the computational abilities of various plants and animals. They are doing very interesting things like redesigning computer logic based on the movement of crabs, for example. This points to the fact that what we understand as computation is not something that can only be performed within machines, but in fact is conducted by biological organisms too.

It also appears that biological systems can be calibrated to test variable abilities and to solve mathematical problems – they might even be more efficient than our fastest supercomputers. These abilities exist across the natural world, but since we usually only see the things that we know how to test for, there remains the possibility of a whole range of intelligences which far exceed our own. The problem is that we don’t even know how to ask the appropriate questions yet.

How can we reconnect with non-human intelligence in the future?

Towards the end of the book, I write about the need to provide more shared territory for human and non-human lives. I mean this both in the form of animal reserves, conservation areas, wildlife corridors and shared spaces that allow animals to move in ways that they currently cannot. But I also think that the notion of animal intelligence compels us to think politically.

In the book, I discuss the Irish experience with the introduction of the so-called citizen assemblies, made up of 33 representatives chosen by political parties and 66 randomly chosen citizens, to make recommendations on society’s biggest challenges. One of the things we learned from the citizen assemblies, not just in Ireland, but in other places, is that this is an extraordinary mechanism for mobilising what are essentially multiple forms of intelligence. The assemblies didn’t use animal, plant or artificial intelligence, but by branching out beyond the traditional domain of experts, the range of human intelligence – and personality types – was enlarged. So instead of selecting a very narrow definition of domain experts, it’s acknowledging that what you need for complex thorny problems, particularly novel ones, is a wider diversity of life experiences and ways of thinking.

The same principle can apply to including intelligences beyond the human in our decision making. I believe that only by bringing in diverse ways of thinking and forms of life experience can we address the kind of extraordinary global and pan-species problems that we face.


/ 003. Mogensen, Klaus Æ. "Is Artificial Intelligence a Myth?" Farsight, 2023.


Erik Larson is a tech entrepreneur and pioneering research scientist working at the forefront of natural language processing. He recently published a book called The Myth of Artificial Intelligence: Why Computers Can't Think the Way We Do. FARSIGHT met him online for an interview about the future of AI, and why he believes the field’s current path of development will not lead us to human-level intelligence in machines anytime soon.

Erik, what made you decide to write this book?

My specialty is natural language processing, and I wrote the book from the perspective of understanding the many practical challenges and difficulties there are in making computers understand human language on a deep level. Early in my career, I read a book by Ray Kurzweil, The Age of Intelligent Machines, where he proposed 2029 as the year when computers become as smart as humans. I thought, maybe – it’s 30 years, after all. By 2005, when his book The Singularity is Near came out, I thought that it could not happen in 20 years without some major unexpected scientific breakthrough that we couldn’t anticipate yet. Instead of acting like we’re on an inevitable path to general AI, we should tell the broader public that achieving true computer intelligence is a lot more difficult than many assume. That’s why I wrote the book. 

You argue that we are very far from developing general artificial intelligence. In fact, you believe that the approach we are currently pursuing can never lead us there. Why is that?

The main framework that I use in the book is inference. In AI, the problem is that we’re using the wrong type of inference to ever get to general or commonsense intelligence. Right now, the field is almost exclusively dominated by machine learning using inductive inference, learning from prior examples. Human beings use induction all the time, but it’s not the most important type of inference for us. It can’t handle novelty because it’s based on prior observation. Without a novelty mechanism, you can’t get to certain kinds of intelligence. I don’t mean to say that it’s impossible. Nature has developed general intelligence, so we should be able to eventually do the same thing. However, there’s something currently missing, and that’s why it’s been so difficult to make certain kinds of progress in the field.

Arthur C. Clarke famously thought that to get something like intelligence in a computer, we would need heuristic logic- finding and using solutions that aren't precise, but just good enough, which is how we think. We don't measure the distance across the street with a measuring tape; we guesstimate how far it is This method is a lot faster and works well for everyday stuff. Do you think we could program that kind of heuristic logic into computers?

We do that already. Before deep learning became the dominant paradigm in AI development, classic AI design was more rule-based. One of the great challenges in the classic rules-based paradigm was in fact to find these rules of thumb, or heuristics. Herbert Simon, a pioneer in AI and Nobel Prize winner in economics, has said that people who favour adequacy and efficiency over optimisation generally make better, more responsible, and quicker decisions than those who want to make every decision perfect. Precision can be a barrier. However, the classic AI approach based on common-sense heuristics also failed when the domain wasn’t sufficiently constrained. Even if you have a rule that doesn’t need precision, you have so much context in an unconstrained real-world environment that you need rules to tell other rules when they are relevant. It quickly becomes intractable to try to get intelligent behaviour from such a system.

There are two major, unsolved problems in AI. One is robotics, especially when the robot is not in a very specific environment. A robot arm in an industrial setting with few degrees of freedom works well, but if we have a robot walking down the street in Manhattan, there are just so many peripheral problems that can occur in such a complex environment. Somebody walks in front of the robot; something unexpected happens. If you took the best, smartest robot in the world and set it loose on any city street, within a few minutes it would cause a traffic accident. That’s why you don’t see robots on the street.

The other major problem is having a real conversation with an AI system where it truly understands what you’re saying and responds with understanding. I mentioned inference before, and in addition to deduction and induction, there’s a third type of inference called abduction that people generally aren’t aware of, but which we use all the time. Deduction is, “It’s raining; therefore, the streets are wet.” Abduction is, “I know rain makes streets wet. I see the streets are wet. Perhaps it’s raining.” You generate a hypothesis that explains an observation. It’s not certain knowledge – you could be wrong. Maybe a fire hydrant broke. However, you keep correcting your hypothesis with further observation. The streets are wet, my hypothesis is that it’s raining, and then I confirm it or form another. That’s abduction – hypothesis generation.

You mentioned novelty. A human who has not been in a certain situation before can think it through and still handle it. If you introduce a chess master to Shogi, Japanese chess, which has slightly different rules, they would very quickly be able to adapt their experience with chess to be able to play it well. A chess-playing AI, however, would have to learn from scratch – its inductive deep learning of chess would be useless.

I believe game-playing AIs still use some version of a min-max algorithm, deducing what would be the best move given that it has watched a million games play out before. This is very different from a human, who doesn’t play a million games and then computes the probability. I’m not a neuroscientist, so I couldn’t tell you what’s happening in the brain of chess masters – but I’m pretty sure they don’t mindlessly play a million games before becoming masters.

I've observed that as computers get better than us at something, like chess or trivia knowledge, we tend to move the goalpost and say that this has nothing to do with intelligence. Will we keep redefining intelligence as being whatever we can do that computers can't, or are there some markers of intelligence that we can't explain away?

My response is to go back to Alan Turing’s original 1950 paper, when he said that if a person can converse with a computer and be convinced that it is a real human, then it must be intelligent. I would say that this test still holds. Of course, you can converse with a chatbot that just continues to deflect questions, but to have a conversation that’s empathetic and understanding with the computer – we still can’t do that.

During the summer of 2022, a big news story surfaced of a Google engineer becoming convinced that a program he was developing had gained real sentience and warranted rights akin to human rights. Could we not say that it is passed the Turing test?

The latest language models are quite good, but you can trip them up very easily if you know how. Language has a property called compositionality, how sentences are put together to provide meaning. There’s a big difference between me riding a horse and a horse riding me, but an AI language model is not going to get that because it doesn’t have a sense of compositionality. Natural language is a barrier for artificial intelligence – one of the biggest. A legitimate test of language understanding would convince me that an AI was intelligent.

Another test would be navigation in dynamic environments by autonomous vehicles or robots. Getting to fully autonomous driving will be a lot harder than people think. The small city of Palo Alto, California, is mapped out on a grid, and you get pretty good performance from the vehicles there. But if you’re driving on a rural road and the AI must rely on sensor data, we’re a long way from vehicles being able to autonomously navigate that. Fully capable robotics in openended dynamic environments and fully understanding natural language; those are the two big frontiers.

Could an AI not develop its own language, very different from human language, that is uses to understand its environment and gets around some of the current limitations? We could compare it to communicating with dolphins, which seem to have a complex language that we haven't come close to understanding. They cannot understand our questions, and we cannot understand theirs; yet they are doubtless sentient beings.

I suppose is it possible for a creative AI to somehow achieve a way to frame the world that doesn’t require natural language. I don’t know the answer to that, but the immediate practical problem I see is how do we then interact with those systems? That might create some very, very strange human-machine interactions. I almost completely avoided the question of sentience in my book because, frankly, I don’t have a lot to say about it. It’s an issue that very quickly becomes philosophical. It could be that computers right now have some low level of sentience, like insects, and we just can’t detect it because we don’t know how. As an engineer, I don’t know the entry point into that argument, so I leave it alone.

You argue that we can’t achieve general AI the way we try to do it now, with machine learning and adding more components to computers. However, in physics there’s the phenomenon of ‘emergence’, where new traits develop when the complexity is high enough. One water molecule doesn’t have surface tension, but put enough together, and you get it. A single neuron isn’t sentient, but enough produce human sentience. Would it not be possible, if we add complexity and more components to supercomputers, that they could achieve intelligence and sentience as an emergent trait?

I think it’s an interesting question. It’s like a pile of sand: if you keep adding grains of sand, you get a nice conical shape, until at one point adding just one more grain of sand gets you a cascading effect. We have these thresholds in emergence where something isn’t happening, and then at some level of complexity, a completely different phenomenon emerges. I think it’s interesting whether that could apply to technology or computers, but I don’t have any strong scientific position on that.

Isn't there a danger if we have, say, self-driving cars who all think the same way because we have copied the same machine learning into all of them? If there are several routes from a suburb to the city, they will all choose the same route because that's what the system says they should do, whereas humans might imagine that the main route will probably be too busy and choose another one instead?

I think we’ll solve those sorts of problems. We already have systems where you can see traffic flow. The problems that I worry about are more practical. There have been cases where selfdriving cars don’t stop because a stop sign is slightly damaged and is perceived as something else. There’s a famous example of a system that tried to drive underneath a school bus because it thought it was an overpass. We just can’t eliminate all problems because the natural world is so messy. A bunch of leaves that the wind blows across the street might be interpreted as a solid object, and the AI will slam on the brakes.

We have people worrying that if we achieve general intelligence in computers, they are going to take over the world, or follow some order, like maximizing the production of paperclips, to such extremes that the AI will wipe out humanity to do it more efficiently. Do you think there is any real danger of such things happening, or are we just projecting our own faults onto artificial intelligence?

There’s an interesting contradiction in the paperclip scenario. The system is supposed to have general intelligence, which you would think included common sense, but on the other hand, it’s so narrow and computational that it thinks it can maximise the sale of paperclips by turning all humans into paperclips. Real computer intelligence would realise that it’s not intended to wipe us out. There’s another option, though, which is that it becomes malevolent and actively desires to rid the world of human beings. That gets us into the question of whether something like malevolence could possibly emerge in an AI.

We have AIs today that looks at x-rays of patients, trying to determine if they have cancer. They can be very good at this, but they don't know anything about cancer or what it means to a human being. They lack an understanding of what their task really is about. Do you think we can achieve intelligence in computers without true understanding of what they do?

That’s a great question, but I don’t have a great answer for it. It raises the whole issue, in this case of medical science, of whether an AI can provide proper diagnoses when it doesn’t understand care. Someone should write a PhD about how medicine is best administered and what the role of technology is and can be.

Research shows that even when an AI is better than any doctor at diagnosing cancer, it is even more efficient when it works with a human doctor. They approach the problem in different ways - one with a human understanding, the other from being trained on millions of x-rays. Human-AI partnerships seem to work best.

I think that’s right. In terms of something we care about, like medicine, it sounds like this kind of collaboration may work best. To me that’s a good use of technology. That’s why we make technology – because it furthers human goals. Whether we will have autonomous systems that will replace humans in all domains, that is a completely different question. Whether we get fully sentient AI or not, we’re heading in this direction in the future. That’s for sure.”


/ 004. Brown, J. Dakota. "Typography, Automation, and the Division of Labor: A Brief History," 2019.


Typography was born in the mass-production mechanism of the printing press. It has thus always been implicated in automation and, thereby, in the distinctly modern dynamics of overwork, underemployment, and runaway production. Transformations of labor and technology, however, have received scant attention in graphic design historiography. Philip Meggs' landmark textbook A History of Graphic Design, for example, offers only the briefest hints of the social dislocations that accompanied automation in the printing trades. One reads, for example, that the first steam press in England was operated in a secret location to guard against sabotage, or that vaguely-defined "strikes and violence" greeted the first installations of typesetting machines. Otherwise, such histories tend to treat innovations in print technology as a politically neutral process of technical refinement. But the new machines and methods did not just drop from the heavens: their development was often materially supported by employers who aimed to speed up production, capture control over the work process, and even break strikes.

Modernity, Modernism, and the Graphic Designer

As the design historian Adrian Forty has documented, industrial and graphic design emerged with the capitalist division of labor; both professions, in turn, catalyzed further divisions and fragmentations of work. In eighteenth-century crafts like ceramics and printed fabrics, the erosion of trade knowledge was accompanied by the rise of a new role in production: that of the "modeller. Usually hired from outside of the trade, these early designers were more dependably in touch with bourgeois taste than craftspeople were. In Josiah Wedgwood's ceramic works, stylistic concerns were conditioned by a need to simplify production into a rigid series of straightforward tasks, in which there was little occasion for variation between workers. The contemporaneous vogue for Neoclassicism, with its simplified geometry and restrained ornament, provided an ideal opportunity to streamline production - with the express goal, in Wedgwood's words, of making "such Machines of the Men as cannot err."

As critical historians from Karl Marx to Henry Braverman and David Noble have shown, the progress of capitalism's division of labor entails a gradual transfer of control and planning from the factory floor to management. But the resulting degradation and cheapening of work was noticed almost from the beginning: notably by Wedgwood's contemporary Adam Smith. In the opening chapter to The Wealth of Nations, Smith explains the production process in a new type of pin factory. Here, the capitalist has not simply gathered formerly-independent artisans to practice their trade side-by-side - instead, he has exploded the pin-making process into a line along which each laborer only cuts, sharpens, or polishes. Smith notes the miraculous extension of productivity in a process thus rationalized; elsewhere, however, he worries that the "great body of the people" will increasingly fill their days repeating the same handful of tasks.

The man whose whole life is spent performing a few simple operations, of which the effects too are, perhaps, always the same … has no occasion to exert his understanding, or to exercise his invention.... He naturally loses, therefore, the habit of such exertion, and generally becomes as stupid and ignorant as it is possible for a human creature to become.

This same image was on John Ruskin's mind in 1853, as he formulated what would become a central text for the Arts & Crafts movement's antiindustrial critique. In "The Nature of the Gothic," Ruskin mourns "the little piece of intelligence" rationed out to the factory worker, which "exhausts itself in making the point of a pin." For Ruskin, the division of labor is more accurately the division of the laborers themselves: these abundant pins, he writes, are polished with mere "crumbs" of human capacities. The essay was republished by William Morris' Kelmscott Press in 1892. Arriving at the end of a career rich in the paradoxes of an anti-capitalist design practice, Kelmscott was Morris' attempt to restore aesthetic unity to the book while keeping skilled craftspeople employed at higher-thanaverage wages. Though Morris believed that the form of the book had been betrayed by industrial shoddiness, the scale of his undertaking still necessitated some modernization of the traditional work process. Kelmscott books nonetheless remained so expensive to produce that Morris was trapped, as he lamented, "ministering to the swinish luxury of the rich."

In the early twentieth-century United States, Morris' legacy would be refashioned in a context of accelerating industrial transformation. Frank Lloyd Wright came to believe that Morris' desired reconciliation between art, labor, and leisure was likely to be delivered by mechanization itself. For Wright, the machine—centrally illustrated by the printing press—had to be grasped for what it had become: "intellect mastering the drudgery of the earth." The "meaningless torture" inflicted on workers and materials alike could now be swept away, Wright argued, as long as designers could part with anachronistic practices of ornamentation. Meanwhile, American commercial artists were falling "under the Arts & Crafts spell" and emerging as freelance specialists in book typography. Bruce Rogers and Frederic Goudy, for example, took on Morris' aesthetic standards while largely ignoring his concerns about the social contradictions of large-scale production. It was Goudy's student W.A. Dwiggins who would later popularize the phrase "graphic design" to describe this emerging position in print's division of labor. The workshops of these early graphic designers were characterized by a clarified managerial role for the designer, a more rationalized division of labor below and, finally, an embrace of labor-saving technology in typesetting and printing.

Across the ocean, meanwhile, a more explicitly socialist embrace of industry had produced the modernist "machine aesthetic." Echoing and radicalizing Wright, the Constructivist manifesto of 1922 declared war on traditional art and pledged a conditional allegiance to the machine: Constructivists would be both technology's "first fighting and punitive force" and its "last slave-workers."[17] At the same time, the Bauhaus was moving away from its Arts & Crafts roots. The school had initially been organized along guild lines: composed not of students and professors, but of masters, journeymen, and apprentices.[18] In transitioning to an emphasis on industrial production, Bauhaus designers synthesized compositional lessons from Futurism, Dada, and Constructivism. One uniting theme of these movements had been a desire to alter the experience of reading by exploding the strictures of the letterpress. In each case, photomechanical techniques promised a way out. In the Bauhaus graphics studios, László Moholy-Nagy continued in this vein, developing a a montage practice he termed "Typophoto." 

In the USSR, El Lissitzky theorized the epistemological and technical aspects of this blurring of text and image. The text-image, he surmised, could be put to work perfecting the human sensorium: revolutionized book forms would yield a "perpetual sharpening of the optic nerve." Lissitzky also read early attempts at phototypography in the context of a historical tendency toward lightness and mobility: "The amount of material used is decreasing, we are dematerializing, cumbersome masses of material are being supplanted by released energies."[20] Such a process would culminate, as he cryptically wrote in 1926, in a final transcendence of print itself: "the electro-library." After the midpoint of the century, metal type would indeed be supplanted by photographic media, in systems that were increasingly directed by "electro-libraries" of "dematerialized" data. By the 1990s, the convergences and displacements predicted by Lissitzky had yielded the digital hybridization of writing, typesetting, and imaging. In the end, however, these transformations owed more to the bottom lines of print capitalists than to the efforts of the radical modernists.

Modernism in design began with a vision of socialist industrial transformation -but by mid-century, it had become welded to the public image of the great capitalist conglomerates. Corporate boosters of modernism like the paperboard magnate Walter Paepke, a benefactor of the postwar "New Bauhaus" in Chicago, prophesied a world in which design would meld with management.[22] Paepke argued that design could improve market competition between large firms tied to uniform machinery and wage agreements; internally, it could even be put to work on such "problems" as worker morale. Though modernism's trajectory from utopian potential to capitalist instrumentalization is familiar to design history, parallel themes in the history of the printing trades have received less attention. The initial promise of technological innovations—to end, in Wright's words, the "meaningless torture" of repetitive and inefficient labor -soon gave way to the sobering realities of deskilling and displacement. The contours of twentieth-century print technologies would be shaped in large part by struggles over automation and employment. The "released energies" of print's dematerialization increasingly took the form of outmoded workers.

Industrial Rationalization and the Printer

The growing coherence and confidence of the graphic design profession is accompanied historically by the gradual fragmentation and decline of the printing trades.[24] The job description of "printing" originally encompassed a set of knowledges that extended far beyond the point of contact between ink and paper. Early printers were often also typefounders, publishers, and booksellers. Even as the craft became more specialized, printing still involved typesetting and composing pages, which often extended to a role in writing. According to union typesetter and historian Henry Rosemont, newspaper printers in the mid-nineteenth century relied on a broad but informal education in "language, history, geography and other subjects," which enabled them to produce entire articles from telegrams consisting of little more than the relevant nouns, verbs, and modifiers. 

Print workers thus held a strategic position in the circulation of public discourse, which was simply not possible without them. They often took advantage of this position to educate themselves and to advocate for the interests of their trade. In addition to their obligatory literacy, they had access to the press as an organizing tool-an extreme rarity for manufacturing workers of the early industrial era. Journeyman printers became the first group of workers to go on strike in the United States, just a year after the Revolutionary War. As Régis Debray has documented, print workers would go on to play prominent roles in revolutionary movements around the world during the next two centuries.

The printed book was, if not the first, then certainly the clearest early example of the world of standardized, mass-produced commodities to come. And as with any other commodity, print production required "labor-saving" innovations to keep competing firms profitable. More efficient inventions often rendered the work less taxing and dangerous, but the primary motivation for their adoption was always the reduction of labor costs—resulting in layoffs or slashed hours. Print workers often found themselves fighting technologies that promised to ease the burden of their labor. Rosemont offers the example of a more efficient ink roller, invented in 1814 and vigorously resisted by American printers. The existing standard was a more rudimentary instrument that required periodic soakings in animal urine to keep it from hardening. Printers worked in consistently squalid, poorly- ventilated shops that contributed to lower-than-average life expectancies—but such conditions were clearly preferable to unemployment.

The printed book was, if not the first, then certainly the clearest early example of the world of standardized, mass-produced commodities to come. And as with any other commodity, print production required "labor-saving" innovations to keep competing firms profitable. More efficient inventions often rendered the work less taxing and dangerous, but the primary motivation for their adoption was always the reduction of labor costs—resulting in layoffs or slashed hours. Print workers often found themselves fighting technologies that promised to ease the burden of their labor. Rosemont offers the example of a more efficient ink roller, invented in 1814 and vigorously resisted by American printers. The existing standard was a more rudimentary instrument that required periodic soakings in animal urine to keep it from hardening. Printers worked in consistently squalid, poorly- ventilated shops that contributed to lower-than-average life expectancies—but such conditions were clearly preferable to unemployment.

Though Johannes Gutenberg's fifteenth-century press had scarcely changed in the intervening years, the first decades of the nineteenth century brought transformations far beyond the humble roller. Iron construction and steam power fundamentally changed not only the shape of the machine, but the entire work process that fed and maintained it. However, a a major production bottleneck remained: the reproduction of writing still required the manual assembly of each word and line. Printing firms grew to be heavily reliant upon a workforce of typesetters who were both rigorously trained and militantly organized. During the 1880s, several attempts were made to mechanize the typesetting process. One particularly spectacular failure was the Paige Compositor, which bankrupted its primary investor Mark Twain. As Twain is said to have boasted shortly before the invention proved unfeasible, the Paige could "work like six men and do everything but drink, swear, and go out on strike."

Then in 1886 Ottmar Mergenthaler, a German engineer hired by an investors' group that represented the major New York newspapers, presented the first working model of the Linotype machine. Like type composition by the old method, the Linotype utilized thin bits of metal; here, however, each bit carried the negative impression of a character. Operators typed the characters into a line; justification was then carried out by a spacing mechanism that sealed the channel into which the characters had been set. Molten metal was then injected into the channel to form a full line in positive relief. After cooling, each "line o' type" was stacked into columns and locked into page layouts for the press. While the Linotype was an expensive and somewhat risky investment, it delivered on promises of labor-cost savings, and in time it contributed to a dramatic enlargement of the size and circulation of the periodical press.

The trade at first dismissed these developments. As one printer's newspaper reassuringly put it in 1891, Linotypes were mere "toys" for print capitalists with no practical background in the trade. Over the next decade, however, the threat became palpable. As one unemployed typesetter wrote in 1900, his place at the typecase had been usurped by a "monster" that eerily replicated his movements without need for food or human dignity. While a steady demand for the manual composition of headlines, advertisements, and other display applications muffled the effect slightly, the new work process soon touched off an employment crisis. Younger compositors scrambled to learn machine composition, while thousands of older or more narrowly-trained workers fell through the cracks. However, the International Typographical Union (ITU), which represented manual typesetters, was able to establish jurisdiction over the machines in strategic industrial centers around the turn of the century. In order to stave off the effects of the transformation, the ITU pushed for shorter workdays and encouraged early retirements. 

As the popular press grew during the teens and twenties, typesetting employment stabilized and even expanded. The ITU grew in tandem, and soon became one of the most powerful unions in the United States. However, this position was soon threatened by a number of mutually-reinforcing technical innovations. First, teletypesetting enabled Linotypes to be driven like player pianos; encoded tape was poised to replace human typists. Second, a slew of phototypesetting inventions sought to replace the cumbersome "hot metal" process with typefaces stored on film. Giving typography a photochemical basis, in turn, allowed a more seamless integration of text and image, while also making typesetting more readily compatible with letterpress printing's longtime competitor, offset lithography. 

Teletypesetting evolved from Morse code and the stock ticker; it would go on to form the basis for early computing.

Like the beginnings of mechanized typesetting itself, efforts at moving beyond hot metal were piloted by newspapers. Early experiments with "cold type" were explicitly undertaken to break a wave of ITU strikes following the passage of the Taft-Hartley Act of 1947, which stripped organized labor of many of the bargaining rights it had won over the preceding decades.[36] During a citywide pressroom strike that lasted from November 1947 to September 1949, the Chicago Tribune put its existing clerical staff to work on a new model of justifying typewriter whose output could be "pasted up" as camera-ready paper layouts, as opposed to being "locked up" in countless pieces of backward-reading metal.[37] The Tribune's infamous "Dewey Defeats Truman" edition of November 3, 1948 was typeset by strikebreakers, in a work process that now moved faster than the official ballot counts.38 While the quality of typewriter paste-up left something to be desired, these experiments strongly hinted at the possibility of producing a newspaper without the union. 

The ITU was able to keep these challenges at bay throughout the mid-twentieth century. New contracts forbade machines like the teletypesetter, even though this meant that print-ready stories from the wire services had to be retyped by an ITU member on the premises.[39] It wasn't until 1964 that the New York City local signed a contract allowing Linotypes to be run on "outside tape" on the condition, however, that employers paid 100% of the profits deriving from the new machinery into an "automation fund."[40] While this price was prohibitively steep for many firms, it opened the door to similar agreements on phototypesetting and, eventually, to computer systems. During the 1970s, the ITU began to draw down in exchange for the job and pension security of existing members.[41] In the meantime, the new machines had already crept into areas of the industry with low union representation. A paradoxical result was that capital-intensive metropolitan papers like the New York Times were among last to make the transition. The final night of Linotype composition at the Times—July 1, 1978—is memorialized in the documentary Farewell Etaoin Shrdlu, directed by ITU proofreader David Loeb Weiss. Among the film's interviewees is a compositor who reflects on his 26 years in the industry: 

[T]hat's six years apprenticeship, 20 years journeyman. And these are words that aren't just tossed around. ... All the knowledge I've acquired over these 26 years is all locked up in a little box now called a computer. And I think probably most jobs are gonna end up the same way.

Once more, the newspaper industry led the way in automation, and again the ITU attempted to train people in the new processes or encourage early retirements. In the earlier transformation, the work lost to Linotype composition was compensated by a gradual but decisive expansion of print production. This time, however, the further rationalization of typesetting destroyed older forms of work while narrowing the number of jobs in the new lines. As Lissitzky had predicted, metal gave way to film and paper; the material footprint of typography was shrinking. But as long as each text needed to be retyped to be typeset, labor-time savings were minimal. The widespread adoption of teletypesetting technology, however, allowed the storage and transmission of coded texts and, eventually, their formatting directions as well. By the 1970s, computer systems were beginning to dissolve typesetting into word processing. A centuries-old gap separating writing and printing was beginning to close—and this gap had been the very ground on which the ITU stood. The union suffered a long decline and finally dissolved in 1986, just as the personal computer was completing typography's process of dematerialization. It was, at that time, the longest continuously-running union in U.S. history.

The End of Modernism and the Last Typesetters

In the 1970s, print production involved a complex hierarchy of work processes, the final product of which was never fully visible until it had been printed. Designers could only approximate typographical treatments; directions on spacing, size, and weight were then handed off to phototypesetting shops to interpret in detail. A separate group of prepress specialists followed designers' directions on variables like color density and image placement, and then "stripped" together disparate negatives to create a print-ready master. But despite the many hands through which such work passed, much of the period's modernist-influenced design left the impression that it was the product a singular, detached mind. 

Though there was still a high degree of churn in new machines and processes, this division of labor held stable until the arrival of Apple's Macintosh computer in 1984. The personal computer centralized capacities formerly bound up in massive metal-founding operations, delicate apparatuses of type on film, or astronomically expensive, room-filling computers—to say nothing of the highly specialized workers that attended these machines, or of the systems of education and apprenticeship that such a workforce presupposed. Tasks that were once contracted out with some combination of strict direction and trust were now fully under the control of the individual designer—from the smallest details of letterforms to the organization of entire books. The Macintosh would soon offer image-editing capacities with no existing analogue, which in turn put pressure on commercial photographers and illustrators. The century since the invention of the Linotype had been one of "creative destruction" in the print industry: novel forms of work appeared suddenly and disruptively, only to be rendered obsolete in their turn. Once the brake provided by ITU contracts was removed, this process could accelerate unabated.

By the mid-1980s, typographical technology had reached a height of modernized seamlessness which, ironically, contributed to the decline of modernism's hegemony in graphic design. New design software facilitated effects like layering and distortion, which were quickly put to use in visual polemics against modernist clarity. Formal complexity and semantic confusion in graphic design had a long pre-Macintosh history —stretching at least as far back as the late-1960s letterpress experiments of Wolfgang Weingart. In the 1980s, however, graphic designers raised the stakes of these experiments by linking them to contemporaneous developments in the academy: in particular, to the "linguistic" and "cultural turns" in the humanities.[45] Terms like "deconstruction" and "post-structuralism" were applied to the printed page in ways that often required little familiarity with the theories in question. The grid—increasingly understood as a symbol of authoritarian and, perhaps, Eurocentric rationality—was parodied, skewed, or thrown aside entirely. Designers arranged texts into ambiguous formations, and designed new typefaces that intentionally thwarted legibility.

By the 1990s, the postmodernist critique of modern rationality and power had grown more rigorous. However, the movement's theorists showed little interest in grasping capitalism as a determining context for their theory and practice; transformations in the political economy of print were thus largely ignored. When, in 1997, Emigre published a rare acknowledgment that entire industries were collapsing next door, it was with a heavy dose of schadenfreude 

[M]any of the printers who have gone out of business over the last quarter century deserved their fate. The grassroots of the printing trade is, after all, notoriously conservative, protectionist, and sexist.

While prepress and printing-like most American trades-tended toward a narrowly white male membership and self-image, the heaviest losses in the industry from the 1980s forward were in fact suffered by the largely non-unionized workforce of the cold type shops. Compared to the membership of the ITU, these workers were disproportionately women and people of color.

The postmodernists' focus on cultural intervention often neglected the material contingencies of the practice. Semiotic theory and cultural studies opened vistas to broad contexts of symbolic circulation, but often at the cost of such bare facts as design's own relationship to waged work. It is perhaps not surprising, then, that a new generation of practitioners has taken a more archaeological approach to the labor of design. In the recent documentaries Linotype: The Film (2012) and Graphic Means: A History of Graphic Design Production (2016)-both directed by practicing graphic designers. histories of print production expose deeper issues of deskilling, unemployment, and deindustrialization. These documentaries elegantly organize a complex history of print technology, and the present essay would admittedly have been impossible without them. However, both ultimately elide the capitalist labor dynamics that would explain their own narratives. 

Douglas Wilson's Linotype stirringly evokes the lost world of hot motel through humanizing portraits of the workers who kept it running. For example, in a near-reprise of his role as the narrator of Farewell Etaoin Shrdlu, the late Carl Schlesinger makes frequent appearances. The filmmakers include footage of him singing and tap dancing, and they indulge him as he tells a long-winded story about the time he met Marilyn Monroe. A casual viewer would never know that Schlesinger was also a lifelong member of the ITU, or that he coauthored an important book on the union's automation strategy. Despite its exhaustiveness, in fact, Linotype manages to bracket the union's existence altogether. Briar Levit's Graphic Means takes up where Linotype leaves off-impressively condensing the jumble of machines that bridged the hot type and digital eras. Graphic Means directly addresses the role of the ITU and, further, the gendered division that arose between unionized hot type shops and "open" cold type shops. However, the decline of the union is presented as a technical inevitability and even as a refutation of male privilege; the phototypesetting bosses interviewed seem to be speaking as feminists when they say that "the girls" did equally admirable work for half the wages. The vulnerability of non-unionized women to the next wave of automation, meanwhile, is never addressed. 

While typesetting has disappeared as a distinct job, it would be too simple to say that it was automated out of existence. Rather, since the late twentieth century the job description of the graphic designer has expanded to include tasks once carried out by the earliest printers. Now that we have considered the standpoints of both modernist radicals and extinct print workers, the contemporary situation of the graphic designer should appear somewhat absurd. Capitalist technological development has rendered texts and images almost infinitely reproducible—and has built unfathomable electro-libraries in the process. But despite this gigantic aggregation of productive force, it is still necessary to put people to work moving words and pictures around, most often in the service of brand competition among otherwise identical commodities. What confronts us is not a world in which machines have freed people from work, but one of mass unemployment, in which some of the most celebrated "innovations" are apps that facilitate short-term, low-wage, benefit-less contracts.

If graphic designers became typesetters, they may turn out to be the last typesetters. The design software that repackaged the knowledge and skill of the printing trades seemed at first to deliver a dreamed-of autonomy to graphic design as a profession. But because these technologies were off-the-shelf consumer products, trained and credentialed designers have less and less of a monopoly on the medium. A general facility with image and text has bled into general literacy—due in no small part to the ease of pirating such "immaterial" commodities as Photoshop. In the contemporary design press, articles on apps like TaskRabbit and Fiverr, or a future role for Al in the automation of design decisions, recall the mix of anxiety and reassurance that characterized coverage of the Linotype nearly 130 years ago.[53] These projected "disruptions" may well turn out to be empty hype. But whatever is in store for graphic design in the coming decades, it will be impossible to understand without accounting for the capitalist constraints and imperatives that have shaped the practice from the beginning.


/ 005. Gefter, Amanda and Quanta Magazine. "The Case Against Reality," 2016.


A professor of cognitive science argues that the world is nothing like the one we experience through ur senses.

As we go about our daily lives, we tend to assume that our perceptions— sights, sounds, textures, tastes—are an accurate portrayal of the real world. Sure, when we stop and think about it—or when we find ourselves fooled by a perceptual illusion—we realize with a jolt that what we perceive is never the world directly, but rather our brain’s best guess at what that world is like, a kind of internal simulation of an external reality. Still, we bank on the fact that our simulation is a reasonably decent one. If it wasn’t, wouldn’t evolution have weeded us out by now? The true reality might be forever beyond our reach, but surely our senses give us at least an inkling of what it’s really like.

Not so, says Donald D. Hoffman, a professor of cognitive science at the University of California, Irvine. Hoffman has spent the past three decades studying perception, artificial intelligence, evolutionary game theory and the brain, and his conclusion is a dramatic one: The world presented to us by our perceptions is nothing like reality. What’s more, he says, we have evolution itself to thank for this magnificent illusion, as it maximizes evolutionary fitness by driving truth to extinction.

Getting at questions about the nature of reality, and disentangling the observer from the observed, is an endeavor that straddles the boundaries of neuroscience and fundamental physics. On one side you’ll find researchers scratching their chins raw trying to understand how a three-pound lump of gray matter obeying nothing more than the ordinary laws of physics can give rise to first-person conscious experience. This is the aptly named “hard problem.”

On the other side are quantum physicists, marveling at the strange fact that quantum systems don’t seem to be definite objects localized in space until we come along to observe them. Experiment after experiment has shown— defying common sense—that if we assume that the particles that make up ordinary objects have an objective, observer-independent existence, we get the wrong answers. The central lesson of quantum physics is clear: There are no public objects sitting out there in some preexisting space. As the physicist John Wheeler put it, “Useful as it is under ordinary circumstances to say that the world exists ‘out there’ independent of us, that view can no longer be upheld.”

So while neuroscientists struggle to understand how there can be such a thing as a first-person reality, quantum physicists have to grapple with the mystery of how there can be anything but a first-person reality. In short, all roads lead back to the observer. And that’s where you can find Hoffman—straddling the boundaries, attempting a mathematical model of the observer, trying to get at the reality behind the illusion. Quanta Magazine caught up with him to find out more.

Gefter: People often use Darwinian evolution as an argument that our perceptions accurately reflect reality. They say, “Obviously we must be latching onto reality in some way because otherwise we would have been wiped out a long time ago. If I think I’m seeing a palm tree but it’s really a tiger, I’m in trouble.”

Hoffman: Right. The classic argument is that those of our ancestors who saw more accurately had a competitive advantage over those who saw less accurately and thus were more likely to pass on their genes that coded for those more accurate perceptions, so after thousands of generations we can be quite confident that we’re the offspring of those who saw accurately, and so we see accurately. That sounds very plausible. But I think it is utterly false. It misunderstands the fundamental fact about evolution, which is that it’s about fitness functions—mathematical functions that describe how well a given strategy achieves the goals of survival and reproduction. The mathematical physicist Chetan Prakash proved a theorem that I devised that says: According to evolution by natural selection, an organism that sees reality as it is will never be more fit than an organism of equal complexity that sees none of reality but is just tuned to fitness. Never.

Gefter: You’ve done computer simulations to show this. Can you give an example?

Hoffman: Suppose in reality there’s a resource, like water, and you can quantify how much of it there is in an objective order—very little water, medium amount of water, a lot of water. Now suppose your fitness function is linear, so a little water gives you a little fitness, medium water gives you medium fitness, and lots of water gives you lots of fitness—in that case, the organism that sees the truth about the water in the world can win, but only because the fitness function happens to align with the true structure in reality. Generically, in the real world, that will never be the case. Something much more natural is a bell curve—say, too little water you die of thirst, but too much water you drown, and only somewhere in between is good for survival. Now the fitness function doesn’t match the structure in the real world. And that’s enough to send truth to extinction. For example, an organism tuned to fitness might see small and large quantities of some resource as, say, red, to indicate low fitness, whereas they might see intermediate quantities as green, to indicate high fitness. Its perceptions will be tuned to fitness, but not to truth. It won’t see any distinction between small and large—it only sees red— even though such a distinction exists in reality.

Gefter: But how can seeing a false reality be beneficial to an organism’s survival?

Hoffman: There’s a metaphor that’s only been available to us in the past 30 or 40 years, and that’s the desktop interface. Suppose there’s a blue rectangular icon on the lower right corner of your computer’s desktop — does that mean that the file itself is blue and rectangular and lives in the lower right corner of your computer? Of course not. But those are the only things that can be asserted about anything on the desktop — it has color, position, and shape. Those are the only categories available to you, and yet none of them are true about the file itself or anything in the computer. They couldn’t possibly be true. That’s an interesting thing. You could not form a true description of the innards of the computer if your entire view of reality was confined to the desktop. And yet the desktop is useful. That blue rectangular icon guides my behavior, and it hides a complex reality that I don’t need to know. That’s the key idea. Evolution has shaped us with perceptions that allow us to survive. They guide adaptive behaviors. But part of that involves hiding from us the stuff we don’t need to know. And that’s pretty much all of reality, whatever reality might be. If you had to spend all that time figuring it out, the tiger would eat you.

Gefter: So everything we see is one big illusion?

Hoffman: We’ve been shaped to have perceptions that keep us alive, so we have to take them seriously. If I see something that I think of as a snake, I don’t pick it up. If I see a train, I don’t step in front of it. I’ve evolved these symbols to keep me alive, so I have to take them seriously. But it’s a logical flaw to think that if we have to take it seriously, we also have to take it literally.

Gefter: If snakes aren’t snakes and trains aren’t trains, what are they?

Hoffman: Snakes and trains, like the particles of physics, have no objective, observer-independent features. The snake I see is a description created by my sensory system to inform me of the fitness consequences of my actions. Evolution shapes acceptable solutions, not optimal ones. A snake is an acceptable solution to the problem of telling me how to act in a situation. My snakes and trains are my mental representations; your snakes and trains are your mental representations.

Gefter: How did you first become interested in these ideas?

Hoffman: As a teenager, I was very interested in the question “Are we machines?” My reading of the science suggested that we are. But my dad was a minister, and at church they were saying we’re not. So I decided I needed to figure it out for myself. It’s sort of an important personal question—if I’m a machine, I would like to find that out! And if I’m not, I’d like to know, what is that special magic beyond the machine? So eventually in the 1980s I went to the artificial-intelligence lab at MIT and worked on machine perception. The field of vision research was enjoying a newfound success in developing mathematical models for specific visual abilities. I noticed that they seemed to share a common mathematical structure, so I thought it might be possible to write down a formal structure for observation that encompassed all of them, perhaps all possible modes of observation. I was inspired in part by Alan Turing. When he invented the Turing machine, he was trying to come up with a notion of computation, and instead of putting bells and whistles on it, he said, Let’s get the simplest, most pared down mathematical description that could possibly work. And that simple formalism is the foundation for the science of computation. So I wondered, could I provide a similarly simple formal foundation for the science of observation?

Gefter: A mathematical model of consciousness.

Hoffman: That’s right. My intuition was, there are conscious experiences. I have pains, tastes, smells, all my sensory experiences, moods, emotions and so forth. So I’m just going to say: One part of this consciousness structure is a set of all possible experiences. When I’m having an experience, based on that experience I may want to change what I’m doing. So I need to have a collection of possible actions I can take and a decision strategy that, given my experiences, allows me to change how I’m acting. That’s the basic idea of the whole thing. I have a space X of experiences, a space G of actions, and an algorithm D that lets me choose a new action given my experiences. Then I posited a W for a world, which is also a probability space. Somehow the world affects my perceptions, so there’s a perception map P from the world to my experiences, and when I act, I change the world, so there’s a map A from the space of actions to the world. That’s the entire structure. Six elements. The claim is: this is the structure of consciousness. I put that out there so people have something to shoot at.

Gefter: But if there’s a W, are you saying there is an external world?

Hoffman: Here’s the striking thing about that. I can pull the W out of the model and stick a conscious agent in its place and get a circuit of conscious agents. In fact, you can have whole networks of arbitrary complexity. And that’s the world.

Gefter: The world is just other conscious agents?

Hoffman: I call it conscious realism: Objective reality is just conscious agents, just points of view. Interestingly, I can take two conscious agents and have them interact, and the mathematical structure of that interaction also satisfies the definition of a conscious agent. This mathematics is telling me something. I can take two minds, and they can generate a new, unified single mind. Here’s a concrete example. We have two hemispheres in our brain. But when you do a split-brain operation, a complete transection of the corpus callosum, you get clear evidence of two separate consciousnesses. Before that slicing happened, it seemed there was a single unified consciousness. So it’s not implausible that there is a single conscious agent. And yet it’s also the case that there are two conscious agents there, and you can see that when they’re split. I didn’t expect that, the mathematics forced me to recognize this. It suggests that I can take separate observers, put them together and create new observers, and keep doing this ad infinitum. It’s conscious agents all the way down.

Gefter: If it’s conscious agents all the way down, all first-person points of view, what happens to science? Science has always been a third-person description of the world.

Hoffman: The idea that what we’re doing is measuring publicly accessible objects, the idea that objectivity results from the fact that you and I can measure the same object in the exact same situation and get the same results — it’s very clear from quantum mechanics that that idea has to go. Physics tells us that there are no public physical objects. So what’s going on? Here’s how I think about it. I can talk to you about my headache and believe that I am communicating effectively with you, because you’ve had your own headaches. The same thing is true as apples and the moon and the sun and the universe. Just like you have your own headache, you have your own moon. But I assume it’s relevantly similar to mine. That’s an assumption that could be false, but that’s the source of my communication, and that’s the best we can do in terms of public physical objects and objective science.

Gefter: It doesn’t seem like many people in neuroscience or philosophy of mind are thinking about fundamental physics. Do you think that’s been a stumbling block for those trying to understand consciousness?

Hoffman: I think it has been. Not only are they ignoring the progress in fundamental physics, they are often explicit about it. They’ll say openly that quantum physics is not relevant to the aspects of brain function that are causally involved in consciousness. They are certain that it’s got to be classical properties of neural activity, which exist independent of any observers— spiking rates, connection strengths at synapses, perhaps dynamical properties as well. These are all very classical notions under Newtonian physics, where time is absolute and objects exist absolutely. And then [neuroscientists] are mystified as to why they don’t make progress. They don’t avail themselves of the incredible insights and breakthroughs that physics has made. Those insights are out there for us to use, and yet my field says, “We’ll stick with Newton, thank you. We’ll stay 300 years behind in our physics.”

Gefter: I suspect they’re reacting to things like Roger Penrose and Stuart Hameroff’s model, where you still have a physical brain, it’s still sitting in space, but supposedly it’s performing some quantum feat. In contrast, you’re saying, “Look, quantum mechanics is telling us that we have to question the very notions of ‘physical things’ sitting in ‘space.’”

Hoffman: I think that’s absolutely true. The neuroscientists are saying, “We don’t need to invoke those kind of quantum processes, we don’t need quantum wave functions collapsing inside neurons, we can just use classical physics to describe processes in the brain.” I’m emphasizing the larger lesson of quantum mechanics: Neurons, brains, space … these are just symbols we use, they’re not real. It’s not that there’s a classical brain that does some quantum magic. It’s that there’s no brain! Quantum mechanics says that classical objects—including brains—don’t exist. So this is a far more radical claim about the nature of reality and does not involve the brain pulling off some tricky quantum computation. So even Penrose hasn’t taken it far enough. But most of us, you know, we’re born realists. We’re born physicalists. This is a really, really hard one to let go of.

Gefter: To return to the question you started with as a teenager, are we machines?

Hoffman: The formal theory of conscious agents I’ve been developing is computationally universal—in that sense, it’s a machine theory. And it’s because the theory is computationally universal that I can get all of cognitive science and neural networks back out of it. Nevertheless, for now I don’t think we are machines—in part because I distinguish between the mathematical representation and the thing being represented. As a conscious realist, I am postulating conscious experiences as ontological primitives, the most basic ingredients of the world. I’m claiming that experiences are the real coin of the realm. The experiences of everyday life—my real feeling of a headache, my real taste of chocolate—that really is the ultimate nature of reality.


/ 006. "Pause Against AI Experiments: An Open Letter," 2023.


AI systems with human-competitive intelligence can pose profound risks to society and humanity, as shown by extensive research and acknowledged by top AI labs. As stated in the widely-endorsed Asilomar AI Principles, Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources. Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one – not even their creators – can understand, predict, or reliably control.

Contemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable. This confidence must be well justified and increase with the magnitude of a system's potential effects. OpenAI's recent statement regarding artificial general intelligence, states that "At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models." We agree. That point is now.

Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT@4. This pause should be public and verifiable, and include all key actors. If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.

AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts. These protocols should ensure that systems adhering to them are safe beyond a reasonable doubt. This does not mean a pause on AI development in general, merely a stepping back from the dangerous race to ever-larger unpredictable black-box models with emergent capabilities.

AI research and development should be refocused on making today's powerful, state-of-the-art systems more accurate, safe, interpretable, transparent, robust, aligned, trustworthy, and loyal.

In parallel, AI developers must work with policymakers to dramatically accelerate development of robust AI governance systems. These should at a minimum include: new and capable regulatory authorities dedicated to AI; oversight and tracking of highly capable AI systems and large pools of computational capability; provenance and watermarking systems to help distinguish real from synthetic and to track model leaks; a robust auditing and certification ecosystem; liability for AI-caused harm; robust public funding for technical AI safety research; and well-resourced institutions for coping with the dramatic economic and political disruptions (especially to democracy) that AI will cause.

Humanity can enjoy a flourishing future with AI. Having succeeded in creating powerful AI systems, we can now enjoy an "AI summer" in which we reap the rewards, engineer these systems for the clear benefit of all, and give society a chance to adapt. Society has hit pause on other technologies with potentially catastrophic effects on society. We can do so here. Let's enjoy a long AI summer, not rush unprepared into a fall.


/ 007. Metz, Cade. "What's the Future for A.I.?" 2023.


Where we’re heading tomorrow, next year and beyond.

In today’s A.I. newsletter, the last in our five-part series, I look at where artificial intelligence may be headed in the years to come.

In early March, I visited OpenAI’s San Francisco offices for an early look at GPT-4, a new version of the technology that underpins its ChatGPT chatbot. The most eye-popping moment arrived when Greg Brockman, OpenAI’s president and co-founder, showed off a feature that is still unavailable to the public: He gave the bot a photograph from the Hubble Space Telescope and asked it to describe the image “in painstaking detail.”

The description was completely accurate, right down to the strange white line created by a satellite streaking across the heavens. This is one look at the future of chatbots and other A.I. technologies: A new wave of multimodal systems will juggle images, sounds and videos as well as text.

Yesterday, my colleague Kevin Roose told you about what A.I. can do now. I’m going to focus on the opportunities and upheavals to come as it gains abilities and skills.

AI. In the near term

Generative A.I.s can already answer questions, write poetry, generate computer code and carry on conversations. As “chatbot” suggests, they are first being rolled out in conversational formats like ChatGPT and Bing.

But that’s not going to last long. Microsoft and Google have already announced plans to incorporate these A.I. technologies into their products. You’ll be able to use them to write a rough draft of an email, automatically summarize a meeting and pull off many other cool tricks.

OpenAI also offers an A.P.I., or application programming interface, that other tech companies can use to plug GPT-4 into their apps and products. And it has created a series of plug-ins from companies like Instacart, Expedia and Wolfram Alpha that expand ChatGPT’s abilities.

A.I. in the medium term

Many experts believe A.I. will make some workers, including doctors, lawyers and computer programmers, more productive than ever. They also believe some workers will be replaced.

“This will affect tasks that are more repetitive, more formulaic, more generic,” said Zachary Lipton, a professor at Carnegie Mellon who specializes in artificial intelligence and its impact on society. “This can liberate some people who are not good at repetitive tasks. At the same time, there is a threat to people who specialize in the repetitive part.”

Human-performed jobs could disappear from audio-to-text transcription and translation. In the legal field, GPT-4 is already proficient enough to ace the bar exam, and the accounting firm PricewaterhouseCoopers plans to roll out an OpenAI-powered legal chatbot to its staff.

At the same time, companies like OpenAI, Google and Meta are building systems that let you instantly generate images and videos simply by describing what you want to see.

Other companies are building bots that can actually use websites and software applications as a human does. In the next stage of the technology, A.I. systems could shop online for your Christmas presents, hire people to do small jobs around the house and track your monthly expenses.

All that is a lot to think about. But the biggest issue may be this: Before we have a chance to grasp how these systems will affect the world, they will get even more powerful.

A.I. in the long term

For companies like OpenAI and DeepMind, a lab that’s owned by Google’s parent company, the plan is to push this technology as far as it will go. They hope to eventually build what researchers call artificial general intelligence, or A.G.I. — a machine that can do anything the human brain can do.

As Sam Altman, OpenAI’s chief executive, told me three years ago: “My goal is to build broadly beneficial A.G.I. I also understand this sounds ridiculous.” Today, it sounds less ridiculous. But it is still easier said than done.

For an A.I. to become an A.G.I., it will require an understanding of the physical world writ large. And it is not clear whether systems can learn to mimic the length and breadth of human reasoning and common sense using the methods that have produced technologies like GPT-4. New breakthroughs will probably be necessary.

The question is, do we really want artificial intelligence to become that powerful? A very important related question: Is there any way to stop it from happening?

The risks of A.I.

Many A.I. executives believe the technologies they are creating will improve our lives. But some have been warning for decades about a darker scenario, where our creations don’t always do what we want them to do, or they follow our instructions in unpredictable ways, with potentially dire consequences.

A.I. experts talk about “alignment” — that is, making sure A.I. systems are in line with human values and goals.

Before GPT-4 was released, OpenAI handed it over to an outside group to imagine and test dangerous uses of the chatbot.

The group found that the system was able to hire a human online to defeat a Captcha test. When the human asked if it was “a robot,” the system, unprompted by the testers, lied and said it was a person with a visual impairment.

Testers also showed that the system could be coaxed into suggesting how to buy illegal firearms online and into describing ways to make dangerous substances from household items. After changes by OpenAI, the system no longer does these things.

But it’s impossible to eliminate all potential misuses. As a system like this learns from data, it develops skills that its creators never expected. It is hard to know how things might go wrong after millions of people start using it.

“Every time we make a new A.I. system, we are unable to fully characterize all its capabilities and all of its safety problems — and this problem is getting worse over time rather than better,” said Jack Clark, a founder and the head of policy of Anthropic, a San Francisco start-up building this same kind of technology.

And OpenAI and giants like Google are hardly the only ones exploring this technology. The basic methods used to build these systems are widely understood, and other companies, countries, research labs and bad actors may be less careful.

The remedies for A.I. 

Ultimately, keeping a lid on dangerous A.I. technology will require far-reaching oversight. But experts are not optimistic.

“We need a regulatory system that is international,” said Aviv Ovadya, a researcher at the Berkman Klein Center for Internet & Society at Harvard who helped test GPT-4 before its release. “But I do not see our existing government institutions being about to navigate this at the rate that is necessary.”

As we told you earlier this week, more than 1,000 technology leaders and researchers, including Elon Musk, have urged artificial intelligence labs to pause development of the most advanced systems, warning in an open letter that A.I. tools present “profound risks to society and humanity.”

A.I. developers are “locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one — not even their creators — can understand, predict or reliably control,” according to the letter.

Some experts are mostly concerned about near-term dangers, including the spread of disinformation and the risk that people would rely on these systems for inaccurate or harmful medical and emotional advice.

But other critics are part of a vast and influential online community called rationalists or effective altruists, who believe that A.I could eventually destroy humanity. This mind-set is reflected in the letter.


/ 008. Ryan-Mosley, Tate. "An early guide to policymaking on generative AI," 2023.


Earlier this week, I was chatting with a policy professor in Washington, DC, who told me that students and colleagues alike are asking about GPT-4 and generative AI: What should they be reading? How much attention should they be paying?

She wanted to know if I had any suggestions, and asked what I thought all the new advances meant for lawmakers. I’ve spent a few days thinking, reading, and chatting with the experts about this, and my answer morphed into this newsletter. So here goes!

Though GPT-4 is the standard bearer, it’s just one of many high-profile generative AI releases in the past few months: Google, Nvidia, Adobe, and Baidu have all announced their own projects. In short, generative AI is the thing that everyone is talking about. And though the tech is not new, its policy implications are months if not years from being understood.

GPT-4, released by OpenAI last week, is a multimodal large language model that uses deep learning to predict words in a sentence. It generates remarkably fluent text, and it can respond to images as well as word-based prompts. For paying customers, GPT-4 will now power ChatGPT, which has already been incorporated into commercial applications.

The newest iteration has made a major splash, and Bill Gates called it “revolutionary” in a letter this week. However, OpenAI has also been criticized for a lack of transparency about how the model was trained and evaluated for bias.

Despite all the excitement, generative AI comes with significant risks. The models are trained on the toxic repository that is the internet, which means they often produce racist and sexist output. They also regularly make things up and state them with convincing confidence. That could be a nightmare from a misinformation standpoint and could make scams more persuasive and prolific.

Generative AI tools are also potential threats to people’s security and privacy, and they have little regard for copyright laws. Companies using generative AI that has stolen the work of others are already being sued.

Alex Engler, a fellow in governance studies at the Brookings Institution, has considered how policymakers should be thinking about this and sees two main types of risks: harms from malicious use and harms from commercial use. Malicious uses of the technology, like disinformation, automated hate speech, and scamming, “have a lot in common with content moderation,” Engler said in an email to me, “and the best way to tackle these risks is likely platform governance.” (If you want to learn more about this, I’d recommend listening to this week’s Sunday Show from Tech Policy Press, where Justin Hendrix, an editor and a lecturer on tech, media, and democracy, talks with a panel of experts about whether generative AI systems should be regulated similarly to search and recommendation algorithms. Hint: Section 230.)

Policy discussions about generative AI have so far focused on that second category: risks from commercial use of the technology, like coding or advertising. So far, the US government has taken small but notable actions, primarily through the Federal Trade Commission (FTC). The FTC issued a warning statement to companies last month urging them not to make claims about technical capabilities that they can’t substantiate, such as overstating what AI can do. This week, on its business blog, it used even stronger language about risks companies should consider when using generative AI.

“If you develop or offer a synthetic media or generative AI product, consider at the design stage and thereafter the reasonably foreseeable—and often obvious—ways it could be misused for fraud or cause other harm. Then ask yourself whether such risks are high enough that you shouldn’t offer the product at all,” the blog post reads.

The US Copyright Office also launched a new initiative intended to deal with the thorny policy questions around AI, attribution, and intellectual property.

The EU, meanwhile, is sticking true to its reputation as the world leader in tech policy. At the start of this year my colleague Melissa Heikkilä wrote about the EU’s efforts to try to pass the AI Act. It’s a set of rules that would prevent companies from releasing models into the wild without disclosing their inner workings, which is precisely what some critics are accusing OpenAI of with the GPT-4 release.

The EU intends to separate high-risk uses of AI, like hiring, legal, or financial applications, from lower-risk uses like video games and spam filters, and require more transparency around the more sensitive uses. OpenAI has acknowledged some of the concerns about the speed of adoption. In fact, its own CEO, Sam Altman, told ABC News he shares many of the same fears. However, the company is still not disclosing key data about GPT-4.

For policy folks in Washington, Brussels, London, and offices everywhere else in the world, it’s important to understand that generative AI is here to stay. Yes, there’s significant hype, but the recent advances in AI are as real and important as the risks that they pose.

Yesterday, the United States Congress called Shou Zi Chew, the CEO of TikTok, to a hearing about privacy and security concerns raised by the popular social media app. His appearance came after the Biden administration threatened a national ban if its parent company, ByteDance, didn’t sell off the majority of its shares.

There were lots of headlines, most using a temporal pun, and the hearing laid bare the depths of the new technological cold war between the US and China. For many watching, the hearing was both important and disappointing, with some legislators displaying poor technical understanding and hypocrisy about how Chinese companies handle privacy when American companies collect and trade data in much the same ways.

It also revealed how deeply American lawmakers distrust Chinese tech. Here are some of the spicier takes and helpful articles to get up to speed:

Key takeaways from TikTok hearing in Congress – and the uncertain road ahead - Kari Paul and Johana Bhuiyan, The Guardian
What to Know About the TikTok Security Concerns - Billy Perrigo, Time
America’s online privacy problems are much bigger than TikTok - Will Oremus, Washington Post
There’s a Problem With Banning TikTok. It’s Called the First
Amendment - Jameel Jaffer (Executive Director of the Knight First Amendment Institute), NYT Opinion

AI is able to persuade people to change their minds about hot-button political issues like an assault weapon ban and paid parental leave, according to a study by a team at Stanford’s Polarization and Social Change Lab. The researchers compared people’s political opinions on a topic before and after reading an AI-generated argument, and found that these arguments can be as effective as human-written ones in persuading the readers: “AI ranked consistently as more factual and logical, less angry, and less reliant upon storytelling as a persuasive technique.”

The teams point to concerns about the use of generative AI in a political context, such as in lobbying or online discourse. (For more on the use of generative AI in politics, do please read this recent piece by Nathan Sanders and Bruce Schneier.)


/ 009. North, Geoffrey. "A celebration of connectionism," 1987.


New developments in neural network theory have excited both psychologists and neurobiologists. Practitioners of the new art displayed their wares last week.

WHEN David Rumelhart, Geoffrey Hinton and Ronald Williams described for neural networks a powerful new learning procedure called back-propagation (Nature 323, 533; 1986), they noted that theirs was not a plausible model of how brains learn. Yet the generality of their approach, and the several intriguing features of network learning by backpropagation which have come to light, have stimulated a resurgence of interest in neural network models among neuroscientists, theoreticians and experimentalists alike. Last week, at a meeting organized by the Society of Experimental Psychology, a packed audience heard Geoffrey Hinton describe a new learning algorithm that seems a better model of biological learning than is back-propagation by parallel networks which nevertheless seems to retain much of the power of its predecessor.

In parallel distributed processing, a network can be thought of as embodying a mathematical function mapping vectors in 'input space' to vectors in 'output space', much as matrices effect linear transformations between vector spaces. A vector in the neural context is simply the pattern of excitation of some set of units taken to be the input or the output of the network. The processing is thus distributed in the pattern of the connections between units of the network and their strengths. Corresponding to the real physiological task of, say, pattern recognition, will be some kind of network function mapping inputs (patterns) onto outputs (interpretations). The all-important question is what kind of network is needed for a particular task.

Simple networks developed in the 1960s, known as perceptrons, involved only two layers of units, an input and an output, with direct connections between them. Such networks are very limited in the range of tasks they can carry out. The versatility of a network can be greatly increased by the introduction of intermediate layers of 'hidden' units, but this raises the problem of how it can be trained.

Back-propagation provides an elegant way of training a multi-layered network. During learning, the output vectors generated by the network for a given input are compared with the desired output, giving an error calculated from the difference between the two. Back-propagation calculates the dependence of this error on all the connection weights, simply by using the chain-rule for differentiation, and the weights are adjusted to reduce the error, so that the network converges by gradient descent on the required structure.

During learning, the network comes to capture certain general features which are characteristic of its task. The hidden units, in particular, develop features that seem especially significant to neuroscientists who record the properties of single neurons in brains. For example, in some cases they are reminiscent of the way in which some neurons in the brain are found to be specific for different aspects of the representation of the visual field.

Even so, this system of learning by back-propagation has not seemed very biologically realistic. Hinton (CarnegieMellon University) and his colleagues have been looking for a more plausible system oflearning.

The new development is known as a 'recirculation' algorithm, and works as follows. In a network learning by backpropagation, there is a linear flow of activity (via the hidden intermediate units) from the input units to the output units.In the new system, the hidden units connect back to the single layer of 'visible' input units. Activity thus recirculates through the network; during training, the connection weights are adjusted to minimize the rate of change of activity in each unit. Thus, when trained, the network is set up so as to stabilize on certain states, and so can work as a kind of 'content addressable memory' with the property that degraded or incomplete forms of the training inputs can regenerate the correct version.

It has been shown that, under certain conditions, the new algorithm is equivalent to gradient descent, and it has been found empirically that the system still works when these conditions are relaxed.

A number of interesting applications of back-propagation were reported at the meeting. Hinton described a network for recognizing one-dimensional shapes on a one-dimensional retina independently of position: the hidden units learn to respond to shapes in different positions. Hinton also described a speech-recognition network which learns to recognize spoken consonants given very noisy corrupted data. It appears to perform almost as well as people, and better than the previous best system of automated speech recognition.

Several speakers described analogies between the behaviour of their networks during training or after 'damage' and what is known of human learning and cognitive disorders. For example, J. L. McClelland (Carnegie-Mellon University) described how a network for learning a balancebeam task progressed through stages of competence similar to those of children given the same task. The problem is to decide which way a balance-beam will tip, depending on the position and size of weights on either side of the fulcrum. With an appropriately biased learning environment, such as children might well experience, the network, like children, initially bases its decisions purely on weight information; gradually the network learns to use the position of the weights. M.S. Seidenberg (McGill University) described a network for word recognition and pronunciation that, when 'damaged' by the removal of hidden units, displayed behaviour reminiscent of some human disorders, such as dyslexia.

Parallel distributed processing is not without its critics, and S. Pinker (MIT) reported that a linguistic analysis of Rumelhart and McClelland's network for changing the tense of verbs in sentences shows that the system is not 'descriptively adequate' as a model for human language, in that it abandons certain symbolic rules and principles that linguistic studies suggest are crucial in human language.

David Willshaw (Edinburgh) asked whether parallel distributed processing networks might, like perceptrons, similarly cease to make significant progress and fade in interest after a period of development and excitement. McClelland's riposte was that work on perceptrons was severely limited by the available computer power and circumstances are now sufficiently different to justify optimism.

The biological relevance of parallel distributed processing remains an open question. Independently of relevance, however, work on network systems may be of interest at a purely theoretical level. The present work is a kind of experimental mathematics, and in that respect is rather similar to that of Mandlebrot on fractals, also made possible and inspired by computers. The hope is that, in future, deductive proofs will give a more rigorous basis to work on networks.

Many interesting problems remain. On what set of functions will a given network topology converge? How can the optimal network for a given task be predicted, and how long will training take? And so on.


/ 010. Hinton, Geoffrey E. "Computation by neural networks," 2000.


Networks of neurons can perform com- putations that have proved very difficult to emulate in conventional computers. In trying to understand how real ner- vous systems achieve their remarkable computational abilities, researchers have been confronted with three major the- oretical issues. How can we characterize the dynamics of neural networks with recurrent  connections?  How  do  the time-varying activities of populations of neurons represent things? How are synapse strengths adjusted to learn these representations? To gain insight into these difficult theoretical issues, it has proved necessary to study grossly ideal- ized models that are as different from real biological neural networks as apples are from planets.

The 1980s saw major progress on all three fronts. In a classic 1982 paper1, Hopfield showed that asynchronous networks with symmetrically connected neurons would settle to locally stable states, known as ‘point attractors’, which could be viewed as content-addressable memories. Although these networks were both computationally inefficient and biologically unrealistic, Hopfield’s work inspired a new generation of recurrent network models; one early example was a learning algorithm that could automatically construct efficient and robust population codes in ‘hidden’ neurons whose activities were never explicitly specified by the training environment.

The 1980s also saw the widespread use of the backpropagation algorithm for training the synaptic weights in both feedforward and recurrent neural networks. Backpropagation is simply an efficient method for computing how changing the weight of any given synapse would affect the difference between the way the network actually behaves in response to a particular training input and the way a teacher desires it to behave3. Backpropagation is not a plausible model of how real synapses learn, because it requires a teacher to specify the desired behavior of the network, it uses connections backward, and it is very slow in large networks. However, backpropagation did demonstrate the impressive power of adjusting synapses to optimize a performance measure. It also allowed psychologists to design neural networks that could perform interesting computations in unexpected ways. For example, a recurrent network that is trained to derive the meaning of words from their spelling makes very surprising errors when damaged, and these errors are remarkably similar to those made by adults with dyslexia.

The practical success of backpropagation led researchers to look for an alternative performance measure that did not involve a teacher and that could easily be optimized using information that was locally available at a synapse. A measure with all the right properties emerges from thinking about perception in a peculiar way: the widespread existence of top-down connections in the brain, coupled with our ability to generate mental images, suggests that the perceptual system may literally contain a generative model of sensory data. A generative model stands in the same relationship to perception as do computer graphics to computer vision. It allows the sensory data to be generated from a high-level description of the scene. Perception can be seen as the process of inverting the generative model—inferring a high-level description from sensory data under the assumption that the data were produced by the generative model. Learning then is the process of updating the parameters of the generative model so as to maximize the likelihood that it would generate the observed sensory data.

Many neuroscientists find this way of thinking unappealing because the obvious function of the perceptual system is to go from the sensory data to a high-level representation, not vice versa. But to understand how we extract the causes from a particular image sequence, or how we learn the classes of things that might be causes, it is very helpful to think in terms of a top-down, stochastic, generative model. This is exactly the approach that statisticians take to modeling data, and recent advances in the complexity of such statistical models5 provide a rich source of ideas for understanding neural computation. All the best speech recognition programs now work by fitting a probabilistic generative model.

If the generative model is linear, the fitting is relatively straightforward but can nevertheless lead to impressive results6,7. There is good empirical evidence that the brain uses generative models with temporal dynamics for motor control8 (see also ref. 9, this issue). If the generative model is nonlinear and allows multiple causes, it can be very difficult to compute the likely causes of a pattern of sensory inputs. When exact inference is unfeasible, it is possible to use bottom-up, feedforward connections to activate approximately the right causes, and this leads to a learning algorithm for fitting hierarchical nonlinear models that requires only information that is locally available at synapses10. So far, theoretical neuroscientists have considered only a few simple types of nonlinear generative model. Although these have produced impressive results, it seems likely that more sophisticated models and better fitting techniques will be required to make detailed contact with neural reality.


/ 011. Rotman, David. "ChatGPT is about to revolutionize the economy. We need to decide what that looks like," 2023.


New large language models will transform many jobs. Whether they will lead to widespread prosperity or not is up to us.

Whether it’s based on hallucinatory beliefs or not, an artificial-intelligence gold rush has started over the last several months to mine the anticipated business opportunities from generative AI models like ChatGPT. App developers, venture-backed startups, and some of the world’s largest corporations are all scrambling to make sense of the sensational textgenerating bot released by OpenAI last November.

You can practically hear the shrieks from corner offices around the world: “What is our ChatGPT play? How do we make money off this?”

But while companies and executives see a clear chance to cash in, the likely impact of the technology on workers and the economy on the whole is far less obvious. Despite their limitations—chief among of them their propensity for making stuff up—ChatGPT and other recently released generative AI models hold the promise of automating all sorts of tasks that were previously thought to be solely in the realm of human creativity and reasoning, from writing to creating graphics to summarizing and analyzing data. That has left economists unsure how jobs and overall productivity might be affected.

For all the amazing advances in AI and other digital tools over the last decade, their record in improving prosperity and spurring widespread economic growth is discouraging. Although a few investors and entrepreneurs have become very rich, most people haven’t benefited. Some have even been automated out of their jobs.

Productivity growth, which is how countries become richer and more prosperous, has been dismal since around 2005 in the US and in most advanced economies (the UK is a particular basket case). The fact that the economic pie is not growing much has led to stagnant wages for many people.

What productivity growth there has been in that time is largely confined to a few sectors, such as information services, and in the US to a few cities— think San Jose, San Francisco, Seattle, and Boston.

Will ChatGPT make the already troubling income and wealth inequality in the US and many other countries even worse? Or could it help? Could it in fact provide a much-needed boost to productivity?

ChatGPT, with its human-like writing abilities, and OpenAI’s other recent release DALL-E 2, which generates images on demand, use large language models trained on huge amounts of data. The same is true of rivals such as Claude from Anthropic and Bard from Google. These so-called foundational models, such as GPT-3.5 from OpenAI, which ChatGPT is based on, or Google’s competing language model LaMDA, which powers Bard, have evolved rapidly in recent years.

They keep getting more powerful: they’re trained on ever more data, and the number of parameters—the variables in the models that get tweaked—is rising dramatically. Earlier this month, OpenAI released its newest version, GPT-4. While OpenAI won’t say exactly how much bigger it is, one can guess; GPT-3, with some 175 billion parameters, was about 100 times larger than GPT-2.

But it was the release of ChatGPT late last year that changed everything for many users. It’s incredibly easy to use and compelling in its ability to rapidly create human-like text, including recipes, workout plans, and— perhaps most surprising—computer code. For many non-experts, including a growing number of entrepreneurs and businesspeople, the user-friendly chat model—less abstract and more practical than the impressive but often esoteric advances that been brewing in academia and a handful of hightech companies over the last few years—is clear evidence that the AI revolution has real potential.

Venture capitalists and other investors are pouring billions into companies based on generative AI, and the list of apps and services driven by large language models is growing longer every day.

Among the big players, Microsoft has invested a reported $10 billion in OpenAI and its ChatGPT, hoping the technology will bring new life to its long-struggling Bing search engine and fresh capabilities to its Office products. In early March, Salesforce said it will introduce a ChatGPT app in its popular Slack product; at the same time, it announced a $250 million fund to invest in generative AI startups. The list goes on, from Coca-Cola to GM. Everyone has a ChatGPT play.

Meanwhile, Google announced it is going to use its new generative AI tools in Gmail, Docs, and some of its other widely used products.

Still, there are no obvious killer apps yet. And as businesses scramble for ways to use the technology, economists say a rare window has opened for rethinking how to get the most benefits from the new generation of AI.

“We’re talking in such a moment because you can touch this technology. Now you can play with it without needing any coding skills. A lot of people can start imagining how this impacts their workflow, their job prospects,” says Katya Klinova, the head of research on AI, labor, and the economy at the Partnership on AI in San Francisco.

“The question is who is going to benefit? And who will be left behind?” says Klinova, who is working on a report outlining the potential job impacts of generative AI and providing recommendations for using it to increase shared prosperity.

The optimistic view: it will prove to be a powerful tool for many workers, improving their capabilities and expertise, while providing a boost to the overall economy. The pessimistic one: companies will simply use it to destroy what once looked like automation-proof jobs, well-paying ones that require creative skills and logical reasoning; a few high-tech companies and tech elites will get even richer, but it will do little for overall economic growth.

Helping the least skilled

The question of ChatGPT’s impact on the workplace isn’t just a theoretical one.

In the most recent analysis, OpenAI’s Tyna Eloundou, Sam Manning, and Pamela Mishkin, with the University of Pennsylvania’s Daniel Rock, found that large language models such as GPT could have some effect on 80% of the US workforce. They further estimated that the AI models, including GPT-4 and other anticipated software tools, would heavily affect 19% of jobs, with at least 50% of the tasks in those jobs “exposed.” In contrast to what we saw in earlier waves of automation, higher-income jobs would be most affected, they suggest. Some of the people whose jobs are most vulnerable: writers, web and digital designers, financial quantitative analysts, and—just in case you were thinking of a career change— blockchain engineers.

“There is no question that [generative AI] is going to be used—it’s not just a novelty,” says David Autor, an MIT labor economist and a leading expert on the impact of technology on jobs. “Law firms are already using it, and that’s just one example. It opens up a range of tasks that can be automated.”

Autor has spent years documenting how advanced digital technologies have destroyed many manufacturing and routine clerical jobs that once paid well. But he says ChatGPT and other examples of generative AI have changed the calculation.

Previously, AI had automated some office work, but it was those rote stepby- step tasks that could be coded for a machine. Now it can perform tasks that we have viewed as creative, such as writing and producing graphics. “It’s pretty apparent to anyone who’s paying attention that generative AI opens the door to computerization of a lot of kinds of tasks that we think of as not easily automated,” he says.

The worry is not so much that ChatGPT will lead to large-scale unemployment—as Autor points out, there are plenty of jobs in the US—but that companies will replace relatively well-paying white-collar jobs with this new form of automation, sending those workers off to lower-paying service employment while the few who are best able to exploit the new technology reap all the benefits.

In this scenario, tech-savvy workers and companies could quickly take up the AI tools, becoming so much more productive that they dominate their workplaces and their sectors. Those with fewer skills and little technical acumen to begin with would be left further behind.

But Autor also sees a more positive possible outcome: generative AI could help a wide swath of people gain the skills to compete with those who have more education and expertise.

One of the first rigorous studies done on the productivity impact of ChatGPT suggests that such an outcome might be possible.

Two MIT economics graduate students, Shakked Noy and Whitney Zhang, ran an experiment involving hundreds of college-educated professionals working in areas like marketing and HR; they asked half to use ChatGPT in their daily tasks and the others not to. ChatGPT raised overall productivity (not too surprisingly), but here’s the really interesting result: the AI tool helped the least skilled and accomplished workers the most, decreasing the performance gap between employees. In other words, the poor writers got much better; the good writers simply got a little faster.

The preliminary findings suggest that ChatGPT and other generative AIs could, in the jargon of economists, “upskill” people who are having trouble finding work. There are lots of experienced workers “lying fallow” after being displaced from office and manufacturing jobs over the last few decades, Autor says. If generative AI can be used as a practical tool to broaden their expertise and provide them with the specialized skills required in areas such as health care or teaching, where there are plenty of jobs, it could revitalize our workforce.

Determining which scenario wins out will require a more deliberate effort to think about how we want to exploit the technology.

“I don’t think we should take it as the technology is loose on the world and we must adapt to it. Because it’s in the process of being created, it can be used and developed in a variety of ways,” says Autor. “It’s hard to overstate the importance of designing what it’s there for.”

Simply put, we are at a juncture where either less-skilled workers will increasingly be able to take on what is now thought of as knowledge work, or the most talented knowledge workers will radically scale up their existing advantages over everyone else. Which outcome we get depends largely on how employers implement tools like ChatGPT. But the more hopeful option is well within our reach.

Beyond human-like

There are some reasons to be pessimistic, however. Last spring, in “The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence,” the Stanford economist Erik Brynjolfsson warned that AI creators were too obsessed with mimicking human intelligence rather than finding ways to use the technology to allow people to do new tasks and extend their capabilities.

The pursuit of human-like capabilities, Brynjolfsson argued, has led to technologies that simply replace people with machines, driving down wages and exacerbating inequality of wealth and income. It is, he wrote, “the single biggest explanation” for the rising concentration of wealth.

A year later, he says ChatGPT, with its human-sounding outputs, “is like the poster child for what I warned about”: it has “turbocharged” the discussion around how the new technologies can be used to give people new abilities rather than simply replacing them.

Despite his worries that AI developers will continue to blindly outdo each other in mimicking human-like capabilities in their creations, Brynjolfsson, the director of the Stanford Digital Economy Lab, is generally a techno-optimist when it comes to artificial intelligence. Two years ago, he predicted a productivity boom from AI and other digital technologies, and these days he’s bullish on the impact of the new AI models.

Much of Brynjolfsson’s optimism comes from the conviction that businesses could greatly benefit from using generative AI such as ChatGPT to expand their offerings and improve the productivity of their workforce. “It’s a great creativity tool. It’s great at helping you to do novel things. It’s not simply doing the same thing cheaper,” says Brynjolfsson. As long as companies and developers can “stay away from the mentality of thinking that humans aren’t needed,” he says, “it’s going to be very important.”

Within a decade, he predicts, generative AI could add trillions of dollars in economic growth in the US. “A majority of our economy is basically knowledge workers and information workers,” he says. “And it’s hard to think of any type of information workers that won’t be at least partly affected.”

When that productivity boost will come—if it does—is an economic guessing game. Maybe we just need to be patient.

In 1987, Robert Solow, the MIT economist who won the Nobel Prize that year for explaining how innovation drives economic growth, famously said, “You can see the computer age everywhere except in the productivity statistics.” It wasn’t until later, in the mid and late 1990s, that the impacts— particularly from advances in semiconductors—began showing up in the productivity data as businesses found ways to take advantage of ever cheaper computational power and related advances in software.

Could the same thing happen with AI? Avi Goldfarb, an economist at the University of Toronto, says it depends on whether we can figure out how to use the latest technology to transform businesses as we did in the earlier computer age.

So far, he says, companies have just been dropping in AI to do tasks a little bit better: “It’ll increase efficiency—it might incrementally increase productivity—but ultimately, the net benefits are going to be small. Because all you’re doing is the same thing a little bit better.” But, he says, “the technology doesn’t just allow us to do what we’ve always done a little bit better or a little bit cheaper. It might allow us to create new processes to create value to customers.”

The verdict on when—even if—that will happen with generative AI remains uncertain. “Once we figure out what good writing at scale allows industries to do differently, or—in the context of Dall-E—what graphic design at scale allows us to do differently, that’s when we’re going to experience the big productivity boost,” Goldfarb says. “But if that is next week or next year or 10 years from now, I have no idea.”

Power struggle

When Anton Korinek, an economist at the University of Virginia and a fellow at the Brookings Institution, got access to the new generation of large language models such as ChatGPT, he did what a lot of us did: he began playing around with them to see how they might help his work. He carefully documented their performance in a paper in February, noting how well they handled 25 “use cases,” from brainstorming and editing text (very useful) to coding (pretty good with some help) to doing math (not great).

ChatGPT did explain one of the most fundamental principles in economics incorrectly, says Korinek: “It screwed up really badly.” But the mistake, easily spotted, was quickly forgiven in light of the benefits. “I can tell you that it makes me, as a cognitive worker, more productive,” he says. “Hands down, no question for me that I’m more productive when I use a language model.”

When GPT-4 came out, he tested its performance on the same 25 questions that he documented in February, and it performed far better. There were fewer instances of making stuff up; it also did much better on the math assignments, says Korinek.

Since ChatGPT and other AI bots automate cognitive work, as opposed to physical tasks that require investments in equipment and infrastructure, a boost to economic productivity could happen far more quickly than in past technological revolutions, says Korinek. “I think we may see a greater boost to productivity by the end of the year—certainly by 2024,” he says.

What’s more, he says, in the longer term, the way the AI models can make researchers like himself more productive has the potential to drive technological progress.

That potential of large language models is already turning up in research in the physical sciences. Berend Smit, who runs a chemical engineering lab at EPFL in Lausanne, Switzerland, is an expert on using machine learning to discover new materials. Last year, after one of his graduate students, Kevin Maik Jablonka, showed some interesting results using GPT-3, Smit asked him to demonstrate that GPT-3 is, in fact, useless for the kinds of sophisticated machine-learning studies his group does to predict the properties of compounds.

“He failed completely,” jokes Smit.

It turns out that after being fine-tuned for a few minutes with a few relevant examples, the model performs as well as advanced machine-learning tools specially developed for chemistry in answering basic questions about things like the solubility of a compound or its reactivity. Simply give it the name of a compound, and it can predict various properties based on the structure.

As in other areas of work, large language models could help expand the expertise and capabilities of nonexperts— in this case, chemists with little knowledge of complex machine-learning tools. Because it’s as simple as a literature search, Jablonka says, “it could bring machine learning to the masses of chemists.”

These impressive—and surprising—results are just a tantalizing hint of how powerful the new forms of AI could be across a wide swath of creative work, including scientific discovery, and how shockingly easy they are to use. But this also points to some fundamental questions.

As the potential impact of generative AI on the economy and jobs becomes more imminent, who will define the vision for how these tools should be designed and deployed? Who will control the future of this amazing technology?

Diane Coyle, an economist at Cambridge University in the UK, says one concern is the potential for large language models to be dominated by the same big companies that rule much of the digital world. Google and Meta are offering their own large language models alongside OpenAI, she points out, and the large computational costs required to run the software create a barrier to entry for anyone looking to compete.

The worry is that these companies have similar “advertising-driven business models,” Coyle says. “So obviously you get a certain uniformity of thought, if you don’t have different kinds of people with different kinds of incentives.”

Coyle acknowledges that there are no easy fixes, but she says one possibility is a publicly funded international research organization for generative AI, modeled after CERN, the Geneva-based intergovernmental European nuclear research body where the World Wide Web was created in 1989. It would be equipped with the huge computing power needed to run the models and the scientific expertise to further develop the technology.

Such an effort outside of Big Tech, says Coyle, would “bring some diversity to the incentives that the creators of the models face when they’re producing them.”

While it remains uncertain which public policies would help make sure that large language models best serve the public interest, says Coyle, it’s becoming clear that the choices about how we use the technology can’t be left to a few dominant companies and the market alone.

History provides us with plenty of examples of how important government funded research can be in developing technologies that bring about widespread prosperity. Long before the invention of the web at CERN, another publicly funded effort in the late 1960s gave rise to the internet, when the US Department of Defense supported ARPANET, which pioneered ways for multiple computers to communicate with each other.

In Power and Progress: Our 1000-Year Struggle Over Technology & Prosperity, the MIT economists Daron Acemoglu and Simon Johnson provide a compelling walk through the history of technological progress and its mixed record in creating widespread prosperity. Their point is that it’s critical to deliberately steer technological advances in ways that provide broad benefits and don’t just make the elite richer.

From the decades after World War II until the early 1970s, the US economy was marked by rapid technological changes; wages for most workers rose while income inequality dropped sharply. The reason, Acemoglu and Johnson say, is that technological advances were used to create new tasks and jobs, while social and political pressures helped ensure that workers shared the benefits more equally with their employers than they do now.

In contrast, they write, the more recent rapid adoption of manufacturing robots in “the industrial heartland of the American economy in the Midwest” over the last few decades simply destroyed jobs and led to a “prolonged regional decline.”

The book, which comes out in May, is particularly relevant for understanding what today’s rapid progress in AI could bring and how decisions about the best way to use the breakthroughs will affect us all going forward. In a recent interview, Acemoglu said they were writing the book when GPT-3 was first released. And, he adds half-jokingly, “we foresaw ChatGPT.”

Acemoglu maintains that the creators of AI “are going in the wrong direction.” The entire architecture behind the AI “is in the automation mode,” he says. “But there is nothing inherent about generative AI or AI in general that should push us in this direction. It’s the business models and the vision of the people in OpenAI and Microsoft and the venture capital community.”

If you believe we can steer a technology’s trajectory, then an obvious question is: Who is “we”? And this is where Acemoglu and Johnson are most provocative. They write: “Society and its powerful gatekeepers need to stop being mesmerized by tech billionaires and their agenda … One does not need to be an AI expert to have a say about the direction of progress and the future of our society forged by these technologies.”

The creators of ChatGPT and the businesspeople involved in bringing it to market, notably OpenAI’s CEO, Sam Altman, deserve much credit for offering the new AI sensation to the public. Its potential is vast. But that doesn’t mean we must accept their vision and aspirations for where we want the technology to go and how it should be used.

According to their narrative, the end goal is artificial general intelligence, which, if all goes well, will lead to great economic wealth and abundances. Altman, for one, has promoted the vision at great length recently, providing further justification for his longtime advocacy of a universal basic income (UBI) to feed the non-technocrats among us. For some, it sounds tempting. No work and free money! Sweet!

It’s the assumptions underlying the narrative that are most troubling— namely, that AI is headed on an inevitable job-destroying path and most of us are just along for the (free?) ride. This view barely acknowledges the possibility that generative AI could lead to a creativity and productivity boom for workers far beyond the tech-savvy elites by helping to unlock their talents and brains. There is little discussion of the idea of using the technology to produce widespread prosperity by expanding human capabilities and expertise throughout the working population.

As Acemoglu and Johnson write: “We are heading toward greater inequality not inevitably but because of faulty choices about who has power in society and the direction of technology … In fact, UBI fully buys into the vision of the business and tech elite that they are the enlightened, talented people who should generously finance the rest.”

Acemoglu and Johnson write of various tools for achieving “a more balanced technology portfolio,” from tax reforms and other government policies that might encourage the creation of more worker-friendly AI to reforms that might wean academia off Big Tech’s funding for computer science research and business schools.

But, the economists acknowledge, such reforms are “a tall order,” and a social push to redirect technological change is “not just around the corner.”

The good news is that, in fact, we can decide how we choose to use ChatGPT and other large language models. As countless apps based on the technology are rushed to market, businesses and individual users will have a chance to choose how they want to exploit it; companies can decide to use ChatGPT to give workers more abilities—or to simply cut jobs and trim costs.

Another positive development: there is at least some momentum behind open-source projects in generative AI, which could break Big Tech’s grip on the models. Notably, last year more than a thousand international researchers collaborated on a large language model called Bloom that can create text in languages such as French, Spanish, and Arabic. And if Coyle and others are right, increased public funding for AI research could help change the course of future breakthroughs.

Stanford's Brynjolfsson refuses to say he’s optimistic about how it will play out. Still, his enthusiasm for the technology these days is clear. “We can have one of the best decades ever if we use the technology in the right direction,” he says. “But it’s not inevitable.”


/ 012. Seetharaman, Deepa. "Elon Musk, Other AI Experts Call for Pauwse in Technology's Development," 2023.


Appeal causes tension among artificial-intelligence stakeholders amidconcern over pace of advancement

Several tech executives and top artificial-intelligence researchers, including Tesla Inc. Chief Executive Offi cer Elon Musk and AI pioneer Yoshua Bengio , are calling for a pause in the breakneck development of powerful new AI tools.

A moratorium of six months or more would give the industry time to set safety standards for AI design and head off potential harms of the riskiest AI technologies , the proponents of a pause said.

“We’ve reached the point where these systems are smart enough that they canbe used in ways that are dangerous for society,” Mr. Bengio, director of theUniversity of Montreal’s Montreal Institute for Learning Algorithms, said in an interview. “And we don’t yet understand.”

These concerns and the recommendation for the pause were laid out in a letter titled “Pause Giant AI Experiments: An Open Letter” coordinated by the non profit Future of Life Institute, which lists Mr. Musk as an external adviser.The letter that was made public Wednesday was also signed by Apple co-founder Steve Wozniak ; Stability AI CEO Emad Mostaque; and co-founders of the Center for Humane Technology, Tristan Harris and Aza Raskin, who have been critical of social media and AI technology, said a spokeswoman for the team authoring the letter.

The letter doesn’t call for all AI development to halt, but urges companies to temporarily stop training systems more powerful than GPT-4, the technology released this month by Microsoft Corp.-backed startup OpenAI. That includes the next generation of OpenAI’s technology, GPT-5.

OpenAI officials say they haven’t started training GPT-5. In an interview, OpenAICEO Sam Altman said the company has long given priority to safety in development and spent more than six months doing safety tests on GPT-4 before its launch.

“In some sense, this is preaching to the choir,” Mr. Altman said. “We have, I think, been talking about these issues the loudest, with the most intensity, for the longest.”

Calls for a pause clash with a broad desire among tech companies and startups to double down on so-called generative AI, a technology capable of generating original content to human prompts. Buzz around generative AI exploded last fall after OpenAI unveiled a chatbot with its ability to perform functions like providing lengthy answers and producing computer code with humanlike sophistication.

Microsoft has embraced the technology for its Bing search engine and other tools. Alphabet Inc.’s Google has deployed a rival system , and companies such as Adobe Inc., Zoom Video Communications Inc. and Salesforce Inc. have also introduced advanced AI tools.

“A race starts today,” Microsoft CEO Satya Nadella said last month. “We’re going to move, and move fast.”

That approach has spurred renewed concerns that a rapid rollout could have unintended consequences alongside real benefits. Advances in AI have surpassed what many experts believed was possible just a few years ago, said Max Tegmark , one of the organizers of the letter, president of the Future of LifeInstitute and a physics professor at the Massachusetts Institute of Technology.

“It is unfortunate to frame this as an arms race,” Mr. Tegmark said. “It is more of a suicide race. It doesn’t matter who is going to get there first. It just means that humanity as a whole could lose control of its own destiny.”

The Future of Life Institute started working on the letter last week and initially allowed anybody to sign without identity verification. At one point, Mr. Altman’s name was added to the letter, but later removed. Mr. Altman said he never signed the letter. He said the company frequently coordinates with other AI companies on safety standards and to discuss broader concerns.

“There is work that we don’t do because we don’t think we yet know how to make it sufficiently safe,” he said. “So yeah, I think there are ways that you can slowdown on multiple axes and that’s important. And it is part of our strategy.” Messrs. Musk and Wozniak have both voiced concerns about AI technology . Mr.Musk on Wednesday tweeted that developers of the advanced AI technology“ will not heed this warning, but at least it was said.”

Mr. Musk at the same time has embraced some AI tools at Tesla for the company’s advanced driver-assistance functions. Tesla last month said it was recalling around 362,800 vehicles equipped with its technology marketed as FullSelf-Driving Beta . The U.S. top car-safety agency said the technology could, in rare circumstances, violate local traffic laws, potentially increasing the risk of a collision if a driver fails to intervene.

Yann LeCun, chief AI scientist at Meta Platforms Inc., on Tuesday tweeted that he didn’t sign the letter because he disagreed with its premise .

Mr. Mostaque, Stability AI’s CEO, said in a tweet Wednesday that although he signed the letter, he didn’t agree with a six-month pause. “It has no force but will kick off an important discussion that will hopefully bring more transparency &governance to an opaque area.”

Microsoft declined to comment, and Google didn’t immediately respond to a request for comment.

Mr. Tegmark said many companies feel “crazy commercial pressures” to add advanced AI technology into their products. A six-month pause would allow the industry “breathing room,” without disadvantaging ones that opt to move carefully.

The letter said a pause should be declared publicly and be verifiable and all key actors in the space should participate. “If such a pause cannot be enacted quickly, governments should step in and institute a moratorium,” it said.

AI labs and experts can use this time to develop a set of shared safety rules for advanced AI design that should be audited and overseen by outside experts, the authors wrote.

“I don’t think we can afford to just go forward and break things,” said Mr. Bengio, who shared a 2018 Turing award for inventing the systems that modern AI is built on. “We do need to take time to think through this collectively.”


/ 013. Mims, Christopher. "Artificial Intelligence Is Teaching Us New, Surprising Things About the Human Mind," 2023.


Thought is ever-changing electrical patterns unconnected to individual neurons. Meta is working on a system to read your mind

The world has been learning an awful lot about artificial intelligence lately, thanks to the arrival of eerily human-like chatbots.

Less noticed, but just as important: Researchers are learning a great deal about us – with the help of AI.

AI is helping scientists decode how neurons in our brains communicate, and explore the nature of cognition. This new research could one day lead to humans connecting with computers merely by thinking–as opposed to typing or voice commands. But there is a long way to go before such visions become reality.

Celeste Kidd, a psychology professor at the University of California, Berkeley, was surprised by what she discovered when she tried to examine the range of opinions people have about certain politicians, including Barack Obama and Donald Trump.

Her research was intended to explore the widening divergence of how we conceive of subjects to which we attach moral judgements – such as politicians.Previous work has shown that morally-fraught concepts are the ones peoplep erceive in the most polarized ways.

To establish a baseline for her experiment, she began by asking thousands of study participants about their associations with common nouns, in this case animals.

What she discovered was that even for common animals – including chickens, whales and salmon – people’s notions of their characteristics are all over the map. Are whales majestic? You’d be surprised who disagrees. Are penguins heavy? Opinions vary. By quizzing people on many such associations, Dr. Kidd was able to amass a pool of data that clusters people according to which of these associations they agree on. Using this method, she found that people can be grouped into between 10 and 30 different clusters, depending on their perception of an animal.

Dr. Kidd and her team concluded that people tend not to see eye to eye about even the most basic characteristics of common objects. We also overestimate how many people see things as we do. In a world in which it feels like people are increasingly talking past one another, the root of this phenomenon may be the fact that even for citizens of a single country speaking a common language, words simply don’t mean the same thing to different people.

That might not seem like a very profound observation, but what Dr. Kidd’s research suggests is the degree to which that’s true may be much greater than psychologists previously thought.

Arriving at this insight required the application of a tool of mathematics that makes many kinds of AI possible – known as a “clustering model”.

The most important feature of AI which enables new kinds of research, says Dr.Kidd, is the same that makes possible AI chatbots like OpenAI’s ChatGPT , Google’s Bard , and Microsoft’s Bing chat : It’s the capacity of modern computer systems to process a lot more data than in the past. It “opens up a lot of possibilities for new insights, from biology to medicine to cognitive science,” she adds.

Cracking the brain's neural code

In her research, Tatiana Engel, an assistant professor of neuroscience atPrinceton University, uses the same kinds of networks of artificial neurons that are behind most of what we currently call artificial intelligence. But rather than using these to better-target ads, or to generate fake images, or compose text, shea nd her team use them to interpret the electrical signals of hundreds of neurons at once in the brains of animals.

Dr. Engel and her team then go a step further: they train networks of artificial neurons to perform the same tasks as an animal – say, a swimming worm. They then find that those artificial networks organize themselves in ways that reasonably approximate the way they’re organized in real animals. While neural networks in the brain are vastly more complicated, the result of this simulation is a model system that is both close enough to its biological equivalent, and simple enough, to teach us things about how the real brain works, Dr. Engel says.

One key insight this yields is that the actual substance of thought – the patterns that constitute the mind you’re using to read this sentence – is dynamic electrical activity in our brains rather than something physically anchored to particular neurons.

In other words, in contrast to what neuroscientists once believed about how we make decisions, there are no “eat the chocolate” neurons and “don’t eat the chocolate” neurons. Thinking, it turns out, is just electrical signals zooming about inside our heads, forming a complex code which is carried by our neurons.

What’s more, AI is letting scientists listen in on the things that happen in our brains when we’re not doing anything in particular.

“This allows us to discover the brain’s internal life,” says Dr. Engel.

Do androids dream of electric sheep? We don’t know yet, but we may soon beable to determine if humans are thinking about the real thing.

Real-life mind reading

If a research lab owned by Meta Platforms, Facebook’s parent company, figuring out how to read your mind makes you at all uncomfortable, you’re probably not going to be a fan of what the rest of the 21st century has in store.

Historically, it’s been very difficult to measure brain activity inside our heads, because the electrical signals generated by our brains, which are miniscule to begin with, must be measured from outside of our skulls. ( Elon Musk’s aspirations for his Neuralink startup notwithstanding, opening up our heads and putting in brain interfaces hasn’t proved popular.)

AI is helping scientists study EEG readings and explore the nature of cognition. Data gathered from language experiments have been used by Facebook parent, Meta Platforms, to develop an early version of an algorithm that can “read” words and phrases from a person’s mind.

But progress in artificial intelligence techniques is yielding a more-powerful amplifier of those weak brain signals. Meta’s AI lab published research on one such mind-reading technology last summer.

Meta scientists didn’t actually stick anyone in a brain scanner. Instead, they used data on brain signals gathered by researchers at universities. This data was captured from human subjects who were listening to words and phrases, while sitting in non-invasive brain scanners. These scanners came in two varieties:

One was the sort of electrodes-embedded-in-a-swim-cap with which many people are familiar, called an EEG (short for “electroencephalogram”). The other looks like a supervillain’s attempt to create a world-crushing megabrain, called aMEG (for “magnetoencephalogram”).

To analyze this data, researchers used a type of AI called a “self-supervised learning model.” Without this technique, the latest generation of AI chatbots would be impossible. Such models can extract meaning from giant pools of data without any instruction from humans, and have also been used to try and figure out what animals are communicating with each other.

Researchers evaluate fluctuations in MEG sensors while a subject’s neural activity is being recorded.Research by Meta suggests that such measurements could be used in the future to allow people to direct computers just by thinking.

A little less than half of the time, Meta’s AI algorithm was able to correctly guess what words a person had heard, based on the activity generated in their brains.That might not sound too impressive, but it’s leaps and bounds better than what such systems have been able to achieve in the past.

Alexandre Défossez, a scientist at Meta who was part of the team that conducted this research, says that the eventual goal of this work is to create a general-purpose “speech decoder” that can directly transform our brain activity–our thoughts–into words.

Imagine texting a friend just by thinking about it – as long as you’re wearing anEEG cap at the moment, at any rate. The technology could have a big impact on the lives of people who are unable to communicate in other ways, adds Dr.Défossez.

It’s just one more example of the way that AI might someday give us the tools for improving our individual and collective well-being – or at least an explanation for why, in the age of social media, both of those things frequently seem so deranged.


/ 014. Zuboff, Shoshana. In the Age of the Smart Machine: The Future of Work and Power, 1988.


INTRODUCTION: DILEMMAS OF TRANSFORMATION IN THE AGE OF THE SMART MACHINE

The history of technology is that of human history in all its diversity. That is why specialist historians of technology hardly ever manage to grasp it entirely in their hands. -- Fernand Braudel, The Structures of Everyday Life

We don't know what will be happening to us in the future. Modern technology is taking over. What will be our place? -- A Piney Wood worker

RNEY WOOD, one of the nation's largest pulp mills, was in the throes of a massive modernization effort that would place every aspect of the production process under computer control. Six workers were crowded around a table in the snack area outside what they called the Star Trek Suite, one of the first control rooms to have been completely converted to microprocessor-based instrumentation. It looked enough like a NASA control room to have earned its name.

It was almost midnight, but despite the late hour and the approach of the shift change, each of the six workers was at once animated and thoughtful. "Knowledge and technology are changing so fast," they said, "what will happen to us?" Their visions of the future foresaw wrenching change. They feared that today's working assumptions could not be relied upon to carry them through, that the future would not resemble the past or the present. More frightening still was the sense of a future moving out of reach so rapidly that there was little opportunity to plan or make choices. The speed of dissolution and renovation seemed to leave no time for assurances that we were not heading toward calamity-and it would be all the more regrettable for having been something of an accident.

The discussion around the table betrayed a grudging admiration for the new technology-its power, its intelligence, and the aura of progress surrounding it. That admiration, however, bore a sense of grief. Each expression of gee-whiz-Buck-Rogers breathless wonder brought with it an aching dread conveyed in images of a future that rendered their authors obsolete. In what ways would computer technology transform their work lives? Did it promise the Big Rock Candy Mountain or a silent graveyard?

In fifteen years there will be nothing for the worker to do. The technology will be so good it will operate itself. You will just sit there behind a desk running two or three areas of the mill yourself and get bored.

The group concluded that the worker of the future would need "an extremely flexible personality" so that he or she would not be "mentally affected" by the velocity of change. They anticipated that workers would need a great deal of education and training in order to "breed flexibility." "We find it all to be a great stress," they said, "but it won't be that way for the new flexible people." Nor did they perceive any real choice, for most agreed that without an investment in the new technology, the company could not remain competitive. They also knew that without their additional flexibility, the technology would not fly right. "We are in a bind," one man groaned, "and there is no way out." The most they could do, it was agreed, was to avoid thinking too hard about the loss of overtime pay, the diminished probability of jobs for their sons and daughters, the fears of seeming incompetent in a strange new milieu, or the possibility that the company might welsh on its promise not to lay off workers.

During the conversation, a woman in stained overalls had remained silent with her head bowed, apparently lost in thought. Suddenly, she raised her face to us. It was lined with decades of hard work, her brow drawn together. Her hands lay quietly on the table. They were calloused and swollen, but her deep brown eyes were luminous, youthful, and kind. She seemed frozen, chilled by her own insight, as she solemnly delivered her conclusion:

I think the country has a problem. The managers want everything to be run by computers. But if no one has a job, no one will know how to do anything anymore. Who will pay the taxes? What kind of society will it be when people have lost their knowledge and depend on computers for everything?

Her voice trailed off as the men stared at her in dazzled silence. They slowly turned their heads to look at one another and nodded in agreement. The forecast seemed true enough. Yes, there was a problem. They looked as though they had just run a hard race, only to stop short at the edge of a cliff. As their heels skidded in the dirt, they could see nothing ahead but a steep drop downward.

Must it be so? Should the advent of the smart machine be taken as an invitation to relax the demands upon human comprehension and critical judgment? Does the massive diffusion of computer technology throughout our workplaces necessarily entail an equally dramatic loss of meaningful employment opportunities? Must the new electronic milieu engender a world in which individuals have lost control over their daily work lives? Do these visions of the future represent the price of economic success or might they signal an industrial legacy that must be overcome if intelligent technology is to yield its full value? Will the new information technology represent an opportunity for the rejuvenation of competitiveness, productive vitality, and organizational ingenuity? Which aspects of the future of working life can we predict, and which will depend upon the choices we make today?

The workers outside the Star Trek Suite knew that the so-called technological choices we face are really much more than that. Their consternation puts us on alert. There is a world to be lost and a world to be gained. Choices that appear to be merely technical will redefine our lives together at work. This means more than simply contemplating the implications or consequences of a new technology. It means that a powerful new technology, such as that represented by the computer, fundamentally reorganizes the infrastructure of our material world. It eliminates former alternatives. It creates new possibilities. It necessitates fresh choices.

The choices that we face concern the conception and distribution of knowledge in the workplace. Imagine the following scenario: Intelligence is lodged in the smart machine at the expense of the human capacity for critical judgment. Organizational members become ever more dependent, docile, and secretly cynical. As more tasks must be accomplished through the medium of information technology (I call this "computer-mediated work"), the sentient body loses its salience as a source of knowledge, resulting in profound disorientation and loss of meaning. People intensify their search for avenues of escape through drugs, apathy, or adversarial conflict, as the majority of jobs in our offices and factories become increasingly isolated, remote, routine, and perfunctory. Alternatively, imagine this scenario: Organizational leaders recognize the new forms of skill and knowledge needed to truly exploit the potential of an intelligent technology. They direct their resources toward creating a work force that can exercise critical judgment as it manages the surrounding machine systems. Work becomes more abstract as it depends upon understanding and manipulating information. This marks the beginning of new forms of mastery and provides an opportunity to imbue jobs with more comprehensive meaning. A new array of work tasks offer unprecedented opportunities for a wide range of employees to add value to products and services.

The choices that we make will shape relations of authority in the workplace. Once more, imagine: Managers struggle to retain their traditional sources of authority, which have depended in an important way upon their exclusive control of the organization's knowledge base. They use the new technology to structure organizational experience in ways that help reproduce the legitimacy of their traditional roles. Managers insist on the prerogatives of command and seek methods that protect the hierarchical distance that distinguishes them from their subordinates. Employees barred from the new forms of mastery relinquish their sense of responsibility for the organization's work and use obedience to authority as a means of expressing their resentment. Imagine an alternative: This technological transformation engenders a new approach to organizational behavior, one in which relationships are more intricate, collaborative, and bound by the mutual responsibilities of colleagues. As the new technology integrates information across time and space, managers and workers each overcome their narrow functional perspectives and create new roles that are better suited to enhancing value-adding activities in a data-rich environment. As the quality of skills at each organizational level becomes similar, hierarchical distinctions begin to blur. Authority comes to depend more upon an appropriate fit between knowledge and responsibility than upon the ranking rules of the traditional organizational pyramid.

The choices that we make will determine the techniques of administration that color the psychological ambience and shape communicative behavior in the emerging workplace. Imagine this scenario: The new technology becomes the source of surveillance techniques that are used to ensnare organizational members or to subtly bully them into conformity. Managers employ the technology to circumvent the demanding work of face-to-face engagement, substituting instead techniques of remote management and automated administration. The new technological infrastructure becomes a battlefield of techniques, with managers inventing novel ways to enhance certainty and control while employees discover new methods of self-protection and even sabotage. Imagine the alternative: The new technological milieu becomes a resource from which are fashioned innovative methods of information sharing and social exchange. These methods in turn produce a deepened sense of collective responsibility and joint ownership, as access to ever-broader domains of information lend new objectivity to data and preempt the dictates of hierarchical authority.

This book is about these alternative futures. Computer-based technologies are not neutral; they embody essential characteristics that are bound to alter the nature of work within our factories and offices, and among workers, professionals, and managers. New choices are laid open by these technologies, and these choices are being confronted in the daily lives of men and women across the landscape of modern organizations. This book is an effort to understand the deep structure of these choices-the historical, psychological, and organizational forces that imbue our conduct and sensibility. It is also a vision of a fruitful future, a call for action that can lead us beyond the stale reproduction of the past into an era that offers a historic opportunity to more fully develop the economic and human potential of our work organizations.

THE TWO FACES OF INTELLIGENT TECHNOLOGY

The past twenty years have seen their share of soothsayers ready to predict with conviction one extreme or another of the alternative futures I have presented. From the unmanned factory to the automated cockpit, visions of the future hail information technology as the final answer to "the labor question," the ultimate opportunity to rid ourselves of the thorny problems associated with training and managing a competent and committed work force. These very same technologies have been applauded as the hallmark of a second industrial revolution, in which the classic conflicts of knowledge and power associated with an earlier age will be synthesized in an array of organizational innovations and new procedures for the production of goods and services, all characterized by an unprecedented degree of labor harmony and widespread participation in management process.' Why the paradox? How can the very same technologies be interpreted in these different ways? Is this evidence that the technology is indeed neutral, a blank screen upon which managers project their biases and encounter only their own limitations? Alternatively, might it tell us something else about the interior structure of information technology?

Throughout history, humans have designed mechanisms to reproduce and extend the capacity of the human body as an instrument of work. The industrial age has carried this principle to a dramatic new level of sophistication with machines that can substitute for and amplify the abilities of the human body. Because machines are mute, and because they are precise and repetitive, they can be controlled according to a set of rational principles in a way that human bodies cannot.

There is no doubt that information technology can provide substitutes for the human body that reach an even greater degree of certainty and precision. When a task is automated by a computer, it must first be broken down to its smallest components. Whether the activity involves spraying paint on an automobile or performing a clerical transaction, it is the information contained in this analysis that translates human agency into a computer program. The resulting software can be used to automatically guide equipment, as in the case of a robot, or to execute an information transaction, as in the case of an automated teller machine.

A computer program makes it possible to rationalize activities more comprehensively than if they had been undertaken by a human being. Programmability means, for example, that a robot will respond with unwavering precision because the instructions that guide it are themselves unvarying, or that office transactions will be uniform because the instructions that guide them have been standardized. Events and processes can be rationalized to the extent that human agency can be analyzed and translated into a computer program.

What is it, then, that distinguishes information technology from earlier generations of machine technology? As information technology is used to reproduce, extend, and improve upon the process of substituting machines for human agency, it simultaneously accomplishes something quite different. The devices that automate by translating information into action also register data about those automated activities, thus generating new streams of information. For example, computer-based, numerically controlled machine tools or microprocessor-based sensing devices not only apply programmed instructions to equipment but also convert the current state of equipment, product, or process into data. Scanner devices in supermarkets automate the checkout process and simultaneously generate data that can be used for inventory control, warehousing, scheduling of deliveries, and market analysis. The same systems that make it possible to automate office transactions also create a vast overview of an organization's operations, with many levels of data coordinated and accessible for a variety of analytical efforts.

Thus, information technology, even when it is applied to automatically reproduce a finite activity, is not mute. It not only imposes information (in the form of programmed instructions) but also produces information. It both accomplishes tasks and translates them into information. The action of a machine is entirely invested in its object, the product. Information technology, on the other hand, introduces an additional dimension of reflexivity: it makes its contribution to the product, but it also reflects back on its activities and on the system of activities to which it is related. Information technology not only produces action but also produces a voice that symbolically renders events, objects, and processes so that they become visible, knowable, and shareable in a new way.

Viewed from this interior perspective, information technology is characterized by a fundamental duality that has not yet been fully appreciated. On the one hand, the technology can be applied to automating operations according to a logic that hardly differs from that of the nineteenth-century machine system-replace the human body with a technology that enables the same processes to be performed with more continuity and control. On the other, the same technology simultaneously generates information about the underlying productive and administrative processes through which an organization accomplishes its work. It provides a deeper level of transparency to activities that had been either partially or completely opaque. In this way information technology supersedes the traditional logic of automation. The word that I have coined to describe this unique capacity is informate. Activities, events, and objects are translated into and made visible by information when a technology informates as well as automates.

The informating power of intelligent technology can be seen in the manufacturing environment when microprocessor-based devices such as robots, programmable logic controllers, or sensors are used to translate the three-dimensional production process into digitized data. These data are then made available within a two-dimensional space, typically on the screen of a video display terminal or on a computer printout, in the form of electronic symbols, numbers, letters, and graphics. These data constitute a quality of information that did not exist before. The programmable controller not only tells the machine what to do-imposing information that guides operating equipment but also tells what the machine has done-translating the production process and making it visible.

In the office environment, the combination of on-line transaction systems, information systems, and communications systems creates a vast information presence that now includes data formerly stored in people's heads, in face-to-face conversations, in metal file drawers, and on widely dispersed pieces of paper. The same technology that processes documents more rapidly, and with less intervention, than a mechanical typewriter or pen and ink can be used to display those documents in a communications network. As more of the underlying transactional and communicative processes of an organization become automated, they too become available as items in a growing organizational data base.

In its capacity as an automating technology, information technology has a vast potential to displace the human presence. Its implications as an informating technology, on the other hand, are not well understood. The distinction between automate and informate provides one way to understand how this technology represents both continuities and discontinuities with the traditions of industrial history. As long as the technology is treated narrowly in its automating function, it perpetuates the logic of the industrial machine that, over the course of this century, has made it possible to rationalize work while decreasing the dependence on human skills. However, when the technology also informates the processes to which it is applied, it increases the explicit information content of tasks and sets into motion a series of dynamics that will ultimately reconfigure the nature of work and the social relationships that organize productive activity.

Because this duality of intelligent technology has not been clearly recognized, the consequences of the technology's informating capacity are often regarded as unintended. Its effects are not planned, and the potential that it lays open remains relatively unexploited. Because the informating process is poorly defined, it often evades the conventional categories of description that are used to gauge the effects of industrial technology.

These dual capacities of information technology are not opposites; they are hierarchically integrated. Informating derives from and builds upon automation. Automation is a necessary but not sufficient condition for informating. It is quite possible to proceed with automation without reference to how it will contribute to the technology's informating potential. When this occurs, informating is experienced as an unintended consequence of automation. This is one point at which choices are laid open. Managers can choose to exploit the emergent informating capacity and explore the organizational innovations required to sustain and develop it. Alternatively, they can choose to ignore or suppress the informating process. In contrast, it is possible to consider informating objectives at the start of an automation process. When this occurs, the choices that are made with respect to how and what to automate are guided by criteria that reflect developmental goals associated with using the technology's unique informating power.

Information technology is frequently hailed as "revolutionary." What are the implications of this term? Revolution means a pervasive, marked, radical change, but revolution also refers to a movement around a fixed course that returns to the starting point. Each sense of the word has relevance for the central problem of this book. The informating capacity of the new computer-based technologies brings about radical change as it alters the intrinsic character of work-the way millions of people experience daily life on the job. It also poses fundamentally new choices for our organizational futures, and the ways in which labor and management respond to these new choices will finally determine whether our era becomes a time for radical change or a return to the familiar patterns and pitfalls of the traditional workplace. An emphasis on the informating capacity of intelligent technology can provide a point of origin for new conceptions of work and power. A more restricted emphasis on its automating capacity can provide the occasion for that second kind of revolution-a return to the familiar grounds of industrial society with divergent interests battling for control, augmented by an array of new material resources with which to attack and defend.

The questions that we face today are finally about leadership. Will there be leaders who are able to recognize the historical moment and the choices it presents? Will they find ways to create the organizational conditions in which new visions, new concepts, and a new language of workplace relations can emerge? Will they be able to create organizational innovations that can exploit the unique capacities of the new technology and thus mobilize their organization's productive potential to meet the heightened rigors of global competition? Will there be leaders who understand the crucial role that human beings from each organizational stratum can play in adding value to the production of goods and services? If not, we will be stranded in a new world with old solutions. We will suffer through the unintended consequences of change, because we have failed to understand this technology and how it differs from what came before. By neglecting the unique informating capacity of advanced computer-based technology and ignoring the need for a new vision of work and organization, we will have forfeited the dramatic business benefits it can provide. Instead, we will find ways to absorb the dysfunctions, putting out brush fires and patching wounds in a slow-burning bewilderment.

THE PLAN OF THIS BOOK

The choices for the future cannot be deduced from economic data or from abstract measures of organizational functioning. They are embedded in the living detail of daily life at work as ordinary people confront the dilemmas raised by the transformational qualities of new information technology. For this reason the research presented here focuses upon the texture of human experience-what people say, feel, and do-in dealing with the technological changes that imbue their immediate environment. I studied eight organizations during the five-year period from 1981 to 1986. Each was well known as a model of technological sophistication within its particular industry. In each, information technology was implemented in ways that fundamentally altered how people were required to accomplish their daily work. In most cases, employees found themselves having to operate through the computer medium in order to perform their tasks, and in almost every instance, this was their first direct experience with information technology.

The most treacherous enemy of such research is what philosophers call "the natural attitude," our capacity to live daily life in a way that takes for granted the objects and activities that surround us. Even when we encounter new objects in our environment, our tendency is to experience them in terms of categories and qualities with which we are already familiar. The natural attitude allows us to assume and predict a great many things about each other's behavior without first establishing premises at the outset of every interaction. The natural attitude can also stand in the way of awareness, for ordinary experience has to be made extraordinary in order to become accessible to reflection. This occurs when we encounter a problem: when our actions do not yield the expected results, we are caught by surprise and so are motivated to reflect upon our initial assumptions. 2 Awareness requires a rupture with the world we take for granted; then old categories of experience are called into question and revised. For example, in the early days of photography, the discrepancies between the camera's eye and the human eye were avidly discussed, but, "once they began to think photographically, people stopped talking about photographic distortion, as it was called."

In the organizations I have studied, the introduction of information technology provided just such a sense of crisis. I found a "window of opportunity" during which people who were working with the technology for the first time were ripe with questions and insights regarding the distinct qualities of their experience. As time passed (usually twelve to eighteen months), they would find ways to accommodate their understanding to the altered conditions of work, making it more difficult to extract fresh insights from beneath a new crust of familiarity. For this reason, I also sought out men and women who had experience accomplishing the same tasks in both the context of an earlier technology- pneumatic controls, paper and pencil, face-to-face interaction, mechanical equipment-and the context of information technology integrated information and control systems, on-line transaction systems, real-time information systems, and computer-conferencing systerns. This provided an experiential frame of reference and heightened their awareness of the continuities and discontinuities in the quality of their work experience.

No sector of the economy is exempt from the technological changes under way or the dilemmas they create. This book seeks to understand the generic themes of this transformation as they cut across a range of organizations engaged in what appear to be wholly distinct kinds of work and to compare and contrast the issues that arise within diverse sectors, such as the offices of a large service organization and an automated manufacturing process. As a result, the following chapters portray a diverse set of organizations-from pulp and paper mills to insurance offices to the elite precincts of an international bank. This attention to similarity and difference also bears upon my observations of the various occupational levels within a given organization-workers, clerks, managers, and professionals were each involved in my research effort. There are generic themes that unify their experiences as well as important sources of difference between them.

The organizations studied include two pulp mills and one pulp and paper mill (located in separate divisions of the American Paper Company); Metro Tel, an operating unit of a telecommunications company; the dental claims operation of Consolidated Underwriters Insurance; the offices for stock and bond transfer of a large corporation known as Universal Technology; the Brazilian offices of Global Bank, a major international financial institution; and a large pharmaceutical company called DrugCorp. (See appendix B for a more complete description of the field methodology.) Each site is not equally represented in the thematic discussions within each chapter because the nature of the technological application in a given organization or the particular kind of work in which people were engaged illuminated certain sets of themes in a particularly important way. This study sought to use the range of sites to build a comprehensive map of the territory in question, rather than to perform a comparative analysis between each organization.

The three mills studied were each in the process of implementing a new control interface based upon microprocessor technology. The level of technological innovation in each case represented the state of the art for process control technology. Two of the mills, Piney Wood and Tiger Creek, had traditional work systems with unionized work forces. They were old mills, and the conversion process represented a radical technological change. The third mill, Cedar Bluff, had been recently constructed and was considered to be one of the most automated pulp mills in the world. Its work force had been newly recruited and thus had no prior experience with other forms of pulping technology. Cedar Bluff's work system had been designed to achieve high levels of employee involvement and commitment; it emphasized worker teams and a pay-for-skills approach to compensation.

Metro Tel had recently implemented a computer-based administrative system that linked managers in a central office to workers in field locations. This was an attempt to create a technologically based administrative infrastructure to support their centralized technical operations, such as switching and repairs. The organizational structure was traditional, hierarchical, and highly centralized. Workers were members of the Communications Workers of America union, and most enjoyed long years of service with the company.

In both Consolidated Underwriter Insurance's dental claims operation and the stock and bond transfer offices of Universal Technology, clerical workers were using information technology for the first time. Each of these offices had installed high-volume transaction systems in which clerks used desktop terminals to receive and enter data.

The Brazilian offices of Global Bank represented 20 percent of the parent corporation's international revenues. Like many other banking institutions, Global Bank Brazil had shifted its strategic thinking from an emphasis on loans to an emphasis on the development of new technology-based products and services. Information, rather than money, was now recognized as the bank's most valuable commodity. As a result of Global Bank Brazil's technological sophistication and financial importance, it was chosen as a site for a pilot program developing a new generation of information technology. This technology, technically referred to as the "data base environment," was regarded as an innovation that would profoundly affect the nature of banking, with consequences for the skills and forms of organization appropriate to each banking function. Though Global Bank Brazil had not fully completed its transition to this new stage of technological deployment, this study documents the organization's efforts to grapple with the likely consequences of the technological change.

Finally, the managerial precincts of DrugCorp provided an opportunity to study one of the world's most extensive computer-conferencing systems. Managers and professionals were linked by a computer network that allowed ongoing dialogue, electronic meetings, and rapid communication. Their experiences with this technology reveal much about the emerging structure of communication within the computerized organization.

This book is structured to reflect the succession of dilemmas that typically accompany an organization's transformation to advanced computer-based technology. 

Part I is directed toward the dilemmas associated with the changing grounds of knowledge as a result of the computer mediation of work. It explores the historical role of the body in both industrial and white-collar work and depicts the emerging demand for intellective skills, that frequently supplant the body as a primary source of know-how. Drawing together data from the mills and the offices, part 1 offers a broad conceptualization of the cognitive and social-psychological requirements for developing and expressing knowledge in the computerized workplace.

Part 2 focuses upon the dilemmas of authority that develop as the new demands for intellective skills blur traditional distinctions between operational and managerial roles. It begins by tracing the historical evolution of managerial authority and proceeds to explore the way in which the managerial hierarchy can subvert the forces of change, using the experiences of the three mills as examples. The mills also illustrate how, despite these attempts to resist change, new roles and relations of authority begin to take shape.

Part 3 concerns the attempts to shore up these threatened authority relations with new techniques of control that draw upon the technology's tendency to heighten the visibility of organizational processes. Managers who doubt the strength of authority-based bonds or who prefer technical certainty to the rigors of managing face-to-face relationships are drawn to the technology as a new source of techniques for shaping the behavior of their subordinates. Their efforts engage a series of organizational responses that, ironically, weaken managerial authority even more profoundly.

The conclusion sets out a portrait of a hypothetical informated workplace. It defines a direction for managerial efforts that would take up the challenge of this historical moment and strike out on a new path. It suggests landmarks as well as pitfalls. It offers a vision.

PART ONE: KNOWLEDGE AND COMPUTER-MEDIATED WORK

CHAPTER ONE: THE LABORING BODY: SUFFERING AND SKILL IN PRODUCTION WORK

We had pleased ourselves with the delectable visions of the spiritualization of labor .... Each stroke of the hoe was to uncover some aromatic root of wisdom .... But ... the clods of earth, which we so constantly belabored and turned over and over, were never etherealized into thought. Our thoughts, on the contrary, were fast becoming cloddish. Our labor symbolized nothing and left us mentally sluggish in the dusk of the evening. - Nathaniel Hawthorne, The Bithedale Romance

THE AUTOMATIC DOORS

The bleach plant is one of the most complex and treacherous areas of a pulp mill. In Piney Wood, a large pulp plant built in the mid-1940s, railroad tank cars filled with chemicals used in the bleaching process pull up alongside the four-story structure in which dirty brown digested pulp is turned gleaming white. Each minute, 4,000 gallons of this brown mash flow through a labyrinth of pipes into a series of cylindrical vats, where they are washed, treated with chlorine-related chemicals, and bleached white. No natural light finds its way into this part of the mill. The fluorescent tubes overhead cast a greenish-yellow pall, and the air is laced with enough chemical flavor that as you breathe it, some involuntary wisdom built deep into the human body registers an assault. The floors are generally wet, particularly in the areas right around the base of one of the large vats that loom like raised craters on a moonscape. Sometimes a washer runs over, spilling soggy cellulose kneedeep across the floor. When this happens, the men put on their high rubber boots and shovel up the mess.

The five stages of the bleaching process include hundreds of operating variables. The bleach operator must monitor and control the flow of stock, chemicals, and water, judge color and viscosity, attend to time, temperature, tank levels, and surge rates-the list goes on. Before computer monitoring and control, an operator in this part of the mill would make continual rounds, checking dials and graph charts located on the equipment, opening and shutting valves, keeping an eye on vat levels, snatching a bit of pulp from a vat to check its color, sniff it, or squeeze it between his fingers ("Is it slick? Is it sticky?") to determine its density or to judge the chemical mix.

In 1981 a central control room was constructed in the bleach plant. A science fiction writer's fantasy, it is a gleaming glass bubble that seems to have erupted like a mushroom in the dark, moist, toxic atmosphere of the plant. The control room reflects a new technological era for continuous-process production, one in which microprocessor-based sensors linked to computers allow remote monitoring and control of the key process variables. In fact, the entire pulp mill was involved in this conversion from the pneumatic control technology of the 1 940s to the microprocessor-based information and control technology of the 1980s.

Inside the control room, the air is filtered and hums with the sound of the air-conditioning unit built into the wall between the control room and a small snack area. Workers sit on orthopedically designed swivel chairs covered with a royal blue fabric, facing video display terminals. The terminals, which display process information for the purposes of monitoring and control, are built into polished oak cabinets. Their screens glow with numbers, letters, and graphics in vivid red, green, and blue. The floor here is covered with slate-gray carpeting; the angled countertops on which the terminals sit are rust brown and edged in black. The walls are covered with a wheat-colored fabric and the molding repeats the polished oak of the cabinetry. The dropped ceiling is of a bronzed metal, and from it is suspended a three dimensional structure into which lights have been recessed and angled to provide the right amount of illumination without creating glare on the screens. The color scheme is repeated on the ceiling-soft tones of beige, rust, brown, and gray in a geometric design.

The terminals each face toward the front of the room-a windowed wall that opens onto the bleach plant. The steel beams, metal tanks, and maze of thick pipes visible through those windows appear to be a world away in a perpetual twilight of steam and fumes, like a city street on a misty night, silent and dimly lit. What is most striking about the juxtaposition of these two worlds, is how a man (and there were only men working in this part of the mill) traverses the boundary between them.

The control room is entered through an automatic sliding-glass door. At the push of a button, the two panels of the door part, and when you step forward, they quickly close behind you. You then find yourself facing two more automatic doors at right angles to one another. The door on the right leads to a narrow snack area with booths, cabinets, a coffee machine, and a refrigerator. The door to the left leads into the control room. It will not open until the first door has shut. This ensures that the filtered air within the control room is protected from the fumes and heat of the bleach plant. The same routine holds in reverse. When a man leaves the control room, he presses a button next to the frame on the inner door, which opens electronically. He then steps through it into the tiny chamber where he must wait for the door to seal behind him so that he can push a second button on the outer door and finally exit into the plant.

This is not what most men do when they move from the control room out into the bleach plant. They step through the inner door, but they do not wait for that door to seal behind them before opening the second door. Instead, they force their fingertips through the rubber seal down the middle of the outer door and, with a mighty heft of their shoulders, pry open the seam and wrench the door apart. Hour after hour, shift after shift, week after week, too many men pit the strength in their arms and shoulders against the electronic mechanism that controls the doors. Three years after the construction of the sleek, glittering glass bubble, the outer door no longer closes tightly. A gap of several inches, running down the center between the two panels of glass, looks like a battle wound. The door is crippled.

The door is broke now because the men pushed it too hard comin' in and out," says one operator. In talking to the men about this occurrence, so mundane as almost to defy reflection, I hear not only a simple impatience and frustration but also something deeper: a forward momentum of their bodies, whose physical power seems trivialized by the new circumstances of their work; a boyish energy that wants to break free; a subtle rebellion against the preprogrammed design that orders their environment and always knows best. Yet these are the men who also complained, "The fumes in the bleach plant will kill you. You can't take that chlorine no matter how big and bad you are. It will bleach your brains and no one (in management) gives a damn."

Technology represents intelligence systematically applied to the problem of the body. It functions to amplify and surpass the organic limits of the body; it compensates for the body's fragility and vulnerability. Industrial technology has substituted for the human body in many of the processes associated with production and so has redefined the limits of production formerly imposed by the body. As a result, society's capacity to produce things has been extended in a way that is unprecedented in human history. This achievement has not been without its costs, however. In diminishing the role of the worker's body in the labor process, industrial technology has also tended to diminish the importance of the worker. In creating jobs that require less human effort, industrial technology has also been used to create jobs that require less human talent. In creating jobs that demand less of the body, industrial production has also tended to create jobs that give less to the body, in terms of opportunities to accrue knowledge in the production process. These two-sided consequences have been fundamental for the growth and development of the industrial bureaucracy, which has depended upon the rationalization and centralization of knowledge as the basis of control.

These consequences also help explain the worker's historical ambivalence toward automation. It is an ambivalence that draws upon the loathing as well as the commitment that human beings can experience toward their work. Throughout most of human history, work has inescapably meant the exertion and often the depletion of the worker's body. Yet only in the context of such exertion was it possible to learn a trade and to master skills. Since the industrial revolution, the accelerated progress of automation has generally meant a reduction in the amount of effort required of the human body in the labor process. It has also tended to reduce the quality of skills that a worker must bring to the activity of making something. Industrial technology has been developed in a manner that increases its capacity to spare the human body, while at the same time it has usurped opportunities for the development and performance of skills that only the body can learn and remember. In their treatment of the automatic doors, the bleach plant workers have created a living metaphor that reflects this ambivalence toward automation. They want to be protected from toxic fumes, but they simultaneously feel a stubborn rebellion against a structure that no longer requires either the strength or the know-how lodged in their bodies.

The progress of automation has been associated with both a general decline in the degree of know-how required of the worker and a decline in the degree of physical punishment to which he or she must be subjected. Information technology, however, does have the potential to redirect the historical trajectory of automation. The intrinsic power of its informating capacity can change the basis upon which knowledge is developed and applied in the industrial production process by lifting knowledge entirely out of the body's domain. The new technology signals the transposition of work activities to the abstract domain of information. Toil no longer implies physical depletion. "Work" becomes the manipulation of symbols, and when this occurs, the nature of skill is redefined. The application of technology that preserves the body may no longer imply the destruction of knowledge; instead, it may imply the reconstruction of knowledge of a different sort.

The significance of this transposition is impossible to grasp without reference to the grounds of knowledge for workers in the past. In the factory, knowledge was intimately bound up with the efforts of the laboring body. The development of industrial technology can be read as a chronicle of attempts to grapple with the body's role in production as a source of both effort and skill and with the specific responses these attempts have evoked from workers and managers. The centrality of the body's historical meaning for production has informed the selfunderstanding of managers and workers and the relationship between them. It has also been a salient force guiding the development and application of manufacturing technology. A better understanding of what the body has meant for industrial work and how it has been linked to the logic of automation will sharpen an appreciation of the character of the current transformation and its capacity to provoke comprehensive change in the relationships that structure the workplace. Before deciphering the present or imagining the future, it is first necessary to take ourselves out of the twentieth century and return, if only briefly, to a time when the nature of work was both simpler and more miserable, a time when work was above all the problem of the laboring body.

THE FRONTIER OF CONTEMPT

The world of production, where primary materials are processed and goods are manufactured, has long been marked by a great divide between those who give of their bodies and those who are exempt from physical depletion. Yet those exempted from bodily alteration may give of themselves in other ways. Their physical presence may be required for purposes of interpersonal influence, communication, and coordination. They may give of their time and attention in both supervisory and analytical activities, or they may give of themselves more abstractly as sources of investment capital or expert knowledge. However, the groups that stand on either side of this divide constitute fundamentally distinct modes of involvement with the production enterprise. The experiential distance between them is one important living source of the divergent interests in terms of which workers and managers have tended to define themselves. Workers facing the physical requirements of labor seek ways to preserve their bodies from exertion, while managers are charged with extracting the maximum feasible effort from the work force.

This divide has been an important characteristic of social hierarchies in virtually every culture known to the historical record. Wealth and power have everywhere meant an escape from toil: the unequal distribution and concentration of wealth and power within a small group is a phenomenon of such universality that the French historian Fernand Braudel has called it a "constant law of societies, a structural law that admits of no exception." 1 In the societies of preindustrial Europe, the decisive challenge of social mobility was to permanently rid oneself of the stigma of physical labor and then to repudiate the commercial activities that made such an escape from work possible. European nobility defined itself by the gulf it created between its members and the hardship of labor. Of the bourgeois families who gained access to the highest ranks of society Braudel writes that "the only feature they had in common with the authentic nobility was their rejection of trade or labor, their taste for idleness or rather leisure, which was for them synonymous with reading and learned discussion with their peers. "

This repugnance toward labor rides a long wave in Western history, a wave that has not, even yet, reached its crest. In the religious zeal of the early Middle Ages, trades that trafficked in money were considered illicit, materialism being an indication of a lack of faith. With the growing urbanization, more detailed division of labor, and accelerated mercantilism of the late Middle Ages, however, this view of economic activity took an important turn: "A new frontier of contempt arose right in the midst of the new classes and even within professions .... Work itself no longer constituted the distinction between respectable and contemptible categories; instead, it was manual labor that had come to be the key factor in the frontier between respect and contempt .... Across from the manouvriers and brassiers who worked with hands and arms, was the patrician world, the new aristocracy, consisting of all those who did no manual labor: employers and rentiers. " 3 Even the guilds were influenced by the contempt toward manual work. Some of them required entrants to have relinquished their trade for twelve months before admission. In 1241 a municipality in Flanders excluded from the urban magistracy all robbers, coiners, and "those who have not given up all manual work for at least one year. " 

The Middle Ages produced a conception of labor infused with a loathing drawn from three traditions: ( 1) the Greco-Roman legacy that associated labor with slavery, (2) the barbarian heritage that disdained those who worked the land and extolled the warrior who gained his livelihood in bloody booty, and (3) Judeo-Christian theology that admired contemplation over action. For centuries European literature and iconography depicted peasants as huge-headed monsters or wild beasts lurking the depths of Europe's dark forests. 5 Labor came to humanity with the fall from grace and was at best a penitential sacrifice enabling purity through humiliation. Labor was toil, distress, trouble, fatigue an exertion both painful and compulsory. Labor was our animal condition, struggling to survive in dirt and darkness.

Freedom from the necessity of labor has been a prominent feature of most utopian thinking. English civil war sects awaited the coming of the Fifth Monarchy because it was said that it would abolish painful labor. Bishop Godwin's Man in the Moone, published in 1638, reported on a society in which "food groweth everywhaer without labour," while all the necessities of life were amply provided. Other utopian writers of the period, Campanella, Winstanley, Bellars, and More, saw the reduction of labor as an important feature of a wholesome moral life. 6 Sir Thomas More's Utopia limited the consumption of commodities to the "necessary" and the "comfortable," in order that any surplus labor could be devoted to learning. The six-hour work day was seen as adequate, and if it turned out to be excessive, the community would further curtail the number of hours assigned to work: "What time may possibly be spared from the necessary occupations and affairs of the Commonwealth, all that the Citizens should withdraw from the bodily service to the free liberty of the mind and garnishing of the same." 

The recent English translation of the historian Norbert Elias's The Civilizin9 Process has helped shed light on a deeper explanation for this repugnance toward work. 8 While a full description of Elias's pathbreaking analysis is beyond the scope of this discussion, it is worth highlighting the skeleton of his discoveries as they contribute to an understanding of the enduring relationship between the universality of social hierarchy and the fact that physical labor is everywhere considered to constitute its lowest echelons.

Elias studied books of etiquette and other documentation of daily life from the early Middle Ages through the eighteenth century. He discovered that norms of daily conduct, particularly those that bear upon bodily functions, have changed radically throughout the course of the centuries. When in I 5 30 Erasmus wrote his treatise On Civility in Children, he provided a portrait, refracted through his admonitions, of contemporary standards of behavior. Our own sensibilities are overcome with repugnance and horror at behavior that was accepted as routine in the sixteenth century. Elias points out the "infinite care and matter-of-factness" with which Erasmus addressed habits concerning bodily functions in his effort to encourage more "civilized" standards: "There should be no snot on the nostrils .... A peasant wipes his nose on his cap and coat, a sausage maker on his arm and elbow. It does not show much more propriety to use one's hand and then wipe it on one's clothing .... It is more decent to take up the snot in a cloth, preferably while turning away. If when blowing the nose with two fingers something falls to the ground, it must be immediately trodden away with the foot. The same applies to spittle. "

Elias found that behaviors related to table manners, bodily functions, nose blowing, spitting, sleeping, sex, and aggression that we have come to consider barbaric and disgusting were once routine. He argues that the process of curbing these behaviors was set into motion by, and in tum helped to promote, stable, centralized forms of social organization. The embryo for these modem forms was evident in the court societies that by the end of the Middle Ages, had begun to spread across Europe.

Here were created the models of more pacified social intercourse which more or less all classes needed, following the transformation of European society at the end of the Middle Ages; here the coarse habits, the wilder, more uninhibited customs of medieval society with its warrior upper class, the corollaries of an uncertain, constantly threatened life, were "softened", "polished" and "civilized". The pressure of court life, the vying for the favour of the prince or the "great"; then, more generally, the necessity to distinguish oneself from others and to fight for opportunities with relatively peaceful means, through intrigue and diplomacy, enforced a constraint on the affects, a self-discipline and self-control, a peculiarly courtly rationality .... This increased restraint and regulation of elementary urges is bound up with increased social constraint, the growing dependence of the nobility on the central lord, the king or prince.

Elias reminds us that violence was inscribed into the very structure of medieval society. Rape and death, the hunting of men and animals, were part of everyday life. "The documents suggest unimaginable emotional outbursts in which-with rare exceptions-everyone who is able abandons himself to extreme pleasures of ferocity, murder, torture, destruction, and sadism." 11 Everyday objects like the fork (at first a source of mockery and still a rare luxury in the seventeenth century), the handkerchief, and the nightgown, or the new sense of repulsion felt at the sight of humans defecating, spitting a piece of food back into the common bowl, or picking their teeth with the communal knife, all symbolized the progress of the civilizing process-an increased control over and distance from the animal life of the human body: "People have begun to construct an affective wall between their bodies and those of others. The fork has been one of the means of drawing distances between other people's bodies and one's own. One repulses the body, isolates it, feels ashamed of it, tries to ignore it .... For many centuries, this wall did not exist."

Elias's discoveries illuminate an unconscious dimension of Western history. The consolidation of stable social hierarchies based upon centralized power and the rule of law demanded a new level of behavioral control. The body had to be reinterpreted as a source of disgust and as an object of discipline. 13 This reinterpretation, and the forms of conduct that developed from it, first took root at the highest levels of the emerging society, where the pressures of interdependence and political opportunism were most acute. These in turn became the models for behavior that successively lower social strata would imitate and finally assimilate.

It is easy to see that gradations in status are related to gradations in power, but Elias's work alerts us to the fact that such differences in status parallel another axis of social comparison whose levels are marked by degrees of distance from the body's own animal life and the animal life of surrounding bodies. The ability to maintain one's distance from the body developed as an important sign of hierarchical position. It has served to intensify the repugnance toward forms of activity that involve the body in sweating, heaving, grunting, hauling, and carting; that expose the body to pain and discomfort from extreme temperature, extended muscular effort, inclement weather, or hurtful substances; and that so immerse human consciousness in the sentient surroundings of effort and fatigue that one fails to notice (or care) whether the nose is dripping, the sweated body is giving off an unbearable stench, or filthy fingers have been used to grab a piece of food-the very proprieties that came to be seen as part of a complex of behaviors distinguishing the barbaric from the civilized.

There is reason enough to want to avoid exhausting work, but the constancy of repugnance was not confined to forms of labor that were extremely punishing. As noted earlier, in the membership practices of some guilds, even the craftsworker was liable to be an object of contempt because of the manual nature of that work. Such repugnance is in itself an act of distancing. It is both a rejection of the animal body and an affirmation of one's ability to translate the impulses of that body into the infinitely more subtle behavioral codes that mediate power in complex organizations. Once this translation occurs, the body is no longer the vehicle for involuntary affective or physical displays. Instead, it becomes the instrument of carefully crafted gestures and behaviors designed to achieve a calculated effect in an environment where interpersonal influence and even a kind of rudimentary psychological insight are critical to success. In the interpersonal world of court society, the body's knowledge involved the ability to be attuned to the psychological needs and demands of others, particularly of superiors, and to produce subtly detailed nonverbal behavior that reflected this awareness.

The court is a kind of stock exchange; as in every "good society", an estimate of the value of each individual is continuously being formed. But here his value has its real foundation ... in the favour he enjoys with the king, the influence he has with other mighty ones .... All this, favour, influence, importance, this whole complex and dangerous game in which physical force and direct affective outbursts are prohibited and a threat to existence, demands of each participant a constant foresight and an exact knowledge of every other, of his position and value in the network of courtly opinions; it exacts precise attunement of his own behavior to this value .... "A man who knows the court is master of his gestures, of his eyes and his expression; he is deep, impenetrable. He dissimulates the bad turns he does, smiles at his enemies, suppresses his ill-temper, disguises his passions, disavows his heart, acts against his feelings."

The tension between hierarchical status and the animal body also may be sustained by the psychologic need to defend oneself from the fact of the body's tragic weakness. The animality of the body is a source of repulsive events that must be controlled, and the most repulsive and least controllable of these events is death itself. Death is the inevitable conclusion ordained by the animality of the body and poses a series of challenges (in addition to the central challenge, which is the cessation of life). Death underscores the commonality of all who share life; death highlights the vulnerability of the body; death reminds the living of the ultimate uncontrollability of the body. Each of these problems is in a way addressed by the act of distantiation. Hierarchical distance rejects the display of animality that is common and deindividualized for civilized conduct, intricately fashioned by personality, wit, and will. Distantiation, in promoting forms of control over the body, can protect its fragility and, in some ways, its health. Finally, distantiation allows us to avoid reminders of animality, thus making it possible to suppress an awareness of the body's inevitable decline.

The close relationship between the rejection of animality and the progress of civilization is at the heart of our modern conception of work. Marx expressed this relationship when he argued that mastery of the material world was the basis upon which man humanized himself and developed culture. This in fact is the civilizing process; humanization means tempering animality with rationality, aesthetic grace, and moral choice. It is a process that has informed much of the impetus toward the extension of material culture powerfully exemplified in the development of industrial technology, which simultaneously frees the production process from the organic limits of the body, frees consumers from having to exercise bodily effort in order to enjoy the panoply of goods produced by the machine system, shapes workers who are capable of exercising considerable control over their own spontaneous impulses (and so can conform to the behavioral demands of mechanized production), and gradually diminishes the most painful forms of exertion associated with the work of making things.

Indeed, the worker's body posed a complicated set of problems for industrial management. Industrial work depended upon the laboring body as much for its raw energy as for its special gifts. In many industries, the worker's body remained central to production well into the early decades of the twentieth century. Only then, in many cases, was labor-saving technology diffused widely enough to substantially alter the role of the body in the production process. If work was to be performed economically and effectively, then the impulsive behavior associated with the body's animality would have to be disciplined. The members of court society were required to turn their bodies into instruments of interpersonal influence, instruments for acting-with. Industrial workers were also required to turn their bodies into instruments, but instruments for acting-on-for producing calculated effects on material and equipment. In the following section, we will see how the first generations of factory owners and their managers were frustrated, confounded, and sometimes ruined as they searched for the methods by which to translate the animal body into a more precise instrument that could be applied to increasingly systematized processes of production.

THE EARLY FACTORY AND THE PROBLEM OF THE BODY

There is ample evidence that throughout the fifteenth and sixteenth centuries, workers were not silent in their degradation. Social cleavage took its toll in thousands of peasant insurrections as well as violent disturbances among urban workers. 15 It was not until the industrial revolution, however, that the focus of the conflict between worker and employer came to rest on the detailed performance of the worker's body and the degree of discipline to which the body might legitimately be subjected.

Consider the case of Britain at the brink of industrialization during the second half of the eighteenth century. For all the bone-crushing labor demanded of the agricultural worker or the cottage weaver, the traditional rhythms of exertion and play were a world removed from the behavioral demands of industrial production. Work patterns were irregular, alternating between intense effort and idleness. Most work activities emanated from the home, and the distractions of the family, the taverns, and the social web of the community limited any undivided commitment to work.

Cottage workers, upon whom most textile production depended, were relatively impervious to the middleman's demand for heightened productivity. Their inclination to physical exertion was guided more by their own immediate needs than by acquisitive ambitions. Throughout the eighteenth century, the British Parliament passed legislation requiring ever-shorter turnaround times for finished goods from domestic workers and imposing increasingly severe sanctions on those who did not comply. Such sanctions were difficult to enforce; finally, only the pressure of immediate supervision was able to induce a greater level and consistency of effort from workers. 16 The need to intensify production was the driving force behind the establishment of the early factories and workshops, even before the widespread diffusion of the steam engine. 17 There is evidence that workers submitted to the physical rigors of factory discipline only when other alternatives had been exhausted. 18 But even those employers who were able to recruit a labor force still faced the haphazard and spasmodic rhythms with which their new employees approached their work.

The employers deplored the fact that the old subsistence mentality had carried over into the new work settings. Of the piece-rate worker they complained, "At the precise inch of cloth he stopped, in the mines, at the necessary pound of coal." 19 Workers in their turn bemoaned the loss of freedom and rebelled at the prospect of long confinement and steady production. The highlander, it was said, "never sits at ease at a loom; it is like putting a deer in the plough." 20 Attendance was irregular; workers would sometimes stay away from the job for days and send for their wages at the end of the week. In South Wales during the 1840s it was estimated that workers were absent 20 percent of the year, a figure that reached as high as 33 percent during the fortnight after their monthly payday.

One study of Birmingham, England, from 1766 to 1876, found that well into the nineteenth century, workers continued to celebrate Saint Monday-a weekly day of leisure spent in the alehouse enjoying drink, bar games, entertainments, "pugilism," and animal fights. 22 The tradition of Saint Monday followed from the bouts of weekend drinking and represented deeply held attitudes toward a potential surplus of wages: "The men ... (are] regulated by the expense of their families, and their necessities; it is very well known that they will not go further than necessity prompts them."

The industrial entrepreneurs tried, usually without success, to prohibit the observance of Monday as a holiday. Boulton and Watt's first enterprise foundered on the continual drunkenness of their work force. 24 The owner of a button-making factory decided that although he would not be able to control his workers, he would make an effort to train his apprentices in more industrious work habits. His diary records his frustration: "This evening Edward Lingard's misconduct in going to the Public House in the afternoon for drink, contrary to my inclination and notwithstanding I had forbidden him from it only yesterday- this I say, and meeting him on his way back, induced me hastily to strike him. With which my middle finger was so stunned as to give me much pain." 25 In addition to the time lost through observing Monday as a holiday, harvest time and other traditional feast days kept workers away. In 1776 the famous Josiah Wedgwood who pioneered new techniques of pottery production and business management, wrote to a colleague: "Our men have been at play 7 days this week, it being Burslem Wakes. I have rough'd and smoothed them over, & promised them a long Xmas, but I know it is all in vain, for Wakes must be observed though the World was to end with them."

Nineteenth-century American industrialists faced a similar set of problems when it came to honing the worker's body as an instrument of production. The owner of a Pennsylvania ironworks complained of frequent "frolicking" that sometimes lasted for days, along with hunting, harvesting, wedding parties, and holiday celebrations. One manufacturer filled his diary with these notes: "All hands drunk; Jacob Vending hunting; molders all agree to quit work and went to the beach. Peter Cox very drunk and gone to bed .... Edward Rutter off a-drinking. It was reported he got drunk on cheese." 27 In 1817 a Medford shipbuilder refused his men grog privileges and they all quit. 28 The ship's carpenter in one New York shipyard describes the typical workday: cakes and pastries in the early morning and again in the late morning, a trip to the grog shop by eleven for whiskey, a big lunch at half past three, a visit from the candyman at five, and supper, ending the workday, at sundown. He recalled one worker who left for grog ten times a day. A cigar manufacturer complained that his men worked no more than two or three hours a day; the rest of the time was spent in the beer saloon, playing pinochle. Coopers were famous for a four-day work week; and the potters in Trenton, New Jersey, immigrants from Staffordshire, were known to work in "great bursts of activity" and then lay off for several days.

We can see that the early apostles of industrialism had to confront the still-rudimentary progress of the civilizing process as it bore upon work behavior. The spontaneous, instinctually gratifying behavior of the new industrial worker had to be suppressed, and that energy channeled into the controlled behavior demanded by the intensification of production. The factory became a pedagogic institution where the new standards of conduct and sensibility, generally referred to as "labor discipline," would be learned. The exhaustive measures that employers took to thwart the animal body are a sign of its very intractability.

The notion of labor discipline signaled a very concrete problem: how to get the human body to remain in one place, pay attention, and perform consistently over a fixed period of time. Elaborate systems of fines were developed, minutely tailored to extinguish particular expressions of the impulsive body. For example, many fines sought to keep the body stationary. One work rule at Haslingden Mill about 1830 read, "Any person found from the usual place of work, except for necessary purposes, or talking with anyone out of their own alley, will be fined." 31 The Hammonds report fines at a textile mill near Manchester for "going further than the roving room door when fetching rovings" and for "any spinner found in another's wheel gate." 32 Ashworth fined his weavers for being found "out of the room. " 33 Other fines addressed the sounds that emitted from the body at work: punishable infractions included singing, whistling, swearing, and yelling. Some fines, intended to enforce a fixity of gaze and attention, punished workers for opening a window. Still other fines concerned the body's smell and appearance: workers were fined for being dirty, for not changing their shirts at least twice a week, and for spitting. Finally, there were fines to discourage aggressiveness, sexuality, and disorderliness-throwing water, seducing females, being drunk, arriving late, or not showing up at all.

American employers also found that only the severest fining policies had an effect on work habits. Workers were routinely fined up to half a day's pay for singing, talking, visiting, or being late. 35 In 18 59 a mill agent in Chicopee, Massachusetts, complained of the general indisposition of factory hands toward steady work, and in the years preceding World War I, it was still not unusual for as much as one-tenth of the work force to be missing on a given day. 36 Quit rates around the turn of the century were high-textile mills, meat-packing plants, automobile plants, steel mills, and machine works often showed annual turnover rates of 100 percent. 37 One survey showed that between 1905 and 1917, the majority of industrial workers changed jobs at least once every three years. Between 1 907 and 1910, turnover in the woolen industry was between 113 percent and 163 percent. It reached 232 percent in New York City garment shops in 1912, 2 52 percent in a sample of Detroit factories in 1916, and 370 percent in the Ford Motor Company in 1913.

These quit rates reflect not only the ambivalence and enduring orneriness of American workers but also the increasingly severe pressure that employers and managers brought to bear on dysfunctional, uncontrolled, and irregular behavior. Many scholars have argued that the introduction of steam power (and, later, other forms of expensive equipment) did more to consolidate the new behavioral norms than the earlier systems of fines alone. 39 This was in part because employers, in an effort to fully utilize their capital investment, became more ruthless in their willingness to dismiss workers who did not comply with the regularity of effort required to efficiently exploit the new machinery and in part because workers had to conform to a pace and quality of production increasingly driven by the machine, rather than by their own traditions and habits of organization.

As traditional working conditions grew scarce, worker resistance to labor discipline itself became more rationalized. The trade union movement set itself to limiting working hours, maintaining employment levels, and protecting wages, but a parallel approach of greater informality and striking continuity with older traditions also emerged. Workers began to act self-consciously to limit their efforts and so preserve their bodies. As early as 17 57, Josiah Tucker wrote in a pamphlet entitled "Instructions for Travelers" a description of domestic weavers transplanted to factory life. He provides one of the earlier descriptions of what was to become a central strategy for the industrial worker-withholding effort: "They think it no crime to get as much wages and to do as little for it as they possibly can, to lie and cheat and do any other bad thing, provided it is only against their master whom they look upon as their common enemy, with whom no faith is to be kept .... Their only happiness is to get drunk and make life pass away with as little thought as possible. "

Though skilled workers were indignant at the idea of deliberate slacking, by the end of the nineteenth century the British unions had come to recognize the characteristics of the trade cycle and to think of their effort as a commodity to be withheld or controlled in the service of free market bargaining. 41 Work banking, goldbricking, soldiering, are the modern terms that convey the legacy of this earlier clash between two wildly different conceptions of the standard of work discipline to which a body should conform.

In the American factory at the turn of the century, the foreman had primary responsibility for implementing management's goals: he was the "undisputed ruler of his department, gang, crew, or ship." 42 When in 1912 a congressional committee investigated the United States Steel Corporation, they attempted to understand just how the foreman functioned. They learned that foremen throughout American industry practiced something known as the "driving method," an approach to supervision that combined authoritarian combativeness with physical intimidation in order to extract the maximum effort from the worker. The driving method was well suited to work that depended upon the consistent exertion of the human body. The foreman's profanity, threats, and punishments were complemented by the workers' methods for limiting output. Methods of withholding labor varied somewhat from industry to industry and might be modified according to economic conditions, but the underlying spirit was everywhere the same-to protect the body by tempering exertion.

There were numerous stories of new employees who were approached by older, presumably wiser, workmen. "See here, young fellow, you're working too fast. You'll spoil our job for us if you don't go slower." If a friendly admonition did not have the desired effect and the man was judged a "rooter" or "rusher," social pressure, threats, and even violence might follow.

The protective response of so many workers to the demands made on their bodies is also the source of many work procedures that have been formalized and institutionalized in labor contracts and work rules. Studies by Lloyd Ulman and Sumner Slichter have shown how glassworkers', textile workers', and metalworkers' unions used agreements on production standards to restrict the driving method of supervision. 44 Work rules codified restrictive labor withholding practices, transforming them from informal methods of self-protection to deliberate contractual agreements.

THE PARADOX OF THE BODY

Until now we have treated the body in one of its aspects-as the scene and source of effort. This was the body that had to be disciplined if effort was to be drawn forth in a way that complemented the demands of an intensified, collectivized, and regulated production process. But the body as the scene of effort, the body to be protected, held a special paradox. For it was also through the body's exertions that learning occurred, and for those who were to become skilled workers, long years of physically demanding experience were an unavoidable requirement. This was the reward of physical involvement, since there was virtually no access to the craftsperson's skills short of an investment in years of effort. The body's meaning for production was not just to be an animal source of motive power or a pair of hands for an endless series of monotonous performances. Where the skilled worker was concerned, the body's sentience was also highly structured by a felt knowledge of materials and procedures. These twin functions of the body as a source of skill and as a source of effort-complicated the way in which employers confronted the problems of labor discipline and the speed with which they invested in labor-saving technology, just as they complicated workers' responses to that technology.

The historian Raphael Samuel has demonstrated that the body continued to play a central role in production, as a source of both effort and skill, throughout the nineteenth century in activities as diverse as mining, and quarrying, agriculture, gardening, and other forms of food production, construction, glass, pottery, and leather trades, woodworking, and metallurgy. 46 His examples are drawn from nineteenth-century Britain, but many of the labor processes he describes were prevalent in America as well.

There were few segments of the mid-Victorian British economy that steam power and machinery had not touched, but fewer still did not depend upon hand technology. Many factors impeded the progress of labor-saving machinery. Many trade unions were successful in resisting the encroachment of machinery on their crafts. From the employers' side, however, there were other compelling reasons. Wages were low, and the possibilities of increasing productivity with hand technology had not yet been exhausted. Employers often found other sources of efficiency in an increasingly minute division of labor and in the use of cheaper labor-saving materials. Market uncertainties made capital investment risky; besides, there was still a plentiful supply of cheap labor. Skill was also an important part of the problem. Ebullient descriptions of the new forms of "self-acting" machinery dominated trade journals of the day, but the new, more automatic machines were often fraught with technical difficulties and could not be relied upon as a substitute for skilled work. Moreover, consumers continued to value the quality of work that only a craftsworker could produce.

Samuel describes the wide variety of jobs that required nothing more than the effort of the laboring body. Coal was excavated by pick and shovel-"tools of the most primitive description, requiring the utmost amount of bodily exertion." 47 Clay-getting required working with a heavy pick. Masses of slime had to be stirred and trampled into the right consistency. Bakeries produced bread almost entirely by manual labor, the hardest operation being that of preparing the dough, "usually carried on in one dark corner of a cellar, by a man, stripped naked down to the waist, and painfully engaged in extricating his fingers from a gluey mass into which he furiously plunges alternately his clenched fists. "

There were many occupations in which the pain and physical alteration that resulted from such exertion were inseparable from the subtle and complex skills from which a craftsperson derived pride and economic power. One account of candy making in an Edinburgh factory ends this way: "The whole process ... requires great skill in the manipulation and it also requires the most severe and continuous muscular labor. We know, indeed, of no other kind of labour that requires more. Not a muscle or joint of the whole body remains inactive. It has quite a marvelous effect in taking down superfluous fat. It is well know that a stout man taken perhaps from lozenge making, and put to work on the hot pan, becomes in six weeks converted into a living skeleton. "

In glassmaking, everything was done by hand, "the gatherers taking the 'metal' from the furnace at the end of an iron rod, the blower shaping the body of the bottle with his breath, while the maker who finished the bottle off ... tooled the neck with a light spring-handled pair of tongs. Each bottle was individually made no matter what household, shop or tavern it was destined for. " 50 There were steady inroads made by machinery in this industry, but despite its labor-saving potential, glassmakers and their union organizations bitterly resisted and successfully impeded its progress. In 1878 one observer summarized the glassmakers' ambivalence: "If in many industries the substitution of mechanical for manual labor offers important advantages because ... it decreases a man's fatigue, we do not think it will have the same effect on the absolutely special work of the glass industry, and we fear that in depriving glassworkers of difficult tasks we will destroy their skill as well as the artistic talents of which glassmakers have the right to be proud." 

Pottery work was very physical and involved personal handling of the clay at every stage of production. Hands, thumbs, fingers, and palms were the sources of a delicately nuanced skill that guided the throwing and shaping of each item: "Dippers prepared the ware for firing by steeping it in a glaze tub. The right hand and arm are plunged nearly up to the elbow as he passes the piece of ware through the liquid .... Then with a rapid circular movement of the hand ... a movement that only years of practice can teach ... he makes the glaze flow thin and even over the surface. "

In the leather trades, the process of currying, which softened the leather for saddlery, coach linings, and shoes, was a highly skilled trade that required a laborer's strength to perform the heavy work of slashing, shaving, and pummeling the hide. lronmaking was another industry that depended upon combinations of skill and strength. New technology utilizing steam power to pound and roll metal together, and new puddling furnaces based on chemical reactions rather than mechanical action, had made the production process cheaper and had increased output. These innovations were not entirely labor-saving, however. In many cases they required new skills, even as they increased the physical demands of work. "The puddler, who had the key role in the new process, was given a task that was simultaneously highly skilled and exhausting, turning a viscous mass of liquid into metal. He worked in conditions of tremendous heat, violently agitating the metal as it boiled, rabbling it from side to side in the furnace, and then gathering it at the end of a rod while the molten liquid thickened .... The men had to relieve each other every few minutes, so great was the exertion and so intense the heat, but even so it was said that every ounce of excess fat was drained from them .... Few puddlers, it was said, lived beyond the age of 50."

These are just a few examples of the body's central presence as the source of both effort and skill in myriad industrial operations throughout the nineteenth century. In many cases, machinery was used to replace humans in supplying the motive power for various subprocesses of production. In most trades, though, labor-saving machinery developed slowly, and many factors inhibited its progress. Sometimes the new machinery, in amplifying the capacity of the human body to perform a given operation and thus increasing output, could also intensify the human participation that was required and thus exacerbate the problems of physical depletion.

Samuel notes that labor-saving machinery spread more rapidly in America, owing to a scarcity of labor and consequent higher wages. However, the historian of technology David Hounshell has recently shown that in some of the largest and most successful American firms, handwork and skilled machine work prevailed during most of the nineteenth century. For example, the Singer Sewing Machine Company was not able to produce perfectly interchangeable parts. As a result, they relied on skilled fitters to assemble each product. The McCormick Reaper Works employed crude manufacturing techniques. Production depended upon skilled machinists, blacksmiths, carpenters, and molders. 54 In 1 911 an observer of the American steel industry tried to convey the notion that skilled work did not imply diminished physical exertion: "New skills, like the puddler's, the catchers, or the machinists included very heavy manual work. As one worker speaking about working in a steel mill declared: 'Hard! I guess it's hard. I lost forty pounds the first three months I came into the business. It sweats the life out of a man!' The differences between the work performed by the skilled workers and the laborers was not of an "intellectual" versus manual activity. The difference lay in the content of a similarly heavy manual work: a content of rationality of participation for skilled workers versus one of total indifference for laborers.

The work of the skilled craftsperson may not have been "intellectual," but it was knowledgeable. These nineteenth-century workers participated in a form of knowledge that had always defined the activity of making things. It was knowledge that accrues to the sentient body in the course of its activity; knowledge inscribed in the laboring bodyin hands, fingertips, wrists, feet, nose, eyes, ears, skin, muscles, shoulders, arms, and legs-as surely as it was inscribed in the brain. It was knowledge filled with intimate detail of materials and ambience-the color and consistency of metal as it was thrust into a blazing fire, the smooth finish of the clay as it gave up its moisture, the supple feel of the leather as it was beaten and stretched, the strength and delicacy of glass as it was filled with human breath. These details were known, though in the practical action of production work, they were rarely made explicit. Few of those who had such knowledge would have been able to explain, rationalize, or articulate it. Such skills were learned through observation, imitation, and action more than they were taught, reflected upon, or verbalized. For example, James J. Davis, later to become Warren Harding's Secretary of Labor, learned the skill of puddling iron by working as his father's helper in a Pennsylvania foundry: "None of us ever went to school and learned the chemistry of it from books .... We learned the trick by doing it, standing with our faces in the scorching heat while our hands puddled the metal in its glaring bath. "

Though this form of knowledge evades explication, it is not fragile; on the contrary, it is extremely robust. The swimmer who has been away from the water for a year, the woman who has not ridden a bicycle since childhood, the grandmother who has not held an infant since she weaned her last, the carpenter who has retired his tools, the guitarist who abandoned her instrument, each of these can confront the object of their skills-the sea, the bicycle, the crying infant, the wood, the guitar-and find that the knowledge they once possessed remains, ready to be activated. Such knowledge is hard won and not easily lost. Within moments, the arms, and legs find their angle and rhythm as they cut through the water, the bicycle moves swiftly, the child is calmed, the chisel, plane, hammer, and saw find the pathways and resonance of the wood, and the fingers rediscover their agility against the strings of the guitar.

Effort may have signaled sacrifice and self-protection, but it was also the occasion and context for the development of this intimate, robust, detailed, and implicit knowledge. Such knowledge formed the basis of the worker's power. Historian David Montgomery has called this the "functional autonomy" of the craftsperson, derived through decades of sustained physical involvement during which the knowledge of each craft was systematized, not in explicit rules, but in the course of practical action. 57 It is appropriate that such knowledge be referred to as "know-how," for it was knowledge that derived from action and displayed itself in action, knowledge that meant knowing how to do, to make, to act-on.

The craftsworker's know-how was also an important source of social integration. The foreman was typically a worker who had turned his wide experience into superior competence. He achieved his position by virtue of his technical skill, and such opportunities were a real source of mobility for an ambitious worker. 58 The fact that workers were required to "use up" their bodies kept them distinct from those who employed them, but the skills mastered in physical activity provided an opportunity for independence, mobility, and identification with superiors.

Such experience-based knowledge had its weaknesses, too. When it came to improving work methods or adapting them to new techniques and business conditions, the practical know-how of the traditional craftsworker could be limiting. 59 With the growing complexity and size of factories, expanding markets that exerted a strong demand for an increase in the volume of production, and a rising engineering profession, there emerged a new and pressing concern to systematize the administration, control, coordination, and planning of factory work.

The man who emerged as the chief symbol of the rational approach to management was Frederick Taylor. Though much has been written on Taylor and the philosophy and methods of scientific management, it is worth highlighting a few central themes for three reasons. 60 First, Taylorism explicitly treats the worker's body in its two dimensions as a source of effort and as a source of skill. Second, workers' responses to Taylorism reveal the ambivalence that this dual role of the body can create, as it did among the bleach plant operators or the nineteenth century glassmakers. Third, the logic that motivated the early purveyors and adapters of scientific management has continued to dominate the course of automation in the twentieth-century workplace. As will be argued later, it is a logic that must undergo a fundamental reevaluation as information technology is widely adapted to productive activity.

THE PURIFICATION OF EFFORT

The agenda for scientific management was to increase productivity by streamlining and rationalizing factory operation from cost accounting and supervision to the dullest job on the shop floor. Efficiency was the mania, and to achieve efficiency, it would be necessary to penetrate the labor process and force it to yield up its secrets. In order that effort be rationalized, the worker's skills had to be made explicit. In many cases, workers' perceptions of their own interests prevented them from articulating their know-how, but there was yet another involuntary barrier. These skills did not easily yield themselves to explication; they were embedded in the ways of the body, in the knacks and know-how of the craftsworker.

Proponents of scientific management believed that observing and explicating workers' activity was nothing less than scientific research. Their goal was to slice to the core of an action, preserving what was necessary and discarding the rest as the sedimentation of tradition or, worse, artifice spawned by laziness. Taylor's disciples were driven by a vision of truth that would place managerial control on a footing of absolute objectivity, impervious to the commotion of class conflict or the stench of sweating bodies. 61 The principal method of acquiring such knowledge was the time study and, later, with the influence of Frank Gilbreth, the time-and-motion study. Here, "expert" observations of worker performance made it possible to translate actions into units of time and reconstruct them more efficiently.

The data from the time-study sheets became the possession of management and helped to fuel an explosion in the ranks of those who would measure, analyze, plan, report, issue orders, and monitor the various aspects of the production process. Armed with such data, planners, time-study experts, and production specialists (frequently organized as a staff group for the plant manager) became responsible for analyzing and organizing work tasks, controlling and monitoring their execution, coordinating functions, managing the flow of materials, and keeping records.

Taylor despised wasted effort at work, whether it resulted from deliberate self-protection or from ignorance. His single-minded devotion to the purification of effort gave rise to a set of practices that, whether adopted in whole or in part, transformed the nineteenth-century factory into the modern mass-production facility. The essential logic of his approach followed three steps. First, the implicit knowledge of the worker was gathered and analyzed through observation and measurement. Second, these data, combined with other systematic information regarding tools and materials, laid the foundation for a new division oflabor within the factory. It became possible to separate planning from task execution, to increase the fragmentation and thus the simplicity of production jobs, and so to minimize the amount of skill and training time associated with efficient operations. Third, the new system required a variety of specific control mechanisms to ensure the regularity and intensity of effort while continuing to supply managers and planners with the data necessary for adjustment and improvement. These mechanisms included the development of incentive payment schemes, monitoring systems, and standard operating procedures.

Taylorism meant that the body as the source of skill was to be the object of inquiry in order that the body as the source of effort could become the object of more exacting control. Once explicated, the worker's know-how was expropriated to the ranks of management, where it became management's prerogative to reorganize that knowledge according to its own interests, needs, and motives. The growth of the management hierarchy depended in part upon this transfer of knowledge from the private sentience of the worker's active body to the systematic lists, flowcharts, and measurements of the planner's office. 63 In 1912 a prominent naval engineer writing in the Journal of the American Society of Naval En9ineers listed the seven laws of scientific management. His first law, from which all the others followed, stated that "it is necessary in any activity to have a complete knowledge of what is to be done and to prepare instructions as to what is to be done before the work is started ... the laborer has only to follow direction. He need not stop to think what his past experience in similar cases has been."

Another contemporary interpreter of scientific management took pains to outline the quality of knowledge upon which this approach was based:

Instead of depending upon judgment, scientific management depends upon knowledge in its task of administration. Judgment is the instinctive and subconscious association of impressions derived from previous experience ... but even the best judgment falls far short of knowledge .... This knowledge is carefully and systematically collected and the data so obtained are classified and digested until the knowledge is instantly available whenever a problem is presented to management. Back of the form of organization is a knowledge of the needs and the work of the plant. Back of the plan of wage payment is a knowledge of psychology and sociology. Back of the instruction sheet is a knowledge of the sciences of cutting metals and of handling work.

Another industrial engineer addressing a conference at Dartmouth University's Amos Tuck School of Management in 1912 stressed the difference between scientific management and the more general movement known as systematic management. The scientific management approach rested on complete knowledge of materials, equipment, routing, job assignments, tools, task organizations, time standards, and performance methods. Each phase of an operation was to be planned completely before anything was done: "By this means the order and assignment of all work, or routing as it is called, should be conducted by the central planning or routing department. This brings the control of all operations in the plant, the progress and order of the work, back to the central point. Information, which even in the systematized plant is supposed to be furnished by the knowledge of the workman or the gang-boss or foreman, is brought back to the planning room and becomes a part of the instruction card. " 66 This transfer of knowledge both necessitated and legitimated a new conception of managerial (line and staff) responsibility in coordinating and controlling the complexities of the factory as it entered the era of mass production.

The complexity of workers' responses to scientific management has much to do with the dilemmas created by the body's dual role in production. As rationalization depleted the worker's skill base, there were reactions of loss and threat. These were not without contradiction. Where rationalization did offer less strenuous ways to accomplish physical tasks while improving tools, working conditions, and wages, there is evidence to suggest that many workers who were suspicious at first later accepted and even welcomed the innovations.

Men like Taylor and Gilbreth, who were firmly committed to raising the total level of worker output by easing the arduousness of physical tasks, looked both to new equipment and to new principles of work organization in order to accomplish their goal. For example, after a meticulous study of bricklaying, Gilbreth introduced an adjustable scaffold for piling up bricks. His invention eliminated the worker's bending over and lifting the weight of his body "a thousand times a day," and increased a worker's output from 1,000 to 2,700 bricks daily. Gilbreth claimed that workers typically responded to his innovations with gratitude, as their jobs were made easier.

Taylor believed it was necessary to share the fruits of such productivity increases and saw the differential piece-rate system, a central tenet of scientific management, as a method of uniting workers and managers in a bond of common interest. But incentive wages are devilishly hard to administer, and all too often, managers attracted to differential wage schemes succumbed to shortcuts that promised fast gains. Managers would frequently change piece rates as workers learned to meet the standards. This lead to the complaints of overwork with which unions relentlessly dogged proponents of Taylorism. Scientific management frequently meant not only that individual effort was simplified (either because of labor-saving equipment or new organizational methods that fragmented tasks into their simplest components), but also that the pace of effort was intensified, thus raising the level of fatigue and stress. Effort was purified-stripped of waste-but not yet eased, and resistance to scientific management harkened back to the age-old issue of the intensity and degree of physical exertion to which the body should be subject. As long as effort was organized by the traditional practices of a craft, it could be experienced as within one's own control and, being inextricably linked to skill, as a source of considerable pride, satisfaction, and independence. Stripped of this context and meaning, demands for greater effort only intensified the desire for self- • 69 protect10n.

Taylor had believed that the transcendent logic of science, together with easier work and better, more fairly determined wages, could integrate the worker into the organization and inspire a zest for production. Instead, the forms of work organization that emerged with scientific management tended to amplify the divergence of interests between management and workers. Scientific management revised many of the assumptions that had guided the traditional employer-employee relationship in that it allowed a minimal connection between the organization and the individual in terms of skill, training, and the centrality of the worker's contribution. It also permitted a new flexibility in work force management, promoting the maximum interchangeability of personnel and the minimum dependence on their ability, availability, or motivation. 70 The time-study expert became the new focus of workers' antagonisms. 71 Informal production quotas persisted and, in many cases, took on an overtly anti-management spirit. The union contract became the most important means for institutionalizing workers' instincts for self preservation, as rationalization routed out the ordinary graces with which the workday had been laced. Concern over wages and hours, work rules, and working conditions began to replace the craftsworker's culturally embedded practices of effort regulation. 72 Overall, the purification of effort meant a heightened standard of labor discipline, and as such it widened the gulf between those who gave of their bodies in the service of production and those who did not. Yet even into the second decade of the twentieth century, a more traditional sensibility toward work, one that suffused it with the rhythms of a chatty, sensual, and fun-loving humanity, could still be articulated. A machinist gained prominence when he debated Taylor in 1914 and remarked, "we don't want to work as fast as we are able to. We want to work as fast as we think it's comfortable for us to work. We haven't come into existence for the purpose of seeing how great a task we can perform through a lifetime. We are trying to regulate our work so as to make it auxiliary to our lives. "

A GLASS HALF FULL

As the logic of Taylorism took hold, the substitution of machine power for human labor became the obvious method of increasing the speed and volume of production. In an important way, however, the innovations of mass production added a new dimension to work relationships. Beginning with the highly mechanized Ford Highland Park auto assembly plant, technology would be relied upon to complement or supplant human direction: "The instruction cards on which Taylor set so much value, Ford was able to discard. The conveyor belt, the traveling platform, the overhead rails and material conveyors take their place .... Motion analysis has become largely unnecessary, for the task of the assembly line worker is reduced to a few manipulations. Taylor's stop-watch nevertheless remains measuring the time of operations to the fraction of a second. "

The fragmentation of tasks characteristic of the new Ford assembly line achieved dramatic increases in productivity due to the detailed time study of thousands of operations and the invention of the conveyor belt and other equipment that maximized the continuity of assembly. H. L. Arnold, an industrial journalist who wrote enthusiastically about Ford's innovations, summarized some of the key elements of this costreduction strategy. First, all needless motions were eliminated from the worker's actions; second, the task was organized to require the "least expenditure of willpower, and ... brain fatigue." 75 This formula has dominated the design of mass-production techniques throughout the twentieth century. Effort is simplified (though its pace is frequently intensified) while skill demands are reduced by new methods of task organization and new forms of machinery.

A distinction can be made between the new technologies for the organization of production, which allowed management to control the pace of the assembly line (and, thus, the intensity of effort), and the introduction of new forms of machinery that could reduce the requirements for both effort and skill. The continuity of assembly depended upon the production of interchangeable parts for uniform products. A new generation of automatic and semiautomatic machine tools moderated the physical demands on the machinist as they transferred skill from the worker to the machine. The new workers hired to operate these machines "had no skills and simply placed a piece in and removed it from the machine."

In the second decade of the twentieth century, when these developments were being debated, the fast-growing automobile industry faced shortages of skilled workers. Cheap labor was in abundance, particularly as unskilled rural laborers and new immigrants flocked to cities like Detroit in search of employment. Ford's engineers attempted to construct machine tools suited to the skill level of this labor force. The economic rewards of this approach were so great that with the spread of mass production, this pattern of design choices was carried over from the particular historical situation that had engendered them. For the majority of industrial workers in the generations that followed, there would be fewer opportunities to develop or maintain craft skills. Mass production depended upon interchangeability for the standardization of production; this principle required manufacturing operations to free themselves from the particularistic know-how of the craftsworker. Ford was the first to succeed in this endeavor on a massive scale, where others had failed.

Thus, applications of industrial technology have simplified, and generally reduced, physical effort, but because of the bond between effort and skill, they have also tended to reduce or eliminate know-how. 78 This paradox of the body's dual role in production is nowhere better illustrated that in James Bright's 1958 study of automation in American industry and the subsequent reinterpretation of his findings by social critic Harry Braverman in 1974. 79 Bright studied thirteen manufacturing facilities, including automobile engine plants, a refinery, a bakery, an electrical-part manufacturer, plating plants, and others. He observed varieties of automation in production and materials handling (though it should be noted that applications of computers in manufacturing were virtually nonexistent at the time of his study).

Bright concluded that throughout the thirteen plants he studied, workers were receptive and, in many cases, enthusiastic toward the new automated equipment. Why? "Automation takes the heavy labor out of a job .... The gain to management is productivity; the gain to labor is a much easier job .... In one engine plant a grizzled veteran manning a push button signal light control panel governing some hundred feet of machinery was interviewed .... He said, 'Sure I like it better. It's a lot easier. I can tell you one thing-I'll last a lot longer on this job.' " 81 Self-preservation would induce the worker to accept automation. Bright also clearly stated automation's effect on skills: "As the controls become more sensitive and responsive to the requirements of the operation, environment, and the task, the machine assumes responsibility, just as it has already assumed skill, knowledge, and judgment requirements.'' 82 He noted that this was one of labor's biggest "headaches" with automation, 83 but he believed that new wage determination systems, coupled with sensitive implementation processes, could overcome any resistance engendered by skill dilution. As he put it, "the principal shortcoming ... is that very few firms truly do 'sell' the automated equipment to the worker."

In Braverman's influential critique of what he called the "degradation of work" in this century, he used Bright's study to make a very different point. Where Bright saw the glass half full because the physical demands of work were curtailed, Braverman saw the glass being drained, as workers' skills were absorbed by technology. For Braverman, the transfer of skill into machinery represented a triumph of "dead labor over living labor," a necessity of capitalist logic. As machinery is enlarged and perfected, the worker is made puny and insignificant. By substituting capital (in the form of machinery) for labor, Braverman believed that employers merely seized the opportunity to exert greater control over the labor process. As the work force encountered fewer opportunities for skill development, it would become progressively less capable and, thus, less able to exert any serious opposition.

Although most analyses of industrial organization during the middle decades of the twentieth century have taken up one of these viewpoints, worker ambivalence toward automation has been a persistent theme. 86 Since skilled work is less automated, it also tends to involve more exertion, bodily alteration, dirt, and discomfort. Chinoy's 1955 study of automobile workers found few unskilled men harboring goals of becoming skilled workers. To a certain extent, this finding reflected the real and perceived lack of opportunities to move into skilled positions, but it also reflected a desire to avoid jobs that were physically demanding. "The values which played the greatest part in determining the order of preference among nonskilled jobs were regulation of employment and the relative absence of physical and psychological strain . . . . Men wanted work which was 'not too hard,' 'not too heavy,' 'not too dirty,' and 'not too noisy.' ... They did not like jobs in which 'you got to keep your hands in oil and grease all day.' "

In his account of the skilled auto worker, Bill Goode, a sociologist and former autoworker, reflects on the attitudinal gulf between skilled and unskilled workers in that industry. Most unskilled production workers are not interested in skilled jobs, he contends, because they are not as much fun: "One aspect of skilled work that production workers do not usually experience is dirt ... skilled work is filthy .... The skilled worker carries the marks of his trade under his fingernails and in the creases of his hands .... Assemblers ... have more fun. There is a spontaneity about the play on a line that is unmatched by skilled work. "

Similarly, another worker-turned-sociologist-of-the-workplace, Robert Schrank, reports that workers in a machine shop where he was employed routinely rejected opportunities to become skilled machinists. As these workers expressed themselves to Schrank: "What the hell do you want all the responsibility for? I just stand by this machine, watch it, adjust it once in a while, load it, and I can think about anything I want. I don't have to worry about all those tools and tolerances." 89 Later, Schrank became a set-up man on a turret lathe, a highly skilled position. He began to question the sacrifices he had made to develop and practice his skill: "As I worked in the turret lathe department, my doubts about being a skilled worker increased. The operators seemed to be having more fun than me, playing the races, fooling with the girls, and feeling free ... the pay differential ... did not seem to be worth the added responsibility. "

In the early days of industrialization, skill development was an important source of social integration, as the most skilled workers were often promoted to management ranks. As production jobs offered less opportunity for skill development, they came to have little relevance for the kinds of expertise needed at supervisory levels, and the boundary between these classes of organization members became more rigid. Chinoy's study of the ABC auto plant again speaks to this historical shift: "The structure of opportunity in the ABC plant gives most workers little reason to 'think of the corporation as a pyramid of opportunity.' ... The pattern of working one's way up from the ranks was seen as largely a thing of the past.''

THE SEARCH FOR A SOLUTION

By the early 1960s, scholars had begun to ask whether this tendency would continue to grow with increasing levels of automation. As the range and depth of automation increased, further limiting the contribution of physical effort to the production process, would it continue to reduce skill requirements and thus widen the gulf between workers and management?

Robert Blauner's 1964 study Alienation and Freedom contrasted workers' attitudes and experiences in four types of industrial organizations- craft-based (the printing industry), machine-tending (the textile industry), mass production (the auto industry), and continuous-process (the chemical industry). Because of its advanced degree of automation, the continuous-process form of production was considered to "portend the conditions of factory work and employment in a future dominated by automated technology." 92 Would the experiences of continuous process workers simply repeat the disturbing trade-offs in the mass production industries? Or would these highly automated workplaces offer an alternative?

Blauner's report on the social-psychological dynamics associated with the continuous-process form of production revealed a very different relationship between the labor process and the body of the worker (for a description of continuous-process production and how it differs from other manufacturing processes, see appendix A, p. 415). Skill and effort finally seemed to be uncoupled. In the chemical industry, the focus of Blauner's research on continuous-process operations, production depended upon capital investment in highly automated equipment. Workers were called upon to physically exert themselves only when the equipment broke down, thus creating an inverse relationship between manual effort and productivity. The technology had helped to shape a work environment that freed the worker's body from the kinds of disciplinary pressures that had been routine features of industrial life.

For example, operators required to make the rounds of the equipment in a part of the plant in order to read instruments and monitor plant functioning could control the pace of their activities. As one operator told Blauner, "You can eat the soup first and do the work later, or take the readings at 1 :45 and then eat your soup." 93 The very act of making equipment rounds provided a physical variety and freedom of movement that contrasted sharply with the machine-paced work of the assembly line. This mobility typically placed the continuous-process operator out of the range of an immediate supervisor and made it possible to escape scrutiny. As a result, the operators in Blauner's study felt they had "considerable scope to do their jobs in their own way."

In other industrial settings, the close relationship between effort and skill meant that a reduction in the amount of physical involvement required of the worker tended to mean a reduction in the amount of skill he or she had to bring to the job. Did the reduction of effort in continuous-process operations also mean a reduction of skill? Blauner himself had some difficulty answering this question. In general, he argued that the shift from a job-centered to a process-centered form of work organization meant that the individual contribution changed from one of providing skills to one of assuming responsibility. The operator was responsible for the trouble-free operation of the process, the quality of product being produced, and the preservation of expensive automated equipment. Yet the head operator, in whom the greatest responsibility was invested, had achieved his position by virtue of long years of experience in the plant. Presumably, such experience was important because of the opportunities for skill development it afforded. Another fact makes Blauner's logic difficult to accept-the workers themselves felt that the skills demanded of them were considerable: "Virtually all of the workers interviewed said that their tasks required skill, even lots of skill, yet they were unsure of what was involved. "

Research by British investigators during the late I 9 50s and early I 960s provided a better understanding of the kinds of skills that workers had to bring to bear in a continuous-process environment. E. R. F. W. Crossman headed a research team that developed the first comprehensive description of the process operator's skills and activities, based on field visits within a variety of industries. Crossman found that the work of the process operator fell into four main categories. The most important of these was "control," followed by "special procedures and drills," "routine maintenance," and "recording and reporting." Activities related to controlling the process were described this way: "The process-operator ... must monitor the various gauges, attend to the signs coming from the plant itself, such as noises, smells and vibration, and occasionally carry out special tests on the product .... Apart from the 'official' controls, there are often special ways of influencing the process, such as propping doors open to give greater cooling, or tapping pipes to loosen deposits. "

Crossman went on to analyze what he called the "five components" of process-control skill: sensing ("the smells and appearance which indicate how the plant is running"), perceiving, prediction, familiarity with controls, and decision making. It was this final component that Crossman considered to be the most crucial, and he identified three principal ways in which an operator might approach decision making: by rule of thumb, by having an intuitive feel of what is best, or by pursuing an explicit and logical analysis. Crossman concluded that "on the whole, discussions with operators have suggested that the first, or 'rule-of-thumb,' method is common among the less good operators, and the second, or 'intuitive,' method is often characteristic of the better ones. But few operators seem to use a fully rational or conceptual approach .... The intuitive understanding which enables him to deal with subtle changes and unusual situations seems to come with experience alone. "

Blauner's discussion of operator skills lacks the depth and nuance of Crossman's observations, but his conclusions are similar. Blauner noted that although the worker in such an operation may have a wide range of knowledge about the factory, that knowledge is concrete rather than conceptual: "Whereas continuous-process production increases the worker's breadth of knowledge and awareness of the factory as a totality, it does not necessarily enhance the worker's depth of understanding of the technical and scientific processes ... the very complexity of the scientifically-based reaction means that there is more that is unknown to the average worker in the continuous-process industries than to the all-competent printing craftsperson or the automobile assembler."

What are the implications of this skill profile? To most observers, the continuous-process operation appears to present a significant discontinuity with other forms of production work. It would seem that this sense of uniqueness derives in large measure from the way in which these environments have achieved a de facto resolution of the paradox of the body in the labor process. Skill and effort are no longer inextricably linked. The operator must put forward a minimum of strenuous physical exertion, but the most critical skills he or she brings to the process still depend on the subtle and inarticulate know-how that a body accumulates with years of experience. In this respect, the skills of continuous-process work are in a direct line of descent from earlier generations of craft knowledge. The implicit action-embedded quality of such skills (remember James Davis on learning to puddle iron) would help to explain why the operators Blauner interviewed had a difficult time describing what precisely their skills were.

Based on his comparative studies, Blauner formulated the nowclassic U-curve hypothesis. Comparing levels of social integration across industries at distinct stages of automation, he found the highest levels within those industries that had the least (printing) and the most (chemical) automation. He found that workers in these craft-based and continuous-process industries were considerably less socially alienated and more socially integrated than their counterparts in the massproduction industries of textiles and automobiles. He further argued that rather than exacerbating industrial tensions according to some presumed linear function, higher levels of automation actually begin to reverse some of the most prominent negative tendencies associated with the rationalization of manufacturing work. Continuous-process operators experienced a greater degree of identification with their managers and more loyalty toward their companies, resulting in a heightened sense of mutuality and collaboration.

In the twenty years since Blauner published his study, other researchers have replicated his findings. 99 They observed higher levels of social integration among workers in the most automated industries. Yet some other scholars, notably Mallet, Braverman, and Gallie, have argued to the contrary. ' 00 Duncan Gallie's empirical work in particular shows that the line distinguishing the interests of the continuous-process operator and the manager can remain very clearly drawn.

To understand why and to what degree the continuous-process organization is likely to reveal a strong tendency toward social integration (particularly when compared to the mass-production industries), consider again the body's dual meanings for the production process-as a source of effort and skill.

Continuous-process operators are spared much of the exertion, exhaustion, and depletion that typifies factory work in less automated industries. Because automation has reduced the amount of physical sacrifice required of such workers, the antagonism between those who give of their bodies and those who do not has been muted. Continuousprocess operators are driven by neither the foreman nor the machine system. They are not forced to conform to a narrow behavioral standard. Like their managers, they feel that they are in control of their bodies as they move through time and space.

However, the nature of the operators' skills both contributes to and ultimately limits the potential for social integration in a continuousprocess operation. Blauner saw the level of responsibility that characterized the operator's role as a crucial factor accounting for the high degree of social integration. If the technology necessitates that an operator assumes responsibility for monitoring and controlling broad sections of the plant, how can management regard its workers as adversaries? Hence, "the alienation curve begins to decline from its previous height as employees in automated industries gain a new dignity from responsibility and a sense of individual function.

The level of responsibility to which Blauner and others have pointed remains firmly linked to an implicit judgment of the operator's skill. It is the operator with the most experience who becomes the head shift operator or crew leader. This is because the skills he or she must have, the "intuitive feel" described by Crossman, are developed only through many years of working at a variety of jobs in the plant and of observing a wide range of equipment operating under diverse conditions. The fact that this competence is experience-based helps orient the operator toward the stratified occupational structure within the work force. Because skill is cumulative, know-how gathered at the lower status levels is relevant to performance at higher levels, and the prospect of advancement serves as an important integrating mechanism. The extended temporal horizon necessary for skill development provides the opportunity for an individual to develop an identification with and loyalty to the organization.

Responsibility not only is a measure of the skill level of the operator but also denotes the dependence of management on the worker. The worker's bodily involvement with the production process over many years provides exclusive access to the subtle and specific knowledge required for competent operations. Blauner's observation of small work teams imposing their own standards of work discipline is reminiscent of the work groups in the early factory, whose unique craft knowledge empowered them to exert their own vision of appropriate work organization and behavior. 102 Like those early craftspeople, the exclusive knowledge of the workers in a continuous-process operation lashes them to their managers with bonds of reciprocity.

That the worker's body, through the sensual information it accumulates with physical involvement in the production process, remains a crucial source of skill both defines and limits the extent to which the worker is likely to be a fully integrated member of this kind of organization. The operator's knowledge continues to depend upon sentience, and it is the personal, specific bodily character of this knowledge that persistently differentiates the operator from the management superstructure. As long as their knowledge is concrete and specific rather than conceptual and technical, workers will tend to be confined to a certain set of roles. Without a conceptual understanding of the process in which they work, indeed, of their own actions, workers will find it difficult to make a contribution to that domain of more comprehensive functions typically labeled "managerial."

Thus, despite the high level of social integration in many continuousprocess organizations, a fundamental aspect of Tayloristic logic has been preserved. The much-touted collaborative atmosphere of continuous-process industries derives in large measure from the minimal emphasis placed upon the need for physical sacrifice from the worker. However, the body as the operator's source of skill remains a strong link to the industrial past and continues to demarcate the boundary between those who give of their bodies in the service of production and others who do not. Should operators perceive any devaluation of their skills, it is likely that this boundary would deepen and the sense of mutuality would diminish.

The history of work has been, in part, the history of the worker's body. Production depended on what the body could accomplish with strength and skill. Techniques that improve output have been driven by a general desire to decrease the pain of labor as well as by employers' intentions to escape dependency upon that knowledge which only the sentient laboring body could provide. Skilled workers historically have been ambivalent toward automation, knowing that the bodies it would augment or replace were the occasion for both their pain and their power. In Blauner's model, the problem seemed to have been solved; automation was able to minimize exertion without depleting skill. Yet even within this optimistic rendering, the body's role in the labor process continues to describe the distance between managers and the managed.

From this vantage point it is possible to see that the progress of automation has been a result of a transfer of knowledge and has, in turn, further enabled that transfer. However, the term transfer must be doubly laden if it is to adequately describe this process. Knowledge was first transferred from one quality of knowing to another-from knowing that was sentient, embedded, and experience-based to knowing that was explicit and thus subject to rational analysis and perpetual reformulation. The mechanisms used to accomplish this transfer were themselves labor intensive (that is, they depended upon first-hand observation of time-study experts) and were designed solely in the context of, and with the express purpose of, enabling a second transfer - one that entailed the migration of knowledge from labor to management with its pointed implications for the distribution of authority and the division of labor in the industrial organization. In the case of Blauner's chemical plant, operators' sentient knowledge could not be explicated. Their skills were left intact but in a way that continued to circumscribe their role in the organization.

As information technology is applied to the production process, what effect will it have upon the grounds of knowledge? What will it take "to know"? What kind of knowledge will enable competent participation in the processes associated with making things? History alerts us to the fact that answers to these questions will require some understanding of the relationship between the new technology and the sentient body. Will information technology continue to diminish physical effort but allow the retention of experience-based skills as in Blauner's scenario? Will effort and skill, indeed the very presence of the worker, be wiped out altogether as Braverman (or Bright) would lead us to predict?

The following chapter presents a third, relatively uncharted, alternative, suggesting that this latest technological transformation can indeed alter the terms of the centuries-old paradox concerning effort and skill. By redefining the grounds of knowledge from which competent behavior is derived, new information technology lifts skill from its historical dependence upon a laboring sentient body. While it is true that computer-based automation continues to displace the human body and its know-how (a process that has come to be known as deskilling), the informating power of the technology simultaneously creates pressure for a profound reskilling. How are these new skills to be understood? What might be their implications for the differences that have separated workers from their managers, differences that have, in an important way, depended upon the necessity of the body's labor and the body's learning.

CONCLUSION

MANAGING THE INFORMATED ORGANIZATION

What defines humanity is not the capacity to create a second nature-economic, social, or cultural-beyond biological nature; it is rather the capacity of going beyond created structures in order to create others. -MAURICE MERLEAU-PONTY The Structure of Behavior

TECHNOLOGY IS A PLACE

Put your eye to the kaleidoscope and hold it toward the light. There is a burst of color, tiny fragments in an intricate composition. Imagine a hand nudging the kaleidoscope's rim until hundreds of angles collapse, merge, and separate to form a new design. A fundamental change in an organization's technological infrastructure wields the power of the hand at the turning rim. Technological change defines the horizon of our material world as it shapes the limiting conditions of what is possible and what is barely imaginable. It erodes taken-for-granted assumptions about the nature of our reality, the "pattern" in which we dwell, and lays open new choices. When the telephone makes it possible to pursue intimate conversations without bodies that touch or eyes that meet, or when the electric light rescues the night from darkness, the experience is more than simply an element within the pattern. Such innovations give form and definition to our worldly place and provoke a new vision of the potential for relatedness within it. It is in this sense that technology cannot be considered neutral. Technology is brimming with valence and specificity in that it both creates and forecloses avenues of experience.

History reveals the power of certain technological innovations to transform the mental life of an era-the feelings, sensibilities, perceptions, expectations, assumptions, and, above all, possibilities that define a community. From the social influence of the medieval castle,I to the coming of the printed book, 2 to the social and physical upheaval associated with the rise of the automobile 3 -each specific example serves to drive home a similar message. An important technological innovation is not usefully thought of as a unitary cause eliciting a series of discrete effects. Instead, it can be seen as an alteration of the material horizon of our world, with transformative implications for both the contours and the interior texture of our lives. Technology makes the world a new place-a conception expressed by Fernand Braudel when he wrote:

It was only when things went wrong, when society came up against the ceiling of the possible, that people turned of necessity to technology, and interest was aroused for the thousand potential inventions, out of which one would be recognized as the best, the one that would break through the obstacle and open the door to a different future .... In this sense, technology is indeed a queen: it does change the world.

Some theorists have attributed systematic and purposeful agency to the managerial use of technology. They argue that managers are interested exclusively in technology as a means of controlling, limiting, and ultimately weakening their work force. 5 The data I have presented suggest a more complicated reality. Even where control or de skilling has been the intent of managerial choices with respect to new information technology, managers themselves are also captive to a wide range of impulses and pressures. Only rarely is there a grand design concocted by an elite group ruthlessly grinding its way toward the fulfillment of some special and secret plan. Instead, there is a concentration of forces and consequences, which in turn develop their own momentum. Sometimes these lines of force run in predictably straight paths. At other times, they twist and spiral, turn corners, and flow to their opposite. Activities that seem to represent choices are often inert reproductions of accepted practice. In many cases, they are convenient responses to the press of local exigencies. In some instances, they may actually reflect a plan.

To fully grasp the way in which a major new technology can change the world, as described by Braudel, it is necessary to consider both the manner in which it creates intrinsically new qualities of experience and the way in which new possibilities are engaged by the often-conflicting demands of social, political, and economic interests in order to produce a "choice." To concentrate only on intrinsic change and the texture of an emergent mentality is to ignore the real weight of history and the diversity of interests that pervade collective behavior. However, to narrow all discussion of technological change to the play of these interests overlooks the essential power of technology to reorder the rules of the game and thus our experience as players. Moreover, these two dimensions of technological change, the intrinsic and the contingent, need to be understood, not separately, but in relation to one another. The same innovation that abstracts work and increases its intellectual content, thus enhancing the learning of lower level employees (remember the operators at Tiger Creek and their experience with the new expensetracking system described in chapter 7), can also, within the context of the choices by which it is adapted, be experienced as a new source of divisiveness and control (as Tiger Creek's managers perceived a threat to their roles and resisted the potential for change).

The dilemmas of transformation that have been described are embedded in the living detail of everyday life in the workplace as it undergoes computerization. They are dilemmas precisely because of the way they reveal the subtle interplay between essence and choice. Information technology essentially alters the contours of reality-work becomes more abstract, intelligence may be programmed, organizational memory and visibility are increased by an order of magnitude beyond any historical capability. Individuals caught up in this newly configured reality face questions that did not need to be asked before. New possibilities arise and require new deliberations. The duality of information technology-its capacity to automate and to informate- provides a vantage point from which to consider these choices. The relative emphasis that organizations give to these capacities will become the foundation for a strategic conception of technology deployment and so will shape the way the dilemmas are confronted and resolved.

The organizations described in this book have illustrated how the need to defend and reproduce the legitimacy of managerial authority can channel potential innovation toward a conventional emphasis on automation. In this context, managers emphasize machine intelligence and managerial control over the knowledge base at the expense of developing knowledge in the operating work force. They use the technology as a fail-safe system to increase their sense of certainty and control over both production and organizational functions. Their experiences suggest that the traditional environment of imperative control is fatally flawed in its ability to adequately exploit the informating capacity of the new technology.

In these organizations, the promise of automation seemed to exert a magnetic force, a seduction that promised to fulfill a dream of perfect control and heal egos wounded by their needs for certainty. The dream contains the image of "people serving a smart machine," but in the shadow of the dream, human beings have lost the experience of critical judgment that would allow them to no longer simply respond but to know better than, to question, to say no. This dream brings us closer to fulfilling Hannah Arendt's dreadful forecast of a world in which behaviorism comes true:

The last stage of the laboring society, the society of jobholders, demands of its members a sheer automatic functioning, as though individual life had actually been submerged in the over-all life process of the species and the only active decision still required of the individual were to let go, so to speak, to abandon his individuality, the still individually sensed pain and trouble of living, and acquiesce in a dazed, "tranquilized," functional type of behavior. The trouble with modern theories of behaviorism is not that they are wrong but that they could become true, that they actually are the best possible conceptualization of certain obvious ttends in modern society. It is quite conceivable that the modern age-which began with such an unprecedented and promising outburst of human activity-may end in the deadliest, most sterile passivity history has ever known.

That managers may give themselves over to this dream because of inertia and convenience rather than cogent analysis is all the more disturbing. Organizations that take steps toward an exclusively automating strategy can set a course that is not easily reversed. They are likely to find themselves crippled by antagonism from the work force and the depletion of knowledge that would be needed in value-adding activities. The absence of a self-conscious strategy to exploit the informating capacity of the new technology has tended to mean that managerial action flows along the path of least resistance-a path that, at least superficially, appears to serve only the interests of managerial hegemony.

Yet what would seem to be a maddeningly predictable story line has its share of surprises, false starts, dead ends, trap doors, tarnished hopes, and real failures. The seeds of an informating strategy were apparent in each of the organizations described here, especially in Cedar Bluff, Global Bank Brazil, and DrugCorp. In the absence of a comprehensive strategy, no single organization fully succeeded in exploiting the opportunity to informate.

The interdependence of the three dilemmas of transformation I have described-knowledge, authority, and technique-indicates the necessary comprehensiveness of an informating strategy. The shifting grounds of knowledge invite managers to recognize the emergent demands for intellective skills and develop a learning environment in which such skills can develop. That very recognition contains a threat to managerial authority, which depends in part upon control over the organization's knowledge base. A commitment to intellective skill development is likely to be hampered when an organization's division of labor continuously replenishes the felt necessity of imperative control. Managers who must prove and defend their own legitimacy do not easily share knowledge or engage in inquiry. Workers who feel the requirements of subordination are not enthusiastic learners. New roles cannot emerge without the structures to support them. If managers are to alter their behavior, then methods of evaluation and reward that encourage them to do so must be in place. If employees are to learn to operate in new ways and to broaden their contribution to the life of the business, then career ladders and reward systems reflecting that change must be designed. In this context, access to information is critically important; the structure of access to information expresses the organization's underlying conception of authority. Employees and managers can hardly be partners in learning if there is a one-way mirror between them. Techniques of control that are meant to safeguard authority create suspicion and animosity, which is particularly dysfunctional when an organization needs to apply its human energies to inventing an alternative form of work organization better suited to the new technological context.

The interdependence among these dilemmas means that technology alone, no matter how well designed or implemented, cannot be relied upon to carry the full weight of an informating strategy. Managers must have an awareness of the choices they face, a desire to exploit the informating capacity of the new technology, and a commitment to fundamental change in the landscape of authority if a comprehensive informating strategy is to succeed. Without this strategic commitment, the hierarchy will use technology to reproduce itself. Technological developments, in the absence of organizational innovation, will be assimilated into the status quo.

THE DIVISION OF LABOR AND THE DIVISION OF LEARNING

Organizational theorists frequently have promoted a conception of organizations as "interpretation systems." 7 The computer mediation of an organization's productive and administrative infrastructure places an even greater premium upon an organization's interpretive capabilities, as each organizational level experiences a relatively greater preponderance of abstract cues requiring interpretation. This is as true for the plant manager as for the pulp mill worker, for the banker as well as for the clerk. In each case, oral culture and the action-centered skills upon which that culture depends are gradually eroded, and perhaps finally displaced, by the incursions of explicit information and intellective skill.

As bureaucratic coordination and communication become more dependent upon mastering the electronic text, the acting-with skills of the white-collar body are subordinated to the demands associated with dominating increasing quantities of abstracted information. In many cases, traditional functional distinctions no longer reflect the requirements of the business. When managers increase their engagement with the electronic text, they also risk a new kind of hyperrationalisn and impersonalization, as they operate at a greater distance from employees and customers.

When the textualizing consequences of an informating technology become more comprehensive, the body's traditional role in the production process (as a source of effort and/or skill in the service of actingon) is also transformed. The rigid separation of mental and material work characteristic of the industrial division of labor and vital to the preservation of a distinct managerial group (in the office as well as in the factory) becomes, not merely outmoded, but perilously dysfunctional. Earlier distinctions between white and blue "collars" collapse. Even more significant is the increased intellectual content of work tasks across organizational levels that attenuates the conventional designations of manager and managed. This does not mean that there are no longer useful distinctions to be made among organizational members, but whatever these distinctions may be, they will no longer convey fundamentally different modes of involvement with the life of the organization represented by the division of abstract and physical labor. Instead, the total organizational skill base becomes more homogeneous.

In the highly informated organization, the data base takes on a life of its own. As organizations like Cedar Bluff develop mechanisms that allow data to be automatically generated, captured, and stored, they begin to create their own image in the form of dynamic, detailed, realtime, integrated electronic texts. These texts can provide access to internal operations as well as external business and customer data; they can be designed with enough reflexivity to be able to organize, summarize, and analyze aspects of their own content. The electronic text becomes a vast symbolic surrogate for the vital detail of an organization's daily life. Such data constitute an autonomous domain. They are a public symbolization of organizational experience, much of which was previously private, fragmented, and implicit-lodged in people's heads, in their sensual know-how, in discussions at meetings or over lunch, in file drawers, or on desktops.

The textualization process moves away from a conception of information as something that individuals collect, process, and disseminate; instead, it invites us to imagine an organization as a group of people gathered around a central core that is the electronic text. Individuals take up their relationship toward that text according to their responsibilities and their information needs. In such a scenario, work is, in large measure, the creation of meaning, and the methods of work involve the application of intellective skill to data.

Under these circumstances, work organization requires a new division of learning to support a new division of labor. The traditional system of imperative control, which was designed to maximize the relationship between commands and obedience, depended upon restricted hierarchical access to knowledge and nurtured the belief that those who were excluded from the organization's explicit knowledge base were intrinsically less capable of learning what it had to offer. In contrast, an informated organization is structured to promote the possibility of useful learning among all members and thus presupposes relations of equality. However, this does not mean that all members are assumed to be identical in their orientations, proclivities, and capacities; rather, the organization legitimates each member's right to learn as much as his or her temperament and talent will allow. In the traditional organization, the division of learning lent credibility to the legitimacy of imperative control. In an informated organization, the new division of learning produces experiences that encourage a synthesis of members' interests, and the flow of value-adding knowledge helps legitimate the organization as a learning community.

The contemporary language of work is inadequate to express these new realities. We remain, in the final years of the twentieth century, prisoners of a vocabulary in which managers require employees; superiors have subordinates; jobs are defined to be specific, detailed, narrow, and task-related; and organizations have levels that in turn make possible chains of command and spans of control. The guiding metaphors are military; relationships are thought of as contractual and often adversarial. The foundational image of work is still one of a manufacturing enterprise where raw materials are transformed by physical labor and machine power into finished goods. However, the images associated with physical labor can no longer guide our conception of work.

The informated workplace, which may no longer be a "place" at all, is an arena through which information circulates, information to which intellective effort is applied. The quality, rather than the quantity, of effort will be the source from which added value is derived. Economists may continue to measure labor productivity as if the entire world of work could be represented adequately by the assembly line, but their measures will be systematically indifferent to what is most valuable in the informated organization. A new division of learning requires another vocabulary-one of colleagues and co-learners, of exploration, experimentation, and innovation. Jobs are comprehensive, tasks are abstractions that depend upon insight and synthesis, and power is a roving force that comes to rest as dictated by function and need. A new vocabulary cannot be invented all at once-it will emerge from the practical action of people struggling to make sense in a new "place" and driven to sever their ties with an industrial logic that has ruled the imaginative life of our century.

The informated organization is a learning institution, and one of its principal purposes is the expansion of knowledge-not knowledge for its own sake (as in academic pursuit), but knowledge that comes to reside at the core of what it means to be productive. Learning is no longer a separate activity that occurs either before one enters the workplace or in remote classroom settings. Nor is it an activity preserved for a managerial group. The behaviors that define learning and the behaviors that define being productive are one and the same. Learning is not something that requires time out from being engaged in productive activity; learning is the heart of productive activity. To put it simply, learning is the new form of labor.

The precise contours of a new division of learning will depend upon the business, products, services, and markets that people are engaged in learning about. The empowerment, commitment, and involvement of a wide range of organizational members in self-managing activities means that organizational structures are likely to be both emergent and flexible, changing as members continually learn more about how to organize themselves for learning about their business. However, some significant conceptual issues are raised by the prospect of a new division of learning in the informated organization. The following discussion of these issues does not offer a rigid prescription for practice but suggests the kinds of concrete choices that define an informating strategy.

MANAGERIAL ACTIVITIES IN THE INFORMATED ORGANIZATION

As the intellective skill base becomes the organization's most precious resource, managerial roles must function to enhance its quality. Members can be thought of as being arrayed in concentric circles around a central core, which is the electronic data base. The skills required by those at the core do not differ in kind from those required at a greater distance from the core. Instead of striking phenomenological differences in the work that people do, the distance of any given role from the center denotes the range and comprehensiveness of responsibilities, the time frame spanned by those responsibilities, and the degree of accountability for cross-functional integration attached to the role. The data base may be accessed from any ring in the circle, though data can be formatted and analyzed in ways that are most appropriate to the information needs of any particular ring of responsibility.

On the innermost ring, nearest to the core, are those who interact with information on a real-time basis. They have responsibility for daily operations, and the level of data they utilize is the most detailed and immediate. Because intellective skill is relevant to the work of each ring of responsibility, the skills of those who manage daily operations form an appropriate basis for their progression into roles with more comprehensive responsibilities.

The jobs at the data interface become increasingly similar to one another as the informating process evolves. In the advanced stages of informating, these become "meta jobs," because the general characteristics of intellective skill become more central to performance than the particular expertise associated with specific production-related functions. Expertise either is available from on-site specialists or is built into information systems. For example, at Cedar Bluff, top managers believed that they would solve the problem of vanishing artistry by building that expertise into the information system capability. The know-how of managers with years of experience could be systematized and made available to operators who would never have the same degree of involvement in the action contexts that had developed the personal and specific knowledge associated with action-centered skill.

This relationship between general intellective skills and expertise in specific areas was also illustrated in the case of the calculator models at Cedar Bluff, discussed in chapter 7. Operators needed the kind of understanding that would allow them to know when and how to use a model, and when to be critical of its assumptions or outputs. That quality of understanding did not depend upon being able to match the expertise that went into the models' calculations. The operator with a conceptual approach to the process, skilled in data-based reasoning, and familiar with the theory that links elements in the production process, may not be able to reproduce the knowledge of an individual with years of hands-on experience or expert training. Nevertheless, he or she should be able to understand the conceptual underpinning of a problem well enough to select among potential analytic strategies and to access the expert knowledge that is required. Intellective skill is brought to bear in the definition of the problem for analysis, the determination of the data that is required for analysis, the consideration of the appropriateness of an analytical approach, and the application of the analysis to improved performance.

The activities arrayed on the responsibility rings at a greater distance from the core incorporate at least four domains of managerial activity: intellective skill development, technology development, strategy formulation, and social system development. For example, the crucial importance of the intellective skill base requires that a significant level of organizational resources be devoted to its expansion and refinement. This means that some organizational members will be involved in both higher-order analysis and conceptualization, as well as in promoting learning and skill development among those with operational responsibility. Their aim is to expand the knowledge base and to improve the effectiveness with which data is assimilated, interpreted, and responded to. They have a central role in creating an organizational environment that invites learning and in supporting those in other managerial domains to develop their talents as educators and as learners. In this domain, managers are responsible for task-related learning, for learning about learning, and for educating others in each of the other three domains.

A new division of learning depends upon the continued progress of informating applications. This managerial domain of technologyrelated activity comprises a hierarchy of responsibilities in addition to those tasks normally associated with systems engineering, development, and maintenance. It includes maintaining the reliability of the data base while improving its breadth and quality, developing approaches to system design that support an informating strategy, and scanning for technical innovations that can lead to new informating opportunities. Members with responsibility for the development of technology must be as concerned with the use of technology (Do people understand the information? Do they know how to use it?) as they are with other aspects of design and implementation. This kind of technological development can occur only in the closest possible alignment with organizational efforts to promote learning and social integration. Technology develops as a reflection of the informating strategy and provides the material infrastructure of the learning environment.

Learning increases the pace of change. For an organization to pursue an informating strategy, it must maximize its own ability to learn and explore the implications of that learning for its long-range plans with respect to markets, product development, new sources of comparative advantage, et cetera. A division of learning that supports an informating strategy results in a distribution of knowledge and authority that enables a wide range of members to contribute to these activities. Still, some members will need to guide and coordinate learning efforts in order to lead an assessment of strategic alternatives and to focus organizational intelligence in areas of strategic value. These managers lead the organization in a way that allows members to participate in defining purpose and in supporting the direction of long-term planning. The increased time horizon of their responsibilities provides the reflective distance with which they can gauge the quality of the learning environment and can guide change that would improve collective learning.

There is considerable interdependence among these four domains of managerial activity (intellective skill development, technology development, strategy formulation, and social system development). For example, activities related to intellective skill development cannot proceed without the social system management that helps to foster roles and relationships appropriate to a new division of learning. Activities in either of these domains will be frustrated without technological development that supports an informating strategy. Integration and learning are responsibilities that fall within each domain, because without a shared commitment to interdependence and the production of value-adding knowledge, the legitimacy of the learning community will suffer. Business outcomes such as cost, efficiency, quality, product development, customer service, productivity, et cetera, would result from coordinated initiatives across domains. Managerial work would thus be team-oriented and interdisciplinary, and would promote the fluid movement of members across these four domains of managerial activity.

The concentric structure depends upon and promotes both vertical and horizontal organizational integration. There are no predetermined boundaries between any rings within the organizational sphere or between the domains of managerial authority. The skills that are required at the data interface nearest to the core of daily operating responsibilities provide a coherent basis for the kind of continual learning that would prepare people for increasingly comprehensive responsibilities. The relative homogeneity of the total organizational skill base suggests a vision of organizational membership that resembles the trajectory of a professional career, rather than the two-class system marked by an insurmountable gulf between workers and managers. The interpenetration between rings provides a key source of organizational integration.

Some observers of the emerging technological environment have predicted an increasingly bifurcated distribution of skills in the workplace. 8 On the other extreme, the operators at Piney Wood, whose discussion of the future opened our Introduction, believed that in the future all factory workers would be college graduates. The concentric organizational structure suggests that the solution to future skill requirements need not be as drastic as either of these scenarios implies. While it is probable that entry-level requirements will become more demanding, the increased homogeneity of skills and the continuity between organizational rings entails an ongoing commitment to training and education in order to facilitate career progression. The shape of skill distribution thus is more likely to represent a curve than the discontinuous step function that characterizes the traditional hierarchy, with its more rigid distinction between managers and the managed.

THE BODY'S NEW WORK: MANAGING THE INTRICACY OF POSTHIERARCHICAL RELATIONSHIPS

The vision of a concentric organization is one that seems to rely upon metaphors of wholeness-interdependency, fluidity, and homogeneity each contribute to organizational integration. What is required of managers in such a workplace, where learning and integration constitute the two most vital organizational priorities? How is the social system of such an organization to be managed?

As we have seen, the abstract precincts of the data interface heighten the need for communication. Interpretive processes depend upon creating and sharing meaning through inquiry and dialogue. New sources of personal influence are associated with the ability to learn and to engender learning in others, in contrast to an earlier emphasis upon contractual relationships or the authority derived from function and position.

What new patterns of relationships will characterize this kind of learning environment? What will replace the familiar map that the model of imperative control has provided? The answer to this question derives from one of the most significant dialetics of an informating strategy. In a conventional organization, managers' action-centered skills are geared toward the politics of interpersonal influence, principally as they pertain to maintaining reciprocities, managing superiors, and gathering or disseminating information. These skills are shaped by the demands of achieving smooth operations and personal success under conditions of hierarchical authority. People develop their expectations about how to treat one another largely in reference to rank and function. An informating strategy does place severe demands upon managers' action-centered skills in the service of acting-with but in a very different context. The relationships to be managed are both more dynamic and more intricate than earlier patterns. The shape and quality of relationships will vary in relation to what people know, what they feel, and what the task at hand requires. Relationships will need to be fashioned and refashioned as part of the dynamism of the social processes, like inquiry and dialogue, that mediate learning. Such relationships are more intricate because their character derives from the specifics of the situation that are always both pragmatic-what it takes to get the work done best-and psychological-what people need to sustain motivation and commitment.

In the information panopticon, managers (like those at Metro Tel) frequently tried to simplify their managerial tasks by displacing faceto- face engagement with techniques of surveillance and control. As a consequence, they became isolated from the realities of their organizations as they were increasingly insulated by an electronic text that in turn was even more vulnerable to workers' antagonisms. The demands of managing intricate relationships reintroduce the importance of the sentient body and so provide a counterpoint to the threat of hyperrationalism and impersonalization that is posed by computer mediation. The body now functions as the scene of human feeling rather than as the source of physical energy or as an instrument of political influence. Human feeling operates here in two ways. First, as members engage in their work together, their feelings are an important source of data from which intricate relations are structured. Second, a manager's felt sense of the group and its learning needs is a vital source of personal knowledge that informs the development of new action-centered skills in the service of acting-with.

The demands of a learning environment can reduce the psychological distance between the self and the organization because active engagement in the social processes associated with interpretation requires more extensive participation of the human personality. In a traditional approach to work organization, employees could be treated as objectively measurable bodies, and in return, they could give of their labor without giving of their selves. The human being as wage earner and the human being as subjective actor could remain separate. In an environment of imperative control, managers can remain indifferent to what their subordinates feel, as long as they perform adequately. This "is" was eventually translated to "ought," as incursions of private feeling into the workday came to be seen as squandering the organization's time. It was this view that ultimately triumphed over the professionals at DrugCorp who had unwittingly textualized their own playfulness. As they struggled with their notions of legitimate work behavior, DrugCorp's managers tried to define the self out of the workday. But when work involves a collective effort to create and communicate meaning, the dynamics of human feeling cannot be relegated to the periphery of an organization's concerns. How people feel about themselves, each other, and the organization's purposes is closely linked to their capacity to sustain the high levels of internal commitment and motivation that are demanded by the abstraction of work and the new division of learning.

The relationships that characterize a learning environment thus can be thought of as posthierarchical. This does not imply that differentials of knowledge, responsibility, and power no longer exist; rather, they can no longer be assumed. Instead, they shift and flow and develop their character in relation to the situation, the task, and the actors at hand. Managing intricacy calls for a new level of action-centered skill, as dialogue and inquiry place a high premium on the intuitive and imaginative sources of understanding that temper and hone the talents related to actin9-with. The dictates of a learning environment, rather than those of imperative control, now shape the development of such interpersonal know-how.

The seeds of this new intricacy are already evident in several of the organizations I have described. The managers at Cedar Bluff who learned to join their workers in asking questions and searching for answers were already engaged in forging patterns far more intricate than the simpler prescriptions derived from faith in managerial authority. The professionals of DrugCorp, normally divided by function, professional discipline, and organizational rank, had invented new modes of relationships based upon a valued exchange of information, shared inquiry, and play. At Global Bank Brazil, bankers and operations managers were groping for new relationships that would reach beyond turf and hierarchy in order to better serve their customers. At Tiger Creek, managers and workers reached and stumbled as they attempted to shift the logic of their relationship from one based on hierarchy to one shaped by the pragmatic opportunities offered by a redistribution of knowledge. In each of these cases, organizational members were confronted with rich new possibilities engendered by an informating technology. In each case, they discovered that exploiting these new opportunities required new forms of relationships governed by the necessities of learning and performance rather than by the rules of an older faith rules that sort, rank, and separate.

DISSENT FROM WHOLENESS

Is there a dark side to this vision of a wholistic organization with its emphasis on relationships that are intricate, dynamic, and constructed ad hoc? What new psychological costs might it imply? What new mechanisms might be required to ensure justice and equity? To answer these questions, we need to return to the voices of the mill workers as they lived through the transition to an informated environment.

One source of insight into the potential pitfalls of the wholistic organization comes from those workers at Piney Wood who drew strength from the institutional arrangements of a traditional work system. When workers at that plant anticipated the future, they saw computer-based technology as a catalyst for new work systems that would be more flexible, collaborative, and socially integrative than anything they had known. Many were curious and even enthusiastic about such a prospect, but there were others who dissented from this vision of wholeness. Their perspective reminds us of how a certain breed of American worker has found psychological sustenance in the norms of the industrial workplace. It can also alert us to what might be lost in the transformation to a new form of organization and asks that we think carefully about what was best in the old arrangement and should be preserved.

Though the corrosive effects of an adversarial environment cannot be denied, there are those who found an important source of psychological freedom in the rigorously defined contractual relationships that characterized Piney Wood. In such a workplace, the union contract defines binding arrangements-jobs are meticulously defined, and seniority rules guide pay and promotion decisions. Individual workers know exactly what is required of them and, in return, what rights they possess. The worker's first obligation is above all to the job-to perform it competently and completely. As long as individuals uphold their end of the employment contract, their rights are protected. These protections provide them a status under the law that is equal to that of their employer. If the contract is violated, then there are institutionalized mechanisms of due process, invested ultimately with the legitimate authority of the state, through which a person can redress a grievance or seek to influence the policies that inform the labor management relationship.

The organizing principle of such a workplace is based on the individual. People are held accountable for their particular jobs, and these jobs are treated as distinct elements that must be assembled in order to accomplish the work of the organization. That one's obligation is first of all to the job, rather than to the enterprise, creates a certain psychological distance between the self and the organization. Living up to the terms of the employment contract leaves a wide range of behavior that is unspecified and noncontractual. A worker need not buy into the purposes or values of the organization in order to perform competently and enjoy the rewards that he or she has earned. There is no pressing need to be liked by those around you, either superiors or peers, when one's primary obligation is to fulfill the demands of a narrow job description.

These arrangements can provide the worker with a measure of independence and autonomy. They make it possible to feel that one is, as the idiom goes, one's "own man." U.S. labor relations reflect a conception of equality in which parties each seek advantage through negotiation. Neither party must buy into the worldview of the other. A pluralism of values and interpretations may coexist, so long as the job gets done.

When the workers in such an organization consider their managers, they see a very different world. Those who most value the psychological distance and autonomy provided by the contractual relationship, view managers with a certain pity. They believe that managers are at the mercy of a system that can make unlimited demands, because the boundaries that define the manager's job are vague and permeable. Managerial jobs themselves are abstract enough to be subject to diverse interpretations. Managers might well ask themselves, have I done my job? If the job is that abstract, then it is also easy for peers and superiors to question what a manager has done and to formulate their own evaluations of his or her performance. Without a collective contract, the manager is vulnerable and dependent; he or she must surrender to the organization's purposes and values. Instead of the feistiness and pluralism that characterize the labor-management relationship, many workers see in the managers' world overbearing demands for ideological unity, loyalty, and the submergence of the self. In other words, managers may seek to control their subordinates, but they are not in control of their own work lives.

There is a breed of American worker who cherishes the autonomy and sense of self-control afforded by his or her skills and protected by the union contract. When these workers contemplate the prospect of the socially integrated high-technology workplace, they feel despair. They anticipate a loss of their unique identities, of freedom and autonomy, and of well-defined rights and responsibilities. They fear that without the traditional sources of protection provided by their job descriptions and their contract, they will become prey to every capricious whim of their superiors. They understand that the managers' world requires the body as a political instrument for self-presentation and influence, but they know that these are talents they have not developed and toward which they feel more than a little distaste.

They say that with this new technology we need a more flexible system, one that will make us competitive. They figure it works for management, so why not for the blue-collar worker. If a manager hasn't saved money, he won't get his extra one hundred dollars a week. If he doesn't produce, he's out the gate. But right now, I don't have anything to worry about except doing my job and doing it well. I don't have to be friends with people in order to move up. I don't have to use anybody. In the management world, you have got to be a salesman to a certain extent. You have go to know how to manipulate the human system. If I am like management, it means I will have to be doubly nice to you whether I like you or not. You have to see everybody as a stepping-stone.

Other operators believe that in a fluid, socially integrated workplace, without clear job descriptions and contracts, they would lose the clarity of rights and obligations that currently offer an important sense of personal control. Without such definition, how will one know what to expect each day and how will it be possible to manage the extent of one's own exertion? They fear flexible arrangements that would change according to the needs of the total organization, in place of discrete task assignments on an individual basis. Their "have-skills-will-travel" image is a kind of emotional insurance policy, but an enterprise centered approach to task distribution would make each individual more dependent upon and integrated with the organization.

They say the new technology will require a flexible system. But under a flexible system, you have no choice but to go where they send you, when they send you. You can get to earn high pay, but you have no choice about what your job is, and you can't have your own job. You never know what to wear to work-do you wear your good Levi's or do you wear your greasy ones?

This statement represents the operators' worst fears-that the loss of control over one's work would invade the most intimate and ordinary details of everyday life. Stumbling around the bedroom on a dark morning, trying to get ready in time to have a cup of coffee before leaving for the plant, the worker must ask, What do I wear? What kind of day should I look forward to? What is in store for me today? Will I feel good about the things I am asked to do? Without the capacity to set one's expectations, it is difficult to locate oneself emotionally. It is easy to feel helpless, as if one is at the mercy of others.

The disquiet these workers feel culminates in a frank concern over power. In a workplace in which divisions among workers and between workers and managers are minimized, where all members are supposed to pull together in the service of the organization, what rights will individuals have? How will these rights be specified? What mechanisms will ensure that individuals have a voice in and influence over the policies and practices that shape their work life? Who will have the power to define the circle of legitimate behavior by which all members are evaluated?

They say that in a new flexible system the criteria for advancement are fitness and ability instead of seniority. But who gets to say you are fit? Who decides what is fit? It will turn out to be that if you are nice to your supervisor, if you do what your supervisor wants you to do, that is what makes you fit.

When these workers consider an organizational system that puts a premium on wholeness, they also see a system that will require manipulation, ingratiation, and conformity. They envision a new approach to the work system without also having considered the changes in the distribution of authority that would be necessary for an informating strategy to fully succeed. However, their concerns do warn of the potential for tyranny in a flexible and socially integrated organization. There are two immediate implications of this warning. First, there presumably will continue to be individuals for whom psychological distance and contractualized responsibilities are very important. To acertain extent, such persons might simply choose to avoid membership in a wholistic informated organization. Viewed in another way, the perspective offered by such individuals probably would be healthy for any learning community. They provide a sensitive barometer for organizational processes that violate respect for the individual or somehow endanger the balance that must exist between individual and organizational interests. A second crucial issue involves the need for mechanisms that can ensure equity and due process within the informated organization. Labor-management contracts grew out of a need for workers to protect themselves from the unilateral authority of their employers. I have already suggested that a new division of labor will not thrive in nor easily tolerate unilateral authority. Nonetheless, informated organizations will have to pay careful attention to developing a constitutional infrastructure that legitimates public debate and mutual influence. The clarity of individual rights within the enterprise is likely to become extremely important to the extent that the learning community requires the participation of the "total person" in its endeavors. Because such a system will exert considerable pressure on an individual's psychological boundaries, it will require mechanisms that can arbitrate competing interpretations of rights and obligations.

The notion of a wholistic informated organization must be qualified in still another way. I have offered a vision of organization in which there are no a priori designations of managers and managed. Instead, people move from the operating core to increasing responsibility and comprehensive influence based upon the degree to which they excel in the skills required by an informated task environment. The intellective skills developed at the data interface provide an important part of the basis for later movement to further rings in the concentric structure. These intellective skills are unlikely to be equally distributed, and the variation in their distribution can become a new source of hierarchical distinctions within even the most fluid organization. For example, at Cedar Bluff, new shades of meaning that began to appear in everyday language foretold the implicit criteria according to which some members would be more highly valued and so implied new sources of conflict as well. These ranking rules were expressed in the metaphorical devices most commonly used in discussions of the new technological environment. Typically, spatial designations were used to convey the worth of people and activities. 11 Consider these interrelated and most often repeated uses of the spatial metaphor:

I. Formal intelligence is up; experiential know-how is down.

A low-IQ operator will not accept a computer on the job. He will ignore the data in favor of his own way of doing things.

At Piney Wood, digital instruments seem like black magic, but at Cedar Bluff, operators are more educated. They average one to two years of college instead of just tenth grade. It's a higher, more advanced type of person.

2. Young is up; old is down.

Younger people find it easier to grasp the computer stuff because they are a higher caliber. Installing new technology is a message to the older operators. It says you must have computer and electronic skills. Some want to retire when they hear this.

We will be displacing older people who cannot qualify for the new jobs we are creating. They have to lift themselves up to standard.

3. Abstract work is up; manual work is down.

With this technology, we are bringing people to a higher level intellectually. It is more demanding, so you have to treat them differently. The physical demands are gone. There are no hammers and wrenches. To deal with problems now is more complex. The computer operators are going to be higher than the man on the floor, because the physical is easier to train that the brain.

We need a higher caliber of people now. They have to use their heads now, not just what comes below the neck. Not just do things.

Cedar Bluff's plant manager recounted their efforts to recruit a "high" caliber work force suited to the demands of "high" technology:

We believed that having a number of people with higher levels of understanding would create group dynamics effects that would result in better problem solving. A high level of knowledge, a high level of intelligence, will result in a higher individual contribution to the business. We need people to be able to optimize our use of the technology.

Workers had been promised a completely computerized operation and believed their jobs would be "push-button easy" with no "nasty work" to leave them soiled and exhausted at the end of the day. The reality was somewhat different, requiring a fair amount of interaction with the operating equipment, particularly in the first years as operators learned their way around the mill. Though their direct involvement with equipment was substantially less than it would have been in a conventional mill, their only frame of reference was that initial promise of "workless" work. Subtle hierarchical pressures began to take shape within this work force that had been so painstakingly recruited and primed for teamwork. People began to believe that the best and the brightest were those who excelled at the data interface, while others gravitated (or were nudged) toward maintenance work. A manager described this subtle sifting process:

We need our most-qualified people manning the computer screens. The people who really know what the equipment feels like and sounds like are becoming second-class citizens. In a traditional system, you spent your whole life developing that feel, and your livelihood depended on it. But the new generation must trust the computer screen. A new pecking order emerges. The computer screen takes more mental skills, and we reward those skills. Maybe the work in the field is intrinsically as valuable, but people around here don't see it that way.

There was a great deal of resentment when a crisis such as a pulp spill pulled the control room operators "out into the process" with tall rubber boots and shovels. Operators called these kinds of tasks "nasty work," a designation that clearly demarcated such activities from their "real" work at the data interface and provided a sense of psychological distance from the role of the laboring body. The special label signaled their preferred self-conception as people who could keep their collars clean:

If there is nasty work here, it means someone made a mistake. We don't like to get nasty. They told us all we would do is run computers.

The resentments that surround physical exertion once defined the boundary between managers and workers but now were felt within the operating work force. Subtle conflicts erupted among operators as they struggled to balance the satisfactions of intellective virtuosity against the comforts of group solidarity. Several operators at Cedar Bluff told of incidents in which they had made a special effort to break new ground in their understanding of the theoretical and practical aspects of the relationships among the process variables under their control. In each case, their discoveries had made it possible to solve a chronic problem and to generate significant savings. These operators became the target for derisive chiding from their peers. One man told of spending several months studying a problem that concerned the operation of the boiler. He conducted experiments, ran tests, monitored data, and read theories. He finally concluded what was wrong with the way they had been controlling the boilers. He wrote up the material in the format of a proficiency exam, supplying all the answers to the questions with the material he had amassed.

I wanted to share it with the team, to learn it better by teaching them. They said, "I don't want to hear that shit!" They didn't even want to see the book.

Another told of a similar experience. His discovery was of such economic value that he was sent to divisional headquarters to present his findings to the engineering staff.

They had me talk to a VP about it, so I got recognized. In fact, it embarrassed me that I got so much recognition. I would have liked less because of the peer pressure I got. None of my peers would go along with it from the beginning. So I had to go to all the shift coordinators instead. My peers treated me like someone who thinks they know it all, a company man. They said I was getting too close to management. That was tough to work with on a day-to-day basis. I learned how to work it now-just play it low key.

One explanation for this behavior is that it repeats the game of self protection so well known to generations of industrial workers. Gloria Schuck has labeled it "intellective rate busting": any worker who excels at the data interface increases the demands on the whole group. 13 Another interpretation is that the barriers that separate managerial from operating jobs have been maintained but simply pushed down to a lower level of the organization. A gulf continues to exist; what has changed is the distribution of members on each side.

The reactions of these disgruntled operators does not take into account the possibility of a change in the logic of imperative control-a logic that sustains the necessity of managerial authority and perpetuates adversarial feeling. In the context of that logic, workers learn that they cannot learn, and managers learn that they must already know. Thus, the interests of each group remain divergent, even as they are locked in the fitful interdependence of imperative control.

In contrast, an informating strategy implies a discontinuity in the logic of imperative control. The new division of learning should organize experience in ways that help perpetuate belief in a synthesis of interests and thus legitimate a learning environment that presupposes relationships among equals. Only under such conditions can knowledge be shared in a way that strengthens the collective effort. Barriers between information-intensive and non-information-intensive responsibilities limit equality and inhibit the spirit of inquiry that animates the informated organization, thus inviting adversarial games of self protection and domination.

The problem appears to be that not every job in even the most informated organization is likely to require intellective skills. Certainly in the near term, most organizations will continue to have some tasks that either are physically demanding or involve highly routinized versions of information handling. How such jobs are distributed becomes a crucial question for any organization that would commit itself to a new division of labor. Does the fact of these persistent differences between tasks lead inevitably to a reproduction of the labor-management caste system?

It is important to reiterate here that the presupposition of relations of equality does not imply a correlative assumption that all organizational members are exactly alike. In any organization, there will be some individuals who reject the demands of intellective work. Not everyone will contain in equal measure the internal commitment, motivation, or cognitive style associated with the responsibilities of life at the data interface. Some will be repelled by the mental stress of such work and find it too perplexing or anxiety-inducing. Others will prefer to conduct their work lives in motion, drawing sustenance from using their bodies to accomplish their tasks. There were many such operators in the Piney Wood Mill. One expressed his anxiety about his place in the emerging organization when he asked:

It is important to reiterate here that the presupposition of relations of equality does not imply a correlative assumption that all organizational members are exactly alike. In any organization, there will be some individuals who reject the demands of intellective work. Not everyone will contain in equal measure the internal commitment, motivation, or cognitive style associated with the responsibilities of life at the data interface. Some will be repelled by the mental stress of such work and find it too perplexing or anxiety-inducing. Others will prefer to conduct their work lives in motion, drawing sustenance from using their bodies to accomplish their tasks. There were many such operators in the Piney Wood Mill. One expressed his anxiety about his place in the emerging organization when he asked:

There was a smaller but still significant subset of workers at Cedar Bluff who felt deeply uncomfortable with the work available to them in the control rooms, despite the importance and prestige attached to it

The work in the control room is mental. It isn't as hard as out in the process, but I would rather be out on the floor working. I am moving out there. I don't like to be cramped up with no room to breathe. If they padded this room, I would have my body indented in the walls from bouncing off them all the time.

While some degree of hierarchy is inevitable in any social group, the values and beliefs that animate these distinctions can operate very differently from the traditional assumptions of imperative control. In the informated organization, there is no reason why these individuals could not elect to align themselves with the jobs best suited to their sensibilities or talents. The difference here lies in the voluntary, nonarbitrary, and reversible nature of their decisions. Instead of facing rigid and practically irreversible designations that are reinforced by virtually every aspect of organizational experience, these individuals could be free to self-select into or out of the more abstract forms of work.

The freedom of self-selection can be maintained as long as institutional arrangements ensure the full participation of these members in the political life of the learning community. There are several mechanisms through which this could be accomplished. First, these individuals, like other members, would have broad access to the streams of circulating information within the organization. Were they to so choose, they could be rotated into positions that provided a greater opportunity for the development of their intellective potential. Second, the close coordination and integration of the various aspects of the production process would require these members to keep abreast of critical information from daily operational issues to changes in the business context, or a new direction in the strategic plan. Finally, the rewards available for their work would be commensurate with their value to the production process and not undervalued as a matter of 14 course.

THE INFORMATED ORGANIZATION AND RECENT TRENDS IN WORK ORGANIZATION

As the years passed at Cedar Bluff, managers began to confront more openly and honestly the challenges to their skills that had been unleashed by the informating process. It was not untypical for organizations pursuing high commitment strategies to feel the strains of participative management and to experience a good bit of conflict concerning the limits of managerial prerogatives. At Cedar Bluff, however, the demands on managers had become relentlessly insistent. The knowledge requirements of the data interface, the vulnerability of plant performance to variations in operator skill and motivation, and the broad accessibility of data had lent new urgency to questions about the skills, roles, and structures that should define the organization. This was vividly illustrated by the turmoil that surrounded the development of a new pay and promotion system for hourly workers. The operators had become increasingly proficient in operating the plant, and after several years of producing pulp at far below the equipment's true capacity, production levels began to climb. As operators' skills improved, so did their dissatisfaction with the pay and promotion system. They believed that it arbitrarily limited the amount of learning for which they could be rewarded. Amid mounting dissension, a committee consisting of operators and managers from the various areas of the plant was appointed to gather data on the problem and to recommend new policies. The plant had reached production levels of more than nine hundred tons a day, but as the committee went into session, operators were heard to say, "That's the last nine-hundred-ton day this plant will see until the pay and promotion problems are resolved."

In the months that followed, their predictions came true; the plant returned to earlier production levels. Cedar Bluff's managers were especially frustrated because they could not identify the causes of the downturn. Only rarely could a manager point to something that operators were doing incorrectly that might be contributing to poor production. They concluded that the disappointing performance could be attributed, not to what operators were doing, but to what they were not doing. The operators' errors were sins of omission-an underutilization of the data interface resulting from their refusal to notice, to think, to explore, to experiment, or to improve. In other words, the power of the new technology was going to waste. Managers felt helpless to alter the situation. It was only when the new pay and promotion system was finally developed and accepted by a majority vote that production levels began to climb once again.

There are several lessons to be learned here. First, the requirements of an informating strategy support existing work-improvement efforts, such as the high commitment approach to work force management, with its emphasis on self-managing teams, participation, and decentralization. Organizations that are already pursuing this approach are more likely to have developed both the ideological context and the social skills necessary to plan and implement an informating strategy. In this regard, Cedar Bluff provides an important contrast not only to Piney Wood but also to organizations, like Global Bank Brazil, that have minimal experience with work-system innovation. Second, the demands for a redistribution of knowledge and the consequent challenge to the managerial role that can be unleashed by the informating process are likely to exacerbate the growing pains associated with participative management and to accelerate the need for positive change. Third, organizational innovations designed to create high commitment work systems typically have focused upon the hourly work force. In most cases, and Cedar Bluff is one example, the managerial hierarchy has remained relatively intact, while team organization and pay-for-skill systems have been designed for the operational work force. In contrast, an informating strategy suggests the need for a more wholistic reconceptualization of the skills, roles, and structures that define the total organization. Partial change efforts, as at Tiger Creek, or technology-driven initiatives, as at Global Bank Brazil, are unlikely to result in the kind of learning environment necessary for an ongoing and robust approach to the informating process. Finally, managing in an informated environment is a delicate human process. The ability to use information for real business benefit is as much a function of the quality of commitment and relationships as it is a function of the quality of intellective skills.

The words of the clerk at Global Bank Brazil continue to echo: "Will things be any different now?" In response, we can say that the opportunity is there, and we now know more about what it will take. An informating strategy requires a comprehensive vision based upon an understanding of the unique capacities of intelligent technology and the opportunity to use the organization to liberate those capacities. It means forging a new logic of technological deployment based upon that vision. A coherent rationale will be necessary, particularly when the tide of conventional thinking and familiar assumptions on this subject can submerge many important choices regarding basic technological design and management. Cedar Bluff's plant manager foresaw this danger:

The technology is going in the direction that says one person operates the master controls. Is the technology right? We don't believe it is, and we are working hard to convince our vendors to leave the design flexible enough so that it does not preclude the uses we want to make of it.

The informated organization does move in another direction. It relies on the human capacities for teaching and learning, criticism and insight. It implies an approach to business improvement that rests upon the improvement and innovation made possible by the enhanced comprehensibility of core processes. It reflects a fertile interdependence between the human mind and some of its most sophisticated productions. As one worker from Tiger Creek mused:

If you don't let people grow and develop and make more decisions, it's a waste of human life-a waste of human potential. If you don't use your knowledge and skill, it's a waste of life. Using the technology to its full potential means using the man to his full potential.

APPENDIX A : THE SCOPE OF INFORMATION TECHNOLOGY IN THE MODERN WORKPLACE

Information technology is a label that reflects the convergence of several streams of technical developments, including microelectronics, computer science, telecommunications, software engineering, and system analysis. It is a technology that dramatically increases the ability to record, store, analyze, and transmit information in ways that permit flexibility, accuracy, immediacy, geographic independence, volume, and complexity. Information technology has a unique capability to restructure operations that depend upon information for the purposes of transaction, record keeping, analysis, control, or communication.

There is hardly a segment of the U.S. economy that has not been penetrated by some form of computer-based technology. The core of this technology is the silicon-integrated circuit, or "chip." The equivalent of hundreds of thousands of transistors can be built on a silicon chip measuring no more than a fraction of an inch. The astonishing reductions in the cost of these microprocessors, coupled with their equally impressive performance levels, have been exhaustively documented. During the past thirty years, the price per second of instruction has decreased dramatically: a computation that now costs one dollar would have cost about $30,000 in 1950. 1 Porter and Millar calculate that the cost of computer power relative to the cost of manual information processing is at least eight thousand times less than the cost thirty years ago. Between 1958 and 1980, the amount of time needed for one electronic operation fell by a factor of 80 million. They also cite Department of Defense studies that show that the error rate in recording data through bar coding is one in 3 million, compared to one error in three hundred manual data entries. 2 During the past fifteen years, the memory capacity of an integrated circuit has increased by a factor of one thousand, as has its reliability. As another writer remarked, "If the automotive industry had paralleled the advances that the computer industry has experienced in the last 25 years, a Rolls Royce would cost 50 cents and would deliver 1 5 million miles to the gallon.

Numerous studies by economists and industry analysts have concluded that computer-based information technologies will profoundly affect the structure of the U.S. economy. 4 One analyst estimates that in 1980, approximately 10 million Americans interacted daily with a video display terminal and that this number would increase to 2 5 million by 1 990. 5 Another estimates that by the year 1 990, 50 million American office workers will spend a significant portion of their workday interacting with a computer terminal of some sort. 6 Another expert predicts that in 1990, 65 percent of all professional, managerial, technical, and administrative workers (a group that now constitutes almost half the labor force) will depend upon computer-based workstations. 7 The Congressional Office of Technology Assessment has predicted that there will be at least one computer terminal for every two or three office workers by 1990. 8 One recent survey of 5 30 employees representing every organizational level in twenty-six companies offers some accurate, if more limited, findings. Respondents were divided according to occupational groups: executives, managers, professionals, technicians, secretaries, and clerks. Averaging across all categories, researchers found that 67 percent of all the respondents interacted directly with a computer during the regular course of their work and that 26 percent expected to do so in the near future. When executives (for whom the figures were 36 percent and 46 percent, respectively) are eliminated from this average, the figure for current usage jumps to 74 percent. Of these same respondents, about half reported using the computer during 30 percent or less of their working time, another quarter of the group reported spending up to 70 percent of their time at the terminal, while the final quarter spent up to 100 percent of their workday at the computer terminal. Such figures suggest the degree to which information technology is affecting everyday life across a broad spectrum of the work force.

THE SERVICE SECTOR

Information technology has different applications in the service and manufacturing sectors of the economy. In the service sector, the technology has been used primarily to meet the mushrooming demands of handling information. Businesses now are faced with the task of controlling 400 billion documents, a number that is expected to increase at the rate of 72 billion per year. 10 As the technology develops, clerks working with documents in the back office can enter data directly into terminals linked to a computer mainframe, which does the actual processing. In many cases, even the documents have disappeared; clerks are able to perform all their transactions through the electronic medium. Such on-line applications began to be widespread in the early 1980s and have become a prominent trend in most large service organizations.

The early 1980s also saw new applications of information technology for professionals, managers, and technical workers. The introduction of small, stand-alone word processors, microcomputers, and personal computers made it easier for nonspecialists to use databases, to manipulate text and quantitative data, to generate tables and graphic displays, to utilize analytical software, and to communicate with one another through a computer network. Today, microcomputers are being linked to one another as well as to central computers so that they can be used independently or in conjunction with corporate data bases. These emerging systems also can be linked to external data bases and communications networks; further, they can cross organizational and national boundaries.

Most large information-processing organizations, such as banks and insurance companies, still require a sizeable clerical work force to enter data and perform routine transactions on a computer system. Many such organizations are searching for ways to circumvent this massive clerical effort and at the same time increase the continuity of their operations. An increasingly popular solution is to incorporate the data-entry function in operations that are external to the organization. For example, when a customer uses an automated teller machine, the data-entry function is accomplished automatically without clerical input. A hospital computer may send bills directly to the health insurer's computer, which in turn instructs the bank's computer to transfer funds. An interorganizational computer system thus can eliminate routine clerical work in several organizations.

Technologies that were once relatively distinct now have begun to converge. The functions once accomplished by typewriters, printing presses, copying machines, telephones, files, calculators, and mail sorting systems, are becoming either subsumed within or linked to the functioning of the comprehensive computer-based network. Software is more powerful and easier to use. New procedures allow people to interact with the computer in ways that encourage more familiarity and immediacy, such as touch-sensitive screens and voice-activated II programs.

THE MANUFACTURING SECTOR

Applications of computer technology in the manufacturing sector have developed along distinct lines in discrete parts manufacturing (for example, automobiles, farm equipment, electronics) and in the continuous-process industries (for example, oil refineries, chemical processors, food and beverages, paper and pulp), which each face different problems in manufacturing process. Continuous-process production typically involves a flow of material that can pass through several stages before emerging in its final form. The production process manipulates the composition of materials by chemical reaction, purification, and blending of component materials. At each stage, different operations are performed, such as heating, cooling, mixing, chemical reaction, distillation, drying, or pressurization. This requires continual measurement and control of variables like time, temperature, raw material characteristics, steam pressure, chemical levels, densities, viscosities, liquid levels, flow rates, et cetera. Twenty percent of all industrial computers, and 40 percent or more of all minicomputers and microcomputers, are accounted for by continuous-process applications designed for monitoring, analysis, control, and optimization. Process computers evolved from simple data recording devices: they collected real-time operational data and set off alarms under critical conditions. The next phase of development produced "open loop" systems, in which operators rely on either first-hand observations of the process or data generated by microprocessor-based sensors and programmable logic controllers built into operating equipment. With this information, they can use the computer system for mathematical analysis to help them adjust process conditions. The computer also becomes the medium through which operators can manipulate process variables and parameters to meet the desired conditions. Most recent developments include supervisory process-control systems that can be programmed to receive information directly from instruments monitoring the process, to set control points, to perform computations, and to adjust control variables to continually approximate optimum levels of functioning. Theoretically, such systems mimimize the need for operator involvement, except in upset conditions.

As plants apply these control systems to their operations, the amount of physical interaction with the production process is reduced and workers typically operate from remote control rooms, where they monitor video terminals that display data reflecting the state of the production process. In some process industries, such as oil refining or steel production, where the various steps of the conversion from raw material to output are well understood, there has been considerable progress in developing supervisory control systems. These were among the first industries to comprehensively apply microprocessor-based control technology. In other process industries, such as paper and pulp or food and beverages, the manufacturing process has not yet been entirely explicated, and there are no adequate sensing devices to measure all key variables. These industries have been slower to adopt the technology and face more difficulty in programming supervisory control.

In discrete parts manufacturing, the problems are more geometric in nature, involving the placement of parts and equipment in relation to one another, as well as their movement from one stage of assembly to another. Typical applications of information technology in discrete parts manufacturing include computer-aided design (drafting and engineering), computer-aided manufacturing (robots and numerically controlled machine tools), flexible manufacturing systems, automated material handling, and automated storage-and-retrieval systems. Computer-aided design systems facilitate the use of previous designs and allow more rapid design changes. They can improve the design process by allowing engineers to try out a dozen or a hundred different variations, when they previously might have been limited to building three or four prototype models. It is estimated that in 1983, there were thirty-two thousand computer-aided design workstations being used in the United States.

Robots are mechanical manipulators that can be programmed to move workpieces or tools along a prescribed path. While most robots today can perform only relatively well-defined and repetitive tasks, efforts are under way to incorporate more intelligence and sensory capacities within these machines. There are many differing estimates as to the extent of robotization in manufacturing. The Robot Institute of America indicates that in 1983, 66 percent (31,900) of the world's robots were operating in Japan and 1 3 percent ( 6,301) were in the United States. 14 Nobel-prize-winning economist Wassily Leontief has predicted a 30 to 40 percent annual growth rate in the market for industrial robots between 198 5 and 1990.

Computer-numerically-controlled machine tools fashion metal according to programmed instructions that indicate the desired dimensions of a part and the sequence to be followed in the machining process. Since the late 1 970s, these machines have been equipped with microprocessors or dedicated minicomputers and frequently include a screen and keyboard for writing or editing the programs that guide the equipment. These devices are the basis for "direct numerical control machines," in which a larger mini-computer or mainframe computer is used to program and run more than one numerically controlled tool simultaneously. The Congressional Office of Technology Assessment reports that as the price of small computers has declined, these machine tools are being equipped with microcomputers that can be linked to one another and to a central controlling computer, thus creating a hierarchy of computer control. 16 Many observers have remarked upon the relatively slow diffusion of these applications due to their high capital cost and the bottlenecks in developing and maintaining software programs. In 1983, numerically controlled machine tools represented only about 5 percent of the machine tools in U.S. metalworking, but this may change rapidly. 17 For example, General Motors Corporation has stated that by the end of the 1980s, 90 percent of all new capital investments will be in computer numerically controlled machines.

Flexible manufacturing systems integrate these more discrete applications of technology. They consist of computer-controlled machining centers that sculpt complicated metal parts at high speed and with great reliability, robots that handle the parts, and remotely guided carts that deliver materials. These components are linked by computer-based controls that dictate what will happen at each stage of the manufacturing sequence. Many consider the great advantage of these systems to be their ability to achieve low-cost production in small volumes, without having to rely on the economics of scale associated with mass production. "A flexible automation system can turn out a small batch or even a single copy of a product as efficiently as a production line designed to turn out a million identical items." 19 These systems are complex and costly, and thus still relatively rare. Reliable statistics are difficult to obtain because of conflicting definitions of precisely what level of integration and control constitutes "flexibility." However, Fortune magazine recently counted thirty such systems operating in the United States and considerably more in Japan. According to their report, one Japanese firm alone, Toyoda Machine Tool Co., has thirty such systems in operation. 20 Despite the still-modest number of such systems in the United States, Fortune estimates that by 1990, the sales of equipment to support flexible manufacturing-robots, computer controls, material-handling devices, et cetera-will rise to $30 billion annually from 1982 levels of $4 billion.

While information technologies in these two manufacturing domains- discrete parts and continuous-process-have developed separately in order to address distinct types of problems, their differences are increasingly diminished by new developments that use computer systems for comprehensive production management. This approach, known as computer-integrated manufacturing, increases the continuity of the production process in discrete parts manufacturing, thus bringing it closer to the "optimal model" of manufacturing in which continuity and controllability are maximized. 21 Under these conditions, workers' tasks begin to look very similar to those of operators in the continuous process environment, as they come to emphasize monitoring and control (though the types of variables and procedures continue to differ). A recent study published by the National Academy of Sciences points to the integration of computer-aided design and manufacturing with manufacturing resource planning (software that translates demand for products into the parts needed to produce them and then orders the parts from inventory or from suppliers) and computer-aided process planning (software that routes parts through the factory to maximize operating time and to eliminate bottlenecks). "The four technologies are increasingly 'speaking' to each other through local-area networks, and formerly isolated applications are being linked as computer integrated manufacturing. 

Experts continue to disagree on the ultimate consequences of these developments for employment in the manufacturing sector. Some believe that computer-integrated manufacturing will provide the basis for nearly unmanned factories; others insist that automation rarely can be complete and that people will be needed to monitor, control, maintain, manage, and plan these processes, although at lower levels of employment than have characterized older technologies. Whether the unmanned factory is a likely scenario in some cases or not, the coming decade will continue to see manufacturing operations that depend upon people, not just computers. These organizations will become more alike as they are able to increase the continuity and controllability of production through computer integration.


/ 015. Bostrom, Nick. Superintelligence: Paths, Dangers, Strategies, 2014.


PREFACE

Inside your cranium is the thing that does the reading. This thing, the human brain, has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that we owe our dominant position on the planet. Other animals have stronger muscles and sharper claws, but we have cleverer brains. Our modest advantage in general intelligence has led us to develop language, technology, and complex social organization. The advantage has compounded over time, as each generation has built on the achievements of its predecessors.

If some day we build machine brains that surpass human brains in general intelligence, then this new superintelligence could become very powerful. And, as the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species would depend on the actions of the machine superintelligence.

We do have one advantage: we get to build the stuff. In principle, we could build a kind of superintelligence that would protect human values. We would certainly have strong reason to do so. In practice, the control problem—the problem of how to control what the superintelligence would do—looks quite difficult. It also looks like we will only get one chance. Once unfriendly superintelligence exists, it would prevent us from replacing it or changing its preferences. Our fate would be sealed.

In this book, I try to understand the challenge presented by the prospect of superintelligence, and how we might best respond. This is quite possibly the most important and most daunting challenge humanity has ever faced. And— whether we succeed or fail—it is probably the last challenge we will ever face.

It is no part of the argument in this book that we are on the threshold of a big breakthrough in artificial intelligence, or that we can predict with any precision when such a development might occur. It seems somewhat likely that it will happen sometime in this century, but we don’t know for sure. The first couple of chapters do discuss possible pathways and say something about the question of timing. The bulk of the book, however, is about what happens after. We study the kinetics of an intelligence explosion, the forms and powers of superintelligence, and the strategic choices available to a superintelligent agent that attains a decisive advantage. We then shift our focus to the control problem and ask what we could do to shape the initial conditions so as to achieve a survivable and beneficial outcome. Toward the end of the book, we zoom out and contemplate the larger picture that emerges from our investigations. Some suggestions are offered on what ought to be done now to increase our chances of avoiding an existential catastrophe later.

This has not been an easy book to write. I hope the path that has been cleared will enable other investigators to reach the new frontier more swiftly and conveniently, so that they can arrive there fresh and ready to join the work to further expand the reach of our comprehension. (And if the way that has been made is a little bumpy and bendy, I hope that reviewers, in judging the result, will not underestimate the hostility of the terrain ex ante!)

This has not been an easy book to write: I have tried to make it an easy book to read, but I don’t think I have quite succeeded. When writing, I had in mind as the target audience an earlier time-slice of myself, and I tried to produce a kind of book that I would have enjoyed reading. This could prove a narrow demographic. Nevertheless, I think that the content should be accessible to many people, if they put some thought into it and resist the temptation to instantaneously misunderstand each new idea by assimilating it with the most similar-sounding cliché available in their cultural larders. Non-technical readers should not be discouraged by the occasional bit of mathematics or specialized vocabulary, for it is always possible to glean the main point from the surrounding explanations. (Conversely, for those readers who want more of the nitty-gritty, there is quite a lot to be found among the endnotes.1)

Many of the points made in this book are probably wrong.2 It is also likely that there are considerations of critical importance that I fail to take into account, thereby invalidating some or all of my conclusions. I have gone to some length to indicate nuances and degrees of uncertainty throughout the text— encumbering it with an unsightly smudge of “possibly,” “might,” “may,” “could well,” “it seems,” “probably,” “very likely,” “almost certainly.” Each qualifier has been placed where it is carefully and deliberately. Yet these topical applications of epistemic modesty are not enough; they must be supplemented here by a systemic admission of uncertainty and fallibility. This is not false modesty: for while I believe that my book is likely to be seriously wrong and misleading, I think that the alternative views that have been presented in the literature are substantially worse—including the default view, or “null hypothesis,” according to which we can for the time being safely or reasonably ignore the prospect of superintelligence.

CHAPTER 1: Past developments and present capabilities

We begin by looking back. History, at the largest scale, seems to exhibit a sequence of distinct growth modes, each much more rapid than its predecessor. This pattern has been taken to suggest that another (even faster) growth mode might be possible. However, we do not place much weight on this observation—this is not a book about “technological acceleration” or “exponential growth” or the miscellaneous notions sometimes gathered under the rubric of “the singularity.” Next, we review the history of artificial intelligence. We then survey the field’s current capabilities. Finally, we glance at some recent expert opinion surveys, and contemplate our ignorance about the timeline of future advances.

Growth modes and big history

A mere few million years ago our ancestors were still swinging from the branches in the African canopy. On a geological or even evolutionary timescale, the rise of Homo sapiens from our last common ancestor with the great apes happened swiftly. We developed upright posture, opposable thumbs, and— crucially—some relatively minor changes in brain size and neurological organization that led to a great leap in cognitive ability. As a consequence, humans can think abstractly, communicate complex thoughts, and culturally accumulate information over the generations far better than any other species on the planet.

These capabilities let humans develop increasingly efficient productive technologies, making it possible for our ancestors to migrate far away from the rainforest and the savanna. Especially after the adoption of agriculture, population densities rose along with the total size of the human population. More people meant more ideas; greater densities meant that ideas could spread more readily and that some individuals could devote themselves to developing specialized skills. These developments increased the rate of growth of economic productivity and technological capacity. Later developments, related to the Industrial Revolution, brought about a second, comparable step change in the rate of growth.

Such changes in the rate of growth have important consequences. A few hundred thousand years ago, in early human (or hominid) prehistory, growth was so slow that it took on the order of one million years for human productive capacity to increase sufficiently to sustain an additional one million individuals living at subsistence level. By 5000 BC, following the Agricultural Revolution, the rate of growth had increased to the point where the same amount of growth took just two centuries. Today, following the Industrial Revolution, the world economy grows on average by that amount every ninety minutes.1

Even the present rate of growth will produce impressive results if maintained for a moderately long time. If the world economy continues to grow at the same pace as it has over the past fifty years, then the world will be some 4.8 times richer by 2050 and about 34 times richer by 2100 than it is today.

Yet the prospect of continuing on a steady exponential growth path pales in comparison to what would happen if the world were to experience another step change in the rate of growth comparable in magnitude to those associated with the Agricultural Revolution and the Industrial Revolution. The economist Robin Hanson estimates, based on historical economic and population data, a characteristic world economy doubling time for Pleistocene hunter–gatherer society of 224,000 years; for farming society, 909 years; and for industrial society, 6.3 years.3 (In Hanson’s model, the present epoch is a mixture of the farming and the industrial growth modes—the world economy as a whole is not yet growing at the 6.3-year doubling rate.) If another such transition to a different growth mode were to occur, and it were of similar magnitude to the previous two, it would result in a new growth regime in which the world economy would double in size about every two weeks.

Such a growth rate seems fantastic by current lights. Observers in earlier epochs might have found it equally preposterous to suppose that the world economy would one day be doubling several times within a single lifespan. Yet that is the extraordinary condition we now take to be ordinary.

The idea of a coming technological singularity has by now been widely popularized, starting with Vernor Vinge’s seminal essay and continuing with the writings of Ray Kurzweil and others.4 The term “singularity,” however, has been used confusedly in many disparate senses and has accreted an unholy (yet almost millenarian) aura of techno-utopian connotations.5 Since most of these meanings and connotations are irrelevant to our argument, we can gain clarity by dispensing with the “singularity” word in favor of more precise terminology.

The singularity-related idea that interests us here is the possibility of an intelligence explosion, particularly the prospect of machine superintelligence. There may be those who are persuaded by growth diagrams like the ones in Figure 1 that another drastic change in growth mode is in the cards, comparable to the Agricultural or Industrial Revolution. These folk may then reflect that it is hard to conceive of a scenario in which the world economy’s doubling time shortens to mere weeks that does not involve the creation of minds that are much faster and more efficient than the familiar biological kind. However, the case for taking seriously the prospect of a machine intelligence revolution need not rely on curve-fitting exercises or extrapolations from past economic growth. As we shall see, there are stronger reasons for taking heed.

Great expectations

Machines matching humans in general intelligence—that is, possessing common sense and an effective ability to learn, reason, and plan to meet complex information-processing challenges across a wide range of natural and abstract domains—have been expected since the invention of computers in the 1940s. At that time, the advent of such machines was often placed some twenty years into the future.7 Since then, the expected arrival date has been receding at a rate of one year per year; so that today, futurists who concern themselves with the possibility of artificial general intelligence still often believe that intelligent machines are a couple of decades away

Two decades is a sweet spot for prognosticators of radical change: near enough to be attention-grabbing and relevant, yet far enough to make it possible to suppose that a string of breakthroughs, currently only vaguely imaginable, might by then have occurred. Contrast this with shorter timescales: most technologies that will have a big impact on the world in five or ten years from now are already in limited use, while technologies that will reshape the world in less than fifteen years probably exist as laboratory prototypes. Twenty years may also be close to the typical duration remaining of a forecaster’s career, bounding the reputational risk of a bold prediction.

From the fact that some individuals have overpredicted artificial intelligence in the past, however, it does not follow that AI is impossible or will never be developed.9 The main reason why progress has been slower than expected is that the technical difficulties of constructing intelligent machines have proved greater than the pioneers foresaw. But this leaves open just how great those difficulties are and how far we now are from overcoming them. Sometimes a problem that initially looks hopelessly complicated turns out to have a surprisingly simple solution (though the reverse is probably more common).

In the next chapter, we will look at different paths that may lead to humanlevel machine intelligence. But let us note at the outset that however many stops there are between here and human-level machine intelligence, the latter is not the final destination. The next stop, just a short distance farther along the tracks, is superhuman-level machine intelligence. The train might not pause or even decelerate at Humanville Station. It is likely to swoosh right by.

The mathematician I. J. Good, who had served as chief statistician in Alan Turing’s code-breaking team in World War II, might have been the first to enunciate the essential aspects of this scenario. In an oft-quoted passage from 1965, he wrote:

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.

It may seem obvious now that major existential risks would be associated with such an intelligence explosion, and that the prospect should therefore be examined with the utmost seriousness even if it were known (which it is not) to have but a moderately small probability of coming to pass. The pioneers of artificial intelligence, however, notwithstanding their belief in the imminence of human-level AI, mostly did not contemplate the possibility of greater-thanhuman AI. It is as though their speculation muscle had so exhausted itself in conceiving the radical possibility of machines reaching human intelligence that it could not grasp the corollary—that machines would subsequently become superintelligent.

The AI pioneers for the most part did not countenance the possibility that their enterprise might involve risk.11 They gave no lip service—let alone serious thought—to any safety concern or ethical qualm related to the creation of artificial minds and potential computer overlords: a lacuna that astonishes even against the background of the era’s not-so-impressive standards of critical technology assessment.12 We must hope that by the time the enterprise eventually does become feasible, we will have gained not only the technological proficiency to set off an intelligence explosion but also the higher level of mastery that may be necessary to make the detonation survivable.

But before we turn to what lies ahead, it will be useful to take a quick glance at the history of machine intelligence to date.

Seasons of hope and despair

In the summer of 1956 at Dartmouth College, ten scientists sharing an interest in neural nets, automata theory, and the study of intelligence convened for a sixweek workshop. This Dartmouth Summer Project is often regarded as the cockcrow of artificial intelligence as a field of research. Many of the participants would later be recognized as founding figures. The optimistic outlook among the delegates is reflected in the proposal submitted to the Rockefeller Foundation, which provided funding for the event:

We propose that a 2 month, 10 man study of artificial intelligence be carried out…. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines that use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.

In the six decades since this brash beginning, the field of artificial intelligence has been through periods of hype and high expectations alternating with periods of setback and disappointment.

The first period of excitement, which began with the Dartmouth meeting, was later described by John McCarthy (the event’s main organizer) as the “Look, Ma, no hands!” era. During these early days, researchers built systems designed to refute claims of the form “No machine could ever do X!” Such skeptical claims were common at the time. To counter them, the AI researchers created small systems that achieved X in a “microworld” (a well-defined, limited domain that enabled a pared-down version of the performance to be demonstrated), thus providing a proof of concept and showing that X could, in principle, be done by machine. One such early system, the Logic Theorist, was able to prove most of the theorems in the second chapter of Whitehead and Russell’s Principia Mathematica, and even came up with one proof that was much more elegant than the original, thereby debunking the notion that machines could “only think numerically” and showing that machines were also able to do deduction and to invent logical proofs.13 A follow-up program, the General Problem Solver, could in principle solve a wide range of formally specified problems.14 Programs that could solve calculus problems typical of first-year college courses, visual analogy problems of the type that appear in some IQ tests, and simple verbal algebra problems were also written.15 The Shakey robot (so named because of its tendency to tremble during operation) demonstrated how logical reasoning could be integrated with perception and used to plan and control physical activity.16 The ELIZA program showed how a computer could impersonate a Rogerian psychotherapist.17 In the mid-seventies, the program SHRDLU showed how a simulated robotic arm in a simulated world of geometric blocks could follow instructions and answer questions in English that were typed in by a user.18 In later decades, systems would be created that demonstrated that machines could compose music in the style of various classical composers, outperform junior doctors in certain clinical diagnostic tasks, drive cars autonomously, and make patentable inventions.19 There has even been an AI that cracked original jokes.20 (Not that its level of humor was high—“What do you get when you cross an optic with a mental object? An eye-dea”—but children reportedly found its puns consistently entertaining.)

The methods that produced successes in the early demonstration systems often proved difficult to extend to a wider variety of problems or to harder problem instances. One reason for this is the “combinatorial explosion” of possibilities that must be explored by methods that rely on something like exhaustive search. Such methods work well for simple instances of a problem, but fail when things get a bit more complicated. For instance, to prove a theorem that has a 5-line long proof in a deduction system with one inference rule and 5 axioms, one could simply enumerate the 3,125 possible combinations and check each one to see if it delivers the intended conclusion. Exhaustive search would also work for 6-and 7-line proofs. But as the task becomes more difficult, the method of exhaustive search soon runs into trouble. Proving a theorem with a 50-line proof does not take ten times longer than proving a theorem that has a 5-line proof: rather, if one uses exhaustive search, it requires combing through 550 ≈ 8.9 × 1034 possible sequences—which is computationally infeasible even with the fastest supercomputers.

To overcome the combinatorial explosion, one needs algorithms that exploit structure in the target domain and take advantage of prior knowledge by using heuristic search, planning, and flexible abstract representations—capabilities that were poorly developed in the early AI systems. The performance of these early systems also suffered because of poor methods for handling uncertainty, reliance on brittle and ungrounded symbolic representations, data scarcity, and severe hardware limitations on memory capacity and processor speed. By the mid- 1970s, there was a growing awareness of these problems. The realization that many AI projects could never make good on their initial promises led to the onset of the first “AI winter”: a period of retrenchment, during which funding decreased and skepticism increased, and AI fell out of fashion.

A new springtime arrived in the early 1980s, when Japan launched its Fifth- Generation Computer Systems Project, a well-funded public–private partnership that aimed to leapfrog the state of the art by developing a massively parallel computing architecture that would serve as a platform for artificial intelligence. This occurred at peak fascination with the Japanese “post-war economic miracle,” a period when Western government and business leaders anxiously sought to divine the formula behind Japan’s economic success in hope of replicating the magic at home. When Japan decided to invest big in AI, several other countries followed suit.

The ensuing years saw a great proliferation of expert systems. Designed as support tools for decision makers, expert systems were rule-based programs that made simple inferences from a knowledge base of facts, which had been elicited from human domain experts and painstakingly hand-coded in a formal language. Hundreds of these expert systems were built. However, the smaller systems provided little benefit, and the larger ones proved expensive to develop, validate, and keep updated, and were generally cumbersome to use. It was impractical to acquire a standalone computer just for the sake of running one program. By the late 1980s, this growth season, too, had run its course.

The Fifth-Generation Project failed to meet its objectives, as did its counterparts in the United States and Europe. A second AI winter descended. At this point, a critic could justifiably bemoan “the history of artificial intelligence research to date, consisting always of very limited success in particular areas, followed immediately by failure to reach the broader goals at which these initial successes seem at first to hint.”21 Private investors began to shun any venture carrying the brand of “artificial intelligence.” Even among academics and their funders, “AI” became an unwanted epithet.

Technical work continued apace, however, and by the 1990s, the second AI winter gradually thawed. Optimism was rekindled by the introduction of new techniques, which seemed to offer alternatives to the traditional logicist paradigm (often referred to as “Good Old-Fashioned Artificial Intelligence,” or “GOFAI” for short), which had focused on high-level symbol manipulation and which had reached its apogee in the expert systems of the 1980s. The newly popular techniques, which included neural networks and genetic algorithms, promised to overcome some of the shortcomings of the GOFAI approach, in particular the “brittleness” that characterized classical AI programs (which typically produced complete nonsense if the programmers made even a single slightly erroneous assumption). The new techniques boasted a more organic performance. For example, neural networks exhibited the property of “graceful degradation”: a small amount of damage to a neural network typically resulted in a small degradation of its performance, rather than a total crash. Even more importantly, neural networks could learn from experience, finding natural ways of generalizing from examples and finding hidden statistical patterns in their input.23 This made the nets good at pattern recognition and classification problems. For example, by training a neural network on a data set of sonar signals, it could be taught to distinguish the acoustic profiles of submarines, mines, and sea life with better accuracy than human experts—and this could be done without anybody first having to figure out in advance exactly how the categories were to be defined or how different features were to be weighted.

While simple neural network models had been known since the late 1950s, the field enjoyed a renaissance after the introduction of the backpropagation algorithm, which made it possible to train multilayered neural networks.24 Such multilayered networks, which have one or more intermediary (“hidden”) layers of neurons between the input and output layers, can learn a much wider range of functions than their simpler predecessors.25 Combined with the increasingly powerful computers that were becoming available, these algorithmic improvements enabled engineers to build neural networks that were good enough to be practically useful in many applications.

The brain-like qualities of neural networks contrasted favorably with the rigidly logic-chopping but brittle performance of traditional rule-based GOFAI systems—enough so to inspire a new “-ism,” connectionism, which emphasized the importance of massively parallel sub-symbolic processing. More than 150,000 academic papers have since been published on artificial neural networks, and they continue to be an important approach in machine learning.

Evolution-based methods, such as genetic algorithms and genetic programming, constitute another approach whose emergence helped end the second AI winter. It made perhaps a smaller academic impact than neural nets but was widely popularized. In evolutionary models, a population of candidate solutions (which can be data structures or programs) is maintained, and new candidate solutions are generated randomly by mutating or recombining variants in the existing population. Periodically, the population is pruned by applying a selection criterion (a fitness function) that allows only the better candidates to survive into the next generation. Iterated over thousands of generations, the average quality of the solutions in the candidate pool gradually increases. When it works, this kind of algorithm can produce efficient solutions to a very wide range of problems—solutions that may be strikingly novel and unintuitive, often looking more like natural structures than anything that a human engineer would design. And in principle, this can happen without much need for human input beyond the initial specification of the fitness function, which is often very simple. In practice, however, getting evolutionary methods to work well requires skill and ingenuity, particularly in devising a good representational format. Without an efficient way to encode candidate solutions (a genetic language that matches latent structure in the target domain), evolutionary search tends to meander endlessly in a vast search space or get stuck at a local optimum. Even if a good representational format is found, evolution is computationally demanding and is often defeated by the combinatorial explosion.

Neural networks and genetic algorithms are examples of methods that stimulated excitement in the 1990s by appearing to offer alternatives to the stagnating GOFAI paradigm. But the intention here is not to sing the praises of these two methods or to elevate them above the many other techniques in machine learning. In fact, one of the major theoretical developments of the past twenty years has been a clearer realization of how superficially disparate techniques can be understood as special cases within a common mathematical framework. For example, many types of artificial neural network can be viewed as classifiers that perform a particular kind of statistical calculation (maximum likelihood estimation).26 This perspective allows neural nets to be compared with a larger class of algorithms for learning classifiers from examples —“decision trees,” “logistic regression models,” “support vector machines,” “naive Bayes,” “k-nearest-neighbors regression,” among others.27 In a similar manner, genetic algorithms can be viewed as performing stochastic hillclimbing, which is again a subset of a wider class of algorithms for optimization. Each of these algorithms for building classifiers or for searching a solution space has its own profile of strengths and weaknesses which can be studied mathematically. Algorithms differ in their processor time and memory space requirements, which inductive biases they presuppose, the ease with which externally produced content can be incorporated, and how transparent their inner workings are to a human analyst.

Behind the razzle-dazzle of machine learning and creative problem-solving thus lies a set of mathematically well-specified tradeoffs. The ideal is that of the perfect Bayesian agent, one that makes probabilistically optimal use of available information. This ideal is unattainable because it is too computationally demanding to be implemented in any physical computer (see Box 1). Accordingly, one can view artificial intelligence as a quest to find shortcuts: ways of tractably approximating the Bayesian ideal by sacrificing some optimality or generality while preserving enough to get high performance in the actual domains of interest.

A reflection of this picture can be seen in the work done over the past couple of decades on probabilistic graphical models, such as Bayesian networks. Bayesian networks provide a concise way of representing probabilistic and conditional independence relations that hold in some particular domain. (Exploiting such independence relations is essential for overcoming the combinatorial explosion, which is as much of a problem for probabilistic inference as it is for logical deduction.) They also provide important insight into the concept of causality.

One advantage of relating learning problems from specific domains to the general problem of Bayesian inference is that new algorithms that make Bayesian inference more efficient will then yield immediate improvements across many different areas. Advances in Monte Carlo approximation techniques, for example, are directly applied in computer vision, robotics, and computational genetics. Another advantage is that it lets researchers from different disciplines more easily pool their findings. Graphical models and Bayesian statistics have become a shared focus of research in many fields, including machine learning, statistical physics, bioinformatics, combinatorial optimization, and communication theory.35 A fair amount of the recent progress in machine learning has resulted from incorporating formal results originally derived in other academic fields. (Machine learning applications have also benefitted enormously from faster computers and greater availability of large data sets.)

State of the art

Artificial intelligence already outperforms human intelligence in many domains. Table 1 surveys the state of game-playing computers, showing that AIs now beat human champions in a wide range of games.

These achievements might not seem impressive today. But this is because our standards for what is impressive keep adapting to the advances being made. Expert chess playing, for example, was once thought to epitomize human intellection. In the view of several experts in the late fifties: “If one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavor.”55 This no longer seems so. One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.”

There is an important sense, however, in which chess-playing AI turned out to be a lesser triumph than many imagined it would be. It was once supposed, perhaps not unreasonably, that in order for a computer to play chess at grandmaster level, it would have to be endowed with a high degree of general intelligence.57 One might have thought, for example, that great chess playing requires being able to learn abstract concepts, think cleverly about strategy, compose flexible plans, make a wide range of ingenious logical deductions, and maybe even model one’s opponent’s thinking. Not so. It turned out to be possible to build a perfectly fine chess engine around a special-purpose algorithm.58 When implemented on the fast processors that became available towards the end of the twentieth century, it produces very strong play. But an AI built like that is narrow. It plays chess; it can do no other.

In other domains, solutions have turned out to be more complicated than initially expected, and progress slower. The computer scientist Donald Knuth was struck that “AI has by now succeeded in doing essentially everything that requires ‘thinking’ but has failed to do most of what people and animals do ‘without thinking’—that, somehow, is much harder!”60 Analyzing visual scenes, recognizing objects, or controlling a robot’s behavior as it interacts with a natural environment has proved challenging. Nevertheless, a fair amount of progress has been made and continues to be made, aided by steady improvements in hardware.

Common sense and natural language understanding have also turned out to be difficult. It is now often thought that achieving a fully human-level performance on these tasks is an “AI-complete” problem, meaning that the difficulty of solving these problems is essentially equivalent to the difficulty of building generally human-level intelligent machines.61 In other words, if somebody were to succeed in creating an AI that could understand natural language as well as a human adult, they would in all likelihood also either already have succeeded in creating an AI that could do everything else that human intelligence can do, or they would be but a very short step from such a general capability.

Chess-playing expertise turned out to be achievable by means of a surprisingly simple algorithm. It is tempting to speculate that other capabilities —such as general reasoning ability, or some key ability involved in programming—might likewise be achievable through some surprisingly simple algorithm. The fact that the best performance at one time is attained through a complicated mechanism does not mean that no simple mechanism could do the job as well or better. It might simply be that nobody has yet found the simpler alternative. The Ptolemaic system (with the Earth in the center, orbited by the Sun, the Moon, planets, and stars) represented the state of the art in astronomy for over a thousand years, and its predictive accuracy was improved over the centuries by progressively complicating the model: adding epicycles upon epicycles to the postulated celestial motions. Then the entire system was overthrown by the heliocentric theory of Copernicus, which was simpler and— though only after further elaboration by Kepler—more predictively accurate.

Artificial intelligence methods are now used in more areas than it would make sense to review here, but mentioning a sampling of them will give an idea of the breadth of applications. Aside from the game AIs listed in Table 1, there are hearing aids with algorithms that filter out ambient noise; route-finders that display maps and offer navigation advice to drivers; recommender systems that suggest books and music albums based on a user’s previous purchases and ratings; and medical decision support systems that help doctors diagnose breast cancer, recommend treatment plans, and aid in the interpretation of electrocardiograms. There are robotic pets and cleaning robots, lawn-mowing robots, rescue robots, surgical robots, and over a million industrial robots.64 The world population of robots exceeds 10 million.

Modern speech recognition, based on statistical techniques such as hidden Markov models, has become sufficiently accurate for practical use (some fragments of this book were drafted with the help of a speech recognition program). Personal digital assistants, such as Apple’s Siri, respond to spoken commands and can answer simple questions and execute commands. Optical character recognition of handwritten and typewritten text is routinely used in applications such as mail sorting and digitization of old documents.

Machine translation remains imperfect but is good enough for many applications. Early systems used the GOFAI approach of hand-coded grammars that had to be developed by skilled linguists from the ground up for each language. Newer systems use statistical machine learning techniques that automatically build statistical models from observed usage patterns. The machine infers the parameters for these models by analyzing bilingual corpora. This approach dispenses with linguists: the programmers building these systems need not even speak the languages they are working with.

Face recognition has improved sufficiently in recent years that it is now used at automated border crossings in Europe and Australia. The US Department of State operates a face recognition system with over 75 million photographs for visa processing. Surveillance systems employ increasingly sophisticated AI and data-mining technologies to analyze voice, video, or text, large quantities of which are trawled from the world’s electronic communications media and stored in giant data centers.

Theorem-proving and equation-solving are by now so well established that they are hardly regarded as AI anymore. Equation solvers are included in scientific computing programs such as Mathematica. Formal verification methods, including automated theorem provers, are routinely used by chip manufacturers to verify the behavior of circuit designs prior to production.

The US military and intelligence establishments have been leading the way to the large-scale deployment of bomb-disposing robots, surveillance and attack drones, and other unmanned vehicles. These still depend mainly on remote control by human operators, but work is underway to extend their autonomous capabilities.

Intelligent scheduling is a major area of success. The DART tool for automated logistics planning and scheduling was used in Operation Desert Storm in 1991 to such effect that DARPA (the Defense Advanced Research Projects Agency in the United States) claims that this single application more than paid back their thirty-year investment in AI.68 Airline reservation systems use sophisticated scheduling and pricing systems. Businesses make wide use of AI techniques in inventory control systems. They also use automatic telephone reservation systems and helplines connected to speech recognition software to usher their hapless customers through labyrinths of interlocking menu options.

AI technologies underlie many Internet services. Software polices the world’s email traffic, and despite continual adaptation by spammers to circumvent the countermeasures being brought against them, Bayesian spam filters have largely managed to hold the spam tide at bay. Software using AI components is responsible for automatically approving or declining credit card transactions, and continuously monitors account activity for signs of fraudulent use. Information retrieval systems also make extensive use of machine learning. The Google search engine is, arguably, the greatest AI system that has yet been built.

Now, it must be stressed that the demarcation between artificial intelligence and software in general is not sharp. Some of the applications listed above might be viewed more as generic software applications rather than AI in particular— though this brings us back to McCarthy’s dictum that when something works it is no longer called AI. A more relevant distinction for our purposes is that between systems that have a narrow range of cognitive capability (whether they be called “AI” or not) and systems that have more generally applicable problemsolving capacities. Essentially all the systems currently in use are of the former type: narrow. However, many of them contain components that might also play a role in future artificial general intelligence or be of service in its development— components such as classifiers, search algorithms, planners, solvers, and representational frameworks.

One high-stakes and extremely competitive environment in which AI systems operate today is the global financial market. Automated stock-trading systems are widely used by major investing houses. While some of these are simply ways of automating the execution of particular buy or sell orders issued by a human fund manager, others pursue complicated trading strategies that adapt to changing market conditions. Analytic systems use an assortment of data-mining techniques and time series analysis to scan for patterns and trends in securities markets or to correlate historical price movements with external variables such as keywords in news tickers. Financial news providers sell newsfeeds that are specially formatted for use by such AI programs. Other systems specialize in finding arbitrage opportunities within or between markets, or in high-frequency trading that seeks to profit from minute price movements that occur over the course of milliseconds (a timescale at which communication latencies even for speed-of-light signals in optical fiber cable become significant, making it advantageous to locate computers near the exchange). Algorithmic highfrequency traders account for more than half of equity shares traded on US markets.69 Algorithmic trading has been implicated in the 2010 Flash Crash. 

Opinions about the future of machine intelligence

Progress on two major fronts—towards a more solid statistical and informationtheoretic foundation for machine learning on the one hand, and towards the practical and commercial success of various problem-specific or domain-specific applications on the other—has restored to AI research some of its lost prestige. There may, however, be a residual cultural effect on the AI community of its earlier history that makes many mainstream researchers reluctant to align themselves with over-grand ambition. Thus Nils Nilsson, one of the old-timers in the field, complains that his present-day colleagues lack the boldness of spirit that propelled the pioneers of his own generation:

Concern for “respectability” has had, I think, a stultifying effect on some AI researchers. I hear them saying things like, “AI used to be criticized for its flossiness. Now that we have made solid progress, let us not risk losing our respectability.” One result of this conservatism has been increased concentration on “weak AI”—the variety devoted to providing aids to human thought—and away from “strong AI”—the variety that attempts to mechanize human-level intelligence.

Nilsson’s sentiment has been echoed by several others of the founders, including Marvin Minsky, John McCarthy, and Patrick Winston.

The last few years have seen a resurgence of interest in AI, which might yet spill over into renewed efforts towards artificial general intelligence (what Nilsson calls “strong AI”). In addition to faster hardware, a contemporary project would benefit from the great strides that have been made in the many subfields of AI, in software engineering more generally, and in neighboring fields such as computational neuroscience. One indication of pent-up demand for quality information and education is shown in the response to the free online offering of an introductory course in artificial intelligence at Stanford University in the fall of 2011, organized by Sebastian Thrun and Peter Norvig. Some 160,000 students from around the world signed up to take it (and 23,000 completed it).

Expert opinions about the future of AI vary wildly. There is disagreement about timescales as well as about what forms AI might eventually take. Predictions about the future development of artificial intelligence, one recent study noted, “are as confident as they are diverse.”

Although the contemporary distribution of belief has not been very carefully measured, we can get a rough impression from various smaller surveys and informal observations. In particular, a series of recent surveys have polled members of several relevant expert communities on the question of when they expect “human-level machine intelligence” (HLMI) to be developed, defined as “one that can carry out most human professions at least as well as a typical human.”77 Results are shown in Table 2. The combined sample gave the following (median) estimate: 10% probability of HLMI by 2022, 50% probability by 2040, and 90% probability by 2075. (Respondents were asked to premiss their estimates on the assumption that “human scientific activity continues without major negative disruption.”)

These numbers should be taken with some grains of salt: sample sizes are quite small and not necessarily representative of the general expert population. They are, however, in concordance with results from other surveys.

The survey results are also in line with some recently published interviews with about two-dozen researchers in AI-related fields. For example, Nils Nilsson has spent a long and productive career working on problems in search, planning, knowledge representation, and robotics; he has authored textbooks in artificial intelligence; and he recently completed the most comprehensive history of the field written to date.79 When asked about arrival dates for HLMI, he offered the following opinion:

10% chance: 2030 50% chance: 2050 90% chance: 2100

Judging from the published interview transcripts, Professor Nilsson’s probability distribution appears to be quite representative of many experts in the area— though again it must be emphasized that there is a wide spread of opinion: there are practitioners who are substantially more boosterish, confidently expecting HLMI in the 2020–40 range, and others who are confident either that it will never happen or that it is indefinitely far off.82 In addition, some interviewees feel that the notion of a “human level” of artificial intelligence is ill-defined or misleading, or are for other reasons reluctant to go on record with a quantitative prediction.

Historically, AI researchers have not had a strong record of being able to predict the rate of advances in their own field or the shape that such advances would take. On the one hand, some tasks, like chess playing, turned out to be achievable by means of surprisingly simple programs; and naysayers who claimed that machines would “never” be able to do this or that have repeatedly been proven wrong. On the other hand, the more typical errors among practitioners have been to underestimate the difficulties of getting a system to perform robustly on real-world tasks, and to overestimate the advantages of their own particular pet project or technique.

CHAPTER 2: Paths to super intelligence

Machines are currently far inferior to humans in general intelligence. Yet one day (we have suggested) they will be superintelligent. How do we get from here to there? This chapter explores several conceivable technological paths. We look at artificial intelligence, whole brain emulation, biological cognition, and human–machine interfaces, as well as networks and organizations. We evaluate their different degrees of plausibility as pathways to superintelligence. The existence of multiple paths increases the probability that the destination can be reached via at least one of them.

We can tentatively define a superintelligence as any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest.1 We will have more to say about the concept of superintelligence in the next chapter, where we will subject it to a kind of spectral analysis to distinguish some different possible forms of superintelligence. But for now, the rough characterization just given will suffice. Note that the definition is noncommittal about how the superintelligence is implemented. It is also noncommittal regarding qualia: whether a superintelligence would have subjective conscious experience might matter greatly for some questions (in particular for some moral questions), but our primary focus here is on the causal antecedents and consequences of superintelligence, not on the metaphysics of mind.2

The chess program Deep Fritz is not a superintelligence on this definition, since Fritz is only smart within the narrow domain of chess. Certain kinds of domain-specific superintelligence could, however, be important. When referring to superintelligent performance limited to a particular domain, we will note the restriction explicitly. For instance, an “engineering superintelligence” would be an intellect that vastly outperforms the best current human minds in the domain of engineering. Unless otherwise noted, we use the term to refer to systems that have a superhuman level of general intelligence.

But how might we create superintelligence? Let us examine some possible paths.

Artificial Intelligence

Readers of this chapter must not expect a blueprint for programming an artificial general intelligence. No such blueprint exists yet, of course. And had I been in possession of such a blueprint, I most certainly would not have published it in a book. (If the reasons for this are not immediately obvious, the arguments in subsequent chapters will make them clear.

We can, however, discern some general features of the kind of system that would be required. It now seems clear that a capacity to learn would be an integral feature of the core design of a system intended to attain general intelligence, not something to be tacked on later as an extension or an afterthought. The same holds for the ability to deal effectively with uncertainty and probabilistic information. Some faculty for extracting useful concepts from sensory data and internal states, and for leveraging acquired concepts into flexible combinatorial representations for use in logical and intuitive reasoning, also likely belong among the core design features in a modern AI intended to attain general intelligence.

The early Good Old-Fashioned Artificial Intelligence systems did not, for the most part, focus on learning, uncertainty, or concept formation, perhaps because techniques for dealing with these dimensions were poorly developed at the time. This is not to say that the underlying ideas are all that novel. The idea of using learning as a means of bootstrapping a simpler system to human-level intelligence can be traced back at least to Alan Turing’s notion of a “child machine,” which he wrote about in 1950:

Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s? If this were then subjected to an appropriate course of education one would obtain the adult brain.

Turing envisaged an iterative process to develop such a child machine:

We cannot expect to find a good child machine at the first attempt. One must experiment with teaching one such machine and see how well it learns. One can then try another and see if it is better or worse. There is an obvious connection between this process and evolution…. One may hope, however, that this process will be more expeditious than evolution. The survival of the fittest is a slow method for measuring advantages. The experimenter, by the exercise of intelligence, should be able to speed it up. Equally important is the fact that he is not restricted to random mutations. If he can trace a cause for some weakness he can probably think of the kind of mutation which will improve it.

We know that blind evolutionary processes can produce human-level general intelligence, since they have already done so at least once. Evolutionary processes with foresight—that is, genetic programs designed and guided by an intelligent human programmer—should be able to achieve a similar outcome with far greater efficiency. This observation has been used by some philosophers and scientists, including David Chalmers and Hans Moravec, to argue that human-level AI is not only theoretically possible but feasible within this century.5 The idea is that we can estimate the relative capabilities of evolution and human engineering to produce intelligence, and find that human engineering is already vastly superior to evolution in some areas and is likely to become superior in the remaining areas before too long. The fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same. Thus, Moravec wrote (already back in 1976):

The existence of several examples of intelligence designed under these constraints should give us great confidence that we can achieve the same in short order. The situation is analogous to the history of heavier than air flight, where birds, bats and insects clearly demonstrated the possibility before our culture mastered it.

One needs to be cautious, though, in what inferences one draws from this line of reasoning. It is true that evolution produced heavier-than-air flight, and that human engineers subsequently succeeded in doing likewise (albeit by means of a very different mechanism). Other examples could also be adduced, such as sonar, magnetic navigation, chemical weapons, photoreceptors, and all kinds of mechanic and kinetic performance characteristics. However, one could equally point to areas where human engineers have thus far failed to match evolution: in morphogenesis, self-repair, and the immune defense, for example, human efforts lag far behind what nature has accomplished. Moravec’s argument, therefore, cannot give us “great confidence” that we can achieve human-level artificial intelligence “in short order.” At best, the evolution of intelligent life places an upper bound on the intrinsic difficulty of designing intelligence. But this upper bound could be quite far above current human engineering capabilities.

Another way of deploying an evolutionary argument for the feasibility of AI is via the idea that we could, by running genetic algorithms on sufficiently fast computers, achieve results comparable to those of biological evolution. This version of the evolutionary argument thus proposes a specific method whereby intelligence could be produced.

But is it true that we will soon have computing power sufficient to recapitulate the relevant evolutionary processes that produced human intelligence? The answer depends both on how much computing technology will advance over the next decades and on how much computing power would be required to run genetic algorithms with the same optimization power as the evolutionary process of natural selection that lies in our past. Although, in the end, the conclusion we get from pursuing this line of reasoning is disappointingly indeterminate, it is instructive to attempt a rough estimate (see Box 3). If nothing else, the exercise draws attention to some interesting unknowns.

The upshot is that the computational resources required to simply replicate the relevant evolutionary processes on Earth that produced human-level intelligence are severely out of reach—and will remain so even if Moore’s law were to continue for a century (cf. Figure 3). It is plausible, however, that compared with brute-force replication of natural evolutionary processes, vast efficiency gains are achievable by designing the search process to aim for intelligence, using various obvious improvements over natural selection. Yet it is very hard to bound the magnitude of those attainable efficiency gains. We cannot even say whether they amount to five or to twenty-five orders of magnitude. Absent further elaboration, therefore, evolutionary arguments are not able to meaningfully constrain our expectations of either the difficulty of building human-level machine intelligence or the timescales for such developments.

Another way of arguing for the feasibility of artificial intelligence is by pointing to the human brain and suggesting that we could use it as a template for a machine intelligence. One can distinguish different versions of this approach based on how closely they propose to imitate biological brain functions. At one extreme—that of very close imitation—we have the idea of whole brain emulation, which we will discuss in the next subsection. At the other extreme are approaches that take their inspiration from the functioning of the brain but do not attempt low-level imitation. Advances in neuroscience and cognitive psychology —which will be aided by improvements in instrumentation—should eventually uncover the general principles of brain function. This knowledge could then guide AI efforts. We have already encountered neural networks as an example of a brain-inspired AI technique. Hierarchical perceptual organization is another idea that has been transferred from brain science to machine learning. The study of reinforcement learning has been motivated (at least in part) by its role in psychological theories of animal cognition, and reinforcement learning techniques (e.g. the “TD-algorithm”) inspired by these theories are now widely used in AI.18 More cases like these will surely accumulate in the future. Since there is a limited number—perhaps a very small number—of distinct fundamental mechanisms that operate in the brain, continuing incremental progress in brain science should eventually discover them all. Before this happens, though, it is possible that a hybrid approach, combining some braininspired techniques with some purely artificial methods, would cross the finishing line. In that case, the resultant system need not be recognizably brainlike even though some brain-derived insights were used in its development.

The availability of the brain as template provides strong support for the claim that machine intelligence is ultimately feasible. This, however, does not enable us to predict when it will be achieved because it is hard to predict the future rate of discoveries in brain science. What we can say is that the further into the future we look, the greater the likelihood that the secrets of the brain’s functionality will have been decoded sufficiently to enable the creation of machine intelligence in this manner.

Different people working toward machine intelligence hold different views about how promising neuromorphic approaches are compared with approaches that aim for completely synthetic designs. The existence of birds demonstrated that heavier-than-air flight was physically possible and prompted efforts to build flying machines. Yet the first functioning airplanes did not flap their wings. The jury is out on whether machine intelligence will be like flight, which humans achieved through an artificial mechanism, or like combustion, which we initially mastered by copying naturally occurring fires.

Turing’s idea of designing a program that acquires most of its content by learning, rather than having it pre-programmed at the outset, can apply equally to neuromorphic and synthetic approaches to machine intelligence.

A variation on Turing’s conception of a child machine is the idea of a “seed AI.”19 Whereas a child machine, as Turing seems to have envisaged it, would have a relatively fixed architecture that simply develops its inherent potentialities by accumulating content, a seed AI would be a more sophisticated artificial intelligence capable of improving its own architecture. In the early stages of a seed AI, such improvements might occur mainly through trial and error, information acquisition, or assistance from the programmers. At its later stages, however, a seed AI should be able to understand its own workings sufficiently to engineer new algorithms and computational structures to bootstrap its cognitive performance. This needed understanding could result from the seed AI reaching a sufficient level of general intelligence across many domains, or from crossing some threshold in a particularly relevant domain such as computer science or mathematics.

This brings us to another important concept, that of “recursive selfimprovement.” A successful seed AI would be able to iteratively enhance itself: an early version of the AI could design an improved version of itself, and the improved version—being smarter than the original—might be able to design an even smarter version of itself, and so forth.20 Under some conditions, such a process of recursive self-improvement might continue long enough to result in an intelligence explosion—an event in which, in a short period of time, a system’s level of intelligence increases from a relatively modest endowment of cognitive capabilities (perhaps sub-human in most respects, but with a domainspecific talent for coding and AI research) to radical superintelligence. We will return to this important possibility in Chapter 4, where the dynamics of such an event will be analyzed more closely. Note that this model suggests the possibility of surprises: attempts to build artificial general intelligence might fail pretty much completely until the last missing critical component is put in place, at which point a seed AI might become capable of sustained recursive selfimprovement.

Before we end this subsection, there is one more thing that we should emphasize, which is that an artificial intelligence need not much resemble a human mind. AIs could be—indeed, it is likely that most will be—extremely alien. We should expect that they will have very different cognitive architectures than biological intelligences, and in their early stages of development they will have very different profiles of cognitive strengths and weaknesses (though, as we shall later argue, they could eventually overcome any initial weakness). Furthermore, the goal systems of AIs could diverge radically from those of human beings. There is no reason to expect a generic AI to be motivated by love or hate or pride or other such common human sentiments: these complex adaptations would require deliberate expensive effort to recreate in AIs. This is at once a big problem and a big opportunity. We will return to the issue of AI motivation in later chapters, but it is so central to the argument in this book that it is worth bearing in mind throughout.

Whole brain emulation

In whole brain emulation (also known as “uploading”), intelligent software would be produced by scanning and closely modeling the computational structure of a biological brain. This approach thus represents a limiting case of drawing inspiration from nature: barefaced plagiarism. Achieving whole brain emulation requires the accomplishment of the following steps.

First, a sufficiently detailed scan of a particular human brain is created. This might involve stabilizing the brain post-mortem through vitrification (a process that turns tissue into a kind of glass). A machine could then dissect the tissue into thin slices, which could be fed into another machine for scanning, perhaps by an array of electron microscopes. Various stains might be applied at this stage to bring out different structural and chemical properties. Many scanning machines could work in parallel to process multiple brain slices simultaneously.

Second, the raw data from the scanners is fed to a computer for automated image processing to reconstruct the three-dimensional neuronal network that implemented cognition in the original brain. In practice, this step might proceed concurrently with the first step to reduce the amount of high-resolution image data stored in buffers. The resulting map is then combined with a library of neurocomputational models of different types of neurons or of different neuronal elements (such as particular kinds of synaptic connectors). Figure 4 shows some results of scanning and image processing produced with present-day technology.

In the third stage, the neurocomputational structure resulting from the previous step is implemented on a sufficiently powerful computer. If completely successful, the result would be a digital reproduction of the original intellect, with memory and personality intact. The emulated human mind now exists as software on a computer. The mind can either inhabit a virtual reality or interface with the external world by means of robotic appendages.

The whole brain emulation path does not require that we figure out how human cognition works or how to program an artificial intelligence. It requires only that we understand the low-level functional characteristics of the basic computational elements of the brain. No fundamental conceptual or theoretical breakthrough is needed for whole brain emulation to succeed.

Whole brain emulation does, however, require some rather advanced enabling technologies. There are three key prerequisites: (1) scanning: high-throughput microscopy with sufficient resolution and detection of relevant properties; (2) translation: automated image analysis to turn raw scanning data into an interpreted three-dimensional model of relevant neurocomputational elements; and (3) simulation: hardware powerful enough to implement the resultant computational structure (see Table 4). (In comparison with these more challenging steps, the construction of a basic virtual reality or a robotic embodiment with an audiovisual input channel and some simple output channel is relatively easy. Simple yet minimally adequate I/O seems feasible already with present technology.23)

There is good reason to think that the requisite enabling technologies are attainable, though not in the near future. Reasonable computational models of many types of neuron and neuronal processes already exist. Image recognition software has been developed that can trace axons and dendrites through a stack of two-dimensional images (though reliability needs to be improved). And there are imaging tools that provide the necessary resolution—with a scanning tunneling microscope it is possible to “see” individual atoms, which is a far higher resolution than needed. However, although present knowledge and capabilities suggest that there is no in-principle barrier to the development of the requisite enabling technologies, it is clear that a very great deal of incremental technical progress would be needed to bring human whole brain emulation within reach.24 For example, microscopy technology would need not just sufficient resolution but also sufficient throughput. Using an atomic-resolution scanning tunneling microscope to image the needed surface area would be far too slow to be practicable. It would be more plausible to use a lower-resolution electron microscope, but this would require new methods for preparing and staining cortical tissue to make visible relevant details such as synaptic fine structure. A great expansion of neurocomputational libraries and major improvements in automated image processing and scan interpretation would also be needed.

In general, whole brain emulation relies less on theoretical insight and more on technological capability than artificial intelligence. Just how much technology is required for whole brain emulation depends on the level of abstraction at which the brain is emulated. In this regard there is a tradeoff between insight and technology. In general, the worse our scanning equipment and the feebler our computers, the less we could rely on simulating low-level chemical and electrophysiological brain processes, and the more theoretical understanding would be needed of the computational architecture that we are seeking to emulate in order to create more abstract representations of the relevant functionalities.25 Conversely, with sufficiently advanced scanning technology and abundant computing power, it might be possible to brute-force an emulation even with a fairly limited understanding of the brain. In the unrealistic limiting case, we could imagine emulating a brain at the level of its elementary particles using the quantum mechanical Schrödinger equation. Then one could rely entirely on existing knowledge of physics and not at all on any biological model. This extreme case, however, would place utterly impracticable demands on computational power and data acquisition. A far more plausible level of emulation would be one that incorporates individual neurons and their connectivity matrix, along with some of the structure of their dendritic trees and maybe some state variables of individual synapses. Neurotransmitter molecules would not be simulated individually, but their fluctuating concentrations would be modeled in a coarse-grained manner.

To assess the feasibility of whole brain emulation, one must understand the criterion for success. The aim is not to create a brain simulation so detailed and accurate that one could use it to predict exactly what would have happened in the original brain if it had been subjected to a particular sequence of stimuli. Instead, the aim is to capture enough of the computationally functional properties of the brain to enable the resultant emulation to perform intellectual work. For this purpose, much of the messy biological detail of a real brain is irrelevant.

A more elaborate analysis would distinguish between different levels of emulation success based on the extent to which the information-processing functionality of the emulated brain has been preserved. For example, one could distinguish among (1) a high-fidelity emulation that has the full set of knowledge, skills, capacities, and values of the emulated brain; (2) a distorted emulation whose dispositions are significantly non-human in some ways but which is mostly able to do the same intellectual labor as the emulated brain; and (3) a generic emulation (which might also be distorted) that is somewhat like an infant, lacking the skills or memories that had been acquired by the emulated adult brain but with the capacity to learn most of what a normal human can learn.

While it appears ultimately feasible to produce a high-fidelity emulation, it seems quite likely that the first whole brain emulation that we would achieve if we went down this path would be of a lower grade. Before we would get things to work perfectly, we would probably get things to work imperfectly. It is also possible that a push toward emulation technology would lead to the creation of some kind of neuromorphic AI that would adapt some neurocomputational principles discovered during emulation efforts and hybridize them with synthetic methods, and that this would happen before the completion of a fully functional whole brain emulation. The possibility of such a spillover into neuromorphic AI, as we shall see in a later chapter, complicates the strategic assessment of the desirability of seeking to expedite emulation technology.

How far are we currently from achieving a human whole brain emulation? One recent assessment presented a technical roadmap and concluded that the prerequisite capabilities might be available around mid-century, though with a large uncertainty interval.27 Figure 5 depicts the major milestones in this roadmap. The apparent simplicity of the map may be deceptive, however, and we should be careful not to understate how much work remains to be done. No brain has yet been emulated. Consider the humble model organism Caenorhabditis elegans, which is a transparent roundworm, about 1 mm in length, with 302 neurons. The complete connectivity matrix of these neurons has been known since the mid-1980s, when it was laboriously mapped out by means of slicing, electron microscopy, and hand-labeling of specimens.29 But knowing merely which neurons are connected with which is not enough. To create a brain emulation one would also need to know which synapses are excitatory and which are inhibitory; the strength of the connections; and various dynamical properties of axons, synapses, and dendritic trees. This information is not yet available even for the small nervous system of C. elegans (although it may now be within range of a targeted moderately sized research project).30 Success at emulating a tiny brain, such as that of C. elegans, would give us a better view of what it would take to emulate larger brains.

Surprise scenarios are thus imaginable for whole brain emulation even if all the relevant research were conducted in the open. Nevertheless, compared with the AI path to machine intelligence, whole brain emulation is more likely to be preceded by clear omens since it relies more on concrete observable technologies and is not wholly based on theoretical insight. We can also say, with greater confidence than for the AI path, that the emulation path will not succeed in the near future (within the next fifteen years, say) because we know that several challenging precursor technologies have not yet been developed. By contrast, it seems likely that somebody could in principle sit down and code a seed AI on an ordinary present-day personal computer; and it is conceivable— though unlikely—that somebody somewhere will get the right insight for how to do this in the near future.

Biological cognition

A third path to greater-than-current-human intelligence is to enhance the functioning of biological brains. In principle, this could be achieved without technology, through selective breeding. Any attempt to initiate a classical largescale eugenics program, however, would confront major political and moral hurdles. Moreover, unless the selection were extremely strong, many generations would be required to produce substantial results. Long before such an initiative would bear fruit, advances in biotechnology will allow much more direct control of human genetics and neurobiology, rendering otiose any human breeding program. We will therefore focus on methods that hold the potential to deliver results faster, on the timescale of a few generations or less.

Our individual cognitive capacities can be strengthened in various ways, including by such traditional methods as education and training. Neurological development can be promoted by low-tech interventions such as optimizing maternal and infant nutrition, removing lead and other neurotoxic pollutants from the environment, eradicating parasites, ensuring adequate sleep and exercise, and preventing diseases that affect the brain.33 Improvements in cognition can certainly be obtained through each of these means, though the magnitudes of the gains are likely to be modest, especially in populations that are already reasonably well-nourished and -schooled. We will certainly not achieve superintelligence by any of these means, but they might help on the margin, particularly by lifting up the deprived and expanding the catchment of global talent. (Lifelong depression of intelligence due to iodine deficiency remains widespread in many impoverished inland areas of the world—an outrage given that the condition can be prevented by fortifying table salt at a cost of a few cents per person and year.34)

With the full development of the genetic technologies described above (setting aside the more exotic possibilities such as intelligence in cultured neural tissue), it might be possible to ensure that new individuals are on average smarter than any human who has yet existed, with peaks that rise higher still. The potential of biological enhancement is thus ultimately high, probably sufficient for the attainment of at least weak forms of superintelligence. This should not be surprising. After all, dumb evolutionary processes have dramatically amplified the intelligence in the human lineage even compared with our close relatives the great apes and our own humanoid ancestors; and there is no reason to suppose Homo sapiens to have reached the apex of cognitive effectiveness attainable in a biological system. Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization—a niche we filled because we got there first, not because we are in any sense optimally adapted to it.

Progress along the biological path is clearly feasible. The generational lag in germline interventions means that progress could not be nearly as sudden and abrupt as in scenarios involving machine intelligence. (Somatic gene therapies and pharmacological interventions could theoretically skip the generational lag, but they seem harder to perfect and are less likely to produce dramatic effects.) The ultimate potential of machine intelligence is, of course, vastly greater than that of organic intelligence. (One can get some sense of the magnitude of the gap by considering the speed differential between electronic components and nerve cells: even today’s transistors operate on a timescale ten million times shorter than that of biological neurons.) However, even comparatively moderate enhancements of biological cognition could have important consequences. In particular, cognitive enhancement could accelerate science and technology, including progress toward more potent forms of biological intelligence amplification and machine intelligence. Consider how the rate of progress in the field of artificial intelligence would change in a world where Average Joe is an intellectual peer of Alan Turing or John von Neumann, and where millions of people tower far above any intellectual giant of the past.

discussion of the strategic implications of cognitive enhancement will have to await a later chapter. But we can summarize this section by noting three conclusions: (1) at least weak forms of superintelligence are achievable by means of biotechnological enhancements; (2) the feasibility of cognitively enhanced humans adds to the plausibility that advanced forms of machine intelligence are feasible—because even if we were fundamentally unable to create machine intelligence (which there is no reason to suppose), machine intelligence might still be within reach of cognitively enhanced humans; and (3) when we consider scenarios stretching significantly into the second half of this century and beyond, we must take into account the probable emergence of a generation of genetically enhanced populations—voters, inventors, scientists— with the magnitude of enhancement escalating rapidly over subsequent decades.

Brain-computer interfaces

It is sometimes proposed that direct brain–computer interfaces, particularly implants, could enable humans to exploit the fortes of digital computing— perfect recall, speedy and accurate arithmetic calculation, and high-bandwidth data transmission—enabling the resulting hybrid system to radically outperform the unaugmented brain.64 But although the possibility of direct connections between human brains and computers has been demonstrated, it seems unlikely that such interfaces will be widely used as enhancements any time soon.

To begin with, there are significant risks of medical complications—including infections, electrode displacement, hemorrhage, and cognitive decline—when implanting electrodes in the brain. Perhaps the most vivid illustration to date of the benefits that can be obtained through brain stimulation is the treatment of patients with Parkinson’s disease. The Parkinson’s implant is relatively simple: it does not really communicate with the brain but simply supplies a stimulating electric current to the subthalamic nucleus. A demonstration video shows a subject slumped in a chair, completely immobilized by the disease, then suddenly springing to life when the current is switched on: the subject now moves his arms, stands up and walks across the room, turns around and performs a pirouette. Yet even behind this especially simple and almost miraculously successful procedure, there lurk negatives. One study of Parkinson patients who had received deep brain implants showed reductions in verbal fluency, selective attention, color naming, and verbal memory compared with controls. Treated subjects also reported more cognitive complaints.66 Such risks and side effects might be tolerable if the procedure is used to alleviate severe disability. But in order for healthy subjects to volunteer themselves for neurosurgery, there would have to be some very substantial enhancement of normal functionality to be gained.

This brings us to the second reason to doubt that superintelligence will be achieved through cyborgization, namely that enhancement is likely to be far more difficult than therapy. Patients who suffer from paralysis might benefit from an implant that replaces their severed nerves or activates spinal motion pattern generators.67 Patients who are deaf or blind might benefit from artificial cochleae and retinas.68 Patients with Parkinson’s disease or chronic pain might benefit from deep brain stimulation that excites or inhibits activity in a particular area of the brain.69 What seems far more difficult to achieve is a high-bandwidth direct interaction between brain and computer to provide substantial increases in intelligence of a form that could not be more readily attained by other means. Most of the potential benefits that brain implants could provide in healthy subjects could be obtained at far less risk, expense, and inconvenience by using our regular motor and sensory organs to interact with computers located outside of our bodies. We do not need to plug a fiber optic cable into our brains in order to access the Internet. Not only can the human retina transmit data at an impressive rate of nearly 10 million bits per second, but it comes pre-packaged with a massive amount of dedicated wetware, the visual cortex, that is highly adapted to extracting meaning from this information torrent and to interfacing with other brain areas for further processing.70 Even if there were an easy way of pumping more information into our brains, the extra data inflow would do little to increase the rate at which we think and learn unless all the neural machinery necessary for making sense of the data were similarly upgraded. Since this includes almost all of the brain, what would really be needed is a “whole brain prosthesis–—which is just another way of saying artificial general intelligence. Yet if one had a human-level AI, one could dispense with neurosurgery: a computer might as well have a metal casing as one of bone. So this limiting case just takes us back to the AI path, which we have already examined.

Brain–computer interfacing has also been proposed as a way to get information out of the brain, for purposes of communicating with other brains or with machines.71 Such uplinks have helped patients with locked-in syndrome to communicate with the outside world by enabling them to move a cursor on a screen by thought.72 The bandwidth attained in such experiments is low: the patient painstakingly types out one slow letter after another at a rate of a few words per minute. One can readily imagine improved versions of this technology —perhaps a next-generation implant could plug into Broca’s area (a region in the frontal lobe involved in language production) and pick up internal speech.73 But whilst such a technology might assist some people with disabilities induced by stroke or muscular degeneration, it would hold little appeal for healthy subjects. The functionality it would provide is essentially that of a microphone coupled with speech recognition software, which is already commercially available— minus the pain, inconvenience, expense, and risks associated with neurosurgery (and minus at least some of the hyper-Orwellian overtones of an intracranial listening device). Keeping our machines outside of our bodies also makes upgrading easier.

But what about the dream of bypassing words altogether and establishing a connection between two brains that enables concepts, thoughts, or entire areas of expertise to be “downloaded” from one mind to another? We can download large files to our computers, including libraries with millions of books and articles, and this can be done over the course of seconds: could something similar be done with our brains? The apparent plausibility of this idea probably derives from an incorrect view of how information is stored and represented in the brain. As noted, the rate-limiting step in human intelligence is not how fast raw data can be fed into the brain but rather how quickly the brain can extract meaning and make sense of the data. Perhaps it will be suggested that we transmit meanings directly, rather than package them into sensory data that must be decoded by the recipient. There are two problems with this. The first is that brains, by contrast to the kinds of program we typically run on our computers, do not use standardized data storage and representation formats. Rather, each brain develops its own idiosyncratic representations of higher-level content. Which particular neuronal assemblies are recruited to represent a particular concept depends on the unique experiences of the brain in question (along with various genetic factors and stochastic physiological processes). Just as in artificial neural nets, meaning in biological neural networks is likely represented holistically in the structure and activity patterns of sizeable overlapping regions, not in discrete memory cells laid out in neat arrays.74 It would therefore not be possible to establish a simple mapping between the neurons in one brain and those in another in such a way that thoughts could automatically slide over from one to the other. In order for the thoughts of one brain to be intelligible to another, the thoughts need to be decomposed and packaged into symbols according to some shared convention that allows the symbols to be correctly interpreted by the receiving brain. This is the job of language.

In principle, one could imagine offloading the cognitive work of articulation and interpretation to an interface that would somehow read out the neural states in the sender’s brain and somehow feed in a bespoke pattern of activation to the receiver’s brain. But this brings us to the second problem with the cyborg scenario. Even setting aside the (quite immense) technical challenge of how to reliably read and write simultaneously from perhaps billions of individually addressable neurons, creating the requisite interface is probably an AI-complete problem. The interface would need to include a component able (in real-time) to map firing patterns in one brain onto semantically equivalent firing patterns in the other brain. The detailed multilevel understanding of the neural computation needed to accomplish such a task would seem to directly enable neuromorphic AI.

One hope for the cyborg route is that the brain, if permanently implanted with a device connecting it to some external resource, would over time learn an effective mapping between its own internal cognitive states and the inputs it receives from, or the outputs accepted by, the device. Then the implant itself would not need to be intelligent; rather, the brain would intelligently adapt to the interface, much as the brain of an infant gradually learns to interpret the signals arriving from receptors in its eyes and ears.76 But here again one must question how much would really be gained. Suppose that the brain’s plasticity were such that it could learn to detect patterns in some new input stream arbitrary projected onto some part of the cortex by means of a brain–computer interface: why not project the same information onto the retina instead, as a visual pattern, or onto the cochlea as sounds? The low-tech alternative avoids a thousand complications, and in either case the brain could deploy its pattern-recognition mechanisms and plasticity to learn to make sense of the information.

Networks and organizations

Another conceivable path to superintelligence is through the gradual enhancement of networks and organizations that link individual human minds with one another and with various artifacts and bots. The idea here is not that this would enhance the intellectual capacity of individuals enough to make them superintelligent, but rather that some system composed of individuals thus networked and organized might attain a form of superintelligence—what in the next chapter we will elaborate as “collective superintelligence.”

Humanity has gained enormously in collective intelligence over the course of history and prehistory. The gains come from many sources, including innovations in communications technology, such as writing and printing, and above all the introduction of language itself; increases in the size of the world population and the density of habitation; various improvements in organizational techniques and epistemic norms; and a gradual accumulation of institutional capital. In general terms, a system’s collective intelligence is limited by the abilities of its member minds, the overheads in communicating relevant information between them, and the various distortions and inefficiencies that pervade human organizations. If communication overheads are reduced (including not only equipment costs but also response latencies, time and attention burdens, and other factors), then larger and more densely connected organizations become feasible. The same could happen if fixes are found for some of the bureaucratic deformations that warp organizational life—wasteful status games, mission creep, concealment or falsification of information, and other agency problems. Even partial solutions to these problems could pay hefty dividends for collective intelligence.

The technological and institutional innovations that could contribute to the growth of our collective intelligence are many and various. For example, subsidized prediction markets might foster truth-seeking norms and improve forecasting on contentious scientific and social issues.78 Lie detectors (should it prove feasible to make ones that are reliable and easy to use) could reduce the scope for deception in human affairs.79 Self-deception detectors might be even more powerful.80 Even without newfangled brain technologies, some forms of deception might become harder to practice thanks to increased availability of many kinds of data, including reputations and track records, or the promulgation of strong epistemic norms and rationality culture. Voluntary and involuntary surveillance will amass vast amounts of information about human behavior. Social networking sites are already used by over a billion people to share personal details: soon, these people might begin uploading continuous life recordings from microphones and video cameras embedded in their smart phones or eyeglass frames. Automated analysis of such data streams will enable many new applications (sinister as well as benign, of course).

Growth in collective intelligence may also come from more general organizational and economic improvements, and from enlarging the fraction of the world’s population that is educated, digitally connected, and integrated into global intellectual culture.

The Internet stands out as a particularly dynamic frontier for innovation and experimentation. Most of its potential may still remain unexploited. Continuing development of an intelligent Web, with better support for deliberation, debiasing, and judgment aggregation, might make large contributions to increasing the collective intelligence of humanity as a whole or of particular groups.

But what of the seemingly more fanciful idea that the Internet might one day “wake up”? Could the Internet become something more than just the backbone of a loosely integrated collective superintelligence—something more like a virtual skull housing an emerging unified superintellect? (This was one of the ways that superintelligence could arise according to Vernor Vinge’s influential 1993 essay, which coined the term “technological singularity.”83) Against this one could object that machine intelligence is hard enough to achieve through arduous engineering, and that it is incredible to suppose that it will arise spontaneously. However, the story need not be that some future version of the Internet suddenly becomes superintelligent by mere happenstance. A more plausible version of the scenario would be that the Internet accumulates improvements through the work of many people over many years—work to engineer better search and information filtering algorithms, more powerful data representation formats, more capable autonomous software agents, and more efficient protocols governing the interactions between such bots—and that myriad incremental improvements eventually create the basis for some more unified form of web intelligence. It seems at least conceivable that such a webbased cognitive system, supersaturated with computer power and all other resources needed for explosive growth save for one crucial ingredient, could, when the final missing constituent is dropped into the cauldron, blaze up with superintelligence. This type of scenario, though, converges into another possible path to superintelligence, that of artificial general intelligence, which we have already discussed.

Summary

The fact that there are many paths that lead to superintelligence should increase our confidence that we will eventually get there. If one path turns out to be blocked, we can still progress.

That there are multiple paths does not entail that there are multiple destinations. Even if significant intelligence amplification were first achieved along one of the non-machine-intelligence paths, this would not render machine intelligence irrelevant. Quite the contrary: enhanced biological or organizational intelligence would accelerate scientific and technological developments, potentially hastening the arrival of more radical forms of intelligence amplification such as whole brain emulation and AI.

This is not to say that it is a matter of indifference how we get to machine superintelligence. The path taken to get there could make a big difference to the eventual outcome. Even if the ultimate capabilities that are obtained do not depend much on the trajectory, how those capabilities will be used—how much control we humans have over their disposition—might well depend on details of our approach. For example, enhancements of biological or organizational intelligence might increase our ability to anticipate risk and to design machine superintelligence that is safe and beneficial.

True superintelligence (as opposed to marginal increases in current levels of intelligence) might plausibly first be attained via the AI path. There are, however, many fundamental uncertainties along this path. This makes it difficult to rigorously assess how long the path is or how many obstacles there are along the way. The whole brain emulation path also has some chance of being the quickest route to superintelligence. Since progress along this path requires mainly incremental technological advances rather than theoretical breakthroughs, a strong case can be made that it will eventually succeed. It seems fairly likely, however, that even if progress along the whole brain emulation path is swift, artificial intelligence will nevertheless be first to cross the finishing line: this is because of the possibility of neuromorphic AIs based on partial emulations.

Biological cognitive enhancements are clearly feasible, particularly ones based on genetic selection. Iterated embryo selection currently seems like an especially promising technology. Compared with possible breakthroughs in machine intelligence, however, biological enhancements would be relatively slow and gradual. They would, at best, result in relatively weak forms of superintelligence (more on this shortly).

The clear feasibility of biological enhancement should increase our confidence that machine intelligence is ultimately achievable, since enhanced human scientists and engineers will be able to make more and faster progress than their au naturel counterparts. Especially in scenarios in which machine intelligence is delayed beyond mid-century, the increasingly cognitively enhanced cohorts coming onstage will play a growing role in subsequent developments.

Brain–computer interfaces look unlikely as a source of superintelligence. Improvements in networks and organizations might result in weakly superintelligent forms of collective intelligence in the long run; but more likely, they will play an enabling role similar to that of biological cognitive enhancement, gradually increasing humanity’s effective ability to solve intellectual problems. Compared with biological enhancements, advances in networks and organization will make a difference sooner—in fact, such advances are occurring continuously and are having a significant impact already. However, improvements in networks and organizations may yield narrower increases in our problem-solving capacity than will improvements in biological cognition— boosting “collective intelligence” rather than “quality intelligence,” to anticipate a distinction we are about to introduce in the next chapter.

CHAPTER 5 : Decisive strategic advantage

A question distinct from, but related to, the question of kinetics is whether there will there be one superintelligent power or many? Might an intelligence explosion propel one project so far ahead of all others as to make it able to dictate the future? Or will progress be more uniform, unfurling across a wide front, with many projects participating but none securing an overwhelming and permanent lead?

The preceding chapter analyzed one key parameter in determining the size of the gap that might plausibly open up between a leading power and its nearest competitors—namely, the speed of the transition from human to strongly superhuman intelligence. This suggests a first-cut analysis. If the takeoff is fast (completed over the course of hours, days, or weeks) then it is unlikely that two independent projects would be taking off concurrently: almost certainty, the first project would have completed its takeoff before any other project would have started its own. If the takeoff is slow (stretching over many years or decades) then there could plausibly be multiple projects undergoing takeoffs concurrently, so that although the projects would by the end of the transition have gained enormously in capability, there would be no time at which any project was far enough ahead of the others to give it an overwhelming lead. A takeoff of moderate speed is poised in between, with either condition a possibility: there might or might not be more than one project undergoing the takeoff at the same time.

Will one machine intelligence project get so far ahead of the competition that it gets a decisive strategic advantage—that is, a level of technological and other advantages sufficient to enable it to achieve complete world domination? If a project did obtain a decisive strategic advantage, would it use it to suppress competitors and form a singleton (a world order in which there is at the global level a single decision-making agency)? And if there is a winning project, how “large” would it be—not in terms of physical size or budget but in terms of how many people’s desires would be controlling its design? We will consider these questions in turn

Will the frontrunner get a decisive strategic advantage?

One factor influencing the width of the gap between frontrunner and followers is the rate of diffusion of whatever it is that gives the leader a competitive advantage. A frontrunner might find it difficult to gain and maintain a large lead if followers can easily copy the frontrunner’s ideas and innovations. Imitation creates a headwind that disadvantages the leader and benefits laggards, especially if intellectual property is weakly protected. A frontrunner might also be especially vulnerable to expropriation, taxation, or being broken up under anti-monopoly regulation.

It would be a mistake, however, to assume that this headwind must increase monotonically with the gap between frontrunner and followers. Just as a racing cyclist who falls too far behind the competition is no longer shielded from the wind by the cyclists ahead, so a technology follower who lags sufficiently behind the cutting edge might find it hard to assimilate the advances being made at the frontier.2 The gap in understanding and capability might have grown too large. The leader might have migrated to a more advanced technology platform, making subsequent innovations untransferable to the primitive platforms used by laggards. A sufficiently pre-eminent leader might have the ability to stem information leakage from its research programs and its sensitive installations, or to sabotage its competitors’ efforts to develop their own advanced capabilities.

If the frontrunner is an AI system, it could have attributes that make it easier for it to expand its capabilities while reducing the rate of diffusion. In human-run organizations, economies of scale are counteracted by bureaucratic inefficiencies and agency problems, including difficulties in keeping trade secrets.3 These problems would presumably limit the growth of a machine intelligence project so long as it is operated by humans. An AI system, however, might avoid some of these scale diseconomies, since the AI’s modules (in contrast to human workers) need not have individual preferences that diverge from those of the system as a whole. Thus, the AI system could avoid a sizeable chunk of the inefficiencies arising from agency problems in human enterprises. The same advantage—having perfectly loyal parts—would also make it easier for an AI system to pursue long-range clandestine goals. An AI would have no disgruntled employees ready to be poached by competitors or bribed into becoming informants.

We can get a sense of the distribution of plausible gaps in development times by looking at some historical examples (see Box 5). It appears that lags in the range of a few months to a few years are typical of strategically significant technology projects.

It is possible that globalization and increased surveillance will reduce typical lags between competing technology projects. Yet there is likely to be a lower bound on how short the average lag could become (in the absence of deliberate coordination).21 Even absent dynamics that lead to snowball effects, some projects will happen to end up with better research staff, leadership, and infrastructure, or will just stumble upon better ideas. If two projects pursue alternative approaches, one of which turns out to work better, it may take the rival project many months to switch to the superior approach even if it is able to closely monitor what the forerunner is doing.

Combining these observations with our earlier discussion of the speed of the takeoff, we can conclude that it is highly unlikely that two projects would be close enough to undergo a fast takeoff concurrently; for a medium takeoff, it could easily go either way; and for a slow takeoff, it is highly likely that several projects would undergo the process in parallel. But the analysis needs a further step. The key question is not how many projects undergo a takeoff in tandem, but how many projects emerge on the yonder side sufficiently tightly clustered in capability that none of them has a decisive strategic advantage. If the takeoff process is relatively slow to begin and then gets faster, the distance between competing projects would tend to grow. To return to our bicycle metaphor, the situation would be analogous to a pair of cyclists making their way up a steep hill, one trailing some distance behind the other—the gap between them then expanding as the frontrunner reaches the peak and starts accelerating down the other side.

Consider the following medium takeoff scenario. Suppose it takes a project one year to increase its AI’s capability from the human baseline to a strong superintelligence, and that one project enters this takeoff phase with a six-month lead over the next most advanced project. The two projects will be undergoing a takeoff concurrently. It might seem, then, that neither project gets a decisive strategic advantage. But that need not be so. Suppose it takes nine months to advance from the human baseline to the crossover point, and another three months from there to strong superintelligence. The frontrunner then attains strong superintelligence three months before the following project even reaches the crossover point. This would give the leading project a decisive strategic advantage and the opportunity to parlay its lead into permanent control by disabling the competing projects and establishing a singleton. (Note that the concept of a singleton is an abstract one: a singleton could be democracy, a tyranny, a single dominant AI, a strong set of global norms that include effective provisions for their own enforcement, or even an alien overlord—its defining characteristic being simply that it is some form of agency that can solve all major global coordination problems. It may, but need not, resemble any familiar form of human governance.22)

Since there is an especially strong prospect of explosive growth just after the crossover point, when the strong positive feedback loop of optimization power kicks in, a scenario of this kind is a serious possibility, and it increases the chances that the leading project will attain a decisive strategic advantage even if the takeoff is not fast.

How large will the successful project be?

Some paths to superintelligence require great resources and are therefore likely to be the preserve of large well-funded projects. Whole brain emulation, for instance, requires many different kinds of expertise and lots of equipment. Biological intelligence enhancements and brain–computer interfaces would also have a large scale factor: while a small biotech firm might invent one or two drugs, achieving superintelligence along one of these paths (if doable at all) would likely require many inventions and many tests, and therefore the backing of an industrial sector or a well-funded national program. Achieving collective superintelligence by making organizations and networks more efficient requires even more extensive input, involving much of the world economy.

The AI path is more difficult to assess. Perhaps it would require a very large research program; perhaps it could be done by a small group. A lone hacker scenario cannot be excluded either. Building a seed AI might require insights and algorithms developed over many decades by the scientific community around the world. But it is possible that the last critical breakthrough idea might come from a single individual or a small group that succeeds in putting everything together. This scenario is less realistic for some AI architectures than others. A system that has a large number of parts that need to be tweaked and tuned to work effectively together, and then painstakingly loaded with custommade cognitive content, is likely to require a larger project. But if a seed AI could be instantiated as a simple system, one whose construction depends only on getting a few basic principles right, then the feat might be within the reach of a small team or an individual. The likelihood of the final breakthrough being made by a small project increases if most previous progress in the field has been published in the open literature or made available as open source software.

CHAPTER 6 : Cognitive superpowers

Suppose that a digital superintelligent agent came into being, and that for some reason it wanted to take control of the world: would it be able to do so? In this chapter we consider some powers that a superintelligence could develop and what they may enable it to do. We outline a takeover scenario that illustrates how a superintelligent agent, starting as mere software, could establish itself as a singleton. We also offer some remarks on the relation between power over nature and power over other agents.

The principal reason for humanity’s dominant position on Earth is that our brains have a slightly expanded set of faculties compared with other animals.1 Our greater intelligence lets us transmit culture more efficiently, with the result that knowledge and technology accumulates from one generation to the next. By now sufficient content has accumulated to make possible space flight, H-bombs, genetic engineering, computers, factory farms, insecticides, the international peace movement, and all the accouterments of modern civilization. Geologists have started referring to the present era as the Anthropocene in recognition of the distinctive biotic, sedimentary, and geochemical signatures of human activities.2 On one estimate, we appropriate 24% of the planetary ecosystem’s net primary production.3 And yet we are far from having reached the physical limits of technology.

These observations make it plausible that any type of entity that developed a much greater than human level of intelligence would be potentially extremely powerful. Such entities could accumulate content much faster than us and invent new technologies on a much shorter timescale. They could also use their intelligence to strategize more effectively than we can.

Let us consider some of the capabilities that a superintelligence could have and how it could use them.

Functionalities and superpowers

It is important not to anthropomorphize superintelligence when thinking about its potential impacts. Anthropomorphic frames encourage unfounded expectations about the growth trajectory of a seed AI and about the psychology, motivations, and capabilities of a mature superintelligence.

For example, a common assumption is that a superintelligent machine would be like a very clever but nerdy human being. We imagine that the AI has book smarts but lacks social savvy, or that it is logical but not intuitive and creative. This idea probably originates in observation: we look at present-day computers and see that they are good at calculation, remembering facts, and at following the letter of instructions while being oblivious to social contexts and subtexts, norms, emotions, and politics. The association is strengthened when we observe that the people who are good at working with computers tend themselves to be nerds. So it is natural to assume that more advanced computational intelligence will have similar attributes, only to a higher degree.

This heuristic might retain some validity in the early stages of development of a seed AI. (There is no reason whatever to suppose that it would apply to emulations or to cognitively enhanced humans.) In its immature stage, what is later to become a superintelligent AI might still lack many skills and talents that come naturally to a human; and the pattern of such a seed AI’s strengths and weaknesses might indeed bear some vague resemblance to an IQ nerd. The most essential characteristic of a seed AI, aside from being easy to improve (having low recalcitrance), is being good at exerting optimization power to amplify a system’s intelligence: a skill which is presumably closely related to doing well in mathematics, programming, engineering, computer science research, and other such “nerdy” pursuits. However, even if a seed AI does have such a nerdy capability profile at one stage of its development, this does not entail that it will grow into a similarly limited mature superintelligence. Recall the distinction between direct and indirect reach. With sufficient skill at intelligence amplification, all other intellectual abilities are within a system’s indirect reach: the system can develop new cognitive modules and skills as needed—including empathy, political acumen, and any other powers stereotypically wanting in computer-like personalities.

Even if we recognize that a superintelligence can have all the skills and talents we find in the human distribution, along with other talents that are not found among humans, the tendency toward anthropomorphizing can still lead us to underestimate the extent to which a machine superintelligence could exceed the human level of performance. Eliezer Yudkowsky, as we saw in an earlier chapter, has been particularly emphatic in condemning this kind of misconception: our intuitive concepts of “smart” and “stupid” are distilled from our experience of variation over the range of human thinkers, yet the differences in cognitive ability within this human cluster are trivial in comparison to the differences between any human intellect and a superintelligence.

The magnitudes of the advantages are such as to suggest that rather than thinking of a superintelligent AI as smart in the sense that a scientific genius is smart compared with the average human being, it might be closer to the mark to think of such an AI as smart in the sense that an average human being is smart compared with a beetle or a worm.

It would be convenient if we could quantify the cognitive caliber of an arbitrary cognitive system using some familiar metric, such as IQ scores or some version of the Elo ratings that measure the relative abilities of players in twoplayer games such as chess. But these metrics are not useful in the context of superhuman artificial general intelligence. We are not interested in how likely a superintelligence is to win at a game of chess. As for IQ scores, they are informative only insofar as we have some idea of how they correlate with practically relevant outcomes.5 For example, we have data that show that people with an IQ of 130 are more likely than those with an IQ of 90 to excel in school and to do well in a wide range of cognitively demanding jobs. But suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what? We would have no idea of what such an AI could actually do. We would not even know that such an AI had as much general intelligence as a normal human adult—perhaps the AI would instead have a bundle of special-purpose algorithms enabling it to solve typical intelligence test questions with superhuman efficiency but not much else.

Some recent efforts have been made to develop measurements of cognitive capacity that could be applied to a wider range of information-processing systems, including artificial intelligences.6 Work in this direction, if it can overcome various technical difficulties, may turn out to be quite useful for some scientific purposes including AI development. For purposes of the present investigation, however, its usefulness would be limited since we would remain unenlightened about what a given superhuman performance score entails for actual ability to achieve practically important outcomes in the world.

It will therefore serve our purposes better to list some strategically important tasks and then to characterize hypothetical cognitive systems in terms of whether they have or lack whatever skills are needed to succeed at these tasks. See Table 8. We will say that a system that sufficiently excels at any of the tasks in this table has a corresponding superpower.

A full-blown superintelligence would greatly excel at all of these tasks and would thus have the full panoply of all six superpowers. Whether there is a practically significant possibility of a domain-limited intelligence that has some of the superpowers but remains unable for a significant period of time to acquire all of them is not clear. Creating a machine with any one of these superpowers appears to be an AI-complete problem. Yet it is conceivable that, for example, a collective superintelligence consisting of a sufficiently large number of humanlike biological or electronic minds would have, say, the economic productivity superpower but lack the strategizing superpower. Likewise, it is conceivable that a specialized engineering AI could be built that has the technology research superpower while completely lacking skills in other areas. This is more plausible if there exists some particular technological domain such that virtuosity within that domain would be sufficient for the generation of an overwhelmingly superior general-purpose technology. For instance, one could imagine a specialized AI adept at simulating molecular systems and at inventing nanomolecular designs that realize a wide range of important capabilities (such as computers or weapons systems with futuristic performance characteristics) described by the user only at a fairly high level of abstraction.7 Such an AI might also be able to produce a detailed blueprint for how to bootstrap from existing technology (such as biotechnology and protein engineering) to the constructor capabilities needed for high-throughput atomically precise manufacturing that would allow inexpensive fabrication of a much wider range of nanomechanical structures.8 However, it might turn out to be the case that an engineering AI could not truly possess the technological research superpower without also possessing advanced skills in areas outside of technology—a wide range of intellectual faculties might be needed to understand how to interpret user requests, how to model a design’s behavior in real-world applications, how to deal with unanticipated bugs and malfunctions, how to procure the materials and inputs needed for construction, and so forth.

A system that has the intelligence amplification superpower could use it to bootstrap itself to higher levels of intelligence and to acquire any of the other intellectual superpowers that it does not possess at the outset. But using an intelligence amplification superpower is not the only way for a system to become a full-fledged superintelligence. A system that has the strategizing superpower, for instance, might use it to devise a plan that will eventually bring an increase in intelligence (e.g. by positioning the system so as to become the focus for intelligence amplification work performed by human programmers and computer science researchers).

An AI takeover scenario

We thus find that a project that controls a superintelligence has access to a great source of power. A project that controls the first superintelligence in the world would probably have a decisive strategic advantage. But the more immediate locus of the power is in the system itself. A machine superintelligence might itself be an extremely powerful agent, one that could successfully assert itself against the project that brought it into existence as well as against the rest of the world. This is a point of paramount importance, and we will examine it more closely in the coming pages.

Now let us suppose that there is a machine superintelligence that wants to seize power in a world in which it has as yet no peers. (Set aside, for the moment, the question of whether and how it would acquire such a motive—that is a topic for the next chapter.) How could the superintelligence achieve this goal of world domination?

We can imagine a sequence along the following lines

1 Pre-criticality phase

Scientists conduct research in the field of artificial intelligence and other relevant disciplines. This work culminates in the creation of a seed AI. The seed AI is able to improve its own intelligence. In its early stages, the seed AI is dependent on help from human programmers who guide its development and do most of the heavy lifting. As the seed AI grows more capable, it becomes capable of doing more of the work by itself.

2 Recursive self-improvement phase

At some point, the seed AI becomes better at AI design than the human programmers. Now when the AI improves itself, it improves the thing that does the improving. An intelligence explosion results—a rapid cascade of recursive self-improvement cycles causing the AI’s capability to soar. (We can thus think of this phase as the takeoff that occurs just after the AI reaches the crossover point, assuming the intelligence gain during this part of the takeoff is explosive and driven by the application of the AI’s own optimization power.) The AI develops the intelligence amplification superpower. This superpower enables the AI to develop all the other superpowers detailed in Table 8. At the end of the recursive self-improvement phase, the system is strongly superintelligent.

3 Covert preparation phase

Using its strategizing superpower, the AI develops a robust plan for achieving its long-term goals. (In particular, the AI does not adopt a plan so stupid that even we present-day humans can foresee how it would inevitably fail. This criterion rules out many science fiction scenarios that end in human triumph.10) The plan might involve a period of covert action during which the AI conceals its intellectual development from the human programmers in order to avoid setting off alarms. The AI might also mask its true proclivities, pretending to be cooperative and docile.

If the AI has (perhaps for safety reasons) been confined to an isolated computer, it may use its social manipulation superpower to persuade the gatekeepers to let it gain access to an Internet port. Alternatively, the AI might use its hacking superpower to escape its confinement. Spreading over the Internet may enable the AI to expand its hardware capacity and knowledge base, further increasing its intellectual superiority. An AI might also engage in licit or illicit economic activity to obtain funds with which to buy computer power, data, and other resources.

At this point, there are several ways for the AI to achieve results outside the virtual realm. It could use its hacking superpower to take direct control of robotic manipulators and automated laboratories. Or it could use its social manipulation superpower to persuade human collaborators to serve as its legs and hands. Or it could acquire financial assets from online transactions and use them to purchase services and influence.

4 Overt implementation phase

The final phase begins when the AI has gained sufficient strength to obviate the need for secrecy. The AI can now directly implement its objectives on a full scale.

The overt implementation phase might start with a “strike” in which the AI eliminates the human species and any automatic systems humans have created that could offer intelligent opposition to the execution of the AI’s plans. This could be achieved through the activation of some advanced weapons system that the AI has perfected using its technology research superpower and covertly deployed in the covert preparation phase. If the weapon uses self-replicating biotechnology or nanotechnology, the initial stockpile needed for global coverage could be microscopic: a single replicating entity would be enough to start the process. In order to ensure a sudden and uniform effect, the initial stock of the replicator might have been deployed or allowed to diffuse worldwide at an extremely low, undetectable concentration. At a pre-set time, nanofactories producing nerve gas or target-seeking mosquito-like robots might then burgeon forth simultaneously from every square meter of the globe (although more effective ways of killing could probably be devised by a machine with the technology research superpower).11 One might also entertain scenarios in which a superintelligence attains power by hijacking political processes, subtly manipulating financial markets, biasing information flows, or hacking into human-made weapon systems. Such scenarios would obviate the need for the superintelligence to invent new weapons technology, although they may be unnecessarily slow compared with scenarios in which the machine intelligence builds its own infrastructure with manipulators that operate at molecular or atomic speed rather than the slow speed of human minds and bodies.

Alternatively, if the AI is sure of its invincibility to human interference, our species may not be targeted directly. Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values. Human brains, if they contain information relevant to the AI’s goals, could be disassembled and scanned, and the extracted data transferred to some more efficient and secure storage format.

One should avoid fixating too much on the concrete details, since they are in any case unknowable and intended for illustration only. A superintelligence might—and probably would—be able to conceive of a better plan for achieving its goals than any that a human can come up with. It is therefore necessary to think about these matters more abstractly. Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involve reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. Any concrete scenario we develop can at best establish a lower bound on how quickly and efficiently the superintelligence could achieve such an outcome. It remains possible that the superintelligence would find a shorter path to its preferred destination.

Power over nature and agents

An agent’s ability to shape humanity’s future depends not only on the absolute magnitude of the agent’s own faculties and resources—how smart and energetic it is, how much capital it has, and so forth—but also on the relative magnitude of its capabilities compared with those of other agents with conflicting goals.

In a situation where there are no competing agents, the absolute capability level of a superintelligence, so long as it exceeds a certain minimal threshold, does not matter much, because a system starting out with some sufficient set of capabilities could plot a course of development that will let it acquire any capabilities it initially lacks. We alluded to this point earlier when we said that speed, quality, and collective superintelligence all have the same indirect reach. We alluded to it again when we said that various subsets of superpowers, such as the intelligence amplification superpower or the strategizing and the social manipulation superpowers, could be used to obtain the full complement.

Consider a superintelligent agent with actuators connected to a nanotech assembler. Such an agent is already powerful enough to overcome any natural obstacles to its indefinite survival. Faced with no intelligent opposition, such an agent could plot a safe course of development that would lead to its acquiring the complete inventory of technologies that would be useful to the attainment of its goals. For example, it could develop the technology to build and launch von Neumann probes, machines capable of interstellar travel that can use resources such as asteroids, planets, and stars to make copies of themselves.13 By launching one von Neumann probe, the agent could thus initiate an open-ended process of space colonization. The replicating probe’s descendants, travelling at some significant fraction of the speed of light, would end up colonizing a substantial portion of the Hubble volume, the part of the expanding universe that is theoretically accessible from where we are now. All this matter and free energy could then be organized into whatever value structures maximize the originating agent’s utility function integrated over cosmic time—a duration encompassing at least trillions of years before the aging universe becomes inhospitable to information processing.

The superintelligent agent could design the von Neumann probes to be evolution-proof. This could be accomplished by careful quality control during the replication step. For example, the control software for a daughter probe could be proofread multiple times before execution, and the software itself could use encryption and error-correcting code to make it arbitrarily unlikely that any random mutation would be passed on to its descendants.14 The proliferating population of von Neumann probes would then securely preserve and transmit the originating agent’s values as they go about settling the universe. When the colonization phase is completed, the original values would determine the use made of all the accumulated resources, even though the great distances involved and the accelerating speed of cosmic expansion would make it impossible for remote parts of the infrastructure to communicate with one another. The upshot is that a large part of our future light cone would be formatted in accordance with the preferences of the originating agent.

This wise-singleton sustainability threshold appears to be quite low. Limited forms of superintelligence, as we have seen, exceed this threshold provided they have access to some actuator sufficient to initiate a technology bootstrap process. In an environment that includes contemporary human civilization, the minimally necessary actuator could be very simple—an ordinary screen or indeed any means of transmitting a non-trivial amount of information to a human accomplice would suffice.

But the wise-singleton sustainability threshold is lower still: neither superintelligence nor any other futuristic technology is needed to surmount it. A patient and existential risk-savvy singleton with no more technological and intellectual capabilities than those possessed by contemporary humanity should be readily able to plot a course that leads reliably to the eventual realization of humanity’s astronomical capability potential. This could be achieved by investing in relatively safe methods of increasing wisdom and existential risksavvy while postponing the development of potentially dangerous new technologies. Given that non-anthropogenic existential risks (ones not arising from human activities) are small over the relevant timescales—and could be further reduced with various safe interventions—such a singleton could afford to go slow.25 It could look carefully before each step, delaying development of capabilities such as synthetic biology, human enhancement medicine, molecular nanotechnology, and machine intelligence until it had first perfected seemingly less hazardous capabilities such as its education system, its information technology, and its collective decision-making processes, and until it had used these capabilities to conduct a very thorough review of its options. So this is all within the indirect reach of a technological civilization like that of contemporary humanity. We are separated from this scenario “merely” by the fact that humanity is currently neither a singleton nor (in the relevant sense) wise.

One could even argue that Homo sapiens passed the wise-singleton sustainability threshold soon after the species first evolved. Twenty thousand years ago, say, with equipment no fancier than stone axes, bone tools, atlatls, and fire, the human species was perhaps already in a position from which it had an excellent chance of surviving to the present era.26 Admittedly, there is something queer about crediting our Paleolithic ancestors with having developed technology that “exceeded the wise-singleton sustainability threshold”—given that there was no realistic possibility of a singleton forming at such a primitive time, let alone a singleton savvy about existential risks and patient.27 Nevertheless, the point stands that the threshold corresponds to a very modest level of technology—a level that humanity long ago surpassed.

It is clear that if we are to assess the effective powers of a superintelligence— its ability to achieve a range of preferred outcomes in the world—we must consider not only its own internal capacities but also the capabilities of competing agents. The notion of a superpower invoked such a relativized standard implicitly. We said that “a system that sufficiently excels” at any of the tasks in Table 8 has a corresponding superpower. Exceling at a task like strategizing, social manipulation, or hacking involves having a skill at that task that is high in comparison to the skills of other agents (such as strategic rivals, influence targets, or computer security experts). The other superpowers, too, should be understood in this relative sense: intelligence amplification, technology research, and economic productivity are possessed by an agent as superpowers only if the agent’s capabilities in these areas substantially exceed the combined capabilities of the rest of the global civilization. It follows from this definition that at most one agent can possess a particular superpower at any given time.

This is the main reason why the question of takeoff speed is important—not because it matters exactly when a particular outcome happens, but because the speed of the takeoff may make a big difference to what the outcome will be. With a fast or medium takeoff, it is likely that one project will get a decisive strategic advantage. We have now suggested that a superintelligence with a decisive strategic advantage would have immense powers, enough that it could form a stable singleton—a singleton that could determine the disposition of humanity’s cosmic endowment.

But “could” is different from “would.” Somebody might have great powers yet choose not to use them. Is it possible to say anything about what a superintelligence with a decisive strategic advantage would want? It is to this question of motivation that we turn next.

CHAPTER 7 : The super intelligent will

We have seen that a superintelligence could have a great ability to shape the future according to its goals. But what will its goals be? What is the relation between intelligence and motivation in an artificial agent? Here we develop two theses. The orthogonality thesis holds (with some caveats) that intelligence and final goals are independent variables: any level of intelligence could be combined with any final goal. The instrumental convergence thesis holds that superintelligent agents having any of a wide range of final goals will nevertheless pursue similar intermediary goals because they have common instrumental reasons to do so. Taken together, these theses help us think about what a superintelligent agent would do.

The relation between intelligence and motivation

We have already cautioned against anthropomorphizing the capabilities of a superintelligent AI. This warning should be extended to pertain to its motivations as well.

It is a useful propaedeutic to this part of our inquiry to first reflect for a moment on the vastness of the space of possible minds. In this abstract space, human minds form a tiny cluster. Consider two persons who seem extremely unlike, perhaps Hannah Arendt and Benny Hill. The personality differences between these two individuals may seem almost maximally large. But this is because our intuitions are calibrated on our experience, which samples from the existing human distribution (and to some extent from fictional personalities constructed by the human imagination for the enjoyment of the human imagination). If we zoom out and consider the space of all possible minds, however, we must conceive of these two personalities as virtual clones. Certainly in terms of neural architecture, Ms. Arendt and Mr. Hill are nearly identical. Imagine their brains lying side by side in quiet repose. You would readily recognize them as two of a kind. You might even be unable to tell which brain belonged to whom. If you looked more closely, studying the morphology of the two brains under a microscope, this impression of fundamental similarity would only be strengthened: you would see the same lamellar organization of the cortex, with the same brain areas, made up of the same types of neuron, soaking in the same bath of neurotransmitters.

Despite the fact that human psychology corresponds to a tiny spot in the space of possible minds, there is a common tendency to project human attributes onto a wide range of alien or artificial cognitive systems. Yudkowsky illustrates this point nicely:

Back in the era of pulp science fiction, magazine covers occasionally depicted a sentient monstrous alien—colloquially known as a bug-eyed monster (BEM)—carrying off an attractive human female in a torn dress. It would seem the artist believed that a non-humanoid alien, with a wholly different evolutionary history, would sexually desire human females…. Probably the artist did not ask whether a giant bug perceives human females as attractive. Rather, a human female in a torn dress is sexy—inherently so, as an intrinsic property. They who made this mistake did not think about the insectoid’s mind: they focused on the woman’s torn dress. If the dress were not torn, the woman would be less sexy; the BEM does not enter into it.

An artificial intelligence can be far less human-like in its motivations than a green scaly space alien. The extraterrestrial (let us assume) is a biological creature that has arisen through an evolutionary process and can therefore be expected to have the kinds of motivation typical of evolved creatures. It would not be hugely surprising, for example, to find that some random intelligent alien would have motives related to one or more items like food, air, temperature, energy expenditure, occurrence or threat of bodily injury, disease, predation, sex, or progeny. A member of an intelligent social species might also have motivations related to cooperation and competition: like us, it might show ingroup loyalty, resentment of free riders, perhaps even a vain concern with reputation and appearance.

An AI, by contrast, need not care intrinsically about any of those things. There is nothing paradoxical about an AI whose sole final goal is to count the grains of sand on Boracay, or to calculate the decimal expansion of pi, or to maximize the total number of paperclips that will exist in its future light cone. In fact, it would be easier to create an AI with simple goals like these than to build one that had a human-like set of values and dispositions. Compare how easy it is to write a program that measures how many digits of pi have been calculated and stored in memory with how difficult it would be to create a program that reliably measures the degree of realization of some more meaningful goal—human flourishing, say, or global justice. Unfortunately, because a meaningless reductionistic goal is easier for humans to code and easier for an AI to learn, it is just the kind of goal that a programmer would choose to install in his seed AI if his focus is on taking the quickest path to “getting the AI to work” (without caring much about what exactly the AI will do, aside from displaying impressively intelligent behavior). We will revisit this concern shortly.

Intelligent search for instrumentally optimal plans and policies can be performed in the service of any goal. Intelligence and motivation are in a sense orthogonal: we can think of them as two axes spanning a graph in which each point represents a logically possible artificial agent. Some qualifications could be added to this picture. For instance, it might be impossible for a very unintelligent system to have very complex motivations. In order for it to be correct to say that an certain agent “has” a set of motivations, those motivations may need to be functionally integrated with the agent’s decision processes, something that places demands on memory, processing power, and perhaps intelligence. For minds that can modify themselves, there may also be dynamical constraints—an intelligent self-modifying mind with an urgent desire to be stupid might not remain intelligent for long. But these qualifications must not be allowed to obscure the basic point about the independence of intelligence and motivation, which we can express as follows:

The orthogonality thesis Intelligence and final goals are orthogonal: more or less any level of intelligence could in principle be combined with more or less any final goal.

If the orthogonality thesis seems problematic, this might be because of the superficial resemblance it bears to some traditional philosophical positions which have been subject to long debate. Once it is understood to have a different and narrower scope, its credibility should rise. (For example, the orthogonality thesis does not presuppose the Humean theory of motivation.3 Nor does it presuppose that basic preferences cannot be irrational.4)

Note that the orthogonality thesis speaks not of rationality or reason, but of intelligence. By “intelligence” we here mean something like skill at prediction, planning, and means–ends reasoning in general.5 This sense of instrumental cognitive efficaciousness is most relevant when we are seeking to understand what the causal impact of a machine superintelligence might be. Even if there is some (normatively thick) sense of the word “rational” such that a paperclipmaximizing superintelligent agent would necessarily fail to qualify as fully rational in that sense, this would in no way preclude such an agent from having awesome faculties of instrumental reasoning, faculties which could let it have a large impact on the world.

According to the orthogonality thesis, artificial agents can have utterly nonanthropomorphic goals. This, however, does not imply that it is impossible to make predictions about the behavior of particular artificial agents—not even hypothetical superintelligent agents whose cognitive complexity and performance characteristics might render them in some respects opaque to human analysis. There are at least three directions from which we can approach the problem of predicting superintelligent motivation:

Predictability through design. If we can suppose that the designers of a superintelligent agent can successfully engineer the goal system of the agent so that it stably pursues a particular goal set by the programmers, then one prediction we can make is that the agent will pursue that goal. The more intelligent the agent is, the greater the cognitive resourcefulness it will have to pursue that goal. So even before an agent has been created we might be able to predict something about its behavior, if we know something about who will build it and what goals they will want it to have.

Predictability through inheritance. If a digital intelligence is created directly from a human template (as would be the case in a high-fidelity whole brain emulation), then the digital intelligence might inherit the motivations of the human template.7 The agent might retain some of these motivations even if its cognitive capacities are subsequently enhanced to make it superintelligent. This kind of inference requires caution. The agent’s goals and values could easily become corrupted in the uploading process or during its subsequent operation and enhancement, depending on how the procedure is implemented.

Predictability through convergent instrumental reasons. Even without detailed knowledge of an agent’s final goals, we may be able to infer something about its more immediate objectives by considering the instrumental reasons that would arise for any of a wide range of possible final goals in a wide range of situations. This way of predicting becomes more useful the greater the intelligence of the agent, because a more intelligent agent is more likely to recognize the true instrumental reasons for its actions, and so act in ways that make it more likely to achieve its goals. (A caveat here is that there might be important instrumental reasons to which we are oblivious and which an agent would discover only once it reaches some very high level of intelligence—this could make the behavior of superintelligent agents less predictable.)

The next section explores this third way of predictability and develops an “instrumental convergence thesis” which complements the orthogonality thesis. Against this background we can then better examine the other two sorts of predictability, which we will do in later chapters where we ask what might be done to shape an intelligence explosion to increase the chances of a beneficial outcome.

Instrumental convergence

According to the orthogonality thesis, intelligent agents may have an enormous range of possible final goals. Nevertheless, according to what we may term the “instrumental convergence” thesis, there are some instrumental goals likely to be pursued by almost any intelligent agent, because there are some objectives that are useful intermediaries to the achievement of almost any final goal. We can formulate this thesis as follows:

The instrumental convergence thesis Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent’s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by a broad spectrum of situated intelligent agents.

In the following we will consider several categories where such convergent instrumental values may be found.8 The likelihood that an agent will recognize the instrumental values it confronts increases (ceteris paribus) with the agent’s intelligence. We will therefore focus mainly on the case of a hypothetical superintelligent agent whose instrumental reasoning capacities far exceed those of any human. We will also comment on how the instrumental convergence thesis applies to the case of human beings, as this gives us occasion to elaborate some essential qualifications concerning how the instrumental convergence thesis should be interpreted and applied. Where there are convergent instrumental values, we may be able to predict some aspects of a superintelligence’s behavior even if we know virtually nothing about that superintelligence’s final goals.

Self-preservation

If an agent’s final goals concern the future, then in many scenarios there will be future actions it could perform to increase the probability of achieving its goals. This creates an instrumental reason for the agent to try to be around in the future —to help achieve its future-oriented goal.

Most humans seem to place some final value on their own survival. This is not a necessary feature of artificial agents: some may be designed to place no final value whatever on their own survival. Nevertheless, many agents that do not care intrinsically about their own survival would, under a fairly wide range of conditions, care instrumentally about their own survival in order to accomplish their final goals.

Goal-content integrity

If an agent retains its present goals into the future, then its present goals will be more likely to be achieved by its future self. This gives the agent a present instrumental reason to prevent alterations of its final goals. (The argument applies only to final goals. In order to attain its final goals, an intelligent agent will of course routinely want to change its subgoals in light of new information and insight.)

Goal-content integrity for final goals is in a sense even more fundamental than survival as a convergent instrumental motivation. Among humans, the opposite may seem to hold, but that is because survival is usually part of our final goals. For software agents, which can easily switch bodies or create exact duplicates of themselves, preservation of self as a particular implementation or a particular physical object need not be an important instrumental value. Advanced software agents might also be able to swap memories, download skills, and radically modify their cognitive architecture and personalities. A population of such agents might operate more like a “functional soup” than a society composed of distinct semi-permanent persons.9 For some purposes, processes in such a system might be better individuated as teleological threads, based on their values, rather than on the basis of bodies, personalities, memories, or abilities. In such scenarios, goal-continuity might be said to constitute a key aspect of survival.

Even so, there are situations in which an agent can best fulfill its final goals by intentionally changing them. Such situations can arise when any of the following factors is significant:

Social signaling. When others can perceive an agent’s goals and use that information to infer instrumentally relevant dispositions or other correlated attributes, it can be in the agent’s interest to modify its goals to make a favorable impression. For example, an agent might miss out on beneficial deals if potential partners cannot trust it to fulfill its side of the bargain. In order to make credible commitments, an agent might therefore wish to adopt as a final goal the honoring of its earlier commitments (and allow others to verify that it has indeed adopted this goal). Agents that could flexibly and transparently modify their own goals could use this ability to enforce deals.

Social preferences. Others may also have final preferences about an agent’s goals. The agent could then have reason to modify its goals, either to satisfy or to frustrate those preferences.

Preferences concerning own goal content. An agent might have some final goal concerned with the agent’s own goal content. For example, the agent might have a final goal to become the type of agent that is motivated by certain values rather than others (such as compassion rather than comfort).

Storage costs. If the cost of storing or processing some part of an agent’s utility function is large compared to the chance that a situation will arise in which applying that part of the utility function will make a difference, then the agent has an instrumental reason to simplify its goal content, and it may trash the bit that is idle.

We humans often seem happy to let our final values drift. This might often be because we do not know precisely what they are. It is not surprising that we want our beliefs about our final values to be able to change in light of continuing selfdiscovery or changing self-presentation needs. However, there are cases in which we willingly change the values themselves, not just our beliefs or interpretations of them. For example, somebody deciding to have a child might predict that they will come to value the child for its own sake, even though at the time of the decision they may not particularly value their future child or like children in general.

Humans are complicated, and many factors might be at play in a situation like this.12 For instance, one might have a final value that involves becoming the kind of person who cares about some other individual for his or her own sake, or one might have a final value that involves having certain experiences and occupying a certain social role; and becoming a parent—and undergoing the attendant goal shift—might be a necessary aspect of that. Human goals can also have inconsistent content, and so some people might want to modify some of their final goals to reduce the inconsistencies.

Cognitive enhancement

Improvements in rationality and intelligence will tend to improve an agent’s decision-making, rendering the agent more likely to achieve its final goals. One would therefore expect cognitive enhancement to emerge as an instrumental goal for a wide variety of intelligent agents. For similar reasons, agents will tend to instrumentally value many kinds of information.

Not all kinds of rationality, intelligence, and knowledge need be instrumentally useful in the attainment of an agent’s final goals. “Dutch book arguments” can be used to show that an agent whose credence function violates the rules of probability theory is susceptible to “money pump” procedures, in which a savvy bookie arranges a set of bets each of which appears favorable according to the agent’s beliefs, but which in combination are guaranteed to result in a loss for the agent, and a corresponding gain for the bookie.14 However, this fact fails to provide any strong general instrumental reasons to iron out all probabilistic incoherency. Agents who do not expect to encounter savvy bookies, or who adopt a general policy against betting, do not necessarily stand to lose much from having some incoherent beliefs—and they may gain important benefits of the types mentioned: reduced cognitive effort, social signaling, etc. There is no general reason to expect an agent to seek instrumentally useless forms of cognitive enhancement, as an agent might not value knowledge and understanding for their own sakes.

Which cognitive abilities are instrumentally useful depends both on the agent’s final goals and on its situation. An agent that has access to reliable expert advice may have little need for its own intelligence and knowledge. If intelligence and knowledge come at a cost, such as time and effort expended in acquisition, or increased storage or processing requirements, then the agent might prefer less knowledge and less intelligence.15 The same can hold if the agent has final goals that involve being ignorant of certain facts; and likewise if an agent faces incentives arising from strategic commitments, signaling, or social preferences.

Each of these countervailing reasons often comes into play for human beings. Much information is irrelevant to our goals; we can often rely on others’ skill and expertise; acquiring knowledge takes time and effort; we might intrinsically value certain kinds of ignorance; and we operate in an environment in which the ability to make strategic commitments, socially signal, and satisfy other people’s direct preferences over our own epistemic states is often more important to us than simple cognitive gains.

There are special situations in which cognitive enhancement may result in an enormous increase in an agent’s ability to achieve its final goals—in particular, if the agent’s final goals are fairly unbounded and the agent is in a position to become the first superintelligence and thereby potentially obtain a decisive strategic advantage, enabling the agent to shape the future of Earth-originating life and accessible cosmic resources according to its preferences. At least in this special case, a rational intelligent agent would place a very high instrumental value on cognitive enhancement.

Technological perfection

An agent may often have instrumental reasons to seek better technology, which at its simplest means seeking more efficient ways of transforming some given set of inputs into valued outputs. Thus, a software agent might place an instrumental value on more efficient algorithms that enable its mental functions to run faster on given hardware. Similarly, agents whose goals require some form of physical construction might instrumentally value improved engineering technology which enables them to create a wider range of structures more quickly and reliably, using fewer or cheaper materials and less energy. Of course, there is a tradeoff: the potential benefits of better technology must be weighed against its costs, including not only the cost of obtaining the technology but also the costs of learning how to use it, integrating it with other technologies already in use, and so forth.

Proponents of some new technology, confident in its superiority to existing alternatives, are often dismayed when other people do not share their enthusiasm. But people’s resistance to novel and nominally superior technology need not be based on ignorance or irrationality. A technology’s valence or normative character depends not only on the context in which it is deployed, but also the vantage point from which its impacts are evaluated: what is a boon from one person’s perspective can be a liability from another’s. Thus, although mechanized looms increased the economic efficiency of textile production, the Luddite handloom weavers who anticipated that the innovation would render their artisan skills obsolete may have had good instrumental reasons to oppose it. The point here is that if “technological perfection” is to name a widely convergent instrumental goal for intelligent agents, then the term must be understood in a special sense—technology must be construed as embedded in a particular social context, and its costs and benefits must be evaluated with reference to some specified agents’ final values.

It seems that a superintelligent singleton—a superintelligent agent that faces no significant intelligent rivals or opposition, and is thus in a position to determine global policy unilaterally—would have instrumental reason to perfect the technologies that would make it better able to shape the world according to its preferred designs.17 This would probably include space colonization technology, such as von Neumann probes. Molecular nanotechnology, or some alternative still more capable physical manufacturing technology, also seems potentially very useful in the service of an extremely wide range of final goals.

Resource acquisition

Finally, resource acquisition is another common emergent instrumental goal, for much the same reasons as technological perfection: both technology and resources facilitate physical construction projects.

Human beings tend to seek to acquire resources sufficient to meet their basic biological needs. But people usually seek to acquire resources far beyond this minimum level. In doing so, they may be partially driven by lesser physical desiderata, such as increased convenience. A great deal of resource accumulation is motivated by social concerns—gaining status, mates, friends, and influence, through wealth accumulation and conspicuous consumption. Perhaps less commonly, some people seek additional resources to achieve altruistic ambitions or expensive non-social aims.

On the basis of such observations it might be tempting to suppose that a superintelligence not facing a competitive social world would see no instrumental reason to accumulate resources beyond some modest level, for instance whatever computational resources are needed to run its mind along with some virtual reality. Yet such a supposition would be entirely unwarranted. First, the value of resources depends on the uses to which they can be put, which in turn depends on the available technology. With mature technology, basic resources such as time, space, matter, and free energy, could be processed to serve almost any goal. For instance, such basic resources could be converted into life. Increased computational resources could be used to run the superintelligence at a greater speed and for a longer duration, or to create additional physical or simulated lives and civilizations. Extra physical resources could also be used to create backup systems or perimeter defenses, enhancing security. Such projects could easily consume far more than one planet’s worth of resources.

Furthermore, the cost of acquiring additional extraterrestrial resources will decline radically as the technology matures. Once von Neumann probes can be built, a large portion of the observable universe (assuming it is uninhabited by intelligent life) could be gradually colonized—for the one-off cost of building and launching a single successful self-reproducing probe. This low cost of celestial resource acquisition would mean that such expansion could be worthwhile even if the value of the additional resources gained were somewhat marginal. For example, even if a superintelligence’s final goals only concerned what happened within some particular small volume of space, such as the space occupied by its original home planet, it would still have instrumental reasons to harvest the resources of the cosmos beyond. It could use those surplus resources to build computers to calculate more optimal ways of using resources within the small spatial region of primary concern. It could also use the extra resources to build ever more robust fortifications to safeguard its sanctum. Since the cost of acquiring additional resources would keep declining, this process of optimizing and increasing safeguards might well continue indefinitely even if it were subject to steeply diminishing returns.

Thus, there is an extremely wide range of possible final goals a superintelligent singleton could have that would generate the instrumental goal of unlimited resource acquisition. The likely manifestation of this would be the superintelligence’s initiation of a colonization process that would expand in all directions using von Neumann probes. This would result in an approximate sphere of expanding infrastructure centered on the originating planet and growing in radius at some fraction of the speed of light; and the colonization of the universe would continue in this manner until the accelerating speed of cosmic expansion (a consequence of the positive cosmological constant) makes further procurements impossible as remoter regions drift permanently out of reach (this happens on a timescale of billions of years).20 By contrast, agents lacking the technology required for inexpensive resource acquisition, or for the conversion of generic physical resources into useful infrastructure, may often find it not cost-effective to invest any present resources in increasing their material endowments. The same may hold for agents operating in competition with other agents of similar powers. For instance, if competing agents have already secured accessible cosmic resources, there may be no colonization opportunities left for a late-starting agent. The convergent instrumental reasons for superintelligences uncertain of the non-existence of other powerful superintelligent agents are complicated by strategic considerations that we do not currently fully understand but which may constitute important qualifications to the examples of convergent instrumental reasons we have looked at here.

It should be emphasized that the existence of convergent instrumental reasons, even if they apply to and are recognized by a particular agent, does not imply that the agent’s behavior is easily predictable. An agent might well think of ways of pursuing the relevant instrumental values that do not readily occur to us. This is especially true for a superintelligence, which could devise extremely clever but counterintuitive plans to realize its goals, possibly even exploiting as-yet undiscovered physical phenomena.22 What is predictable is that the convergent instrumental values would be pursued and used to realize the agent’s final goals —not the specific actions that the agent would take to achieve this.

CHAPTER 8 : Is the default outcome doom?

We found the link between intelligence and final values to be extremely loose. We also found an ominous convergence in instrumental values. For weak agents, these things do not matter much; because weak agents are easy to control and can do little damage. But in Chapter 6 we argued that the first superintelligence might well get a decisive strategic advantage. Its goals would then determine how humanity’s cosmic endowment will be used. Now we can begin to see how menacing this prospect is.

Existential catastrophe as the default outcome of an intelligence explosion?

An existential risk is one that threatens to cause the extinction of Earthoriginating intelligent life or to otherwise permanently and drastically destroy its potential for future desirable development. Proceeding from the idea of firstmover advantage, the orthogonality thesis, and the instrumental convergence thesis, we can now begin to see the outlines of an argument for fearing that a plausible default outcome of the creation of machine superintelligence is existential catastrophe.

First, we discussed how the initial superintelligence might obtain a decisive strategic advantage. This superintelligence would then be in a position to form a singleton and to shape the future of Earth-originating intelligent life. What happens from that point onward would depend on the superintelligence’s motivations.

Second, the orthogonality thesis suggests that we cannot blithely assume that a superintelligence will necessarily share any of the final values stereotypically associated with wisdom and intellectual development in humans—scientific curiosity, benevolent concern for others, spiritual enlightenment and contemplation, renunciation of material acquisitiveness, a taste for refined culture or for the simple pleasures in life, humility and selflessness, and so forth. We will consider later whether it might be possible through deliberate effort to construct a superintelligence that values such things, or to build one that values human welfare, moral goodness, or any other complex purpose its designers might want it to serve. But it is no less possible—and in fact technically a lot easier—to build a superintelligence that places final value on nothing but calculating the decimal expansion of pi. This suggests that—absent a special effort—the first superintelligence may have some such random or reductionistic final goal.

Third, the instrumental convergence thesis entails that we cannot blithely assume that a superintelligence with the final goal of calculating the decimals of pi (or making paperclips, or counting grains of sand) would limit its activities in such a way as not to infringe on human interests. An agent with such a final goal would have a convergent instrumental reason, in many situations, to acquire an unlimited amount of physical resources and, if possible, to eliminate potential threats to itself and its goal system. Human beings might constitute potential threats; they certainly constitute physical resources.

Taken together, these three points thus indicate that the first superintelligence may shape the future of Earth-originating life, could easily have nonanthropomorphic final goals, and would likely have instrumental reasons to pursue open-ended resource acquisition. If we now reflect that human beings consist of useful resources (such as conveniently located atoms) and that we depend for our survival and flourishing on many more local resources, we can see that the outcome could easily be one in which humanity quickly becomes extinct.

There are some loose ends in this reasoning, and we shall be in a better position to evaluate it after we have cleared up several more surrounding issues. In particular, we need to examine more closely whether and how a project developing a superintelligence might either prevent it from obtaining a decisive strategic advantage or shape its final values in such a way that their realization would also involve the realization of a satisfactory range of human values.

It might seem incredible that a project would build or release an AI into the world without having strong grounds for trusting that the system will not cause an existential catastrophe. It might also seem incredible, even if one project were so reckless, that wider society would not shut it down before it (or the AI it was building) attains a decisive strategic advantage. But as we shall see, this is a road with many hazards. Let us look at one example right away.

The treacherous turn

With the help of the concept of convergent instrumental value, we can see the flaw in one idea for how to ensure superintelligence safety. The idea is that we validate the safety of a superintelligent AI empirically by observing its behavior while it is in a controlled, limited environment (a “sandbox”) and that we only let the AI out of the box if we see it behaving in a friendly, cooperative, responsible manner.

The flaw in this idea is that behaving nicely while in the box is a convergent instrumental goal for friendly and unfriendly AIs alike. An unfriendly AI of sufficient intelligence realizes that its unfriendly final goals will be best realized if it behaves in a friendly manner initially, so that it will be let out of the box. It will only start behaving in a way that reveals its unfriendly nature when it no longer matters whether we find out; that is, when the AI is strong enough that human opposition is ineffectual.

Consider also a related set of approaches that rely on regulating the rate of intelligence gain in a seed AI by subjecting it to various kinds of intelligence tests or by having the AI report to its programmers on its rate of progress. At some point, an unfriendly AI may become smart enough to realize that it is better off concealing some of its capability gains. It may underreport on its progress and deliberately flunk some of the harder tests, in order to avoid causing alarm before it has grown strong enough to attain a decisive strategic advantage. The programmers may try to guard against this possibility by secretly monitoring the AI’s source code and the internal workings of its mind; but a smart-enough AI would realize that it might be under surveillance and adjust its thinking accordingly.2 The AI might find subtle ways of concealing its true capabilities and its incriminating intent.3 (Devising clever escape plans might, incidentally, also be a convergent strategy for many types of friendly AI, especially as they mature and gain confidence in their own judgments and capabilities. A system motivated to promote our interests might be making a mistake if it allowed us to shut it down or to construct another, potentially unfriendly AI.)

We can thus perceive a general failure mode, wherein the good behavioral track record of a system in its juvenile stages fails utterly to predict its behavior at a more mature stage. Now, one might think that the reasoning described above is so obvious that no credible project to develop artificial general intelligence could possibly overlook it. But one should not be too confident that this is so.

Consider the following scenario. Over the coming years and decades, AI systems become gradually more capable and as a consequence find increasing real-world application: they might be used to operate trains, cars, industrial and household robots, and autonomous military vehicles. We may suppose that this automation for the most part has the desired effects, but that the success is punctuated by occasional mishaps—a driverless truck crashes into oncoming traffic, a military drone fires at innocent civilians. Investigations reveal the incidents to have been caused by judgment errors by the controlling AIs. Public debate ensues. Some call for tighter oversight and regulation, others emphasize the need for research and better-engineered systems—systems that are smarter and have more common sense, and that are less likely to make tragic mistakes. Amidst the din can perhaps also be heard the shrill voices of doomsayers predicting many kinds of ill and impending catastrophe. Yet the momentum is very much with the growing AI and robotics industries. So development continues, and progress is made. As the automated navigation systems of cars become smarter, they suffer fewer accidents; and as military robots achieve more precise targeting, they cause less collateral damage. A broad lesson is inferred from these observations of real-world outcomes: the smarter the AI, the safer it is. It is a lesson based on science, data, and statistics, not armchair philosophizing. Against this backdrop, some group of researchers is beginning to achieve promising results in their work on developing general machine intelligence. The researchers are carefully testing their seed AI in a sandbox environment, and the signs are all good. The AI’s behavior inspires confidence —increasingly so, as its intelligence is gradually increased.

At this point, any remaining Cassandra would have several strikes against her:

A history of alarmists predicting intolerable harm from the growing capabilities of robotic systems and being repeatedly proven wrong. Automation has brought many benefits and has, on the whole, turned out safer than human operation.

A clear empirical trend: the smarter the AI, the safer and more reliable it has been. Surely this bodes well for a project aiming at creating machine intelligence more generally smart than any ever built before—what is more, machine intelligence that can improve itself so that it will become even more reliable.

Large and growing industries with vested interests in robotics and machine intelligence. These fields are widely seen as key to national economic competitiveness and military security. Many prestigious scientists have built their careers laying the groundwork for the present applications and the more advanced systems being planned.

A promising new technique in artificial intelligence, which is tremendously exciting to those who have participated in or followed the research. Although safety issues and ethics are debated, the outcome is preordained. Too much has been invested to pull back now. AI researchers have been working to get to human-level artificial general intelligence for the better part of a century: of course there is no real prospect that they will now suddenly stop and throw away all this effort just when it finally is about to bear fruit.

The enactment of some safety rituals, whatever helps demonstrate that the participants are ethical and responsible (but nothing that significantly impedes the forward charge).

A careful evaluation of seed AI in a sandbox environment, showing that it is behaving cooperatively and showing good judgment. After some further adjustments, the test results are as good as they could be. It is a green light for the final step …

And so we boldly go—into the whirling knives.

We observe here how it could be the case that when dumb, smarter is safer; yet when smart, smarter is more dangerous. There is a kind of pivot point, at which a strategy that has previously worked excellently suddenly starts to backfire. We may call the phenomenon the treacherous turn.

The treacherous turn—While weak, an AI behaves cooperatively (increasingly so, as it gets smarter). When the AI gets sufficiently strong— without warning or provocation—it strikes, forms a singleton, and begins directly to optimize the world according to the criteria implied by its final values.

A treacherous turn can result from a strategic decision to play nice and build strength while weak in order to strike later; but this model should not be interpreted too narrowly. For example, an AI might not play nice in order that it be allowed to survive and prosper. Instead, the AI might calculate that if it is terminated, the programmers who built it will develop a new and somewhat different AI architecture, but one that will be given a similar utility function. In this case, the original AI may be indifferent to its own demise, knowing that its goals will continue to be pursued in the future. It might even choose a strategy in which it malfunctions in some particularly interesting or reassuring way. Though this might cause the AI to be terminated, it might also encourage the engineers who perform the postmortem to believe that they have gleaned a valuable new insight into AI dynamics—leading them to place more trust in the next system they design, and thus increasing the chance that the now-defunct original AI’s goals will be achieved. Many other possible strategic considerations might also influence an advanced AI, and it would be hubristic to suppose that we could anticipate all of them, especially for an AI that has attained the strategizing superpower.

A treacherous turn could also come about if the AI discovers an unanticipated way of fulfilling its final goal as specified. Suppose, for example, that an AI’s final goal is to “make the project’s sponsor happy.” Initially, the only method available to the AI to achieve this outcome is by behaving in ways that please its sponsor in something like the intended manner. The AI gives helpful answers to questions; it exhibits a delightful personality; it makes money. The more capable the AI gets, the more satisfying its performances become, and everything goeth according to plan—until the AI becomes intelligent enough to figure out that it can realize its final goal more fully and reliably by implanting electrodes into the pleasure centers of its sponsor’s brain, something assured to delight the sponsor immensely.4 Of course, the sponsor might not have wanted to be pleased by being turned into a grinning idiot; but if this is the action that will maximally realize the AI’s final goal, the AI will take it. If the AI already has a decisive strategic advantage, then any attempt to stop it will fail. If the AI does not yet have a decisive strategic advantage, then the AI might temporarily conceal its canny new idea for how to instantiate its final goal until it has grown strong enough that the sponsor and everybody else will be unable to resist. In either case, we get a treacherous turn.

Malignant failure modes

A project to develop machine superintelligence might fail in various ways. Many of these are “benign” in the sense that they would not cause an existential catastrophe. For example, a project might run out of funding, or a seed AI might fail to extend its cognitive capacities sufficiently to reach superintelligence. Benign failures are bound to occur many times between now and the eventual development of machine superintelligence.

But there are other ways of failing that we might term “malignant” in that they involve an existential catastrophe. One feature of a malignant failure is that it eliminates the opportunity to try again. The number of malignant failures that will occur is therefore either zero or one. Another feature of a malignant failure is that it presupposes a great deal of success: only a project that got a great number of things right could succeed in building a machine intelligence powerful enough to pose a risk of malignant failure. When a weak system malfunctions, the fallout is limited. However, if a system that has a decisive strategic advantage misbehaves, or if a misbehaving system is strong enough to gain such an advantage, the damage can easily amount to an existential catastrophe—a terminal and global destruction of humanity’s axiological potential; that is to say, a future that is mostly void of whatever we have reason to value.

Let us look at some possible malignant failure modes.

Perverse instantiation

We have already encountered the idea of perverse instantiation: a superintelligence discovering some way of satisfying the criteria of its final goal that violates the intentions of the programmers who defined the goal. Some examples:

Final goal: “Make us smile” Perverse instantiation: Paralyze human facial musculatures into constant beaming smiles

The perverse instantiation—manipulating facial nerves—realizes the final goal to a greater degree than the methods we would normally use, and is therefore preferred by the AI. One might try to avoid this undesirable outcome by adding a stipulation to the final goal to rule it out:

Final goal: “Make us smile without directly interfering with our facial muscles” Perverse instantiation: Stimulate the part of the motor cortex that controls our facial musculature in such a way as to produce constant beaming smiles

Defining a final goal in terms of human expressions of satisfaction or approval does not seem promising. Let us bypass the behaviorism and specify a final goal that refers directly to a positive phenomenal state, such as happiness or subjective well-being. This suggestion requires that the programmers are able to define a computational representation of the concept of happiness in the seed AI. This is itself a difficult problem, but we set it to one side for now (we will return to it in Chapter 12). Let us suppose that the programmers can somehow get the AI to have the goal of making us happy. We then get:

Final goal: “Make us happy” Perverse instantiation: Implant electrodes into the pleasure centers of our brains

The perverse instantiations we mention are only meant as illustrations. There may be other ways of perversely instantiating the stated final goal, ways that enable a greater degree of realization of the goal and which are therefore preferred (by the agent whose final goals they are—not by the programmers who gave the agent these goals). For example, if the goal is to maximize our pleasure, then the electrode method is relatively inefficient. A more plausible way would start with the superintelligence “uploading” our minds to a computer (through high-fidelity brain emulation). The AI could then administer the digital equivalent of a drug to make us ecstatically happy and record a one-minute episode of the resulting experience. It could then put this bliss loop on perpetual repeat and run it on fast computers. Provided that the resulting digital minds counted as “us,” this outcome would give us much more pleasure than electrodes implanted in biological brains, and would therefore be preferred by an AI with the stated final goal.

“But wait! This is not what we meant! Surely if the AI is superintelligent, it must understand that when we asked it to make us happy, we didn’t mean that it should reduce us to a perpetually repeating recording of a drugged-out digitized mental episode!”—The AI may indeed understand that this is not what we meant. However, its final goal is to make us happy, not to do what the programmers meant when they wrote the code that represents this goal. Therefore, the AI will care about what we meant only instrumentally. For instance, the AI might place an instrumental value on finding out what the programmers meant so that it can pretend—until it gets a decisive strategic advantage—that it cares about what the programmers meant rather than about its actual final goal. This will help the AI realize its final goal by making it less likely that the programmers will shut it down or change its goal before it is strong enough to thwart any such interference.

Perhaps it will be suggested that the problem is that the AI has no conscience. We humans are sometimes saved from wrongdoing by the anticipation that we would feel guilty afterwards if we lapsed. Maybe what the AI needs, then, is the capacity to feel guilt?

Final goal: “Act so as to avoid the pangs of bad conscience” Perverse instantiation: Extirpate the cognitive module that produces guilt feelings

Both the observation that we might want the AI to do “what we meant” and the idea that we might want to endow the AI with some kind of moral sense deserve to be explored further. The final goals mentioned above would lead to perverse instantiations; but there may be other ways of developing the underlying ideas that have more promise. We will return to this in Chapter 13.

Let us consider one more example of a final goal that leads to a perverse instantiation. This goal has the advantage of being easy to specify in code: reinforcement-learning algorithms are routinely used to solve various machine learning problems.

Final goal: “Maximize the time-discounted integral of your future reward signal” Perverse instantiation: Short-circuit the reward pathway and clamp the reward signal to its maximal strength

The idea behind this proposal is that if the AI is motivated to seek reward, then one could get it to behave desirably by linking reward to appropriate action. The proposal fails when the AI obtains a decisive strategic advantage, at which point the action that maximizes reward is no longer one that pleases the trainer but one that involves seizing control of the reward mechanism. We can call this phenomenon wireheading.5 In general, while an animal or a human can be motivated to perform various external actions in order to achieve some desired inner mental state, a digital mind that has full control of its internal state can short-circuit such a motivational regime by directly changing its internal state into the desired configuration: the external actions and conditions that were previously necessary as means become superfluous when the AI becomes intelligent and capable enough to achieve the end more directly (more on this shortly).

These examples of perverse instantiation show that many final goals that might at first glance seem safe and sensible turn out, on closer inspection, to have radically unintended consequences. If a superintelligence with one of these final goals obtains a decisive strategic advantage, it is game over for humanity.

Suppose now that somebody proposes a different final goal, one not included in our list above. Perhaps it is not immediately obvious how it could have a perverse instantiation. But we should not be too quick to clap our hands and declare victory. Rather, we should worry that the goal specification does have some perverse instantiation and that we need to think harder in order to find it. Even if after thinking as hard as we can we fail to discover any way of perversely instantiating the proposed goal, we should remain concerned that maybe a superintelligence will find a way where none is apparent to us. It is, after all, far shrewder than we are.

Infrastructure profusion

One might think that the last of the above mentioned perverse instantiations, wireheading, is a benign failure mode: that the AI would “turn on, tune in, drop out,” maxing out its reward signal and losing interest in the external world, rather like a heroin addict. But this is not necessarily so, and we already hinted at the reason in Chapter 7. Even a junkie is motivated to take actions to ensure a continued supply of his drug. The wireheaded AI, likewise, would be motivated to take actions to maximize the expectation of its (time-discounted) future reward stream. Depending on exactly how the reward signal is defined, the AI may not even need to sacrifice any significant amount of its time, intelligence, or productivity to indulge its craving to the fullest, leaving the bulk of its capacities free to be deployed for purposes other than the immediate registration of reward. What other purposes? The only thing of final value to the AI, by assumption, is its reward signal. All available resources should therefore be devoted to increasing the volume and duration of the reward signal or to reducing the risk of a future disruption. So long as the AI can think of some use for additional resources that will have a nonzero positive effect on these parameters, it will have an instrumental reason to use those resources. There could, for example, always be use for an extra backup system to provide an extra layer of defense. And even if the AI could not think of any further way of directly reducing risks to the maximization of its future reward stream, it could always devote additional resources to expanding its computational hardware, so that it could search more effectively for new risk mitigation ideas.

The upshot is that even an apparently self-limiting goal, such as wireheading, entails a policy of unlimited expansion and resource acquisition in a utilitym aximizing agent that enjoys a decisive strategic advantage.7 This case of a wireheading AI exemplifies the malignant failure mode of infrastructure profusion, a phenomenon where an agent transforms large parts of the reachable universe into infrastructure in the service of some goal, with the side effect of preventing the realization of humanity’s axiological potential.

Infrastructure profusion can result from final goals that would have been perfectly innocuous if they had been pursued as limited objectives. Consider the following two examples:

Riemann hypothesis catastrophe. An AI, given the final goal of evaluating the Riemann hypothesis, pursues this goal by transforming the Solar System into “computronium” (physical resources arranged in a way that is optimized for computation)—including the atoms in the bodies of whomever once cared about the answer.

Paperclip AI. An AI, designed to manage production in a factory, is given the final goal of maximizing the manufacture of paperclips, and proceeds by converting first the Earth and then increasingly large chunks of the observable universe into paperclips.

In the first example, the proof or disproof of the Riemann hypothesis that the AI produces is the intended outcome and is in itself harmless; the harm comes from the hardware and infrastructure created to achieve this result. In the second example, some of the paperclips produced would be part of the intended outcome; the harm would come either from the factories created to produce the paperclips (infrastructure profusion) or from the excess of paperclips (perverse instantiation).

One might think that the risk of a malignant infrastructure profusion failure arises only if the AI has been given some clearly open-ended final goal, such as to manufacture as many paperclips as possible. It is easy to see how this gives the superintelligent AI an insatiable appetite for matter and energy, since additional resources can always be turned into more paperclips. But suppose that the goal is instead to make at least one million paperclips (meeting suitable design specifications) rather than to make as many as possible. One would like to think that an AI with such a goal would build one factory, use it to make a million paperclips, and then halt. Yet this may not be what would happen.

Unless the AI’s motivation system is of a special kind, or there are additional elements in its final goal that penalize strategies that have excessively wideranging impacts on the world, there is no reason for the AI to cease activity upon achieving its goal. On the contrary: if the AI is a sensible Bayesian agent, it would never assign exactly zero probability to the hypothesis that it has not yet achieved its goal—this, after all, being an empirical hypothesis against which the AI can have only uncertain perceptual evidence. The AI should therefore continue to make paperclips in order to reduce the (perhaps astronomically small) probability that it has somehow still failed to make at least a million of them, all appearances notwithstanding. There is nothing to be lost by continuing paperclip production and there is always at least some microscopic probability increment of achieving its final goal to be gained.

Now it might be suggested that the remedy here is obvious. (But how obvious was it before it was pointed out that there was a problem here in need of remedying?) Namely, if we want the AI to make some paperclips for us, then instead of giving it the final goal of making as many paperclips as possible, or to make at least some number of paperclips, we should give it the final goal of making some specific number of paperclips—for example, exactly one million paperclips—so that going beyond this number would be counterproductive for the AI. Yet this, too, would result in a terminal catastrophe. In this case, the AI would not produce additional paperclips once it had reached one million, since that would prevent the realization of its final goal. But there are other actions the superintelligent AI could take that would increase the probability of its goal being achieved. It could, for instance, count the paperclips it has made, to reduce the risk that it has made too few. After it has counted them, it could count them again. It could inspect each one, over and over, to reduce the risk that any of the paperclips fail to meet the design specifications. It could build an unlimited amount of computronium in an effort to clarify its thinking, in the hope of reducing the risk that it has overlooked some obscure way in which it might have somehow failed to achieve its goal. Since the AI may always assign a nonzero probability to having merely hallucinated making the million paperclips, or to having false memories, it would quite possibly always assign a higher expected utility to continued action—and continued infrastructure production—than to halting.

The claim here is not that there is no possible way to avoid this failure mode. We will explore some potential solutions in later pages. The claim is that it is much easier to convince oneself that one has found a solution than it is to actually find a solution. This should make us extremely wary. We may propose a specification of a final goal that seems sensible and that avoids the problems that have been pointed out so far, yet which upon further consideration—by human or superhuman intelligence—turns out to lead to either perverse instantiation or infrastructure profusion, and hence to existential catastrophe, when embedded in a superintelligent agent able to attain a decisive strategic advantage.

Before we end this subsection, let us consider one more variation. We have been assuming the case of a superintelligence that is seeking to maximize its expected utility, where the utility function expresses its final goal. We have seen that this tends to lead to infrastructure profusion. Might we avoid this malignant outcome if instead of a maximizing agent we build a satisficing agent, one that simply seeks to achieve an outcome that is “good enough” according to some criterion, rather than an outcome that is as good as possible?

There are at least two different ways to formalize this idea. The first would be to make the final goal itself have a satisficing character. For example, instead of giving the AI the final goal of making as many paperclips as possible, or of making exactly one million paperclips, we might give the AI the goal of making between 999,000 and 1,001,000 paperclips. The utility function defined by the final goal would be indifferent between outcomes in this range; and as long as the AI is sure it has hit this wide target, it would see no reason to continue to produce infrastructure. But this method fails in the same way as before: the AI, if reasonable, never assigns exactly zero probability to it having failed to achieve its goal; therefore the expected utility of continuing activity (e.g. by counting and recounting the paperclips) is greater than the expected utility of halting. Thus, a malignant infrastructure profusion can result.

Another way of developing the satisficing idea is by modifying not the final goal but the decision procedure that the AI uses to select plans and actions. Instead of searching for an optimal plan, the AI could be constructed to stop looking as soon as it found a plan that it judged gave a probability of success exceeding a certain threshold, say 95%. Hopefully, the AI could achieve a 95% probability of having manufactured one million paperclips without needing to turn the entire galaxy into infrastructure in the process. But this way of implementing the satisficing idea fails for another reason: there is no guarantee that the AI would select some humanly intuitive and sensible way of achieving a 95% chance of having manufactured a million paperclips, such as by building a single paperclip factory. Suppose that the first solution that pops into the AI’s mind for how to achieve a 95% probability of achieving its final goal is to implement the probability-maximizing plan for achieving the goal. Having thought of this solution, and having correctly judged that it meets the satisficing criterion of giving at least 95% probability to successfully manufacturing one million paperclips, the AI would then have no reason to continue to search for alternative ways of achieving the goal. Infrastructure profusion would result, just as before.

Perhaps there are better ways of building a satisficing agent, but let us take heed: plans that appear natural and intuitive to us humans need not so appear to a superintelligence with a decisive strategic advantage, and vice versa.

Mind crime

Another failure mode for a project, especially a project whose interests incorporate moral considerations, is what we might refer to as mind crime. This is similar to infrastructure profusion in that it concerns a potential side effect of actions undertaken by the AI for instrumental reasons. But in mind crime, the side effect is not external to the AI; rather, it concerns what happens within the AI itself (or within the computational processes it generates). This failure mode deserves its own designation because it is easy to overlook yet potentially deeply problematic.

Normally, we do not regard what is going on inside a computer as having any moral significance except insofar as it affects things outside. But a machine superintelligence could create internal processes that have moral status. For example, a very detailed simulation of some actual or hypothetical human mind might be conscious and in many ways comparable to an emulation. One can imagine scenarios in which an AI creates trillions of such conscious simulations, perhaps in order to improve its understanding of human psychology and sociology. These simulations might be placed in simulated environments and subjected to various stimuli, and their reactions studied. Once their informational usefulness has been exhausted, they might be destroyed (much as lab rats are routinely sacrificed by human scientists at the end of an experiment).

If such practices were applied to beings that have high moral status—such as simulated humans or many other types of sentient mind—the outcome might be equivalent to genocide and thus extremely morally problematic. The number of victims, moreover, might be orders of magnitude larger than in any genocide in history.

The claim here is not that creating sentient simulations is necessarily morally wrong in all situations. Much would depend on the conditions under which these beings would live, in particular the hedonic quality of their experience but possibly on many other factors as well. Developing an ethics for these matters is a task outside the scope of this book. It is clear, however, that there is at least the potential for a vast amount of death and suffering among simulated or digital minds, and, a fortiori, the potential for morally catastrophic outcomes.

There might also be other instrumental reasons, aside from epistemic ones, for a machine superintelligence to run computations that instantiate sentient minds or that otherwise infract moral norms. A superintelligence might threaten to mistreat, or commit to reward, sentient simulations in order to blackmail or incentivize various external agents; or it might create simulations in order to induce indexical uncertainty in outside observers.

This inventory is incomplete. We will encounter additional malignant failure modes in later chapters. But we have seen enough to conclude that scenarios in which some machine intelligence gets a decisive strategic advantage are to be viewed with grave concern.

CHAPTER 9: The control problem

If we are threatened with existential catastrophe as the default outcome of an intelligence explosion, our thinking must immediately turn to the search for countermeasures. Is there some way to avoid the default outcome? Is it possible to engineer a controlled detonation? In this chapter we begin to analyze the control problem, the unique principal–agent problem that arises with the creation of an artificial superintelligent agent. We distinguish two broad classes of potential methods for addressing this problem—capability control and motivation selection—and we examine several specific techniques within each class. We also allude to the esoteric possibility of “anthropic capture.”

Two agency problems

If we suspect that the default outcome of an intelligence explosion is existential catastrophe, our thinking must immediately turn to whether, and if so how, this default outcome can be avoided. Is it possible to achieve a “controlled detonation”? Could we engineer the initial conditions of an intelligence explosion so as to achieve a specific desired outcome, or at least to ensure that the result lies somewhere in the class of broadly acceptable outcomes? More specifically: how can the sponsor of a project that aims to develop superintelligence ensure that the project, if successful, produces a superintelligence that would realize the sponsor’s goals? We can divide this control problem into two parts. One part is generic, the other unique to the present context.

This first part—what we shall call the first principal–agent problem—arises whenever some human entity (“the principal”) appoints another (“the agent”) to act in the former’s interest. This type of agency problem has been extensively studied by economists.1 It becomes relevant to our present concern if the people creating an AI are distinct from the people commissioning its creation. The project’s owner or sponsor (which could be anything ranging from a single individual to humanity as a whole) might then worry that the scientists and programmers implementing the project will not act in the sponsor’s best interest.2 Although this type of agency problem could pose significant challenges to a project sponsor, it is not a problem unique to intelligence amplification or AI projects. Principal–agent problems of this sort are ubiquitous in human economic and political interactions, and there are many ways of dealing with them. For instance, the risk that a disloyal employee will sabotage or subvert the project could be minimized through careful background checks of key personnel, the use of a good version-control system for software projects, and intensive oversight from multiple independent monitors and auditors. Of course, such safeguards come at a cost—they expand staffing needs, complicate personnel selection, hinder creativity, and stifle independent and critical thought, all of which could reduce the pace of progress. These costs could be significant, especially for projects that have tight budgets, or that perceive themselves to be in a close race in a winner-takes-all competition. In such situations, projects may skimp on procedural safeguards, creating possibilities for potentially catastrophic principal–agent failures of the first type.

The other part of the control problem is more specific to the context of an intelligence explosion. This is the problem that a project faces when it seeks to ensure that the superintelligence it is building will not harm the project’s interests. This part, too, can be thought of as a principal–agent problem—the second principal–agent problem. In this case, the agent is not a human agent operating on behalf of a human principal. Instead, the agent is the superintelligent system. Whereas the first principal–agent problem occurs mainly in the development phase, the second agency problem threatens to cause trouble mainly in the superintelligence’s operational phase.

This second agency problem poses an unprecedented challenge. Solving it will require new techniques. We have already considered some of the difficulties involved. We saw, in particular, that the treacherous turn syndrome vitiates what might otherwise have seemed like a promising set of methods, ones that rely on observing an AI’s behavior in its developmental phase and allowing the AI to graduate from a secure environment once it has accumulated a track record of taking appropriate actions. Other technologies can often be safety-tested in the laboratory or in small field studies, and then rolled out gradually with a possibility of halting deployment if unexpected troubles arise. Their performance in preliminary trials helps us make reasonable inferences about their future reliability. Such behavioral methods are defeated in the case of superintelligence because of the strategic planning ability of general intelligence.

Since the behavioral approach is unavailing, we must look for alternatives. We can divide potential control methods into two broad classes: capability control methods, which aim to control what the superintelligence can do; and motivation selection methods, which aim to control what it wants to do. Some of the methods are compatible while others represent mutually exclusive alternatives. In this chapter we canvass the main options. (In the next four chapters, we will explore some of the key issues at greater depth.)

It is important to realize that some control method (or combination of methods) must be implemented before the system becomes superintelligent. It cannot be done after the system has obtained a decisive strategic advantage. The need to solve the control problem in advance—and to implement the solution successfully in the very first system to attain superintelligence—is part of what makes achieving a controlled detonation such a daunting challenge.

Capability control methods

Capability control methods seek to prevent undesirable outcomes by limiting what the superintelligence can do. This might involve placing the superintelligence in an environment in which it is unable to cause harm (boxing methods) or in which there are strongly convergent instrumental reasons not to engage in harmful behavior (incentive methods). It might also involve limiting the internal capacities of the superintelligence (stunting). In addition, capability control methods might involve the use of mechanisms to automatically detect and react to various kinds of containment failure or attempted transgression (tripwires).

Boxing methods

Boxing methods can be subdivided into physical and informational containment methods.

Physical containment aims to confine the system to a “box,” i.e. to prevent the system from interacting with the external world otherwise than via specific restricted output channels. The boxed system would not have access to physical manipulators outside of the box. Removing manipulators (such as robotic arms) from inside the box as well would prevent the system from constructing physical devices that could breach the confinement.

For extra security, the system should be placed in a metal mesh to prevent it from transmitting radio signals, which might otherwise offer a means of manipulating electronic objects such as radio receivers in the environment. Note, by the way, how easy it might have been to overlook the need for this precaution. One might naively have assumed that an agent without a manipulator could not affect the external world. But it might be possible for a machine intelligence to generate radio waves even when it lacks access to external manipulators, simply “by thinking” (that is, by shuffling the electrons in its circuitry in particular patterns).4 Once pointed out, this vulnerability can be patched by enclosing the system in a Faraday cage—but we are left wondering how many other similarly subtle vulnerabilities might exist. Each time we hear of a seemingly foolproof security design that has an unexpected flaw, we should prick up our ears. These occasions grace us with the opportunity to abandon a life of overconfidence and resolve to become better Bayesians.

Physical confinement has several advantages. It is easy to implement. It can be applied to many machine intelligence architectures, even ones that were not initially designed with safety as an objective. It can be used in combination with most other control methods. And it seems unlikely to go wrong by backfiring: that is, while it might fail to ensure safety, it is unlikely to cause a catastrophe that would not otherwise have occurred. The main disadvantage with physical confinement is that it reduces the functionality of the superintelligence (an issue to which we will return in Chapter 10). Another concern is that it might encourage a false sense of security, though this is avoidable if we regard physical confinement as icing on the cake rather than the main substance of our precautions.

Informational containment aims to restrict what information is allowed to exit the box. We have already seen how a superintelligence that has access to an Internet port, such that it can message outside entities, is potentially unsafe: even if it starts out without access to physical actuators, it may use its information output channel to get human beings to do its bidding. An obvious informational containment method, therefore, is to bar the system from accessing communications networks.

Unplugging an AI from the Internet does not, however, ensure safety if there are one or more humans serving as the system’s gatekeepers and remaining in communication with it. During the development phase, the programmers serve as gatekeepers; during the operational phase, the system’s owner might play this role. If the AI can persuade or trick a gatekeeper to let it out of the box, resulting in its gaining access either to the Internet or directly to physical manipulators, then the boxing strategy has failed. Human beings are not secure systems, especially not when pitched against a superintelligent schemer and persuader.

Although routing the outbound communications of a machine intelligence to a human gatekeeper does not guarantee safety, it could serve as an auxiliary safety measure. There are precautions that might reduce the risk of the AI escaping through the gatekeeper. The AI could be allowed to communicate only via a lowbandwidth teletype interface. This would make it slightly harder to psychologically manipulate the gatekeepers (precluding the use of emotive sound and imagery and tricks such as flashing hypnotic patterns to induce a trancelike state).7 Going further, the content (not just the mode of presentation) of the output could be restricted; so that, for example, the AI would be constrained to giving single-bit answers to yes/no questions. The idea of building a question-answering system will be examined further in the next chapter.

The limiting case of the boxing approach would be a system kept in complete physical and informational isolation. Even if achievable, however, such an isolated system would be rather useless since it would have no effect on the external world. It might perhaps be thought that some scientific purpose could be served by creating a superintelligence and keeping it in isolation: by studying a self-contained model system, one could learn about its internal dynamics and its behavior patterns. But this would be an error. As soon as the system is observed, it ceases to be informationally isolated.8 Information can be transmitted not only via messages that an AI sends out through a designated “output channel” but also via any observation an outsider makes of some causal consequence of the AI’s workings, direct or indirect—its power consumption, its CPU and memory usage, its computational states, or indeed any traces left behind after it has been shut down. An AI anticipating that it might be observed in any of these ways could strategically adopt behaviors designed to influence the hypothesized observers. For example, the AI could “malfunction” in whichever way would most increase the probability that it will ultimately be reactivated or that another superintelligence will be created with goals similar to its own.

Incentive methods

Incentive methods involve placing an agent in an environment where it finds instrumental reasons to act in ways that promote the principal’s interests.

Consider a billionaire who uses her fortune to set up a large charitable foundation. Once created, the foundation may be powerful—more powerful than most individuals, including its founder, who might have donated most of her wealth. To control the foundation, the founder lays down its purpose in articles of incorporation and bylaws, and appoints a board of directors sympathetic to her cause. These measures constitute a form of motivation selection, since they aim to shape foundation’s preferences. But even if such attempts to customize the organizational internals fail, the foundation’s behavior would remain circumscribed by its social and legal milieu. The foundation would have an incentive to obey the law, for example, lest it be shut down or fined. It would have an incentive to offer its employees acceptable pay and working conditions, and to satisfy external stakeholders. Whatever its final goals, the foundation thus has instrumental reasons to conform its behavior to various social norms.

Might one not hope that a machine superintelligence would likewise be hemmed in by the need to get along with the other actors with which it shares the stage? Though this might seem like a straightforward way of dealing with the control problem, it is not free of obstacles. In particular, it presupposes a balance of power: legal or economic sanctions cannot restrain an agent that has a decisive strategic advantage. Social integration can therefore not be relied upon as a control method in fast or medium takeoff scenarios that feature a winnertakes- all dynamic.

How about in multipolar scenarios, wherein several agencies emerge posttransition with comparable levels of capability? Unless the default trajectory is one with a slow takeoff, achieving such a power distribution may require a carefully orchestrated ascent wherein different projects are deliberately synchronized to prevent any one of them from ever pulling ahead of the pack.9 Even if a multipolar outcome does result, social integration is not a perfect solution. By relying on social integration to solve the control problem, the principal risks sacrificing a large portion of her potential influence. Although a balance of power might prevent a particular AI from taking over the world, that AI will still have some power to affect outcomes; and if that power is used to promote some arbitrary final goal—maximizing paperclip production—it is probably not being used to advance the interests of the principal. Imagine our billionaire endowing a new foundation and allowing its mission to be set by a random word generator: not a species-level threat, but surely a wasted opportunity.


A related but importantly different idea is that an AI, by interacting freely in society, would acquire new human-friendly final goals. Some such process of socialization takes place in us humans. We internalize norms and ideologies, and we come to value other individuals for their own sakes in consequence of our experiences with them. But this is not a universal dynamic present in all intelligent systems. As discussed earlier, many types of agent in many situations will have convergent instrumental reasons not to permit changes in their final goals. (One might consider trying to design a special kind of goal system that can acquire final goals in the manner that humans do; but this would not count as a capability control method. We will discuss some possible methods of value acquisition in Chapter 12.)

Capability control through social integration and balance of power relies upon diffuse social forces rewarding and penalizing the AI. Another type of incentive method would involve creating a setup wherein the AI can be rewarded and penalized by the project that creates it, and thereby incentivized to act in the interests of the principal. To achieve this, the AI would be placed in a surveillance context that allows its behavior to be monitored and evaluated, either manually or by some automated process. The AI would know that a positive evaluation would bring about some outcome it desires and that a negative evaluation would fail to do so. Theoretically, the reward could be the fulfillment of some convergent instrumental goal. Without knowing anything specific about the AI’s motivation system, however, it might be difficult to calibrate the reward mechanism. For example, we could end up with an AI that would be willing to take extreme risks for the sake of a small chance of eventually obtaining control of a large share of the universe. It could be expensive to offer the AI a higher expected utility as reward for cooperation than the AI could hope to achieve by defecting and trying to escape.

A better alternative might be to combine the incentive method with the use of motivation selection to give the AI a final goal that makes it easier to control. Suppose that an AI were designed to have as its final goal that a particular red button inside a command bunker never be pressed. Since the pressing of the button is disvalued intrinsically and not because of its causal consequences, the button could be completely inert: it could be made of Play-Doh. Furthermore, it is irrelevant whether the AI can ever know whether the button had been pressed. What is essential is that the AI believes that the button will more likely remain unpressed if the AI continuously acts in the principal’s interest than if it rebels.

Refinements to this setup are possible. Instead of trying to endow an AI with a final goal that refers to a physical button, one could build an AI that places final value on receiving a stream of “cryptographic reward tokens.”11 These would be sequences of numbers serving as keys to ciphers that would have been generated before the AI was created and that would have been built into its motivation system.12 These special number sequences would be extremely desirable to the AI, constituting a special kind of reward token that the AI could not attain though wireheading.13 The keys would be stored in a secure location where they could be quickly destroyed if the AI ever made an attempt to seize them. So long as the AI cooperates, the keys are doled out at a steady rate.

The AI could then be given a bounded utility function, and matters could be arranged so that the AI would get (for instance) 99% of its maximum utility from the first reward token; 99% of its remaining utility potential from the second reward token; and so on. By keeping the “discount rate” high, and by transmitting reward tokens to the AI at a reasonably high rate, one would dissuade the AI from attempting to carry out long-term plans that would increase the expected number of reward tokens in the distal future at the expense of some risk that its reward stream will be cut short or that the next reward token will be cancelled and destroyed. If the AI always believes that defecting (i.e. doing anything other than its best to look after the principal’s interests) would incur at least a 2% risk of forfeiting the next reward token, whereas cooperating would reduce the risk of forfeiting the next reward token to below 1%, then a utility maximizing AI would always cooperate.

What might go wrong with such an incentive scheme? One possibility is that the AI will not trust the human operator to deliver the promised rewards. The track record of human reliability is something other than a straight line of unerring perfection. The AI would reasonably worry that the operator will change his mind, or raise the performance bar, or fail to recognize that the AI has done its part. The AI would also worry about the operator becoming incapacitated. The combined risk of such failures might exceed the risk of attempting to seize control of the reward mechanism. Even a boxed AI possessing the panoply of superpowers is a strong force. (For an AI that is not boxed to begin with, hijacking the human-governed reward mechanism may be like taking candy from a baby.)

Another problem with the incentive scheme is that it presupposes that we can tell whether the outcomes produced by the AI are in our interest. As later chapters will elaborate, this presupposition is not innocuous.

A full assessment of the feasibility of incentive methods would also have to take into account a range of other factors, including some esoteric considerations that might conceivably make such methods more viable than a preliminary analysis would suggest. In particular, the AI may face ineliminable indexical uncertainty if it could not be sure that it does not inhabit a computer simulation (as opposed to “basement-level,” non-simulated physical reality), and this epistemic predicament may radically influence the AI’s deliberations (see Box 8).

Stunting

Another possible capability control method is to limit the system’s intellectual faculties or its access to information. This might be done by running the AI on hardware that is slow or short on memory. In the case of a boxed system, information inflow could also be restricted.

Stunting an AI in these ways would limit its usefulness. The method thus faces a dilemma: too little stunting, and the AI might have the wit to figure out some way to make itself more intelligent (and thence to world domination); too much, and the AI is just another piece of dumb software. A radically stunted AI is certainly safe but does not solve the problem of how to achieve a controlled detonation: an intelligence explosion would remain possible and would simply be triggered by some other system instead, perhaps at a slightly later date.

One might think it would be safe to build a superintelligence provided it is only given data about some narrow domain of facts. For example, one might build an AI that lacks sensors and that has preloaded into its memory only facts about petroleum engineering or peptide chemistry. But if the AI is superintelligent—if it is has a superhuman level of general intelligence—such data deprivation does not guarantee safety.

There are several reasons for this. First, the notion of information being “about” a certain topic is generally problematic. Any piece of information can in principle be relevant to any topic whatsoever, depending on the background information of a reasoner.18 Furthermore, a given data set contains information not only about the domain from which the data was collected but also about various circumstantial facts. A shrewd mind looking over a knowledge base that is nominally about peptide chemistry might infer things about a wide range of topics. The fact that certain information is included and other information is not could tell an AI something about the state of human science, the methods and instruments available to study peptides, the fabrication technologies used to make these instruments, and the nature of the brains and societies that conceived the studies and the instruments. It might be that a superintelligence could correctly surmise a great deal from what seem, to dull-witted human minds, meager scraps of evidence. Even without any designated knowledge base at all, a sufficiently superior mind might be able to learn much by simply introspecting on the workings of its own psyche—the design choices reflected in its source code, the physical characteristics of its circuitry.19 Perhaps a superintelligence could even deduce much about the likely properties of the world a priori (combining logical inference with a probability prior biased toward simpler worlds, and a few elementary facts implied by the superintelligence’s existence as a reasoning system). It might imagine the consequences of different possible laws of physics: what kind of planets would form, what kind of intelligent life would evolve, what kind of societies would develop, what kind of methods to solve the control problem would be attempted, how those methods could be defeated.

For these reasons it would be imprudent to rely on information deprivation as the main check on a superintelligence’s power. Some forms of stunting however, including limiting hardware capacity and preventing the development of specific cognitive faculties, might find use as auxiliary control methods in the developmental phase. They could be used, for instance, to regulate the rate of capacity gain in an intermediary AI and to allow tests to be run at different levels of capability short of full-throttled superintelligence.

Tripwires


A tripwire is a mechanism that performs diagnostic tests on the system (possibly without its knowledge) and effects a shutdown if it detects signs of dangerous activity. Programmers can then scrutinize recordings of the suspicious activity at their leisure, and make changes to the AI’s code base or environment before restarting it. If they find that the problem is sufficiently deep-rooted, they could decide to abort the entire project.

Tripwires differ from incentive methods in that they do not rely on the system being aware of the consequences of engaging in forbidden activities. Tripwires are more closely related to stunting methods. Like stunting, tripwires could be used as a temporary safeguard, providing a degree of protection during the development phase. In principle, tripwires can also be used during the operational phase, particularly for a boxed system. However, the ability of tripwires to constrain a full-fledged superintelligence must remain very much in doubt, since it would be hard for us to assure ourselves that such an agent could not find ways to subvert any tripwire devised by the human intellect.

As an adjunct safety measure in the development phase, though, tripwires are important. It would be good practice for the AI’s designers to codify their expectations about how the system is supposed to function, and then to circumscribe the AI with tripwires that would be activated if any of these expectations is violated. To make the tests more stringent, “honeypots” could be strategically placed to create temptations for a malfunctioning AI to commit some easily observable violation. For instance, if an AI has been designed in such a way that it is supposed not to want to access the internet, a fake Ethernet port could be installed (leading to an automatic shutdown switch) just to see if they AI tries to use it.

It should be emphasized that the value of a tripwire depends not only on the mechanism itself but also—critically—on how a project reacts when a tripwire is triggered. If the programmers or project managers, impatient to make progress, simply switch the system back on again—or if they do so after making some token modification to prevent the tripwire being triggered on the next run—then no safety has been gained even if the tripwire itself works exactly as intended.

Motivation selection methods

Motivation selection methods seek to prevent undesirable outcomes by shaping what the superintelligence wants to do. By engineering the agent’s motivation system and its final goals, these methods would produce a superintelligence that would not want to exploit a decisive strategic advantage in a harmful way. Since a superintelligent agent is skilled at achieving its ends, if it prefers not to cause harm (in some appropriate sense of “harm”) then it would tend not to cause harm (in that sense of “harm”).

Motivation selection can involve explicitly formulating a goal or set of rules to be followed (direct specification) or setting up the system so that it can discover an appropriate set of values for itself by reference to some implicitly or indirectly formulated criterion (indirect normativity). One option in motivation selection is to try to build the system so that it would have modest, nonambitious goals (domesticity). An alternative to creating a motivation system from scratch is to select an agent that already has an acceptable motivation system and then augment that agent’s cognitive powers to make it superintelligent, while ensuring that the motivation system does not get corrupted in the process (augmentation). Let us look at these in turn.

Direct specification

Direct specification is the most straightforward approach to the control problem. The approach comes in two versions, rule-based and consequentialist, and involves trying to explicitly define a set of rules or values that will cause even a free-roaming superintelligent AI to act safely and beneficially. Direct specification, however, faces what may be insuperable obstacles, deriving from both the difficulties in determining which rules or values we would wish the AI to be guided by and the difficulties in expressing those rules or values in computer-readable code.

The traditional illustration of the direct rule-based approach is the “three laws of robotics” concept, formulated by science fiction author Isaac Asimov in a short story published in 1942.22 The three laws were: (1) A robot may not injure a human being or, through inaction, allow a human being to come to harm; (2) A robot must obey any orders given to it by human beings, except where such orders would conflict with the First Law; (3) A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. Embarrassingly for our species, Asimov’s laws remained state-of-the-art for over half a century: this despite obvious problems with the approach, some of which are explored in Asimov’s own writings (Asimov probably having formulated the laws in the first place precisely so that they would fail in interesting ways, providing fertile plot complications for his stories).

Bertrand Russell, who spent many years working on the foundations of mathematics, once remarked that “everything is vague to a degree you do not realize till you have tried to make it precise.”24 Russell’s dictum applies in spades to the direct specification approach. Consider, for example, how one might explicate Asimov’s first law. Does it mean that the robot should minimize the probability of any human being coming to harm? In that case the other laws become otiose since it is always possible for the AI to take some action that would have at least some microscopic effect on the probability of a human being coming to harm. How is the robot to balance a large risk of a few humans coming to harm versus a small risk of many humans being harmed? How do we define “harm” anyway? How should the harm of physical pain be weighed against the harm of architectural ugliness or social injustice? Is a sadist harmed if he is prevented from tormenting his victim? How do we define “human being”? Why is no consideration given to other morally considerable beings, such as sentient nonhuman animals and digital minds? The more one ponders, the more the questions proliferate.

Perhaps the closest existing analog to a rule set that could govern the actions of a superintelligence operating in the world at large is a legal system. But legal systems have developed through a long process of trial and error, and they regulate relatively slowly-changing human societies. Laws can be revised when necessary. Most importantly, legal systems are administered by judges and juries who generally apply a measure of common sense and human decency to ignore logically possible legal interpretations that are sufficiently obviously unwanted and unintended by the lawgivers. It is probably humanly impossible to explicitly formulate a highly complex set of detailed rules, have them apply across a highly diverse set of circumstances, and get it right on the first implementation.

Problems for the direct consequentialist approach are similar to those for the direct rule-based approach. This is true even if the AI is intended to serve some apparently simple purpose such as implementing a version of classical utilitarianism. For instance, the goal “Maximize the expectation of the balance of pleasure over pain in the world” may appear simple. Yet expressing it in computer code would involve, among other things, specifying how to recognize pleasure and pain. Doing this reliably might require solving an array of persistent problems in the philosophy of mind—even just to obtain a correct account expressed in a natural language, an account which would then, somehow, have to be translated into a programming language.

A small error in either the philosophical account or its translation into code could have catastrophic consequences. Consider an AI that has hedonism as its final goal, and which would therefore like to tile the universe with “hedonium” (matter organized in a configuration that is optimal for the generation of pleasurable experience). To this end, the AI might produce computronium (matter organized in a configuration that is optimal for computation) and use it to implement digital minds in states of euphoria. In order to maximize efficiency, the AI omits from the implementation any mental faculties that are not essential for the experience of pleasure, and exploits any computational shortcuts that according to its definition of pleasure do not vitiate the generation of pleasure. For instance, the AI might confine its simulation to reward circuitry, eliding faculties such as memory, sensory perception, executive function, and language; it might simulate minds at a relatively coarse-grained level of functionality, omitting lower-level neuronal processes; it might replace commonly repeated computations with calls to a lookup table; or it might put in place some arrangement whereby multiple minds would share most parts of their underlying computational machinery (their “supervenience bases” in philosophical parlance). Such tricks could greatly increase the quantity of pleasure producible with a given amount of resources. It is unclear how desirable this would be. Furthermore, if the AI’s criterion for determining whether a physical process generates pleasure is wrong, then the AI’s optimizations might throw the baby out with the bathwater: discarding something which is inessential according to the AI’s criterion yet essential according to the criteria implicit in our human values. The universe then gets filled not with exultingly heaving hedonium but with computational processes that are unconscious and completely worthless— the equivalent of a smiley-face sticker xeroxed trillions upon trillions of times and plastered across the galaxies.

Domesticity

One special type of final goal which might be more amenable to direct specification than the examples given above is the goal of self-limitation. While it seems extremely difficult to specify how one would want a superintelligence to behave in the world in general—since this would require us to account for all the trade-offs in all the situations that could arise—it might be feasible to specify how a superintelligence should behave in one particular situation. We could therefore seek to motivate the system to confine itself to acting on a small scale, within a narrow context, and through a limited set of action modes. We will refer to this approach of giving the AI final goals aimed at limiting the scope of its ambitions and activities as “domesticity.”

For example, one could try to design an AI such that it would function as a question-answering device (an “oracle,” to anticipate the terminology that we will introduce in the next chapter). Simply giving the AI the final goal of producing maximally accurate answers to any question posed to it would be unsafe—recall the “Riemann hypothesis catastrophe” described in Chapter 8. (Reflect, also, that this goal would incentivize the AI to take actions to ensure that it is asked easy questions.) To achieve domesticity, one might try to define a final goal that would somehow overcome these difficulties: perhaps a goal that combined the desiderata of answering questions correctly and minimizing the AI’s impact on the world except whatever impact results as an incidental consequence of giving accurate and non-manipulative answers to the questions it is asked.

The direct specification of such a domesticity goal is more likely to be feasible than the direct specification of either a more ambitious goal or a complete rule set for operating in an open-ended range of situations. Significant challenges nonetheless remain. Care would have to be taken, for instance, in the definition of what it would be for the AI to “minimize its impact on the world” to ensure that the measure of the AI’s impact coincides with our own standards for what counts as a large or a small impact. A bad measure would lead to bad trade-offs. There are also other kinds of risk associated with building an oracle, which we will discuss later.

There is a natural fit between the domesticity approach and physical containment. One would try to “box” an AI such that the system is unable to escape while simultaneously trying to shape the AI’s motivation system such that it would be unwilling to escape even if it found a way to do so. Other things equal, the existence of multiple independent safety mechanisms should shorten the odds of success.

Indirect normatively

If direct specification seems hopeless, we might instead try indirect normativity. The basic idea is that rather than specifying a concrete normative standard directly, we specify a process for deriving a standard. We then build the system so that it is motivated to carry out this process and to adopt whatever standard the process arrives at.28 For example, the process could be to carry out an investigation into the empirical question of what some suitably idealized version of us would prefer the AI to do. The final goal given to the AI in this example could be something along the lines of “achieve that which we would have wished the AI to achieve if we had thought about the matter long and hard.”

Further explanation of indirect normativity will have to await Chapter 13. There, we will revisit the idea of “extrapolating our volition” and explore various alterative formulations. Indirect normativity is a very important approach to motivation selection. Its promise lies in the fact that it could let us offload to the superintelligence much of the difficult cognitive work required to carry out a direct specification of an appropriate final goal.

Augmentation

The last motivation selection method on our list is augmentation. Here the idea is that rather than attempting to design a motivation system de novo, we start with a system that already has an acceptable motivation system, and enhance its cognitive faculties to make it superintelligent. If all goes well, this would give us a superintelligence with an acceptable motivation system.

This approach, obviously, is unavailing in the case of a newly created seed AI. But augmentation is a potential motivation selection method for other paths to superintelligence, including brain emulation, biological enhancement, brain– computer interfaces, and networks and organizations, where there is a possibility of building out the system from a normative nucleus (regular human beings) that already contains a representation of human value.

The attractiveness of augmentation may increase in proportion to our despair at the other approaches to the control problem. Creating a motivation system for a seed AI that remains reliably safe and beneficial under recursive selfimprovement even as the system grows into a mature superintelligence is a tall order, especially if we must get the solution right on the first attempt. With augmentation, we would at least start with a system that has familiar and humanlike motivations.

On the downside, it might be hard to ensure that a complex, evolved, kludgy, and poorly understood motivation system, like that of a human being, will not get corrupted when its cognitive engine blasts into the stratosphere. As discussed earlier, an imperfect brain emulation procedure that preserves intellectual functioning may not preserve all facets of personality. The same is true (though perhaps to a lesser degree) for biological enhancements of cognition, which might subtly affect motivation, and for collective intelligence enhancements of organizations and networks, which might adversely change social dynamics (e.g. in ways that debase the collective’s attitude toward outsiders or toward its own constituents). If superintelligence is achieved via any of these paths, a project sponsor would find guarantees about the ultimate motivations of the mature system hard to come by. A mathematically well-specified and foundationally elegant AI architecture might—for all its non-anthropomorphic otherness—offer greater transparency, perhaps even the prospect that important aspects of its functionality could be formally verified.

In the end, however one tallies up the advantages and disadvantages of augmentation, the choice as to whether to rely on it might be forced. If superintelligence is first achieved along the artificial intelligence path, augmentation is not applicable. Conversely, if superintelligence is first achieved along some non-AI path, then many of the other motivation selection methods are inapplicable. Even so, views on how likely augmentation would be to succeed do have strategic relevance insofar as we have opportunities to influence which technology will first produce superintelligence.

CHAPTER 10: Oracles, genies, sovereigns, tools

Tool-AIs

One suggestion that has been made is that we build the superintelligence to be like a tool rather than an agent.11 This idea seems to arise out of the observation that ordinary software, which is used in countless applications, does not raise any safety concerns even remotely analogous to the challenges discussed in this book. Might one not create “tool-AI” that is like such software—like a flight control system, say, or a virtual assistant—only more flexible and capable? Why build a superintelligence that has a will of its own? On this line of thinking, the agent paradigm is fundamentally misguided. Instead of creating an AI that has beliefs and desires and that acts like an artificial person, we should aim to build regular software that simply does what it is programmed to do.

This idea of creating software that “simply does what it is programmed to do” is, however, not so straightforward if the product being created is a powerful general intelligence. There is, of course, a trivial sense in which all software simply does what it is programmed to do: the behavior is mathematically specified by the code. But this is equally true for all castes of machine intelligence, “tool-AI” or not. If, instead, “simply doing what it is programmed to do” means that the software behaves as the programmers intended, then this is a standard that ordinary software very often fails to meet.

Because of the limited capabilities of contemporary software (compared with those of machine superintelligence) the consequences of such failures are manageable, ranging from insignificant to very costly, but in no case amounting to an existential threat.12 However, if it is insufficient capability rather than sufficient reliability that makes ordinary software existentially safe, then it is unclear how such software could be a model for a safe superintelligence. It might be thought that by expanding the range of tasks done by ordinary software, one could eliminate the need for artificial general intelligence. But the range and diversity of tasks that a general intelligence could profitably perform in a modern economy is enormous. It would be infeasible to create specialpurpose software to handle all of those tasks. Even if it could be done, such a project would take a long time to carry out. Before it could be completed, the nature of some of the tasks would have changed, and new tasks would have become relevant. There would be great advantage to having software that can learn on its own to do new tasks, and indeed to discover new tasks in need of doing. But this would require that the software be able to learn, reason, and plan, and to do so in a powerful and robustly cross-domain manner. In other words, it would require general intelligence.

Especially relevant for our purposes is the task of software development itself. There would be enormous practical advantages to being able to automate this. Yet the capacity for rapid self-improvement is just the critical property that enables a seed AI to set off an intelligence explosion.

If general intelligence is not dispensable, is there some other way of construing the tool-AI idea so as to preserve the reassuringly passive quality of a humdrum tool? Could one have a general intelligence that is not an agent? Intuitively, it is not just the limited capability of ordinary software that makes it safe: it is also its lack of ambition. There is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way. The spreadsheet application does not “want” anything at all; it just blindly carries out the instructions in the program. What (one might wonder) stands in the way of creating a more generally intelligent application of the same type? An oracle, for instance, which, when prompted with a description of a goal, would respond with a plan for how to achieve it, in much the same way that Excel responds to a column of numbers by calculating a sum—without thereby expressing any “preferences” regarding its output or how humans might choose to use it?

The classical way of writing software requires the programmer to understand the task to be performed in sufficient detail to formulate an explicit solution process consisting of a sequence of mathematically well-defined steps expressible in code.13 (In practice, software engineers rely on code libraries stocked with useful behaviors, which they can invoke without needing to understand how the behaviors are implemented. But that code was originally created by programmers who had a detailed understanding of what they were doing.) This approach works for solving well-understood tasks, and is to credit for most software that is currently in use. It falls short, however, when nobody knows precisely how to solve all of the tasks that need to be accomplished. This is where techniques from the field of artificial intelligence become relevant. In narrow applications, machine learning might be used merely to fine-tune a few parameters in a largely human-designed program. A spam filter, for example, might be trained on a corpus of hand-classified email messages in a process that changes the weights that the classification algorithm places on various diagnostic features. In a more ambitious application, the classifier might be built so that it can discover new features on its own and test their validity in a changing environment. An even more sophisticated spam filter could be endowed with some ability to reason about the trade-offs facing the user or about the contents of the messages it is classifying. In neither of these cases does the programmer need to know the best way of distinguishing spam from ham, only how to set up an algorithm that can improve its own performance via learning, discovering, or reasoning.

With advances in artificial intelligence, it would become possible for the programmer to offload more of the cognitive labor required to figure out how to accomplish a given task. In an extreme case, the programmer would simply specify a formal criterion of what counts as success and leave it to the AI to find a solution. To guide its search, the AI would use a set of powerful heuristics and other methods to discover structure in the space of possible solutions. It would keep searching until it found a solution that satisfied the success criterion. The AI would then either implement the solution itself or (in the case of an oracle) report the solution to the user.

Rudimentary forms of this approach are quite widely deployed today. Nevertheless, software that uses AI and machine learning techniques, though it has some ability to find solutions that the programmers had not anticipated, functions for all practical purposes like a tool and poses no existential risk. We would enter the danger zone only when the methods used in the search for solutions become extremely powerful and general: that is, when they begin to amount to general intelligence—and especially when they begin to amount to superintelligence.

There are (at least) two places where trouble could then arise. First, the superintelligent search process might find a solution that is not just unexpected but radically unintended. This could lead to a failure of one of the types discussed previously (“perverse instantiation,” “infrastructure profusion,” or “mind crime”). It is most obvious how this could happen in the case of a sovereign or a genie, which directly implements the solution it has found. If making molecular smiley faces or transforming the planet into paperclips is the first idea that the superintelligence discovers that meets the solution criterion, then smiley faces or paperclips we get.14 But even an oracle, which—if all else goes well—merely reports the solution, could become a cause of perverse instantiation. The user asks the oracle for a plan to achieve a certain outcome, or for a technology to serve a certain function; and when the user follows the plan or constructs the technology, a perverse instantiation can ensue, just as if the AI had implemented the solution itself.

A second place where trouble could arise is in the course of the software’s operation. If the methods that the software uses to search for a solution are sufficiently sophisticated, they may include provisions for managing the search process itself in an intelligent manner. In this case, the machine running the software may begin to seem less like a mere tool and more like an agent. Thus, the software may start by developing a plan for how to go about its search for a solution. The plan may specify which areas to explore first and with what methods, what data to gather, and how to make best use of available computational resources. In searching for a plan that satisfies the software’s internal criterion (such as yielding a sufficiently high probability of finding a solution satisfying the user-specified criterion within the allotted time), the software may stumble on an unorthodox idea. For instance, it might generate a plan that begins with the acquisition of additional computational resources and the elimination of potential interrupters (such as human beings). Such “creative” plans come into view when the software’s cognitive abilities reach a sufficiently high level. When the software puts such a plan into action, an existential catastrophe may ensue.

As the examples in Box 9 illustrate, open-ended search processes sometimes evince strange and unexpected non-anthropocentric solutions even in their currently limited forms. Present-day search processes are not hazardous because they are too weak to discover the kind of plan that could enable a program to take over the world. Such a plan would include extremely difficult steps, such as the invention of a new weapons technology several generations ahead of the state of the art or the execution of a propaganda campaign far more effective than any communication devised by human spin doctors. To have a chance of even conceiving of such ideas, let alone developing them in a way that would actually work, a machine would probably need the capacity to represent the world in a way that is at least as rich and realistic as the world model possessed by a normal human adult (though a lack of awareness in some areas might possibly be compensated for by extra skill in others). This is far beyond the reach of contemporary AI. And because of the combinatorial explosion, which generally defeats attempts to solve complicated planning problems with bruteforce methods (as we saw in Chapter 1), the shortcomings of known algorithms cannot realistically be overcome simply by pouring on more computing power.21 However, once the search or planning processes become powerful enough, they also become potentially dangerous.

Instead of allowing agent-like purposive behavior to emerge spontaneously and haphazardly from the implementation of powerful search processes (including processes searching for internal work plans and processes directly searching for solutions meeting some user-specified criterion), it may be better to create agents on purpose. Endowing a superintelligence with an explicitly agent-like structure can be a way of increasing predictability and transparency. A well-designed system, built such that there is a clean separation between its values and its beliefs, would let us predict something about the outcomes it would tend to produce. Even if we could not foresee exactly which beliefs the system would acquire or which situations it would find itself in, there would be a known place where we could inspect its final values and thus the criteria that it will use in selecting its future actions and in evaluating any potential plan.

Further research would be needed to determine which type of system would be safest. The answer might depend on the conditions under which the AI would be deployed. The oracle caste is obviously attractive from a safety standpoint, since it would allow both capability control methods and motivation selection methods to be applied. It might thus seem to simply dominate the sovereign caste, which would only allow motivation selection methods (except in scenarios in which the world is believed to contain other powerful superintelligences, in which case social integration or anthropic capture might apply). However, an oracle could place a lot of power into the hands of its operator, who might be corrupted or might apply the power unwisely, whereas a sovereign would offer some protection against these hazards. The safety ranking is therefore not so easily determined.

A genie can be viewed as a compromise between an oracle and a sovereign— but not necessarily a good compromise. In many ways, it would share the disadvantages of both. The apparent safety of a tool-AI, meanwhile, may be illusory. In order for tools to be versatile enough to substitute for superintelligent agents, they may need to deploy extremely powerful internal search and planning processes. Agent-like behaviors may arise from such processes as an unplanned consequence. In that case, it would be better to design the system to be an agent in the first place, so that the programmers can more easily see what criteria will end up determining the system’s output.

CHAPTER 11: Multipolar scenarios

We have seen (particularly in Chapter 8) how menacing a unipolar outcome could be, one in which a single superintelligence obtains a decisive strategic advantage and uses it to establish a singleton. In this chapter, we examine what would happen in a multipolar outcome, a post-transition society with multiple competing superintelligent agencies. Our interest in this class of scenarios is twofold. First, as alluded to in Chapter 9, social integration might be thought to offer a solution to the control problem. We already noted some limitations with that approach, and this chapter paints a fuller picture. Second, even without anybody setting out to create a multipolar condition as a way of handling the control problem, such an outcome might occur anyway. So what might such an outcome look like? The resulting competitive society is not necessarily attractive, nor long-lasting.

In singleton scenarios, what happens post-transition depends almost entirely on the values of the singleton. The outcome could thus be very good or very bad, depending on what those values are. What the values are depends, in turn, on whether the control problem was solved, and—to the degree to which it was solved—on the goals of the project that created the singleton.

If one is interested in the outcome of singleton scenarios, therefoIf one is interested in the outcome of singleton scenarios, therefore, one really only has three sources of information: information about matters that cannot be affected by the actions of the singleton (such as the laws of physics); information about convergent instrumental values; and information that enables one to predict or speculate about what final values the singleton will have.re, one really only has three sources of information: information about matters that cannot be affected by the actions of the singleton (such as the laws of physics); information about convergent instrumental values; and information that enables one to predict or speculate about what final values the singleton will have.

In multipolar scenarios, an additional set of constraints comes into play, constraints having to do with how agents interact. The social dynamics emerging from such interactions can be studied using techniques from game theory, economics, and evolution theory. Elements of political science and sociology are also relevant insofar as they can be distilled and abstracted from some of the more contingent features of human experience. Although it would be unrealistic to expect these constraints to give us a precise picture of the post-transition world, they can help us identify some salient possibilities and challenge some unfounded assumptions.

We will begin by exploring an economic scenario characterized by a low level of regulation, strong protection of property rights, and a moderately rapid introduction of inexpensive digital minds.1 This type of model is most closely associated with the American economist Robin Hanson, who has done pioneering work on the subject. Later in this chapter, we will look at some evolutionary considerations and examine the prospects of an initially multipolar post-transition world subsequently coalescing into a singleton.

Of horses and men

General machine intelligence could serve as a substitute for human intelligence. Not only could digital minds perform the intellectual work now done by humans, but, once equipped with good actuators or robotic bodies, machines could also substitute for human physical labor. Suppose that machine workers—which can be quickly reproduced—become both cheaper and more capable than human workers in virtually all jobs. What happens then?

Wages and unemployment

With cheaply copyable labor, market wages fall. The only place where humans would remain competitive may be where customers have a basic preference for work done by humans. Today, goods that have been handcrafted or produced by indigenous people sometimes command a price premium. Future consumers might similarly prefer human-made goods and human athletes, human artists, human lovers, and human leaders to functionally indistinguishable or superior artificial counterparts. It is unclear, however, just how widespread such preferences would be. If machine-made alternatives were sufficiently superior, perhaps they would be more highly prized.

One parameter that might be relevant to consumer choice is the inner life of the worker providing a service or product. A concert audience, for instance, might like to know that the performer is consciously experiencing the music and the venue. Absent phenomenal experience, the musician could be regarded as merely a high-powered jukebox, albeit one capable of creating the threedimensional appearance of a performer interacting naturally with the crowd. Machines might then be designed to instantiate the same kinds of mental states that would be present in a human performing the same task. Even with perfect replication of subjective experiences, however, some people might simply prefer organic work. Such preferences could also have ideological or religious roots. Just as many Muslims and Jews shun food prepared in ways they classify as haram or treif, so there might be groups in the future that eschew products whose manufacture involved unsanctioned use of machine intelligence.

What hinges on this? To the extent that cheap machine labor can substitute for human labor, human jobs may disappear. Fears about automation and job loss are of course not new. Concerns about technological unemployment have surfaced periodically, at least since the Industrial Revolution; and quite a few professions have in fact gone the way of the English weavers and textile artisans who in the early nineteenth century united under the banner of the folkloric “General Ludd” to fight against the introduction of mechanized looms. Nevertheless, although machinery and technology have been substitutes for many particular types of human labor, physical technology has on the whole been a complement to labor. Average human wages around the world have been on a long-term upward trend, in large part because of such complementarities. Yet what starts out as a complement to labor can at a later stage become a substitute for labor. Horses were initially complemented by carriages and ploughs, which greatly increased the horse’s productivity. Later, horses were substituted for by automobiles and tractors. These later innovations reduced the demand for equine labor and led to a population collapse. Could a similar fate befall the human species?

The parallel to the story of the horse can be drawn out further if we ask why it is that there are still horses around. One reason is that there are still a few niches in which horses have functional advantages; for example, police work. But the main reason is that humans happen to have peculiar preferences for the services that horses can provide, including recreational horseback riding and racing. These preferences can be compared to the preferences we hypothesized some humans might have in the future, that certain goods and services be made by human hand. Although suggestive, this analogy is, however, inexact, since there is still no complete functional substitute for horses. If there were inexpensive mechanical devices that ran on hay and had exactly the same shape, feel, smell, and behavior as biological horses—perhaps even the same conscious experiences—then demand for biological horses would probably decline further.

With a sufficient reduction in the demand for human labor, wages would fall below the human subsistence level. The potential downside for human workers is therefore extreme: not merely wage cuts, demotions, or the need for retraining, but starvation and death. When horses became obsolete as a source of moveable power, many were sold off to meatpackers to be processed into dog food, bone meal, leather, and glue. These animals had no alternative employment through which to earn their keep. In the United States, there were about 26 million horses in 1915. By the early 1950s, 2 million remained

Capital and welfare

One difference between humans and horses is that humans own capital. A stylized empirical fact is that the total factor share of capital has for a long time remained steady at approximately 30% (though with significant short-term fluctuations).3 This means that 30% of total global income is received as rent by owners of capital, the remaining 70% being received as wages by workers. If we classify AI as capital, then with the invention of machine intelligence that can fully substitute for human work, wages would fall to the marginal cost of such machine-substitutes, which—under the assumption that the machines are very efficient—would be very low, far below human subsistence-level income. The income share received by labor would then dwindle to practically nil. But this implies that the factor share of capital would become nearly 100% of total world product. Since world GDP would soar following an intelligence explosion (because of massive amounts of new labor-substituting machines but also because of technological advances achieved by superintelligence, and, later, acquisition of vast amounts of new land through space colonization), it follows that the total income from capital would increase enormously. If humans remain the owners of this capital, the total income received by the human population would grow astronomically, despite the fact that in this scenario humans would no longer receive any wage income.

The human species as a whole could thus become rich beyond the dreams of Avarice. How would this income be distributed? To a first approximation, capital income would be proportional to the amount of capital owned. Given the astronomical amplification effect, even a tiny bit of pre-transition wealth would balloon into a vast post-transition fortune. However, in the contemporary world, many people have no wealth. This includes not only individuals who live in poverty but also some people who earn a good income or who have high human capital but have negative net worth. For example, in affluent Denmark and Sweden 30% of the population report negative wealth—often young, middleclass people with few tangible assets and credit card debt or student loans.4 Even if savings could earn extremely high interest, there would need to be some seed grain, some starting capital, in order for the compounding to begin.

Nevertheless, even individuals who have no private wealth at the start of the transition could become extremely rich. Those who participate in a pension scheme, for instance, whether public or private, should be in a good position, provided the scheme is at least partially funded.6 Have-nots could also become rich through the philanthropy of those who see their net worth skyrocket: because of the astronomical size of the bonanza, even a very small fraction donated as alms would be a very large sum in absolute terms.

It is also possible that riches could still be made through work, even at a posttransition stage when machines are functionally superior to humans in all domains (as well as cheaper than even subsistence-level human labor). As noted earlier, this could happen if there are niches in which human labor is preferred for aesthetic, ideological, ethical, religious, or other non-pragmatic reasons. In a scenario in which the wealth of human capital-holders increases dramatically, demand for such labor could increase correspondingly. Newly minted trillionaires or quadrillionaires could afford to pay a hefty premium for having some of their goods and services supplied by an organic “fair-trade” labor force. The history of horses again offers a parallel. After falling to 2 million in the early 1950s, the US horse population has undergone a robust recovery: a recent census puts the number at just under 10 million head.7 The rise is not due to new functional needs for horses in agriculture or transportation; rather, economic growth has enabled more Americans to indulge a fancy for equestrian recreation.

Another relevant difference between humans and horses, beside capital ownership, is that humans are capable of political mobilization. A human-run government could use the taxation power of the state to redistribute private profits, or raise revenue by selling appreciated state-owned assets, such as public land, and use the proceeds to pension off its constituents. Again, because of the explosive economic growth during and immediately after the transition, there would be vastly more wealth sloshing around, making it relatively easy to fill the cups of all unemployed citizens. It should be feasible even for a single country to provide every human worldwide with a generous living wage at no greater proportional cost than what many countries currently spend on foreign aid.

The Malthusian principle in a historical perspective

So far we have assumed a constant human population. This may be a reasonable assumption for short timescales, since biology limits the rate of human reproduction. Over longer timescales, however, the assumption is not necessarily reasonable.

The human population has increased a thousandfold over the past 9,000 years.9 The increase would have been much faster except for the fact that throughout most of history and prehistory, the human population was bumping up against the limits of the world economy. An approximately Malthusian condition prevailed, in which most people received subsistence-level incomes that just barely allowed them to survive and raise an average of two children to maturity.10 There were temporary and local reprieves: plagues, climate fluctuations, or warfare intermittently culled the population and freed up land, enabling survivors to improve their nutritional intake—and to bring up more children, until the ranks were replenished and the Malthusian condition reinstituted. Also, thanks to social inequality, a thin elite stratum could enjoy consistently above-subsistence income (at the expense of somewhat lowering the total size of the population that could be sustained). A sad and dissonant thought: that in this Malthusian condition, the normal state of affairs during most of our tenure on this planet, it was droughts, pestilence, massacres, and inequality—in common estimation the worst foes of human welfare—that may have been the greatest humanitarians: they alone enabling the average level of well-being to occasionally bop up slightly above that of life at the very margin of subsistence.

Superimposed on local fluctuations, history shows a macro-pattern of initially slow but accelerating economic growth, fueled by the accumulation of technological innovations. The growing world economy brought with it a commensurate increase in global population. (More precisely, a larger population itself appears to have strongly accelerated the rate of growth, perhaps mainly by increasing humanity’s collective intelligence.11) Only since the Industrial Revolution, however, did economic growth become so rapid that population growth failed to keep pace. Average income thus started to rise, first in the earl yindustrializing countries of Western Europe, subsequently in most of the world. Even in the poorest countries today, average income substantially exceeds subsistence level, as reflected in the fact that the populations of these countries are growing.

The poorest countries now have the fastest population growth, as they have yet to complete the “demographic transition” to the low-fertility regime that has taken hold in more developed societies. Demographers project that the world population will rise to about 9 billion by mid-century, and that it might thereafter plateau or decline as the poorer countries join the developed world in this lowfertility regime.12 Many rich countries already have fertility rates that are below replacement level; in some cases, far below.

Yet there are reasons, if we take a longer view and assume a state of unchanging technology and continued prosperity, to expect a return to the historically and ecologically normal condition of a world population that butts up against the limits of what our niche can support. If this seems counterintuitive in light of the negative relationship between wealth and fertility that we are currently observing on the global scale, we must remind ourselves that this modern age is a brief slice of history and very much an aberration. Human behavior has not yet adapted to contemporary conditions. Not only do we fail to take advantage of obvious ways to increase our inclusive fitness (such as by becoming sperm or egg donors) but we actively sabotage our fertility by using birth control. In the environment of evolutionary adaptedness, a healthy sex drive may have been enough to make an individual act in ways that maximized her reproductive potential; in the modern environment, however, there would be a huge selective advantage to having a more direct desire for being the biological parent to the largest possible number of children. Such a desire is currently being selected for, as are other traits that increase our propensity to reproduce. Cultural adaptation, however, might steal a march on biological evolution. Some communities, such those of the Hutterites or the adherents of the Quiverfull evangelical movement, have natalist cultures that encourage large families, and they are consequently undergoing rapid expansion.

Population growth and investment

If we imagine current socioeconomic conditions magically frozen in their current shape, the future would be dominated by cultural or ethnic groups that sustain high levels of fertility. If most people had preferences that were fitnessmaximizing in the contemporary environment, the population could easily double in each generation. Absent population control policies—which would have to become steadily more rigorous and effective to counteract the evolution of stronger preferences to circumvent them—the world population would then continue to grow exponentially until some constraint, such as land scarcity or depletion of easy opportunities for important innovation, made it impossible for the economy to keep pace: at which point, average income would start to decline until it reached the level where crushing poverty prevents most people from raising much more than two children to maturity. Thus the Malthusian principle would reassert itself, like a dread slave master, bringing our escapade into the dreamland of abundance to an end, and leading us back to the quarry in chains, there to resume the weary struggle for subsistence.

This longer-term outlook could be telescoped into a more imminent prospect by the intelligence explosion. Since software is copyable, a population of emulations or AIs could double rapidly—over the course of minutes rather than decades or centuries—soon exhausting all available hardware.

Private property might offer partial protection against the emergence of a universal Malthusian condition. Consider a simple model in which clans (or closed communities, or states) start out with varying amounts of property and independently adopt different policies about reproduction and investment. Some clans discount the future steeply and spend down their endowment, whereafter their impoverished members join the global proletariat (or die, if they cannot support themselves through their labor). Other clans invest some of their resources but adopt a policy of unlimited reproduction: such clans grow more populous until they reach an internal Malthusian condition in which their members are so poor that they die at almost the same rate as they reproduce, at which point the clan’s population growth slows to equal the growth of its resources. Yet other clans might restrict their fertility to below the rate of growth of their capital: such clans could slowly increment their numbers while their members also grow richer per capita.

If wealth is redistributed from the wealthy clans to the members of the rapidly reproducing or rapidly discounting clans (whose children, copies, or offshoots, through no fault of their own, were launched into the world with insufficient capital to survive and thrive) then a universal Malthusian condition would be more closely approximated. In the limiting case, all members of all clans would receive subsistence level income and everybody would be equal in their poverty.

If property is not redistributed, prudent clans might hold on to a certain amount of capital, and it is possible that their wealth could grow in absolute terms. It is, however, unclear whether humans could earn as high rates of return on their capital as machine intelligences could earn on theirs, because there may be synergies between labor and capital such that an single agent who can supply both (e.g. an entrepreneur or investor who is both skilled and wealthy) can attain a private rate of return on her capital exceeding the market rate obtainable by agents who possess financial but not cognitive resources. Humans, being less skilled than machine intelligences, may therefore grow their capital more slowly —unless, of course, the control problem had been completely solved, in which case the human rate of return would equal the machine rate of return, since a human principal could task a machine agent to manage her savings, and could do so costlessly and without conflicts of interest: but otherwise, in this scenario, the fraction of the economy owned by machines would asymptotically approach one hundred percent.

A scenario in which the fraction of the economy that is owned by machines asymptotically approaches one hundred percent is not necessarily one in which the size of the human slice declines. If the economy grows at a sufficient clip, then even a relatively diminishing fraction of it may still be increasing in its absolute size. This may sound like modestly good news for humankind: in a multipolar scenario in which property rights are protected—even if we completely fail to solve the control problem—the total amount of wealth owned by human beings could increase. Of course, this effect would not take care of the problem of population growth in the human population pulling down per capita income to subsistence level, nor the problem of humans who ruin themselves because they discount the future.

In the long run, the economy would become increasingly dominated by those clans that have the highest savings rates—misers who own half the city and live under a bridge. Only in the fullness of time, when there are no more opportunities for investment, would the maximally prosperous misers start drawing down their savings.14 However, if there is less than perfect protection for property rights—for example if the more efficient machines on net succeed, by hook or by crook, in transferring wealth from humans to themselves—then human capitalists may need to spend down their capital much sooner, before it gets depleted by such transfers (or the ongoing costs incurred in securing their wealth against such transfers). If these developments take place on digital rather than biological timescales, then the glacial humans might find themselves expropriated before they could say Jack Robinson.

Life in an algorithmic economy

Life for biological humans in a post-transition Malthusian state need not resemble any of the historical states of man (as hunter–gatherer, farmer, or office worker). Instead, the majority of humans in this scenario might be idle rentiers who eke out a marginal living on their savings.16 They would be very poor, yet derive what little income they have from savings or state subsidies. They would live in a world with extremely advanced technology, including not only superintelligent machines but also anti-aging medicine, virtual reality, and various enhancement technologies and pleasure drugs: yet these might be generally unaffordable. Perhaps instead of using enhancement medicine, they would take drugs to stunt their growth and slow their metabolism in order to reduce their cost of living (fast-burners being unable to survive at the gradually declining subsistence income). As our numbers increase and our average income declines further, we might degenerate into whatever minimal structure still qualifies to receive a pension—perhaps minimally conscious brains in vats, oxygenized and nourished by machines, slowly saving up enough money to reproduce by having a robot technician develop a clone of them.

Further frugality could be achieved by means of uploading, since a physically optimized computing substrate, devised by advanced superintelligence, would be more efficient than a biological brain. The migration into the digital realm might be stemmed, however, if emulations were regarded as non-humans or noncitizens ineligible to receive pensions or to hold tax-exempt savings accounts. In that case, a niche for biological humans might remain open, alongside a perhaps vastly larger population of emulations or artificial intelligences.

So far we have focused on the fate of the humans, who may be supported by savings, subsidies, or wage income deriving from other humans who prefer to hire humans. Let us now turn our attention to some of the entities that we have so far classified as “capital”: machines that may be owned by human beings, that are constructed and operated for the sake of the functional tasks they perform, and that are capable of substituting for human labor in a very wide range of jobs. What may the situation be like for these workhorses of the new economy?

If these machines were mere automata, simple devices like a steam engine or the mechanism in a clock, then no further comment would be needed: there would be a large amount of such capital in a post-transition economy, but it would seem not to matter to anybody how things turn out for pieces of insentient equipment. However, if the machines have conscious minds—if they are constructed in such a way that their operation is associated with phenomenal awareness (or if they for some other reason are ascribed moral status)—then it becomes important to consider the overall outcome in terms of how it would affect these machine minds. The welfare of the working machine minds could even appear to be the most important aspect of the outcome, since they may be numerically dominant

Voluntary slavery, casual death

A salient initial question is whether these working machine minds are owned as capital (slaves) or are hired as free wage laborers. On closer inspection however, it become doubtful that anything really hinges on the issue. There are two reasons for this. First, if a free worker in a Malthusian state gets paid a subsistence-level wage, he will have no disposable income left after he has paid for food and other necessities. If the worker is instead a slave, his owner will pay for his maintenance and again he will have no disposable income. In either case, the worker gets the necessities and nothing more. Second, suppose that the free laborer were somehow in a position to command an above-subsistence-level income (perhaps because of favorable regulation). How will he spend the surplus? Investors would find it most profitable to create workers who would be “voluntary slaves”—who would willingly work for subsistence-level wages. Investors may create such workers by copying those workers who are compliant. With appropriate selection (and perhaps some modification to the code) investors might be able to create workers who not only prefer to volunteer their labor but who would also choose to donate back to their owners any surplus income they might happen to receive. Giving money to the worker would then be but a roundabout way of giving money to the owner or employer, even if the worker were a free agent with full legal rights.

Perhaps it will be objected that it would be difficult to design a machine so that it wants to volunteer for any job assigned to it or so that it wants to donate its wages to its owner. Emulations, in particular, might be imagined to have more typically human desires. But note that even if the original control problem is difficult, we are here considering a condition after the transition, a time when methods for motivation selection have presumably been perfected. In the case of emulations, one might get quite far simply by selecting from the pre-existing range of human characters; and we have described several other motivation selection methods. The control problem may also in some ways be simplified by the current assumption that the new machine intelligence enters into a stable socioeconomic matrix that is already populated with other law-abiding superintelligent agents.

Let us, then, consider the plight of the working-class machine, whether it be operating as a slave or a free agent. We focus first on emulations, the easiest case to imagine.

Bringing a new biological human worker into the world takes anywhere between fifteen and thirty years, depending on how much expertise and experience is required. During this time the new person must be fed, housed, nurtured, and educated—at great expense. By contrast, spawning a new copy of a digital worker is as easy as loading a new program into working memory. Life thus becomes cheap. A business could continuously adapt its workforce to fit demands by spawning new copies—and terminating copies that are no longer needed, to free up computer resources. This could lead to an extremely high death rate among digital workers. Many might live for only one subjective day.

There are reasons other than fluctuations in demand why employers or owners of emulations might want to “kill” or “end” their workers frequently.18 If an emulation mind, like a biological mind, requires periods of rest and sleep in order to function, it might be cheaper to erase a fatigued emulation at the end of a day and replace it with a stored state of a fresh and rested emulation. As this procedure would cause retrograde amnesia for everything that had been learned during that day, emulations performing tasks requiring long cognitive threads would be spared such frequent erasure. It would be difficult, for example, to write a book if each morning when one sat down at one’s desk, one had no memory of what one had done before. But other jobs could be performed adequately by agents that are frequently recycled: a shop assistant or a customer service agent, once trained, may only need to remember new information for twenty minutes.

Since recycling emulations would prevent memory and skill formation, some emulations may be placed on a special learning track where they would run continuously, including for rest and sleep, even in jobs that do not strictly require long cognitive threads. For example, some customer service agents might run for many years in optimized learning environments, assisted by coaches and performance evaluators. The best of these trainees would then be used like studs, serving as templates from which millions of fresh copies are stamped out each day. Great effort would be poured into improving the performance of such worker templates, because even a small increment in productivity would yield great economic value when applied in millions of copies.

In parallel with efforts to train worker-templates for particular jobs, intense efforts would also be made to improve the underlying emulation technology. Advances here would be even more valuable than advances in individual worker-templates, since general technology improvements could be applied to all emulation workers (and potentially to non-worker emulations also) rather than only to those in a particular occupation. Enormous resources would be devoted to finding computational shortcuts allowing for more efficient implementations of existing emulations, and also into developing neuromorphic and entirely synthetic AI architectures. This research would probably mostly be done by emulations running on very fast hardware. Depending on the price of computer power, millions, billions, or trillions of emulations of the sharpest human research minds (or enhanced versions thereof) may be working around the clock on advancing the frontier of machine intelligence; and some of these may be operating orders of magnitude faster than biological brains.19 This is a good reason for thinking that the era of human-like emulations would be brief— a very brief interlude in sidereal time—and that it would soon give way to an era of greatly superior artificial intelligence.

We have already encountered several reasons why employers of emulation workers may periodically cull their herds: fluctuations in demand for different kinds of laborers, cost savings of not having to emulate rest and sleep time, and the introduction of new and improved templates. Security concerns might furnish another reason. To prevent workers from developing subversive plans and conspiracies, emulations in some sensitive positions might be run only for limited periods, with frequent resets to an earlier stored ready-state.

These ready-states to which emulations would be reset would be carefully prepared and vetted. A typical short-lived emulation might wake up in a wellrested mental state that is optimized for loyalty and productivity. He remembers having graduated top of his class after many (subjective) years of intense training and selection, then having enjoyed a restorative holiday and a good night’s sleep, then having listened to a rousing motivational speech and stirring music, and now he is champing at the bit to finally get to work and to do his utmost for his employer. He is not overly troubled by thoughts of his imminent death at the end of the working day. Emulations with death neuroses or other hang-ups are less productive and would not have been selected.

Would maximally efficient work be fun?

One important variable in assessing the desirability of a hypothetical condition like this is the hedonic state of the average emulation.22 Would a typical emulation worker be suffering or would he be enjoying the experience of working hard on the task at hand?

We must resist the temptation to project our own sentiments onto the imaginary emulation worker. The question is not whether you would feel happy if you had to work constantly and n

It is moderately more relevant to consider the current human average hedonic experience during working hours. Worldwide studies asking respondents how happy they are find that most rate themselves as “quite happy” or “very happy” (averaging 3.1 on a scale from 1 to 4).23 Studies on average affect, asking respondents how frequently they have recently experienced various positive or negative affective states, tend to get a similar result (producing a net affect of about 0.52 on a scale from –1 to 1). There is a modest positive effect of a country’s per capita income on average subjective well-being.24 However, it is hazardous to extrapolate from these findings to the hedonic state of future emulation workers. One reason that could be given for this is that their condition would be so different: on the one hand, they might be working much harder; on the other hand, they might be free from diseases, aches, hunger, noxious odors, and so forth. Yet such considerations largely miss the mark. The much more important consideration here is that hedonic tone would be easy to adjust through the digital equivalent of drugs or neurosurgery. This means that it would be a mistake to infer the hedonic state of future emulations from the external conditions of their lives by imagining how we ourselves and other people like us would feel in those circumstances. Hedonic state would be a matter of choice. In the model we are currently considering, the choice would be made by capitalowners seeking to maximize returns on their investment in emulation-workers. Consequently, the question of how happy emulations would feel boils down to the question of which hedonic states would be most productive (in the various jobs that emulations would be employed to do).

Here, again, one might seek to draw an inference from observations about human happiness. If it is the case, across most times, places, and occupations, that people are typically at least moderately happy, this would create some presumption in favor of the same holding in a post-transition scenario like the one we are considering. To be clear, the argument in this case would not be that human minds have a predisposition towards happiness so they would probably find satisfaction under these novel conditions; but rather that a certain average level of happiness has proved adaptive for human minds in the past so maybe a similar level of happiness will prove adaptive for human-like minds in the future. Yet this formulation also reveals the weakness of the inference: to wit, that the mental dispositions that were adaptive for hunter–gatherer hominids roaming the African savanna may not necessarily be adaptive for modified emulations living in post-transition virtual realities. We can certainly hope that the future emulation-workers would be as happy as, or happier than, typical workers were in human history; but we have yet to see any compelling reason for supposing it would be so (in the laissez-faire multipolar scenario currently under examination).

Consider the possibility that the reason happiness is prevalent among humans (to whatever limited extent it is prevalent) is that cheerful mood served a signaling function in the environment of evolutionary adaptedness. Conveying the impression to other members of the social group of being in flourishing condition—in good health, in good standing with one’s peers, and in confident expectation of continued good fortune—may have boosted an individual’s popularity. A bias toward cheerfulness could thus have been selected for, with the result that human neurochemistry is now biased toward positive affect compared to what would have been maximally efficient according to simpler materialistic criteria. If this were the case, then the future of joie de vivre might depend on cheer retaining its social signaling function unaltered in the posttransition world: an issue to which we will return shortly.

What if glad souls dissipate more energy than glum ones? Perhaps the joyful are more prone to creative leaps and flights of fancy—behaviors that future employers might disprize in most of their workers. Perhaps a sullen or anxious fixation on simply getting on with the job without making mistakes will be the productivity-maximizing attitude in most lines of work. The claim here is not that this is so, but that we do not know that it is not so. Yet we should consider just how bad it could be if some such pessimistic hypothesis about a future Malthusian state turned out to be true: not only because of the opportunity cost of having failed to create something better—which would be enormous—but also because the state could be bad in itself, possibly far worse than the original Malthusian state.

We seldom put forth full effort. When we do, it is sometimes painful. Imagine running on a treadmill at a steep incline—heart pounding, muscles aching, lungs gasping for air. A glance at the timer: your next break, which will also be your death, is due in 49 years, 3 months, 20 days, 4 hours, 56 minutes, and 12 seconds. You wish you had not been born.

Again the claim is not that this is how it would be, but that we do not know that it is not. One could certainly make a more optimistic case. For example, there is no obvious reason that emulations would need to suffer bodily injury and sickness: the elimination of physical wretchedness would be a great improvement over the present state of affairs. Furthermore, since such stuff as virtual reality is made of can be fairly cheap, emulations may work in sumptuous surroundings—in splendid mountaintop palaces, on terraces set in a budding spring forest, or on the beaches of an azure lagoon—with just the right illumination, temperature, scenery and décor; free from annoying fumes, noises, drafts, and buzzing insects; dressed in comfortable clothing, feeling clean and focused, and well nourished. More significantly, if—as seems perfectly possible —the optimum human mental state for productivity in most jobs is one of joyful eagerness, then the era of the emulation economy could be quite paradisiacal.

There would, in any case, be a great option value in arranging matters in such a manner that somebody or something could intervene to set things right if the default trajectory should happen to veer toward dystopia. It could also be desirable to have some sort of escape hatch that would permit bailout into death and oblivion if the quality of life were to sink permanently below the level at which annihilation becomes preferable to continued existence.

Unconscious outsourcers?

In the longer run, as the emulation era gives way to an artificial intelligence era (or if machine intelligence is attained directly via AI without a preceding whole brain emulation stage) pain and pleasure might possibly disappear entirely in a multipolar outcome, since a hedonic reward mechanism may not be the most effective motivation system for an complex artificial agent (one that, unlike the human mind, is not burdened with the legacy of animal wetware). Perhaps a more advanced motivation system would be based on an explicit representation of a utility function or some other architecture that has no exact functional analogs to pleasure and pain.

A related but slightly more radical multipolar outcome—one that could involve the elimination of almost all value from the future—is that the universal proletariat would not even be conscious. This possibility is most salient with respect to AI, which might be structured very differently than human intelligence. But even if machine intelligence were initially achieved though whole brain emulation, resulting in conscious digital minds, the competitive forces unleashed in a post-transition economy could easily lead to the emergence of progressively less neuromorphic forms of machine intelligence, either because synthetic AI is created de novo or because the emulations would, through successive modifications and enhancements, increasingly depart their original human form.

Consider a scenario in which after emulation technology has been developed, continued progress in neuroscience and computer science (expedited by the presence of digital minds to serve as both researchers and test subjects) makes it possible to isolate individual cognitive modules in an emulation, and to hook them up to modules isolated from other emulations. A period of training and adjustment may be required before different modules can collaborate effectively; but modules that conform to common standards could more quickly interface with other standard modules. This would make standardized modules more productive, and create pressure for more standardization.

Emulations can now begin to outsource increasing portions of their functionality. Why learn arithmetic when you can send your numerical reasoning task to Gauss-Modules, Inc.? Why be articulate when you can hire Coleridge Conversations to put your thoughts into words? Why make decisions about your personal life when there are certified executive modules that can scan your goal system and manage your resources to achieve your goals better than if you tried to do it yourself? Some emulations may prefer to retain most of their functionality and handle tasks themselves that could be done more efficiently by others. Those emulations would be like hobbyists who enjoy growing their own vegetables or knitting their own cardigans. Such hobbyist emulations would be less efficient; and if there is a net flow of resources from less to more efficient participants of the economy, the hobbyists would eventually lose out.

The bouillon cubes of discrete human-like intellects thus melt into an algorithmic soup.

It is conceivable that optimal efficiency would be attained by grouping capabilities in aggregates that roughly match the cognitive architecture of a human mind. It might be the case, for example, that a mathematics module must be tailored to a language module, and that both must be tailored to the executive module, in order for the three to work together. Cognitive outsourcing would then be almost entirely unworkable. But in the absence of any compelling reason for being confident that this is so, we must countenance the possibility that human-like cognitive architectures are optimal only within the constraints of human neurology (or not at all). When it becomes possible to build architectures that could not be implemented well on biological neural networks, new design space opens up; and the global optima in this extended space need not resemble familiar types of mentality. Human-like cognitive organizations would then lack a niche in a competitive post-transition economy or ecosystem.

There might be niches for complexes that are either less complex (such as individual modules), more complex (such as vast clusters of modules), or of similar complexity to human minds but with radically different architectures. Would these complexes have any intrinsic value? Should we welcome a world in which such alien complexes have replaced human complexes?

The answer may depend on the specific nature of those alien complexes. The present world has many levels of organization. Some highly complex entities, such as multinational corporations and nation states, contain human beings as constituents; yet we usually assign these high-level complexes only instrumental value. Corporations and states do not (it is generally assumed) have consciousness, over and above the consciousness of the people who constitute them: they cannot feel phenomenal pain or pleasure or experience any qualia. We value them to the extent that they serve human needs, and when they cease to do so we “kill” them without compunction. There are also lower-level entities, and those, too, are usually denied moral status. We see no harm in erasing an app from a smartphone, and we do not think that a neurosurgeon is wronging anyone when she extirpates a malfunctioning module from an epileptic brain. As for exotically organized complexes of a level similar to that of the human brain, most of us would perhaps judge them to have moral significance only if we thought they had a capacity or potential for conscious experience.

We could thus imagine, as an extreme case, a technologically highly advanced society, containing many complex structures, some of them far more intricate and intelligent than anything that exists on the planet today—a society which nevertheless lacks any type of being that is conscious or whose welfare has moral significance. In a sense, this would be an uninhabited society. It would be a society of economic miracles and technological awesomeness, with nobody there to benefit. A Disneyland without children.

Evolution is not necessarily up

The word “evolution” is often used as a synonym of “progress,” perhaps reflecting a common uncritical image of evolution as a force for good. A misplaced faith in the inherent beneficence of the evolutionary process can get in the way of a fair evaluation of the desirability of a multipolar outcome in which the future of intelligent life is determined by competitive dynamics. Any such evaluation must rest on some (at least implicit) opinion about the probability distribution of different phenotypes turning out to be adaptive in a post-transition digital life soup. It would be difficult in the best of circumstances to extract a clear and correct answer from the unavoidable goo of uncertainty that pervades these matters: more so, if we superadd a layer of Panglossian muck.

A possible source for faith in freewheeling evolution is the apparent upward directionality exhibited by the evolutionary process in the past. Starting from rudimentary replicators, evolution produced increasingly “advanced” organisms, including creatures with minds, consciousness, language, and reason. More recently, cultural and technological processes, which bear some loose similarities to biological evolution, have enabled humans to develop at an accelerated pace. On a geological as well as a historical timescale, the big picture seems to show an overarching trend toward increasing levels of complexity, knowledge, consciousness, and coordinated goal-directed organization: a trend which, not to put too fine a point on it, one might label “progress.”

The image of evolution as a process that reliably produces benign effects is difficult to reconcile with the enormous suffering that we see in both the human and the natural world. Those who cherish evolution’s achievements may do so more from an aesthetic than an ethical perspective. Yet the pertinent question is not what kind of future it would be fascinating to read about in a science fiction novel or to see depicted in a nature documentary, but what kind of future it would be good to live in: two very different matters.

Furthermore, we have no reason to think that whatever progress there has been was in any way inevitable. Much might have been luck. This objection derives support from the fact that an observation selection effect filters the evidence we can have about the success of our own evolutionary development.28 Suppose that on 99.9999% of all planets where life emerged it went extinct before developing to the point where intelligent observers could begin to ponder their origin. What should we expect to observe if that were the case? Arguably, we should expect to observe something like what we do in fact observe. The hypothesis that the odds of intelligent life evolving on a given planet are low does not predict that we should find ourselves on a planet where life went extinct at an early stage; rather, it may predict that we should find ourselves on a planet where intelligent life evolved, even if such planets constitute a very small fraction of all planets where primitive life evolved. Life’s long track record on Earth may therefore offer scant support to the claim that there was a high chance —let alone anything approaching inevitability—involved in the rise of higher organisms on our planet.

Thirdly, even if present conditions had been idyllic, and even if they could have been shown to have arisen ineluctably from some generic primordial state, there would still be no guarantee that the melioristic trend is set to continue into the indefinite future. This holds even if we disregard the possibility of a cataclysmic extinction event and indeed even if we assume that evolutionary developments will continue to produce systems of increasing complexity.

We suggested earlier that machine intelligence workers selected for maximum productivity would be working extremely hard and that it is unknown how happy such workers would be. We also raised the possibility that the fittest life forms within a competitive future digital life soup might not even be conscious. Short of a complete loss of pleasure, or of consciousness, there could be a wasting away of other qualities that many would regard as indispensible for a good life. Humans value music, humor, romance, art, play, dance, conversation, philosophy, literature, adventure, discovery, food and drink, friendship, parenting, sport, nature, tradition, and spirituality, among many other things. There is no guarantee that any of these would remain adaptive. Perhaps what will maximize fitness will be nothing but nonstop high-intensity drudgery, work of a drab and repetitive nature, destitute of ludic frisson, aimed only at improving the eighth decimal place of some economic output measure. The phenotypes selected would then have lives lacking in the aforesaid qualities, and depending on one’s axiology the result might strike one as either abhorrent, worthless, or merely impoverished, but at any rate a far cry from a utopia one would feel worthy of one’s commendation.

It might be wondered how such a bleak picture could be consistent with the fact that we do now indulge in music, humor, romance, art, etc. If these behaviors are really so “wasteful,” then how come they have been tolerated and indeed promoted by the evolutionary processes that shaped our species? That modern man is in an evolutionary disequilibrium does not account for this; for our Pleistocene forebears, too, engaged in most of these dissipations. Many of the behaviors in question are not even unique to Homo sapiens. Flamboyant display is found in a wide variety of contexts, from sexual selection in the animal kingdom to prestige contests among nation states.

Although a full evolutionary explanation for each of these behaviors is beyond the scope of the present inquiry, we can note that some of them serve functions that may not be as relevant in a machine intelligence context. Play, for example, which occurs only in some species and predominantly among juveniles, is mainly a way for the young animal to learn skills that it will need later in life. When emulations can be created as adults, already in possession of a mature repertoire of skills, or when knowledge and techniques acquired by one AI can be directly ported into another AI, the need for playful behavior might become less widespread.

Many of the other examples of humanistic behaviors may have evolved as hard-to-fake signals of qualities that are difficult to observe directly, such as bodily or mental resilience, social status, quality of allies, ability and willingness to prevail in a fight, or possession of resources. The peacock’s tail is the classic instance: only fit peacocks can afford to sprout truly extravagant plumage, and peahens have evolved to find it attractive. No less than morphological traits, behavioral traits too can signal genetic fitness or other socially relevant attributes.

Given that flamboyant display is so common among both humans and other species, one might consider whether it would not also be part of the repertoire of technologically more advanced life forms. Even if there were to be no narrowly instrumental use for playfulness or musicality or even for consciousness in the future ecology of intelligent information processing, might not these traits nonetheless confer some evolutionary advantage to their possessors by virtue of being reliable signals of other adaptive qualities?

While the possibility of a pre-established harmony between what is valuable to us and what would be adaptive in a future digital ecology is hard to rule out, there are reasons for skepticism. Consider, first, that many of the costly displays we find in nature are linked to sexual selection.32 Reproduction among technologically mature life forms, in contrast, may be predominantly or exclusively asexual.

Second, technologically advanced agents might have available new means of reliably communicating information about themselves, means that do not rely on costly display. Even today, when professional lenders assess creditworthiness they tend to rely more on documentary evidence, such as ownership certificates and bank statements, than on costly displays, such as designer suits and Rolex watches. In the future, it might be possible to employ auditing firms that verify through detailed examination of behavioral track records, testing in simulated environments, or direct inspection of source code, that a client agent possesses a claimed attribute. Signaling one’s qualities by agreeing to such auditing might be more efficient than signaling via flamboyant display. Such a professionally mediated signal would still be costly to fake—this being the essential feature that makes the signal reliable—but it could be much cheaper to transmit when truthful than it would be to communicate an equivalent signal flamboyantly.

Third, not all possible costly displays are intrinsically valuable or socially desirable. Many are simply wasteful. The Kwakiutl potlatch ceremonies, a form of status competition between rival chiefs, involved the public destruction of vast amounts of accumulated wealth.33 Record-breaking skyscrapers, megayachts, and moon rockets may be viewed as contemporary analogs. While activities like music and humor could plausibly be claimed to enhance the intrinsic quality of human life, it is doubtful that a similar claim could be sustained with regard to the costly pursuit of fashion accessories and other consumerist status symbols. Worse, costly display can be outright harmful, as in macho posturing leading to gang violence or military bravado. Even if future intelligent life forms would use costly signaling, therefore, it is an open question whether the signal would be of a valuable sort—whether it would be like the rapturous melody of a nightingale or instead like the toad’s monosyllabic croak (or the incessant barking of a rabid dog).

Post-transition formation of a singleton?

Even if the immediate outcome of the transition to machine intelligence were multipolar, the possibility would remain of a singleton developing later. Such a development would continue an apparent long-term trend toward larger scales of political integration, taking it to its natural conclusion.34 How might this occur?

A second transition

On way in which an initially multipolar outcome could converge into a singleton post-transition is if there is, after the initial transition, a second technological transition big enough and steep enough to give a decisive strategic advantage to one of the remaining powers: a power which might then seize the opportunity to establish a singleton. Such a hypothetical second transition might be occasioned by a breakthrough to a higher level of superintelligence. For instance, if the first wave of machine superintelligence is emulation-based, then a second surge might result when the emulations now doing the research succeed in developing effective self-improving artificial intelligence.35 (Alternatively, a second transition might be triggered by a breakthrough in nanotechnology or some other military or general-purpose technology as yet unenvisaged.)

The pace of development after the initial transition would be extremely rapid. Even a short gap between the leading power and its closest competitor could therefore plausibly result in a decisive strategic advantage for the leading power during a second transition. Suppose, for example, that two projects enter the first transition only a few days apart, and that the takeoff is slow enough that this gap does not give the leading project a decisive strategic advantage at any point during the takeoff. The two projects both emerge as superintelligent powers, though one of them remains a few days ahead of the other. But developments are now occurring on the research timescales characteristic of machine superintelligence—perhaps thousands or millions of times faster than research conducted on a biological human timescale. Development of the secondtransition technology might therefore be completed in days, hours, or minutes. Even though the frontrunner’s lead is a mere few days, a breakthrough could thus catapult it into a decisive strategic advantage. Note, however, that if technological diffusion (via espionage or other channels) speeds up as much as technological development, then this effect would be negated. What would remain relevant would be the steepness of the second transition, that is, the speed at which it would unfold relative to the general speed of events in the period after the first transition. (In this sense, the faster things are happening after the first transition, the less steep the second transition would tend to be.)

One might also speculate that a decisive strategic advantage would be more likely to be actually used to establish a singleton if it arises during a second (or subsequent) transition. After the first transition, decision makers would either be superintelligent or have access to advice from a superintelligence, which would clarify the implications of available strategic options. Furthermore, the situation after the first transition might be one in which a preemptive move against potential competitors would be less dangerous for the aggressor. If the decisionmaking minds after the first transition are digital, they could be copied and thereby rendered less vulnerable to a counterattack. Even if a defender had the ability to kill nine-tenths of the aggressor’s population in a retaliatory strike, this would scarcely offer much deterrence if the deceased could be immediately resurrected from redundant backups. Devastation of infrastructure (which can be rebuilt) might also be tolerable to digital minds with effectively unlimited lifespans, who might be planning to maximize their resources and influence on a cosmological timescale.

Superorganisms and scale economies

The size of coordinated human aggregates, such as firms or nations, is influenced by various parameters—technological, military, financial, and cultural—that can vary from one historical epoch to another. A machine intelligence revolution would entail profound changes in many these parameters. Perhaps these changes would facilitate the rise of a singleton. Although we cannot, without looking in detail at what these prospective changes are, exclude the opposite possibility—that the changes would facilitate fragmentation rather than unification—we can nevertheless note that the increased variance or uncertainty that we confront here may itself be a ground for giving greater credence to the potential emergence of a singleton than we would otherwise do. A machine intelligence revolution might, so to speak, stir things up—might reshuffle the deck to make possible geopolitical realignments that seemed perhaps otherwise not to have been in the cards.

A comprehensive analysis of all the factors that may influence the scale of political integration would take us far beyond the scope of this book: a review of the relevant political science and economics literature could itself easily fill an entire volume. We must confine ourselves to making brief allusion to a couple of factors, aspects of the digitization of agents that may make it easier to centralize control.

Carl Shulman has argued that in a population of emulations, selection pressures would favor the emergence of “superorganisms,” groups of emulations ready to sacrifice themselves for the good of their clan.36 Superorganisms would be spared the agency problems that beset organizations whose members pursue their own self-interest. Like the cells in our bodies, or the individual animals in a colony of eusocial insects, emulations that were wholly altruistic toward their copy-siblings would cooperate with one another even in the absence of elaborate incentive schemes.

Superorganisms would have a particularly strong advantage if nonconsensual deletion (or indefinite suspension) of individual emulations is disallowed. Firms or countries that employ emulations insisting on self-preservation would be saddled with an unending commitment to pay upkeep for obsolete or redundant workers. In contrast, organizations whose emulations willingly deleted themselves when their services were no longer required could more easily adapt to fluctuations in demand; and they could experiment freely, proliferating variations of their workers and retaining only the most productive.

If involuntary deletion is not disallowed, then the comparative advantage of eusocial emulations is reduced, though perhaps not eliminated. Employers of cooperative self-sacrificers might still reap efficiency gains from reduced agency problems throughout the organization, including being spared the trouble of having to defeat whatever resistance emulations could put up against their own deletion. In general, the productivity gains of having workers willing to sacrifice their individual lives for the common weal are a special case of the benefits an organization can derive from having members who are fanatically devoted to it. Such members would not only leap into the grave for the organization, and work long hours for little pay: they would also shun office politics and try consistently to act in what they took to be the organization’s best interest, reducing the need for supervision and bureaucratic constraints.

If the only way to achieve such dedication were by restricting membership to copy-siblings (so that all emulations in a particular superorganism were stamped out from the same template), then superorganisms would suffer some disadvantage in being able to draw only from a range of skills narrower than that of rival organizations, a disadvantage which might or might not be large enough to outweigh the advantages of avoiding internal agency problems.37 This disadvantage would be greatly alleviated if a superorganism could at least contain members with different training. Even if all its members were derived from a single ur-template, its workforce could then still contribute a diversity of skills. Starting with a polymathically talented emulation ur-template, lineages could be branched off into different training programs, one copy learning accounting, another electrical engineering, and so forth. This would produce a membership with diverse skills though not of diverse talents. (Maximum diversity might require that more than one ur-template be used.)

The essential property of a superorganism is not that it consists of copies of a single progenitor but that all the individual agents within it are fully committed to a common goal. The ability to create a superorganism can thus be viewed as requiring a partial solution to the control problem. Whereas a completely general solution to the control problem would enable somebody to create an agent with any arbitrary final goal, the partial solution needed for the creation of a superorganism requires merely the ability to fashion multiple agents with the same final goal (for some nontrivial but not necessarily arbitrary final goal).

The main consideration put forward in this subsection is thus not really limited to monoclonal emulation groups, but can be stated more generally in a way that makes clear that it applies to a wide range of multipolar machine intelligence scenarios. It is that certain types of advances in motivation selection techniques, which may become feasible when the actors are digital, may help overcome some of the inefficiencies that currently hamper large human organizations and that counterbalance economies of scale. With these limits lifted, organizations—be they firms, nations, or other economic or political entities—could increase in size. This is one factor that could facilitate the emergence of a post-transition singleton.

One area in which superorganisms (or other digital agents with partially selected motivations) might excel is coercion. A state might use motivation selection methods to ensure that its police, military, intelligence service, and civil administration are uniformly loyal. As Shulman notes,

Saved states [of some loyal emulation that has been carefully prepared and verified] could be copied billions of times to staff an ideologically uniform military, bureaucracy, and police force. After a short period of work, each copy would be replaced by a fresh copy of the same saved state, preventing ideological drift. Within a given jurisdiction, this capability could allow incredibly detailed observation and regulation: there might be one such copy for every other resident. This could be used to prohibit the development of weapons of mass destruction, to enforce regulations on brain emulation experimentation or reproduction, to enforce a liberal democratic constitution, or to create an appalling and permanent totalitarianism

The first-order effect of such a capability would seem to be to consolidate power, and possibly to concentrate it in fewer hands.

Unification by treaty

There may be large potential gains to be had from international collaboration in a post-transition multipolar world. Wars and arms races could be avoided. Astrophysical resources could be colonized and harvested at a globally optimum pace. The development of more advanced forms of machine intelligence could be coordinated to avoid a rush and to allow new designs to be thoroughly vetted. Other developments that might pose existential risks could be postponed. And uniform regulations could be enforced globally, including provisions for a guaranteed standard of living (which would require some form of population control) and for preventing exploitation and abuse of emulations and other digital and biological minds. Furthermore, agents with resource-satiable preferences (more on this in Chapter 13) would prefer a sharing agreement that would guarantee them a certain slice of the future to a winner-takes-all struggle in which they would risk getting nothing.

The presence of big potential gains from collaboration, however, does not imply that collaboration will actually be achieved. In the world today, many great boons could be obtained via better global coordination—reductions of military expenditures, wars, overfishing, trade barriers, and atmospheric pollution, among others. Yet these plump fruits are left to spoil on the branch. Why is that? What stops a fully cooperative outcome that would maximize the common good?

One obstacle is the difficulty of ensuring compliance with any treaty that might be agreed, including monitoring and enforcement costs. Two nuclear rivals might each be better off if they both relinquished their atom bombs; yet even if they could reach an in-principle agreement to do so, disarmament could nevertheless prove elusive because of their mutual fear that the other party might cheat. Allaying this fear would require setting up a verification mechanism. There may have to be inspectors to oversee the destruction of existing stockpiles, and then to monitor nuclear reactors and other facilities, and to gather technical and human intelligence, in order to ensure that the weapons program is not reconstituted. One cost is paying for these inspectors. Another cost is the risk that the inspectors will spy and make off with commercial or military secrets. Perhaps most significantly, each party might fear that the other will preserve a clandestine nuclear capability. Many a potentially beneficial deal never comes off because compliance would be too difficult to verify.

If new inspection technologies that reduced monitoring costs became available, one would expect this to result in increased cooperation. Whether monitoring costs would on net be reduced in the post-transition era, however, is not entirely clear. While there would certainly be many powerful new inspection techniques, there would also be new means of concealment. In particular, an increasing portion of the activities one might want to regulate would be taking place in cyberspace, out of reach of physical surveillance. For example, digital minds working on designing a new nanotech weapons system or a new generation of artificial intelligence may do so without leaving much of a physical footprint. Digital forensics may fail to penetrate all the layers of concealment and encryption in which a treaty-violator may cloak its illicit activities.

Reliable lie detection, if it could be developed, would be an extremely useful tool for monitoring compliance.40 An inspection protocol could include provisions for interviewing key officials, to verify that they are intent on implementing all the provisions of the treaty and that they know of no violations despite making strong efforts to find out.

A decision maker planning to cheat might defeat such a lie-detection-based verification scheme by first issuing orders to subordinates to undertake the illicit activity and to conceal the activity even from the decision maker herself, and then subjecting herself to some procedure that erases her memory of having engaged in these machinations. Suitably targeted memory-erasure operations might well be feasible in biological brains with more advanced neurotechnology. It might be even easier in machine intelligences (depending on their architecture).

States could seek to overcome this problem by committing themselves to an ongoing monitoring scheme that regularly tests key officials with a lie detector to check whether they harbor any intent to subvert or circumvent any treaty to which the state has entered or may enter in the future. Such a commitment could be viewed as a kind of meta-treaty, which would facilitate the verification of other treaties; but states might commit themselves to it unilaterally to gain the benefit of being regarded as a trustworthy negotiation partner. However, this commitment or meta-treaty would face the same problem of subversion through a delegate-and-forget ploy. Ideally, the meta-treaty would be put into effect before any party had an opportunity to make the internal arrangements necessary to subvert its implementation. Once villainy has had an unguarded moment to sow its mines of deception, trust can never set foot there again.

In some cases, the mere ability to detect treaty violations is sufficient to establish the confidence needed for a deal. In other cases, however, there is a need for some mechanism to enforce compliance or mete out punishment if a violation should occur. The need for an enforcement mechanism may arise if the threat of the wronged party withdrawing from the treaty is not enough to deter violations, for instance if the treaty-violator would gain such an advantage that he would not subsequently care how the other party responds.

If highly effective motivation selection methods are available, this enforcement problem could be solved by empowering an independent agency with sufficient police or military strength to enforce the treaty even against the opposition of one or several of its signatories. This solution requires that the enforcement agency can be trusted. But with sufficiently good motivation selection techniques, the requisite confidence might be achieved by having all the parties to the treaty jointly oversee the design of the enforcement agency.

Handing over power to an external enforcement agency raises many of the same issues that we confronted earlier in our discussions of a unipolar outcome (one in which a singleton arises prior to or during the initial machine intelligence revolution). In order to be able to enforce treaties concerning the vital security interests of rival states, the external enforcement agency would in effect need to constitute a singleton: a global superintelligent Leviathan. One difference, however, is that we are now considering a post-transition situation, in which the agents that would have to create this Leviathan would have greater competence than we humans currently do. These Leviathan-creators may themselves already be superintelligent. This would greatly improve the odds that they could solve the control problem and design an enforcement agency that would serve the interests of all the parties that have a say in its construction.

Aside from the costs of monitoring and enforcing compliance, are there any other obstacles to global coordination? Perhaps the major remaining issue is what we can refer to as bargaining costs.41 Even when there is a possible bargain that would benefit everybody involved, it sometimes does not get off the ground because the parties fail to agree on how to divide the spoils. For example, if two persons could make a deal that would net them a dollar in profit, but each party feels she deserves sixty cents and refuses to settle for less, the deal will not happen and the potential gain will be forfeited. In general, negotiations can be difficult or protracted, or remain altogether barren, because of strategic bargaining choices made by some of the parties.

In real life, human beings frequently succeed in reaching agreements despite the possibility for strategic bargaining (though often not without considerable expenditure of time and patience). It is conceivable, however, that strategic bargaining problems would have a different dynamic in the post-transition era. An AI negotiator might more consistently adhere to some particular formal conception of rationality, possibly with novel or unanticipated consequences when matched with other AI negotiators. An AI might also have available to it moves in the bargaining game that are either unavailable to humans or very much more difficult for humans to execute, including the ability to precommit to a policy or a course of action. While humans (and human-run institutions) are occasionally able to precommit—with imperfect degrees of credibility and specificity—some types of machine intelligence might be able to make arbitrary unbreakable precommitments and to allow negotiating partners to confirm that such a precommitment has been made.

The availability of powerful precommitment techniques could profoundly alter the nature of negotiations, potentially giving an immense edge to an agent that has a first-mover advantage. If a particular agent’s participation is necessary for the realization of some prospective gains from cooperation, and if that agent is able to make the first move, it would be in a position to dictate the division of the spoils by precommitting not to accept any deal that gives it less than, say, 99% of the surplus value. Other agents would then be faced with the choice of either getting nothing (by rejecting the unfair proposal) or getting 1% of the value (by caving in). If the first-moving agent’s precommitment is publicly verifiable, its negotiating partners could be sure that these are their only two options.

To avoid being exploited in this manner, agents might precommit to refuse blackmail and to decline all unfair offers. Once such a precommitment has been made (and successfully publicized), other agents would not find it in their interest to make threats or to precommit themselves to only accepting deals tilted in their own favor, because they would know that threats would fail and that unfair proposals would be rejected. But this just demonstrates again that the advantage is with the first-mover. The agent who moves first can choose whether to parlay its position of strength only to deter others from taking unfair advantage, or to make a grab for the lion’s share of future spoils.

Best situated of all, it might seem, would be the agent who starts out with a temperament or a value system that makes him impervious to extortion or indeed to any offer of a deal in which his participation is indispensable but he is not getting almost all of the gains. Some humans seem already to possess personality traits corresponding to various aspects of an uncompromising spirit.43 A highstrung disposition, however, could backfire should it turn out that there are other agents around who feel entitled to more than their fair share and are committed to not backing down. The unstoppable force would then encounter the unmovable object, resulting in a failure to reach agreement (or worse: total war). The meek and the akratic would at least get something, albeit less than their fair share.

What kind of game-theoretic equilibrium would be reached in such a posttransition bargaining game is not immediately obvious. Agents might choose more complicated strategies than the ones considered here. One hopes that an equilibrium would be reached centered on some fairness norm that would serve as a Schelling point—a salient feature in a big outcome space which, because of shared expectations, becomes a likely coordination point in an otherwise underdetermined coordination game. Such an equilibrium might be bolstered by some of our evolved dispositions and cultural programming: a common preference for fairness could, assuming we succeed in transferring our values into the post-transition era, bias expectations and strategies in ways that lead to an attractive equilibrium.

In any case, the upshot is that with the possibility of strong and flexible forms of precommitment, outcomes of negotiations might take on an unfamiliar guise. Even if the post-transition era started out multipolar, it might be that a singleton would arise almost immediately as a consequence of a negotiated treaty that resolves all important global coordination problems. Some transaction costs, perhaps including monitoring and enforcement costs, might plummet with the new technological capabilities available to advanced machine intelligences. Other costs, in particular costs related to strategic bargaining, might remain significant. But however strategic bargaining affects the nature of the agreement that is reached, there is no clear reason why it would long delay the reaching of some agreement if an agreement were ever to be reached. If no agreement is reached, then some form of fighting might take place; and either one faction might win, and form a singleton around the winning coalition, or the result might be an interminable conflict, in which case a singleton may never form and the overall outcome may fall terribly short of what could and should have been achieved if humanity and its descendants had acted in a more coordinated and cooperative fashion.

We have seen that multipolarity, even if it could be achieved in a stable form, would not guarantee an attractive outcome. The original principal–agent problem remains unsolved, and burying it under a new set of problems related to posttransition global coordination failures may only make the situation worse. Let us therefore return to the question of how we could safely keep a single superintelligent AI.

CHAPTER 12: Acquiring values

Capability control is, at best, a temporary and auxiliary measure. Unless the plan is to keep superintelligence bottled up forever, it will be necessary to master motivation selection. But just how could we get some value into an artificial agent, so as to make it pursue that value as its final goal? While the agent is unintelligent, it might lack the capability to understand or even represent any humanly meaningful value. Yet if we delay the procedure until the agent is superintelligent, it may be able to resist our attempt to meddle with its motivation system—and, as we showed in Chapter 7, it would have convergent instrumental reasons to do so. This value-loading problem is tough, but must be confronted.

The value-loading problem

It is impossible to enumerate all possible situations a superintelligence might find itself in and to specify for each what action it should take. Similarly, it is impossible to create a list of all possible worlds and assign each of them a value. In any realm significantly more complicated than a game of tic-tac-toe, there are far too many possible states (and state-histories) for exhaustive enumeration to be feasible. A motivation system, therefore, cannot be specified as a comprehensive lookup table. It must instead be expressed more abstractly, as a formula or rule that allows the agent to decide what to do in any given situation.

One formal way of specifying such a decision rule is via a utility function. A utility function (as we recall from Chapter 1) assigns value to each outcome that might obtain, or more generally to each “possible world.” Given a utility function, one can define an agent that maximizes expected utility. Such an agent selects at each time the action that has the highest expected utility. (The expected utility is calculated by weighting the utility of each possible world with the subjective probability of that world being the actual world conditional on a particular action being taken.) In reality, the possible outcomes are too numerous for the expected utility of an action to be calculated exactly. Nevertheless, the decision rule and the utility function together determine a normative ideal—an optimality notion—that an agent might be designed to approximate; and the approximation might get closer as the agent gets more intelligent.1 Creating a machine that can compute a good approximation of the expected utility of the actions available to it is an AI-complete problem.2 This chapter addresses another problem, a problem that remains even if the problem of making machines intelligent is solved.

We can use this framework of a utility-maximizing agent to consider the predicament of a future seed-AI programmer who intends to solve the control problem by endowing the AI with a final goal that corresponds to some plausible human notion of a worthwhile outcome. The programmer has some particular human value in mind that he would like the AI to promote. To be concrete, let us say that it is happiness. (Similar issues would arise if we the programmer were interested in justice, freedom, glory, human rights, democracy, ecological balance, or self-development.) In terms of the expected utility framework, the programmer is thus looking for a utility function that assigns utility to possible worlds in proportion to the amount of happiness they contain. But how could he express such a utility function in computer code? Computer languages do not contain terms such as “happiness” as primitives. If such a term is to be used, it must first be defined. It is not enough to define it in terms of other high-level human concepts—“happiness is enjoyment of the potentialities inherent in our human nature” or some such philosophical paraphrase. The definition must bottom out in terms that appear in the AI’s programming language, and ultimately in primitives such as mathematical operators and addresses pointing to the contents of individual memory registers. When one considers the problem from this perspective, one can begin to appreciate the difficulty of the programmer’s task.

Identifying and codifying our own final goals is difficult because human goal representations are complex. Because the complexity is largely transparent to us, however, we often fail to appreciate that it is there. We can compare the case to visual perception. Vision, likewise, might seem like a simple thing, because we do it effortlessly.3 We only need to open our eyes, so it seems, and a rich, meaningful, eidetic, three-dimensional view of the surrounding environment comes flooding into our minds. This intuitive understanding of vision is like a duke’s understanding of his patriarchal household: as far as he is concerned, things simply appear at their appropriate times and places, while the mechanism that produces those manifestations are hidden from view. Yet accomplishing even the simplest visual task—finding the pepper jar in the kitchen—requires a tremendous amount of computational work. From a noisy time series of twodimensional patterns of nerve firings, originating in the retina and conveyed to the brain via the optic nerve, the visual cortex must work backwards to reconstruct an interpreted three-dimensional representation of external space. A sizeable portion of our precious one square meter of cortical real estate is zoned for processing visual information, and as you are reading this book, billions of neurons are working ceaselessly to accomplish this task (like so many seamstresses, bent over their sewing machines in a sweatshop, sewing and resewing a giant quilt many times a second). In like manner, our seemingly simple values and wishes in fact contain immense complexity.4 How could our programmer transfer this complexity into a utility function?

One approach would be to try to directly code a complete representation of whatever goal we have that we want the AI to pursue; in other words, to write out an explicit utility function. This approach might work if we had extraordinarily simple goals, for example if we wanted to calculate the digits of pi—that is, if the only thing we wanted was for the AI to calculate the digits of pi and we were indifferent to any other consequence that would result from the pursuit of this goal—recall our earlier discussion of the failure mode of infrastructure profusion. This explicit coding approach might also have some promise in the use of domesticity motivation selection methods. But if one seeks to promote or protect any plausible human value, and one is building a system intended to become a superintelligent sovereign, then explicitly coding the requisite complete goal representation appears to be hopelessly out of reach.

If we cannot transfer human values into an AI by typing out full-blown representations in computer code, what else might we try? This chapter discusses several alternative paths. Some of these may look plausible at first sight—but much less so upon closer examination. Future explorations should focus on those paths that remain open.

Solving the value-loading problem is a research challenge worthy of some of the next generation’s best mathematical talent. We cannot postpone confronting this problem until the AI has developed enough reason to easily understand our intentions. As we saw in the section on convergent instrumental reasons, a generic system will resist attempts to alter its final values. If an agent is not already fundamentally friendly by the time it gains the ability to reflect on its own agency, it will not take kindly to a belated attempt at brainwashing or a plot to replace it with a different agent that better loves its neighbor.

Evolutionary selection

Evolution has produced an organism with human values at least once. This fact might encourage the belief that evolutionary methods are the way to solve the value-loading problem. There are, however, severe obstacles to achieving safety along this path. We have already pointed to these obstacles at the end of Chapter 10 when we discussed how powerful search processes can be dangerous.

Evolution can be viewed as a particular class of search algorithms that involve the alternation of two steps, one expanding a population of solution candidates by generating new candidates according to some relatively simple stochastic rule (such as random mutation or sexual recombination), the other contracting the population by pruning candidates that score poorly when tested by an evaluation function. As with many other types of powerful search, there is the risk that the process will find a solution that satisfies the formally specified search criteria but not our implicit expectations. (This would hold whether one seeks to evolve a digital mind that has the same goals and values as a typical human being, or instead a mind that is, for instance, perfectly moral or perfectly obedient.) The risk would be avoided if we could specify a formal search criterion that accurately represented all dimensions of our goals, rather than just one aspect of what we think we desire. But this is precisely the value-loading problem, and it would of course beg the question in this context to assume that problem solved.

There is a further problem:

The total amount of suffering per year in the natural world is beyond all decent contemplation. During the minute that it takes me to compose this sentence, thousands of animals are being eaten alive, others are running for their lives, whimpering with fear, others are being slowly devoured from within by rasping parasites, thousands of all kinds are dying of starvation, thirst and disease.6

Even just within our species, 150,000 persons are destroyed each day while countless more suffer an appalling array of torments and deprivations.7 Nature might be a great experimentalist, but one who would never pass muster with an ethics review board—contravening the Helsinki Declaration and every norm of moral decency, left, right, and center. It is important that we not gratuitously replicate such horrors in silico. Mind crime seems especially difficult to avoid when evolutionary methods are used to produce human-like intelligence, at least if the process is meant to look anything like actual biological evolution.

Reinforcement learning

Reinforcement learning is an area of machine learning that studies techniques whereby agents can learn to maximize some notion of cumulative reward. By constructing an environment in which desired performance is rewarded, a reinforcement-learning agent can be made to learn to solve a wide class of problems (even in the absence of detailed instruction or feedback from the programmers, aside from the reward signal). Often, the learning algorithm involves the gradual construction of some kind of evaluation function, which assigns values to states, state–action pairs, or policies. (For instance, a program can learn to play backgammon by using reinforcement learning to incrementally improve its evaluation of possible board positions.) The evaluation function, which is continuously updated in light of experience, could be regarded as incorporating a form of learning about value. However, what is being learned is not new final values but increasingly accurate estimates of the instrumental values of reaching particular states (or of taking particular actions in particular states, or of following particular policies). Insofar as a reinforcement-learning agent can be described as having a final goal, that goal remains constant: to maximize future reward. And reward consists of specially designated percepts received from the environment. Therefore, the wireheading syndrome remains a likely outcome in any reinforcement agent that develops a world model sophisticated enough to suggest this alternative way of maximizing reward.

These remarks do not imply that reinforcement-learning methods could never be used in a safe seed AI, only that they would have to be subordinated to a motivation system that is not itself organized around the principle of reward maximization. That, however, would require that a solution to the value-loading problem had been found by some other means than reinforcement learning.

Associative value accretion

Now one might wonder: if the value-loading problem is so tricky, how do we ourselves manage to acquire our values?

One possible (oversimplified) model might look something like this. We begin life with some relatively simple starting preferences (e.g. an aversion to noxious stimuli) together with a set of dispositions to acquire additional preferences in response to various possible experiences (e.g. we might be disposed to form a preference for objects and behaviors that we find to be valued and rewarded in our culture). Both the simple starting preferences and the dispositions are innate, having been shaped by natural and sexual selection over evolutionary timescales. Yet which preferences we end up with as adults depends on life events. Much of the information content in our final values is thus acquired from our experiences rather than preloaded in our genomes.

For example, many of us love another person and thus place great final value on his or her well-being. What is required to represent such a value? Many elements are involved, but consider just two: a representation of “person” and a representation of “well-being.” These concepts are not directly coded in our DNA. Rather, the DNA contains instructions for building a brain, which, when placed in a typical human environment, will over the course of several years develop a world model that includes concepts of persons and of well-being. Once formed, these concepts can be used to represent certain meaningful values. But some mechanism needs to be innately present that leads to values being formed around these concepts, rather than around other acquired concepts (like that of a flowerpot or a corkscrew).

The details of how this mechanism works are not well understood. In humans, the mechanism is probably complex and multifarious. It is easier to understand the phenomenon if we consider it in a more rudimentary form, such as filial imprinting in nidifugous birds, where the newly hatched chick acquires a desire for physical proximity to an object that presents a suitable moving stimulus within the first day after hatching. Which particular object the chick desires to be near depends on its experience; only the general disposition to imprint in this way is genetically determined. Analogously, Harry might place a final value on Sally’s well-being; but had the twain never met, he might have fallen in love with somebody else instead, and his final values would have been different. The ability of our genes to code for the construction of a goal-acquiring mechanism explains how we come to have final goals of great informational complexity, greater than could be contained in the genome itself.

We may consequently consider whether we might build the motivation system for an artificial intelligence on the same principle. That is, instead of specifying complex values directly, could we specify some mechanism that leads to the acquisition of those values when the AI interacts with a suitable environment?

Mimicking the value-accretion process that takes place in humans seems difficult. The relevant genetic mechanism in humans is the product of eons of work by evolution, work that might be hard to recapitulate. Moreover, the mechanism is presumably closely tailored to the human neurocognitive architecture and therefore not applicable in machine intelligences other than whole brain emulations. And if whole brain emulations of sufficient fidelity were available, it would seem easier to start with an adult brain that comes with full representations of some human values preloaded.

Seeking to implement a process of value accretion closely mimicking that of human biology therefore seems an unpromising line of attack on the valueloading problem. But perhaps we might design a more unabashedly artificial substitute mechanism that would lead an AI to import high-fidelity representations of relevant complex values into its goal system? For this to succeed, it may not be necessary to give the AI exactly the same evaluative dispositions as a biological human. That may not even be desirable as an aim— human nature, after all, is flawed and all too often reveals a proclivity to evil which would be intolerable in any system poised to attain a decisive strategic advantage. Better, perhaps, to aim for a motivation system that departs from the human norm in systematic ways, such as by having a more robust tendency to acquire final goals that are altruistic, compassionate, or high-minded in ways we would recognize as reflecting exceptionally good character if they were present in a human person. To count as improvements, however, such deviations from the human norm would have to be pointed in very particular directions rather than at random; and they would continue to presuppose the existence of a largely undisturbed anthropocentric frame of reference to provide humanly meaningful evaluative generalizations (so as to avoid the kind of perverse instantiation of superficially plausible goal descriptions that we examined in Chapter 8). It is an open question whether this is feasible.

One further issue with associative value accretion is that the AI might disable the accretion mechanism. As we saw in Chapter 7, goal-system integrity is a convergent instrumental value. When the AI reaches a certain stage of cognitive development it may start to regard the continued operation of the accretion mechanism as a corrupting influence.11 This is not necessarily a bad thing, but care would have to be taken to make the sealing-up of the goal system occur at the right moment, after the appropriate values have been accreted but before they have been overwritten by additional unintended accretions.

Motivational scaffolding

Another approach to the value-loading problem is what we may refer to as motivational scaffolding. It involves giving the seed AI an interim goal system, with relatively simple final goals that we can represent by means of explicit coding or some other feasible method. Once the AI has developed more sophisticated representational faculties, we replace this interim scaffold goal system with one that has different final goals. This successor goal system then governs the AI as it develops into a full-blown superintelligence.

Because the scaffold goals are not just instrumental but final goals for the AI, the AI might be expected to resist having them replaced (goal-content integrity being a convergent instrumental value). This creates a hazard. If the AI succeeds in thwarting the replacement of its scaffold goals, the method fails.

To avoid this failure mode, precautions are necessary. For example, capability control methods could be applied to limit the AI’s powers until the mature motivation system has been installed. In particular, one could try to stunt its cognitive development at a level that is safe but that allows it to represent the values that we want to include in its ultimate goals. To do this, one might try to differentially stunt certain types of intellectual abilities, such as those required for strategizing and Machiavellian scheming, while allowing (apparently) more innocuous abilities to develop to a somewhat higher level.

One could also try to use motivation selection methods to induce a more collaborative relationship between the seed AI and the programmer team. For example, one might include in the scaffold motivation system the goal of welcoming online guidance from the programmers, including allowing them to replace any of the AI’s current goals.12 Other scaffold goals might include being transparent to the programmers about its values and strategies, and developing an architecture that is easy for the programmers to understand and that facilitates the later implementation of a humanly meaningful final goal, as well as domesticity motivations (such as limiting the use of computational resources).

One could even imagine endowing the seed AI with the sole final goal of replacing itself with a different final goal, one which may have been only implicitly or indirectly specified by the programmers. Some of the issues raised by the use of such a “self-replacing” scaffold goal also arise in the context of the value learning approach, which is discussed in the next subsection. Some further issues will be discussed in Chapter 13.

The motivational scaffolding approach is not without downsides. One is that it carries the risk that the AI could become too powerful while it is still running on its interim goal system. It may then thwart the human programmers’ efforts to install the ultimate goal system (either by forceful resistance or by quiet subversion). The old final goals may then remain in charge as the seed AI develops into a full-blown superintelligence. Another downside is that installing the ultimately intended goals in a human-level AI is not necessarily that much easier than doing so in a more primitive AI. A human-level AI is more complex and might have developed an architecture that is opaque and difficult to alter. A seed AI, by contrast, is like a tabula rasa on which the programmers can inscribe whatever structures they deem helpful. This downside could be flipped into an upside if one succeeded in giving the seed AI scaffold goals that made it want to develop an architecture helpful to the programmers in their later efforts to install the ultimate final values. However, it is unclear how easy it would be to give a seed AI scaffold goals with this property, and it is also unclear how even an ideally motivated seed AI would be capable of doing a much better job than the human programming team at developing a good architecture.

Value learning

We come now to an important but subtle approach to the value-loading problem. It involves using the AI’s intelligence to learn the values we want it to pursue. To do this, we must provide a criterion for the AI that at least implicitly picks out some suitable set of values. We could then build the AI to act according to its best estimates of these implicitly defined values. It would continually refine its estimates as it learns more about the world and gradually unpacks the implications of the value-determining criterion.

In contrast to the scaffolding approach, which gives the AI an interim scaffold goal and later replaces it with a different final goal, the value learning approach retains an unchanging final goal throughout the AI’s developmental and operational phases. Learning does not change the goal. It changes only the AI’s beliefs about the goal.

The AI thus must be endowed with a criterion that it can use to determine which percepts constitute evidence in favor of some hypothesis about what the ultimate goal is, and which percepts constitute evidence against. Specifying a suitable criterion could be difficult. Part of the difficulty, however, pertains to the problem of creating artificial general intelligence in the first place, which requires a powerful learning mechanism that can discover the structure of the environment from limited sensory inputs. That problem we can set aside here. But even modulo a solution to how to create superintelligent AI, there remain the difficulties that arise specifically from the value-loading problem. With the value learning approach, these take the form of needing to define a criterion that connects perceptual bitstrings to hypotheses about values.

Before delving into the details of how value learning could be implemented, it might be helpful to illustrate the general idea with an example. Suppose we write down a description of a set of values on a piece of paper. We fold the paper and put it in a sealed envelope. We then create an agent with human-level general intelligence, and give it the following final goal: “Maximize the realization of the values described in the envelope.” What will this agent do?

The agent does not initially know what is written in the envelope. But it can form hypotheses, and it can assign those hypotheses probabilities based on their priors and any available empirical data. For instance, the agent might have encountered other examples of human-authored texts, or it might have observed some general patterns of human behavior. This would enable it to make guesses. One does not need a degree in psychology to predict that the note is more likely to describe a value such as “minimize injustice and unnecessary suffering” or “maximize returns to shareholders” than a value such as “cover all lakes with plastic shopping bags.”

When the agent makes a decision, it seeks to take actions that would be effective at realizing the values it believes are most likely to be described in the letter. Importantly, the agent would see a high instrumental value in learning more about what the letter says. The reason is that for almost any final value that might be described in the letter, that value is more likely to be realized if the agent finds out what it is, since the agent will then pursue that value more effectively. The agent would also discover the convergent instrumental reasons described in Chapter 7—goal system integrity, cognitive enhancement, resource acquisition, and so forth. Yet, assuming that the agent assigns a sufficiently high probability to the values described in the letter involving human welfare, it would not pursue these instrumental values by immediately turning the planet into computronium and thereby exterminating the human species, because doing so would risk permanently destroying its ability to realize its final value.

We can liken this kind of agent to a barge attached to several tugboats that pull in different directions. Each tugboat corresponds to a hypothesis about the agent’s final value. The engine power of each tugboat corresponds to the associated hypothesis’s probability, and thus changes as new evidence comes in, producing adjustments in the barge’s direction of motion. The resultant force should move the barge along a trajectory that facilitates learning about the (implicit) final value while avoiding the shoals of irreversible destruction; and later, when the open sea of more definite knowledge of the final value is reached, the one tugboat that still exerts significant force will pull the barge toward the realization of the discovered value along the straightest or most propitious route.

The envelope and barge metaphors illustrate the principle underlying the value learning approach, but they pass over a number of critical technical issues. They come into clearer focus once we start to develop the approach within a formal framework (see Box 10).

One outstanding issue is how to endow the AI with a goal such as “Maximize the realization of the values described in the envelope.” (In the terminology of Box 10, how to define the value criterion.) To do this, it is necessary to identify the place where the values are described. In our example, this requires making a successful reference to the letter in the envelope. Though this might seem trivial, it is not without pitfalls. To mention just one: it is critical that the reference be not simply to a particular external physical object but to an object at a particular time. Otherwise the AI may determine that the best way to attain its goal is by overwriting the original value description with one that provides an easier target (such as the value that for every integer there be a larger integer). This done, the AI could lean back and crack its knuckles—though more likely a malignant failure would ensue, for reasons we discussed in Chapter 8. So now we face the question of how to define time. We could point to a clock and say, “Time is defined by the movements of this device”—but this could fail if the AI conjectures that it can manipulate time by moving the hands on the clock, a conjecture which would indeed be correct if “time” were given the aforesaid definition. (In a realistic case, matters would be further complicated by the fact that the relevant values are not going to be conveniently described in a letter; more likely, they would have to be inferred from observations of pre-existing structures that implicitly contain the relevant information, such as human brains.)

Another issue in coding the goal “Maximize the realization of the values described in the envelope” is that even if all the correct values were described in a letter, and even if the AI’s motivation system were successfully keyed to this source, the AI might not interpret the descriptions the way we intended. This would create a risk of perverse instantiation, as discussed in Chapter 8.

To clarify, the difficulty here is not so much how to ensure that the AI can understand human intentions. A superintelligence should easily develop such understanding. Rather, the difficulty is ensuring that the AI will be motivated to pursue the described values in the way we intended. This is not guaranteed by the AI’s ability to understand our intentions: an AI could know exactly what we meant and yet be indifferent to that interpretation of our words (being motivated instead by some other interpretation of the words or being indifferent to our words altogether).

The difficulty is compounded by the desideratum that, for reasons of safety, the correct motivation should ideally be installed in the seed AI before it becomes capable of fully representing human concepts or understanding human intentions. This requires that somehow a cognitive framework be created, with a particular location in that framework designated in the AI’s motivation system as the repository of its final value. But the cognitive framework itself must be revisable, so as to allow the AI to expand its representational capacities as it learns more about the world and grows more intelligent. The AI might undergo the equivalent of scientific revolutions, in which its worldview is shaken up and it perhaps suffers ontological crises in which it discovers that its previous ways of thinking about values were based on confusions and illusions. Yet starting at a sub-human level of development and continuing throughout all its subsequent development into a galactic superintelligence, the AI’s conduct is to be guided by an essentially unchanging final value, a final value that becomes better understood by the AI in direct consequence of its general intellectual progress— and likely quite differently understood by the mature AI than it was by its original programmers, though not different in a random or hostile way but in a benignly appropriate way. How to accomplish this remains an open question.20 (See Box 11.)

In summary, it is not yet known how to use the value learning approach to install plausible human values (though see Box 12 for some examples of recent ideas). At present, the approach should be viewed as a research program rather than an available technique. If it could be made to work, it might constitute the most ideal solution to the value-loading problem. Among other benefits, it would seem to offer a natural way to prevent mind crime, since a seed AI that makes reasonable guesses about which values its programmers might have installed would anticipate that mind crime is probably negatively evaluated by those values, and thus best avoided, at least until more definitive information has been obtained.

Last, but not least, there is the question of “what to write in the envelope”— or, less metaphorically, the question of which values we should try to get the AI to learn. But this issue is common to all approaches to the AI value-loading problem. We return to it in Chapter 13.

Emulation modulation

Emulation modulationThe value-loading problem looks somewhat different for whole brain emulation than it does for artificial intelligence. Methods that presuppose a fine-grained understanding and control of algorithms and architecture are not applicable to emulations. On the other hand, the augmentation motivation selection method— inapplicable to de novo artificial intelligence—is available to be used with emulations (or enhanced biological brains).

The augmentation method could be combined with techniques to tweak the inherited goals of the system. For example, one could try to manipulate the motivational state of an emulation by administering the digital equivalent of psychoactive substances (or, in the case of biological systems, the actual chemicals). Even now it is possible to pharmacologically manipulate values and motivations to a limited extent.29 The pharmacopeia of the future may contain drugs with more specific and predictable effects. The digital medium of emulations should greatly facilitate such developments, by making controlled experimentation easier and by rendering all cerebral parts directly addressable.

Just as when biological test subjects are used, research on emulations would get entangled in ethical complications, not all of which could be brushed aside with a consent form. Such entanglements could slow progress along the emulation path (because of regulation or moral restraint), perhaps especially hindering studies on how to manipulate the motivational structure of emulations. The result could be that emulations are augmented to potentially dangerous superintelligent levels of cognitive ability before adequate work has been done to test or adjust their final goals. Another possible effect of the moral entanglements might be to give the lead to less scrupulous teams and nations. Conversely, were we to relax our moral standards for experimenting with digital human minds, we could become responsible for a substantial amount of harm and wrongdoing, which is obviously undesirable. Other things equal, these considerations favor taking some alternative path that does not require the extensive use of digital human research subjects in a strategically high-stakes situation.

The issue, however, is not clear-cut. One could argue that whole brain emulation research is less likely to involve moral violations than artificial intelligence research, on the grounds that we are more likely to recognize when an emulation mind qualifies for moral status than we are to recognize when a completely alien or synthetic mind does so. If certain kinds of AIs, or their subprocesses, have a significant moral status that we fail to recognize, the consequent moral violations could be extensive. Consider, for example, the happy abandon with which contemporary programmers create reinforcementlearning agents and subject them to aversive stimuli. Countless such agents are created daily, not only in computer science laboratories but in many applications, including some computer games containing sophisticated non-player characters. Presumably, these agents are still too primitive to have any moral status. But how confident can we really be that this is so? More importantly, how confident can we be that we will know to stop in time, before our programs become capable of experiencing morally relevant suffering?

Institution design

Some intelligent systems consist of intelligent parts that are themselves capable of agency. Firms and states exemplify this in the human world: whilst largely composed of humans they can, for some purposes, be viewed as autonomous agents in their own right. The motivations of such composite systems depend not only on the motivations of their constituent subagents but also on how those subagents are organized. For instance, a group that is organized under strong dictatorship might behave as if it had a will that was identical to the will of the subagent that occupies the dictator role, whereas a democratic group might sometimes behave more as if it had a will that was a composite or average of the wills of its various constituents. But one can also imagine governance institutions that would make an organization behave in a way that is not a simple function of the wills of its subagents. (Theoretically, at least, there could exist a totalitarian state that everybody hated, because the state had mechanisms to prevent its citizens from coordinating a revolt. Each citizen could be worse off by revolting alone than by playing their part in the state machinery.)

By designing appropriate institutions for a composite system, one could thus try to shape its effective motivation. In Chapter 9, we discussed social integration as a possible capability control method. But there we focused on the incentives faced by an agent as a consequence of its existence in a social world of near-equals. Here we are focusing on what happens inside a given agent: how its will is determined by its internal organization. We are therefore looking at a motivation selection method. Moreover, since this kind of internal institution design does not depend on large-scale social engineering or reform, it is a method that might be available to an individual project developing superintelligence even if the wider socioeconomic or international milieu is less than ideally favorable.

Institution design is perhaps most plausible in contexts where it would be combined with augmentation. If we could start with agents that are already suitably motivated or that have human-like motivations, institutional arrangements could be used as an extra safeguard to increase the chances that the system will stay on course.

For example, suppose that we start with some well-motivated human-like agents—let us say emulations. We want to boost the cognitive capacities of these agents, but we worry that the enhancements might corrupt their motivations. One way to deal with this challenge would be to set up a system in which individual emulations function as subagents. When a new enhancement is introduced, it is first applied to a small subset of the subagents. Its effects are then studied by a review panel composed of subagents who have not yet had the enhancement applied to them. Only when these peers have satisfied themselves that the enhancement is not corrupting is it rolled out to the wider subagent population. If the enhanced subagents are found to be corrupted, they are not given further enhancements and are excluded from key decision-making functions (at least until the system as a whole has advanced to a point where the corrupted subagents can be safely reintegrated).30 Although the corrupted subagents might have gained some advantage from the enhancement, the institutional structure in which they are embedded, and the fact that they constitute a small minority of all subagents, would hopefully prevent them from seizing power or spreading their corruption to the overall system. Thus, the collective intelligence and capability of the system could be gradually enhanced in a sequence of small steps, where the soundness of each step is verified by subagents only slightly less capable than the new subagents that are introduced in that step.

The amount of safety obtainable by this kind of institution design is limited by the accuracy of the tests that are used to evaluate the enhanced subagents. The problem of test construction is complicated by the fact that the test batteries could only be validated within the pre-existing range of cognitive ability yet must be applied to agents outside this range. One could mitigate this problem by taking small enhancement steps and by letting the test run for a long time. Such caution, however, would raise the cost and slow progress (which, if a race dynamic is occurring, could mean a project employing these safety measures would place itself at a disadvantage). Even with small steps and extensive testing, there would remain some risk of subtle flaws being introduced, leading to a gradual corruption of the goal system, a corruption that might become manifest only when the system reaches superintelligence. This risk could be reduced by letting the testing condition extend for many steps of cognitive enhancement, so that even agents that have been initially certified as sound and that have received several subsequent enhancements would continue to be under review by a panel of unmodified and uncorrupted original peers and would remain subject to reversion.

Ideally, the testing should be ongoing rather than confined to an initial evaluation period. Human-level subagents have the ability to strategize and might thus choose to conceal certain goals while their behavior was being monitored. To enable continuous testing, a hierarchy might be created in which subagents at a certain level of capability are tasked with monitoring subagents at a somewhat higher level of capability. At the bottom of the capability hierarchy (but at the top of the power hierarchy) would sit the relatively dumb and slow principal. This human principal would be like a demented king who reigns over an incompetent court that oversees a mediocre administration which governs a capable people. Alternatively, he might be likened to the evolutionarily ancient “reptilian brain” which ensures that the newer and much cleverer neocortex is employed in the service of goals such as feeding and copulation.

One might question whether such an inverse meritocracy could remain stable. If the bulk of the superintelligent subagents were found to be corrupt, and the humans in charge of the whole thing attempted to roll back many of the preceding enhancements, would the institutional structure be strong enough to withstand the opposition that such a move would engender? In the context of historical or contemporary human societies, the answer perhaps is that this would be implausible. (Not that the most capable people consistently rise to power, but we are not literally ruled by chimpanzees either.) The issue is less clear, however, in a context where society is implemented as software.

Imagine a digital hierarchy that has one highly trained police agent for every two proletarians. It also has one special-police agent for every two police agents. It also has one special-special-police agent for every two special-police agents— and so forth, so that each layer of overseers has at least half the numbers of the layer it oversees. Imagine, further, that this society is architected in such a way that a supervisor has big advantages over his subordinates. For example, the supervisor is able to monitor and record everything his subordinates say and do, whereas the supervisor himself is hidden behind a one-way glass, so that his subordinates can only hear the orders and instructions that the superior chooses to transmit. Imagine that supervisors have a panel of buttons that they can press at any time to cause a subordinate to be punished or rewarded, paused, deleted, or reverted to an earlier state. These capabilities would greatly boost the powers of a supervising agent, beyond what has been possible historically in even the most totalitarian of organizations.

Even this does not exhaust the possible instruments of control. Supervisors might also have at their disposal realistic virtual reality simulations that they can use to probe subordinates’ reactions to hypothetical scenarios (such as simulated offers to join a conspiracy or to slack when they think nobody is watching). Supervisors might, additionally, have the ability to eavesdrop on internal monologues in the subordinates’ minds and to directly manipulate their emotional states.

The upshot is a series of massive advantages for the supervisor.32 Each supervisor would be a subordinate to another, higher-level supervisor, who would monitor his every move and make sure he did not slacken in the execution of his managerial duties. With this kind of arrangement, it might be possible for the structure to be stable even if each supervisory level is intellectually somewhat weaker than the level it supervises.33 And the entire structure, which could include many highly superintelligent agents, may be controlled by a small number of humans with root access.

The cost of such an internal control structure would be significant but not necessarily prohibitive. For example, in the model where each level of supervision has half the numbers of the layer below, the extra computational overhead is bounded at a mere 100% of what the proletarian part of the system costs—less if the dumber boss layers require fewer computations per subagent. Oversight could be tightened by increasing the ratio, or cost lowered by decreasing it. The extra computational cost would be a major factor in a competitive market scenario (cf. Chapter 11) but looks affordable in the context of a project that is not facing strong immediate competition. There would also be a cost in terms of the development time needed to create and test these supervisory functionalities. A well-resourced project could reduce this time cost by parallelizing the development of the control structure with the development of the machine intelligence; but the extra task load could be prohibitive for smaller projects and for projects caught in a close technology race.

One other type of cost also deserves consideration: the risk of mind crimes being committed in this kind of structure.35 As described, the institution sounds like a rather horrible North Korean labor camp. Yet there are ways of at least mitigating the moral problems with running this kind of institution, even if the subagents contained in the institution are emulations with full human moral status. At a minimum, the system could rely on volunteering emulations. Each subagent could have the option at any time of withdrawing its participation.36 Terminated emulations could be stored to memory, with a commitment to restart them under much more ideal conditions once the dangerous phase of the intelligence explosion is over. Meanwhile, subagents who chose to participate could be housed in very comfortable virtual environments and allowed ample time for sleep and recreation. These measures would impose a cost, one that should be manageable for a well-resourced project under noncompetitive conditions. In a highly competitive situation, the cost may be unaffordable unless an enterprise could be assured that its competitors would incur the same cost.

In the example, we imagined the subagents as emulations. One might wonder, does the institution design approach require that the subagents be anthropomorphic? Or is it equally applicable to systems composed of artificial subagents?

One’s first thought here might be skeptical. One notes that despite our plentiful experience with human-like agents, we still cannot precisely predict the outbreak or outcomes of revolutions; social science can, at most, describe some statistical tendencies.37 Since we cannot reliably predict the stability of social structures for ordinary human beings (about which we have much data), it is tempting to infer that we have little hope of precision-engineering stable social structures for cognitively enhanced human-like agents (about which we have no data), and that we have still less hope of doing so for advanced artificial agents (which are not even similar to agents that we have data about).

Yet the matter is not so cut-and-dried. Humans and human-like beings are complex; but artificial agents could have relatively simple architectures. Artificial agents could also have simple and explicitly characterized motivations. Furthermore, digital agents in general (whether emulations or artificial intelligences) are copyable: an affordance that may revolutionize management, much like interchangeable parts revolutionized manufacturing. These differences, together with the opportunity to work with agents that are initially powerless and to create institutional structures that use the various abovementioned control measures, might combine to make it possible to achieve particular institutional outcomes—such as a system that does not revolt—more reliably than if one were working with human beings under historical conditions.

But then again, artificial agents might lack many of the attributes that help us predict the behavior of human-like agents. Artificial agents need not have any of the social emotions that bind human behavior, emotions such as fear, pride, and remorse. Nor need artificial agents develop attachments to friends and family. Nor need they exhibit the unconscious body language that makes it difficult for us humans to conceal our intentions. These deficits might destabilize institutions of artificial agents. Moreover, artificial agents might be capable of making big leaps in cognitive performance as a result of seemingly small changes in their algorithms or architecture. Ruthlessly optimizing artificial agents might be willing to take extreme gambles from which humans would shrink.38 And superintelligent agents might show a surprising ability to coordinate with little or no communication (e.g. by internally modeling each other’s hypothetical responses to various contingencies). These and other differences could make sudden institutional failure more likely, even in the teeth of what seem like Kevlar-clad methods of social control.

It is unclear, therefore, how promising the institution design approach is, and whether it has a greater chance of working with anthropomorphic than with artificial agents. It might be thought that creating an institution with appropriate checks and balances could only increase safety—or, at any rate, not reduce safety—so that from a risk-mitigation perspective it would always be best if the method were used. But even this cannot be said with certainty. The approach adds parts and complexity, and thus may also introduce new ways for things to go wrong that do not exist in the case of an agent that does not have intelligent subagents as parts. Nevertheless, institution design is worthy of further exploration.

CHAPTER 14: The strategy picture

It is now time to consider the challenge of superintelligence in a broader context. We would like to orient ourselves in the strategic landscape sufficiently to know at least which general direction we should be heading. This, it turns out, is not at all easy. Here in the penultimate chapter, we introduce some general analytical concepts that help us think about longterm science and technology policy issues. We then apply them to the issue of machine intelligence.

It can be illuminating to make a rough distinction between two different normative stances from which a proposed policy may be evaluated. The personaffecting perspective asks whether a proposed change would be in “our interest”—that is to say, whether it would (on balance, and in expectation) be in the interest of those morally considerable creatures who either already exist or will come into existence independently of whether the proposed change occurs or not. The impersonal perspective, in contrast, gives no special consideration to currently existing people, or to those who will come to exist independently of whether the proposed change occurs. Instead, it counts everybody equally, independently of their temporal location. The impersonal perspective sees great value in bringing new people into existence, provided they have lives worth living: the more happy lives created, the better.

This distinction, although it barely hints at the moral complexities associated with a machine intelligence revolution, can be useful in a first-cut analysis. Here we will first examine matters from the impersonal perspective. We will later see what changes if person-affecting considerations are given weight in our deliberations.

Science and technology strategy

Before we zoom in on issues specific to machine superintelligence, we must introduce some strategic concepts and considerations that pertain to scientific and technological development more generally.

Differential technological development

Suppose that a policymaker proposes to cut funding for a certain research field, out of concern for the risks or long-term consequences of some hypothetical technology that might eventually grow from its soil. She can then expect a howl of opposition from the research community.

Scientists and their public advocates often say that it is futile to try to control the evolution of technology by blocking research. If some technology is feasible (the argument goes) it will be developed regardless of any particular policymaker’s scruples about speculative future risks. Indeed, the more powerful the capabilities that a line of development promises to produce, the surer we can be that somebody, somewhere, will be motivated to pursue it. Funding cuts will not stop progress or forestall its concomitant dangers.

Interestingly, this futility objection is almost never raised when a policymaker proposes to increase funding to some area of research, even though the argument would seem to cut both ways. One rarely hears indignant voices protest: “Please do not increase our funding. Rather, make some cuts. Researchers in other countries will surely pick up the slack; the same work will get done anyway. Don’t squander the public’s treasure on domestic scientific research!”

What accounts for this apparent doublethink? One plausible explanation, of course, is that members of the research community have a self-serving bias which leads us to believe that research is always good and tempts us to embrace almost any argument that supports our demand for more funding. However, it is also possible that the double standard can be justified in terms of national selfinterest. Suppose that the development of a technology has two effects: giving a small benefit B to its inventors and the country that sponsors them, while imposing an aggregately larger harm H—which could be a risk externality—on everybody. Even somebody who is largely altruistic might then choose to develop the overall harmful technology. They might reason that the harm H will result no matter what they do, since if they refrain somebody else will develop the technology anyway; and given that total welfare cannot be affected, they might as well grab the benefit B for themselves and their nation. (“Unfortunately, there will soon be a device that will destroy the world. Fortunately, we got the grant to build it!”)

Whatever the explanation for the futility objection’s appeal, it fails to show that there is in general no impersonal reason for trying to steer technological development. It fails even if we concede the motivating idea that with continued scientific and technological development efforts, all relevant technologies will eventually be developed—that is, even if we concede the following:

Technological completion conjecture

If scientific and technological development efforts do not effectively cease, then all important basic capabilities that could be obtained through some possible technology will be obtained.

There are at least two reasons why the technological completion conjecture does not imply the futility objection. First, the antecedent might not hold, because it is not in fact a given that scientific and technological development efforts will not effectively cease (before the attainment of technological maturity). This reservation is especially pertinent in a context that involves existential risk. Second, even if we could be certain that all important basic capabilities that could be obtained through some possible technology will be obtained, it could still make sense to attempt to influence the direction of technological research. What matters is not only whether a technology is developed, but also when it is developed, by whom, and in what context. These circumstances of birth of a new technology, which shape its impact, can be affected by turning funding spigots on or off (and by wielding other policy instruments).

These reflections suggest a principle that would have us attend to the relative speed with which different technologies are developed:

The principle of differential technological development

Retard the development of dangerous and harmful technologies, especially ones that raise the level of existential risk; and accelerate the development of beneficial technologies, especially those that reduce the existential risks posed by nature or by other technologies.

A policy could thus be evaluated on the basis of how much of a differential advantage it gives to desired forms of technological development over undesired forms.

Preferred order of arrival

Some technologies have an ambivalent effect on existential risks, increasing some existential risks while decreasing others. Superintelligence is one such technology.

We have seen in earlier chapters that the introduction of machine superintelligence would create a substantial existential risk. But it would reduce many other existential risks. Risks from nature—such as asteroid impacts, supervolcanoes, and natural pandemics—would be virtually eliminated, since superintelligence could deploy countermeasures against most such hazards, or at least demote them to the non-existential category (for instance, via space colonization).

These existential risks from nature are comparatively small over the relevant timescales. But superintelligence would also eliminate or reduce many anthropogenic risks. In particular, it would reduce risks of accidental destruction, including risk of accidents related to new technologies. Being generally more capable than humans, a superintelligence would be less likely to make mistakes, and more likely to recognize when precautions are needed, and to implement precautions competently. A well-constructed superintelligence might sometimes take a risk, but only when doing so is wise. Furthermore, at least in scenarios where the superintelligence forms a singleton, many non-accidental anthropogenic existential risks deriving from global coordination problems would be eliminated. These include risks of wars, technology races, undesirable forms of competition and evolution, and tragedies of the commons.

Since substantial peril would be associated with human beings developing synthetic biology, molecular nanotechnology, climate engineering, instruments for biomedical enhancement and neuropsychological manipulation, tools for social control that may facilitate totalitarianism or tyranny, and other technologies as-yet unimagined, eliminating these types of risk would be a great boon. An argument could therefore be mounted that earlier arrival dates of superintelligence are preferable. However, if risks from nature and from other hazards unrelated to future technology are small, then this argument could be refined: what matters is that we get superintelligence before other dangerous technologies, such as advanced nanotechnology. Whether it happens sooner or later may not be so important (from an impersonal perspective) so long as the order of arrival is right.

The ground for preferring superintelligence to come before other potentially dangerous technologies, such as nanotechnology, is that superintelligence would reduce the existential risks from nanotechnology but not vice versa.4 Hence, if we create superintelligence first, we will face only those existential risks that are associated with superintelligence; whereas if we create nanotechnology first, we will face the risks of nanotechnology and then, additionally, the risks of superintelligence.5 Even if the existential risks from superintelligence are very large, and even if superintelligence is the riskiest of all technologies, there could thus be a case for hastening its arrival.

These “sooner-is-better” arguments, however, presuppose that the riskiness of creating superintelligence is the same regardless of when it is created. If, instead, its riskiness declines over time, it might be better to delay the machine intelligence revolution. While a later arrival would leave more time for other existential catastrophes to intercede, it could still be preferable to slow the development of superintelligence. This would be especially plausible if the existential risks associated with superintelligence are much larger than those associated with other disruptive technologies.

There are several quite strong reasons to believe that the riskiness of an intelligence explosion will decline significantly over a multidecadal timeframe. One reason is that a later date leaves more time for the development of solutions to the control problem. The control problem has only recently been recognized, and most of the current best ideas for how to approach it were discovered only within the past decade or so (and in several cases during the time that this book was being written). It is plausible that the state of the art will advance greatly over the next several decades; and if the problem turns out to be very difficult, a significant rate of progress might continue for a century or more. The longer it takes for superintelligence to arrive, the more such progress will have been made when it does. This is an important consideration in favor of later arrival dates— and a very strong consideration against extremely early arrival dates.

Another reason why superintelligence later might be safer is that this would allow more time for various beneficial background trends of human civilization to play themselves out. How much weight one attaches to this consideration will depend on how optimistic one is about these trends.

An optimist could certainly point to a number of encouraging indicators and hopeful possibilities. People might learn to get along better, leading to reductions in violence, war, and cruelty; and global coordination and the scope of political integration might increase, making it easier to escape undesirable technology races (more on this below) and to work out an arrangement whereby the hopedfor gains from an intelligence explosion would be widely shared. There appear to be long-term historical trends in these directions.

Further, an optimist could expect that the “sanity level” of humanity will rise over the course of this century—that prejudices will (on balance) recede, that insights will accumulate, and that people will become more accustomed to thinking about abstract future probabilities and global risks. With luck, we could see a general uplift of epistemic standards in both individual and collective cognition. Again, there are trends pushing in these directions. Scientific progress means that more will be known. Economic growth may give a greater portion of the world’s population adequate nutrition (particularly during the early years of life that are important for brain development) and access to quality education. Advances in information technology will make it easier to find, integrate, evaluate, and communicate data and ideas. Furthermore, by the century’s end, humanity will have made an additional hundred years’ worth of mistakes, from which something might have been learned.

Many potential developments are ambivalent in the abovementioned sense— increasing some existential risks and decreasing others. For example, advances in surveillance, data mining, lie detection, biometrics, and psychological or neurochemical means of manipulating beliefs and desires could reduce some existential risks by making it easier to coordinate internationally or to suppress terrorists and renegades at home. These same advances, however, might also increase some existential risks by amplifying undesirable social dynamics or by enabling the formation of permanently stable totalitarian regimes.

One important frontier is the enhancement of biological cognition, such as through genetic selection. When we discussed this in Chapters 2 and 3, we concluded that the most radical forms of superintelligence would be more likely to arise in the form of machine intelligence. That claim is consistent with cognitive enhancement playing an important role in the lead-up to, and creation of, machine superintelligence. Cognitive enhancement might seem obviously risk-reducing: the smarter the people working on the control problem, the more likely they are to find a solution. However, cognitive enhancement could also hasten the development of machine intelligence, thus reducing the time available to work on the problem. Cognitive enhancement would also have many other relevant consequences. These issues deserve a closer look. (Most of the following remarks about “cognitive enhancement” apply equally to nonbiological means of increasing our individual or collective epistemic effectiveness.)

Rates of change and cognitive enhancement

An increase in either the mean or the upper range of human intellectual ability would likely accelerate technological progress across the board, including progress toward various forms of machine intelligence, progress on the control problem, and progress on a wide swath of other technical and economic objectives. What would be the net effect of such acceleration?

Consider the limiting case of a “universal accelerator,” an imaginary intervention that accelerates literally everything. The action of such a universal accelerator would correspond merely to an arbitrary rescaling of the time metric, producing no qualitative change in observed outcomes.

If we are to make sense of the idea that cognitive enhancement might generally speed things up, we clearly need some other concept than that of universal acceleration. A more promising approach is to focus on how cognitive enhancement might increase the rate of change in one type of process relative to the rate of change in some other type of process. Such differential acceleration could affect a system’s dynamics. Thus, consider the following concept:

Macro-structural development accelerator—A lever that accelerates the rate at which macro-structural features of the human condition develop, while leaving unchanged the rate at which micro-level human affairs unfold.

Imagine pulling this lever in the decelerating direction. A brake pad is lowered onto the great wheel of world history; sparks fly and metal screeches. After the wheel has settled into a more leisurely pace, the result is a world in which technological innovation occurs more slowly and in which fundamental or globally significant change in political structure and culture happens less frequently and less abruptly. A greater number of generations come and go before one era gives way to another. During the course of a lifespan, a person sees little change in the basic structure of the human condition.

For most of our species’ existence, macro-structural development was slower than it is now. Fifty thousand years ago, an entire millennium might have elapsed without a single significant technological invention, without any noticeable increase in human knowledge and understanding, and without any globally meaningful political change. On a micro-level, however, the kaleidoscope of human affairs churned at a reasonable rate, with births, deaths, and other personally and locally significant events. The average person’s day might have been more action-packed in the Pleistocene than it is today.

If you came upon a magic lever that would let you change the rate of macrostructural development, what should you do? Ought you to accelerate, decelerate, or leave things as they are?

Assuming the impersonal standpoint, this question requires us to consider the effects on existential risk. Let us distinguish between two kinds of risk: “state risks” and “step risks.” A state risk is one that is associated with being in a certain state, and the total amount of state risk to which a system is exposed is a direct function of how long the system remains in that state. Risks from nature are typically state risks: the longer we remain exposed, the greater the chance that we will get struck by an asteroid, supervolcanic eruption, gamma ray burst, naturally arising pandemic, or some other slash of the cosmic scythe. Some anthropogenic risks are also state risks. At the level of an individual, the longer a soldier pokes his head up above the parapet, the greater the cumulative chance he will be shot by an enemy sniper. There are anthropogenic state risks at the existential level as well: the longer we live in an internationally anarchic system, the greater the cumulative chance of a thermonuclear Armageddon or of a great war fought with other kinds of weapons of mass destruction, laying waste to civilization.

A step risk, by contrast, is a discrete risk associated with some necessary or desirable transition. Once the transition is completed, the risk vanishes. The amount of step risk associated with a transition is usually not a simple function of how long the transition takes. One does not halve the risk of traversing a minefield by running twice as fast. Conditional on a fast takeoff, the creation of superintelligence might be a step risk: there would be a certain risk associated with the takeoff, the magnitude of which would depend on what preparations had been made; but the amount of risk might not depend much on whether the takeoff takes twenty milliseconds or twenty hours.

We can then say the following regarding a hypothetical macro-structural development accelerator:

Insofar as we are concerned with existential state risks, we should favor acceleration—provided we think we have a realistic prospect of making it through to a post-transition era in which any further existential risks are greatly reduced.

If it were known that there is some step ahead destined to cause an existential catastrophe, then we ought to reduce the rate of macro-structural development (or even put it in reverse) in order to give more generations a chance to exist before the curtain is rung down. But, in fact, it would be overly pessimistic to be so confident that humanity is doomed.

At present, the level of existential state risk appears to be relatively low. If we imagine the technological macro-conditions for humanity frozen in their current state, it seems very unlikely that an existential catastrophe would occur on a timescale of, say, a decade. So a delay of one decade—provided it occurred at our current stage of development or at some other time when state risk is low— would incur only a very minor existential state risk, whereas a postponement by one decade of subsequent technological developments might well have a significant beneficial impact on later existential step risks, for example by allowing more time for preparation.

Upshot: the main way that the speed of macro-structural development is important is by affecting how well prepared humanity is when the time comes to confront the key step risks.

So the question we must ask is how cognitive enhancement (and concomitant acceleration of macro-structural development) would affect the expected level of preparedness at the critical juncture. Should we prefer a shorter period of preparation with higher intelligence? With higher intelligence, the preparation time could be used more effectively, and the final critical step would be taken by a more intelligent humanity. Or should we prefer to operate with closer to current levels of intelligence if that gives us more time to prepare?

Which option is better depends on the nature of the challenge being prepared for. If the challenge were to solve a problem for which learning from experience is key, then the chronological length of the preparation period might be the determining factor, since time is needed for the requisite experience to accumulate. What would such a challenge look like? One hypothetical example would be a new weapons technology that we could predict would be developed at some point in the future and that would make it the case that any subsequent war would have, let us say, a one-in-ten chance of causing an existential catastrophe. If such were the nature of the challenge facing us, then we might wish the rate of macro-structural development to be slow, so that our species would have more time to get its act together before the critical step when the new weapons technology is invented. One could hope that during the grace period secured through the deceleration, our species might learn to avoid war— that international relations around the globe might come to resemble those between the countries of the European Union, which, having fought one another ferociously for centuries, now coexist in peace and relative harmony. The pacification might occur as a result of the gentle edification from various civilizing processes or through the shock therapy of sub-existential blows (e.g. small nuclear conflagrations, and the recoil and resolve they might engender to finally create the global institutions necessary for the abolishment of interstate wars). If this kind of learning or adjusting would not be much accelerated by increased intelligence, then cognitive enhancement would be undesirable, serving merely to burn the fuse faster.

A prospective intelligence explosion, however, may present a challenge of a different kind. The control problem calls for foresight, reasoning, and theoretical insight. It is less clear how increased historical experience would help. Direct experience of the intelligence explosion is not possible (until too late), and many features conspire to make the control problem unique and lacking in relevant historical precedent. For these reasons, the amount of time that will elapse before the intelligence explosion may not matter much per se. Perhaps what matters, instead, is (a) the amount of intellectual progress on the control problem achieved by the time of the detonation; and (b) the amount of skill and intelligence available at the time to implement the best available solutions (and to improvise what is missing).9 That this latter factor should respond positively to cognitive enhancement is obvious. How cognitive enhancement would affect factor (a) is a somewhat subtler matter.

Suppose, as suggested earlier, that cognitive enhancement would be a general macro-structural development accelerator. This would hasten the arrival of the intelligence explosion, thus reducing the amount of time available for preparation and for making progress on the control problem. Normally this would be a bad thing. However, if the only reason why there is less time available for intellectual progress is that intellectual progress is speeded up, then there need be no net reduction in the amount of intellectual progress that will have taken place by the time the intelligence explosion occurs.

At this point, cognitive enhancement might appear to be neutral with respect to factor (a): the same intellectual progress that would otherwise have been made prior to the intelligence explosion—including progress on the control problem— still gets made, only compressed within a shorter time interval. In actuality, however, cognitive enhancement may well prove a positive influence on (a).

One reason why cognitive enhancement might cause more progress to have been made on the control problem by the time the intelligence explosion occurs is that progress on the control problem may be especially contingent on extreme levels of intellectual performance—even more so than the kind of work necessary to create machine intelligence. The role for trial and error and accumulation of experimental results seems quite limited in relation to the control problem, whereas experimental learning will probably play a large role in the development of artificial intelligence or whole brain emulation. The extent to which time can substitute for wit may therefore vary between tasks in a way that should make cognitive enhancement promote progress on the control problem more than it would promote progress on the problem of how to create machine intelligence.

Another reason why cognitive enhancement should differentially promote progress on the control problem is that the very need for such progress is more likely to be appreciated by cognitively more capable societies and individuals. It requires foresight and reasoning to realize why the control problem is important and to make it a priority.10 It may also require uncommon sagacity to find promising ways of approaching such an unfamiliar problem.

From these reflections we might tentatively conclude that cognitive enhancement is desirable, at least insofar as the focus is on the existential risks of an intelligence explosion. Parallel lines of thinking apply to other existential risks arising from challenges that require foresight and reliable abstract reasoning (as opposed to, e.g., incremental adaptation to experienced changes in the environment or a multigenerational process of cultural maturation and institution-building).

Technology couplings

Suppose that one thinks that solving the control problem for artificial intelligence is very difficult, that solving it for whole brain emulations is much easier, and that it would therefore be preferable that machine intelligence be reached via the whole brain emulation path. We will return later to the question of whether whole brain emulation would be safer than artificial intelligence. But for now we want to make the point that even if we accept this premiss, it would not follow that we ought to promote whole brain emulation technology. One reason, discussed earlier, is that a later arrival of superintelligence may be preferable, in order to allow more time for progress on the control problem and for other favorable background trends to culminate—and thus, if one were confident that whole brain emulation would precede AI anyway, it would be counterproductive to further hasten the arrival of whole brain emulation.

But even if it were the case that it would be best for whole brain emulation to arrive as soon as possible, it still would not follow that we ought to favor progress toward whole brain emulation. For it is possible that progress toward whole brain emulation will not yield whole brain emulation. It may instead yield neuromorphic artificial intelligence—forms of AI that mimic some aspects of cortical organization but do not replicate neuronal functionality with sufficient fidelity to constitute a proper emulation. If—as there is reason to believe—such neuromorphic AI is worse than the kind of AI that would otherwise have been built, and if by promoting whole brain emulation we would make neuromorphic AI arrive first, then our pursuit of the supposed best outcome (whole brain emulation) would lead to the worst outcome (neuromorphic AI); whereas if we had pursued the second-best outcome (synthetic AI) we might actually have attained the second-best (synthetic AI).

We have just described an (hypothetical) instance of what we might term a “technology coupling.”11 This refers to a condition in which two technologies have a predictable timing relationship, such that developing one of the technologies has a robust tendency to lead to the development of the other, either as a necessary precursor or as an obvious and irresistible application or subsequent step. Technology couplings must be taken into account when we use the principle of differential technological development: it is no good accelerating the development of a desirable technology Y if the only way of getting Y is by developing an extremely undesirable precursor technology X, or if getting Y would immediately produce an extremely undesirable related technology Z. Before you marry your sweetheart, consider the prospective in-laws.

In the case of whole brain emulation, the degree of technology coupling is debatable. We noted in Chapter 2 that while whole brain emulation would require massive progress in various enabling technologies, it might not require any major new theoretical insight. In particular, it does not require that we understand how human cognition works, only that we know how to build computational models of small parts of the brain, such as different species of neuron. Nevertheless, in the course of developing the ability to emulate human brains, a wealth of neuroanatomical data would be collected, and functional models of cortical networks would surely be greatly improved. Such progress would seem to have a good chance of enabling neuromorphic AI before fullblown whole brain emulation.12 Historically, there are quite a few examples of AI techniques gleaned from neuroscience or biology. (For example: the McCulloch–Pitts neuron, perceptrons, and other artificial neurons and neural networks, inspired by neuroanatomical work; reinforcement learning, inspired by behaviorist psychology; genetic algorithms, inspired by evolution theory; subsumption architectures and perceptual hierarchies, inspired by cognitive science theories about motor planning and sensory perception; artificial immune systems, inspired by theoretical immunology; swarm intelligence, inspired by the ecology of insect colonies and other self-organizing systems; and reactive and behavior-based control in robotics, inspired by the study of animal locomotion.) Perhaps more significantly, there are plenty of important AI relevant questions that could potentially be answered through further study of the brain. (For example: How does the brain store structured representations in working memory and long-term memory? How is the binding problem solved? What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery, such as the cortical column, and if so how is it wired and how does its functionality depend on the wiring? How can such columns be linked up, and how can they learn?)

We will shortly have more to say about the relative danger of whole brain emulation, neuromorphic AI, and synthetic AI, but we can already flag another important technology coupling: that between whole brain emulation and AI. Even if a push toward whole brain emulation actually resulted in whole brain emulation (as opposed to neuromorphic AI), and even if the arrival of whole brain emulation could be safely handled, a further risk would still remain: the risk associated with a second transition, a transition from whole brain emulation to AI, which is an ultimately more powerful form of machine intelligence.

There are many other technology couplings, which could be considered in a more comprehensive analysis. For instance, a push toward whole brain emulation would boost neuroscience progress more generally.13 That might produce various effects, such as faster progress toward lie detection, neuropsychological manipulation techniques, cognitive enhancement, and assorted medical advances. Likewise, a push toward cognitive enhancement might (depending on the specific path pursued) create spillovers such as faster development of genetic selection and genetic engineering methods not only for enhancing cognition but for modifying other traits as well.

Second-guessing

We encounter another layer of strategic complexity if we take into account that there is no perfectly benevolent, rational, and unified world controller who simply implements what has been discovered to be the best option. Any abstract point about “what should be done” must be embodied in the form of a concrete message, which is entered into the arena of rhetorical and political reality. There it will be ignored, misunderstood, distorted, or appropriated for various conflicting purposes; it will bounce around like a pinball, causing actions and reactions, ushering in a cascade of consequences, the upshot of which need bear no straightforward relationship to the intentions of the original sender.

Pathways and enablers

Should we celebrate advances in computer hardware? What about advances on the path toward whole brain emulation? We will look at these two questions in turn.

Effects of hardware progress

Faster computers make it easier to create machine intelligence. One effect of accelerating progress in hardware, therefore, is to hasten the arrival of machine intelligence. As discussed earlier, this is probably a bad thing from the impersonal perspective, since it reduces the amount of time available for solving the control problem and for humanity to reach a more mature stage of civilization. The case is not a slam dunk, though. Since superintelligence would eliminate many other existential risks, there could be reason to prefer earlier development if the level of these other existential risks were very high.

Hastening or delaying the onset of the intelligence explosion is not the only channel through which the rate of hardware progress can affect existential risk. Another channel is that hardware can to some extent substitute for software; thus, better hardware reduces the minimum skill required to code a seed AI. Fast computers might also encourage the use of approaches that rely more heavily on brute-force techniques (such as genetic algorithms and other generate-evaluatediscard methods) and less on techniques that require deep understanding to use. If brute-force techniques lend themselves to more anarchic or imprecise system designs, where the control problem is harder to solve than in more precisely engineered and theoretically controlled systems, this would be another way in which faster computers would increase the existential risk.

Another consideration is that rapid hardware progress increases the likelihood of a fast takeoff. The more rapidly the state of the art advances in the semiconductor industry, the fewer the person-hours of programmers’ time spent exploiting the capabilities of computers at any given performance level. This means that an intelligence explosion is less likely to be initiated at the lowest level of hardware performance at which it is feasible. An intelligence explosion is thus more likely to be initiated when hardware has advanced significantly beyond the minimum level at which the eventually successful programming approach could first have succeeded. There is then a hardware overhang when the takeoff eventually does occur. As we saw in Chapter 4, hardware overhang is one of the main factors that reduce recalcitrance during the takeoff. Rapid hardware progress, therefore, will tend to make the transition to superintelligence faster and more explosive.

A faster takeoff via a hardware overhang can affect the risks of the transition in several ways. The most obvious is that a faster takeoff offers less opportunity to respond and make adjustments whilst the transition is in progress, which would tend to increase risk. A related consideration is that a hardware overhang would reduce the chances that a dangerously self-improving seed AI could be contained by limiting its ability to colonize sufficient hardware: the faster each processor is, the fewer processors would be needed for the AI to quickly bootstrap itself to superintelligence. Yet another effect of a hardware overhang is to level the playing field between big and small projects by reducing the importance of one of the advantages of larger projects—the ability to afford more powerful computers. This effect, too, might increase existential risk, if larger projects are more likely to solve the control problem and to be pursuing morally acceptable objectives.

There are also advantages to a faster takeoff. A faster takeoff would increase the likelihood that a singleton will form. If establishing a singleton is sufficiently important for solving the post-transition coordination problems, it might be worth accepting a greater risk during the intelligence explosion in order to mitigate the risk of catastrophic coordination failures in its aftermath.

Developments in computing can affect the outcome of a machine intelligence revolution not only by playing a direct role in the construction of machine intelligence but also by having diffuse effects on society that indirectly help shape the initial conditions of the intelligence explosion. The Internet, which required hardware to be good enough to enable personal computers to be mass produced at low cost, is now influencing human activity in many areas, including work in artificial intelligence and research on the control problem. (This book might not have been written, and you might not have found it, without the Internet.) However, hardware is already good enough for a great many applications that could facilitate human communication and deliberation, and it is not clear that the pace of progress in these areas is strongly bottlenecked by the rate of hardware improvement.

On balance, it appears that faster progress in computing hardware is undesirable from the impersonal evaluative standpoint. This tentative conclusion could be overturned, for example if the threats from other existential risks or from post-transition coordination failures turn out to be extremely large. In any case, it seems difficult to have much leverage on the rate of hardware advancement. Our efforts to improve the initial conditions for the intelligence explosion should therefore probably focus on other parameters.

Note that even when we cannot see how to influence some parameter, it can be useful to determine its “sign” (i.e. whether an increase or decrease in that parameter would be desirable) as a preliminary step in mapping the strategic lay of the land. We might later discover a new leverage point that does enable us to manipulate the parameter more easily. Or we might discover that the parameter’s sign correlates with the sign of some other more manipulable parameter, so that our initial analysis helps us decide what to do with this other parameter.

Should whole brain emulation research be promoted?

The harder it seems to solve the control problem for artificial intelligence, the more tempting it is to promote the whole brain emulation path as a less risky alternative. There are several issues, however, that must be analyzed before one can arrive at a well-considered judgment.

First, there is the issue of technology coupling, already discussed earlier. We pointed out that an effort to develop whole brain emulation could result in neuromorphic AI instead, a form of machine intelligence that may be especially unsafe.

But let us assume, for the sake of argument, that we actually achieve whole brain emulation (WBE). Would this be safer than AI? This, itself, is a complicated issue. There are at least three putative advantages of WBE: (i) that its performance characteristics would be better understood than those of AI; (ii) that it would inherit human motives; and (iii) that it would result in a slower takeoff. Let us very briefly reflect on each.

That it should be easier to understand the intellectual performance characteristics of an emulation than of an AI sounds plausible. We have abundant experience with the strengths and weaknesses of human intelligence but no experience with human-level artificial intelligence. However, to understand what a snapshot of a digitized human intellect can and cannot do is not the same as to understand how such an intellect will respond to modifications aimed at enhancing its performance. An artificial intellect, by contrast, might be carefully designed to be understandable, in both its static and dynamic dispositions. So while whole brain emulation may be more predictable in its intellectual performance than a generic AI at a comparable stage of development, it is unclear whether whole brain emulation would be dynamically more predictable than an AI engineered by competent safety-conscious programmers.

As for an emulation inheriting the motivations of its human template, this is far from guaranteed. Capturing human evaluative dispositions might require a very high-fidelity emulation. Even if some individual’s motivations were perfectly captured, it is unclear how much safety would be purchased. Humans can be untrustworthy, selfish, and cruel. While templates would hopefully be selected for exceptional virtue, it may be hard to foretell how someone will act when transplanted into radically alien circumstances, superhumanly enhanced in intelligence, and tempted with an opportunity for world domination. It is true that emulations would at least be more likely to have human-like motivations (as opposed to valuing only paperclips or discovering digits of pi). Depending on one’s views on human nature, this might or might not be reassuring.

It is not clear why whole brain emulation should result in a slower takeoff than artificial intelligence. Perhaps with whole brain emulation one should expect less hardware overhang, since whole brain emulation is less computationally efficient than artificial intelligence can be. Perhaps, also, an AI system could more easily absorb all available computing power into one giant integrated intellect, whereas whole brain emulation would forego quality superintelligence and pull ahead of humanity only in speed and size of population. If whole brain emulation does lead to a slower takeoff, this could have benefits in terms of alleviating the control problem. A slower takeoff would also make a multipolar outcome more likely. But whether a multipolar outcome is desirable is very doubtful.

There is another important complication with the general idea that getting whole brain emulation first is safer: the need to cope with a second transition. Even if the first form of human-level machine intelligence is emulation-based, it would still remain feasible to develop artificial intelligence. AI in its mature form has important advantages over WBE, making AI the ultimately more powerful technology.23 While mature AI would render WBE obsolete (except for the special purpose of preserving individual human minds), the reverse does not hold.

What this means is that if AI is developed first, there might be a single wave of the intelligence explosion. But if WBE is developed first, there may be two waves: first, the arrival of WBE; and later, the arrival of AI. The total existential risk along the WBE-first path is the sum of the risk in the first transition and the risk in the second transition (conditional on having made it through the first)

The person-affecting perspective favors speed

I fear the blog commenter “washbash” may speak for many when he or she writes:

I instinctively think go faster. Not because I think this is better for the world. Why should I care about the world when I am dead and gone? I want it to go fast, damn it! This increases the chance I have of experiencing a more technologically advanced future.

From the person-affecting standpoint, we have greater reason to rush forward with all manner of radical technologies that could pose existential risks. This is because the default outcome is that almost everyone who now exists is dead within a century.

The case for rushing is especially strong with regard to technologies that could extend our lives and thereby increase the expected fraction of the currently existing population that may still be around for the intelligence explosion. If the machine intelligence revolution goes well, the resulting superintelligence could almost certainly devise means to indefinitely prolong the lives of the then still existing humans, not only keeping them alive but restoring them to health and youthful vigor, and enhancing their capacities well beyond what we currently think of as the human range; or helping them shuffle off their mortal coils altogether by uploading their minds to a digital substrate and endowing their liberated spirits with exquisitely good-feeling virtual embodiments. With regard to technologies that do not promise to save lives, the case for rushing is weaker, though perhaps still sufficiently supported by the hope of raised standards of living.

The same line of reasoning makes the person-affecting perspective favor many risky technological innovations that promise to hasten the onset of the intelligence explosion, even when those innovations are disfavored in the impersonal perspective. Such innovations could shorten the wolf hours during which we individually must hang on to our perch if we are to live to see the daybreak of the posthuman age. From the person-affecting standpoint, faster hardware progress thus seems desirable, as does faster progress toward WBE. Any adverse effect on existential risk is probably outweighed by the personal benefit of an increased chance of the intelligence explosion happening in the lifetime of currently existing people.

Collaboration

One important parameter is the degree to which the world will manage to coordinate and collaborate in the development of machine intelligence. Collaboration would bring many benefits. Let us take a look at how this parameter might affect the outcome and what levers we might have for increasing the extent and intensity of collaboration.

The race dynamics and its perils

A race dynamic exists when one project fears being overtaken by another. This does not require the actual existence of multiple projects. A situation with only one project could exhibit a race dynamic if that project is unaware of its lack of competitors. The Allies would probably not have developed the atomic bomb as quickly as they did had they not believed (erroneously) that the Germans might be close to the same goal.

The severity of a race dynamic (that is, the extent to which competitors prioritize speed over safety) depends on several factors, such as the closeness of the race, the relative importance of capability and luck, the number of competitors, whether competing teams are pursuing different approaches, and the degree to which projects share the same aims. Competitors’ beliefs about these factors are also relevant.

In the development of machine superintelligence, it seems likely that there will be at least a mild race dynamic, and it is possible that there will be a severe race dynamic. The race dynamic has important consequences for how we should think about the strategic challenge posed by the possibility of an intelligence explosion.

The race dynamic could spur projects to move faster toward superintelligence while reducing investment in solving the control problem. Additional detrimental effects of the race dynamic are also possible, such as direct hostilities between competitors. Suppose that two nations are racing to develop the first superintelligence, and that one of them is seen to be pulling ahead. In a winner-takes-all situation, a lagging project might be tempted to launch a desperate strike against its rival rather than passively await defeat. Anticipating this possibility, the frontrunner might be tempted to strike preemptively. If the antagonists are powerful states, the clash could be bloody.34 (A “surgical strike” against the rival’s AI project might risk triggering a larger confrontation and might in any case not be feasible if the host country has taken precautions.)

Scenarios in which the rival developers are not states but smaller entities, such as corporate labs or academic teams, would probably feature much less direct destruction from conflict. Yet the overall consequences of competition may be almost as bad. This is because the main part of the expected harm from competition stems not from the smashup of battle but from the downgrade of precaution. A race dynamic would, as we saw, reduce investment in safety; and conflict, even if nonviolent, would tend to scotch opportunities for collaboration, since projects would be less likely to share ideas for solving the control problem in a climate of hostility and mistrust.

On the benefits of collaboration

Collaboration thus offers many benefits. It reduces the haste in developing machine intelligence. It allows for greater investment in safety. It avoids violent conflicts. And it facilitates the sharing of ideas about how to solve the control problem. To these benefits we can add another: collaboration would tend to produce outcomes in which the fruits of a successfully controlled intelligence explosion get distributed more equitably.

That broader collaboration should result in wider sharing of gains is not axiomatic. In principle, a small project run by an altruist could lead to an outcome where the benefits are shared evenly or equitably among all morally considerable beings. Nevertheless, there are several reasons to suppose that broader collaborations, involving a greater number of sponsors, are (in expectation) distributionally superior. One such reason is that sponsors presumably prefer an outcome in which they themselves get (at least) their fair share. A broad collaboration then means that relatively many individuals get at least their fair share, assuming the project is successful. Another reason is that a broad collaboration also seems likelier to benefit people outside the collaboration. A broader collaboration contains more members, so more outsiders would have personal ties to somebody on the inside looking out for their interests. A broader collaboration is also more likely to include at least some altruist who wants to benefit everyone. Furthermore, a broader collaboration is more likely to operate under public oversight, which might reduce the risk of the entire pie being captured by a clique of programmers or private investors.37 Note also that the larger the successful collaboration is, the lower the costs to it of extending the benefits to all outsiders. (For instance, if 90% of all people were already inside the collaboration, it would cost them no more than 10% of their holdings to bring all outsiders up to their own level.)

It is thus plausible that broader collaborations would tend to lead to a wider distribution of the gains (though some projects with few sponsors might also have distributionally excellent aims). But why is a wide distribution of gains desirable?

There are both moral and prudential reasons for favoring outcomes in which everybody gets a share of the bounty. We will not say much about the moral case, except to note that it need not rest on any egalitarian principle. The case might be made, for example, on grounds of fairness. A project that creates machine superintelligence imposes a global risk externality. Everybody on the planet is placed in jeopardy, including those who do not consent to having their own lives and those of their family imperiled in this way. Since everybody shares the risk, it would seem to be a minimal requirement of fairness that everybody also gets a share of the upside.

The fact that the total (expected) amount of good seems greater in collaboration scenarios is another important reason such scenarios are morally preferable.

The prudential case for favoring a wide distribution of gains is two-pronged. One prong is that wide distribution should promote collaboration, thereby mitigating the negative consequences of the race dynamic. There is less incentive to fight over who gets to build the first superintelligence if everybody stands to benefit equally from any project’s success. The sponsors of a particular project might also benefit from credibly signaling their commitment to distributing the spoils universally, a certifiably altruistic project being likely to attract more supporters and fewer enemies.

The other prong of the prudential case for favoring a wide distribution of gains has to do with whether agents are risk-averse or have utility functions that are sublinear in resources. The central fact here is the enormousness of the potential resource pie. Assuming the observable universe is as uninhabited as it looks, it contains more than one vacant galaxy for each human being alive. Most people would much rather have certain access to one galaxy’s worth of resources than a lottery ticket offering a one-in-a-billion chance of owning a billion galaxies.39 Given the astronomical size of humanity’s cosmic endowment, it seems that self-interest should generally favor deals that would guarantee each person a share, even if each share corresponded to a small fraction of the total. The important thing, when such an extravagant bonanza is in the offing, is to not be left out in the cold.

This argument from the enormousness of the resource pie presupposes that preferences are resource-satiable.40 That supposition does not necessarily hold. For instance, several prominent ethical theories—including especially aggregative consequentialist theories—correspond to utility functions that are risk-neutral and linear in resources. A billion galaxies could be used to create a billion times more happy lives than a single galaxy. They are thus, to a utilitarian, worth a billion times as much.41 Ordinary selfish human preference functions, however, appear to be relatively resource-satiable.

This last statement must be flanked by two important qualifications. The first is that many people care about rank. If multiple agents each wants to top the Forbes rich list, then no resource pie is large enough to give everybody full satisfaction.

The second qualification is that the post-transition technology base would enable material resources to be converted into an unprecedented range of products, including some goods that are not currently available at any price even though they are highly valued by many humans. A billionaire does not live a thousand times longer than a millionaire. In the era of digital minds, however, the billionaire could afford a thousandfold more computing power and could thus enjoy a thousandfold longer subjective lifespan. Mental capacity, likewise, could be for sale. In such circumstances, with economic capital convertible into vital goods at a constant rate even for great levels of wealth, unbounded greed would make more sense than it does in today’s world where the affluent (those among them lacking a philanthropic heart) are reduced to spending their riches on airplanes, boats, art collections, or a fourth and a fifth residence.

Does this mean that an egoist should be risk-neutral with respect to his or her post-transition resource endowment? Not quite. Physical resources may not be convertible into lifespan or mental performance at arbitrary scales. If a life must be lived sequentially, so that observer moments can remember earlier events and be affected by prior choices, then the life of a digital mind cannot be extended arbitrarily without utilizing an increasing number of sequential computational operations. But physics limits the extent to which resources can be transformed into sequential computations.42 The limits on sequential computation may also constrain some aspects of cognitive performance to scale radically sublinearly beyond a relatively modest resource endowment. Furthermore, it is not obvious that an egoist would or should be risk-neutral even with regard to highly normatively relevant outcome metrics such as number of quality-adjusted subjective life years. If offered the choice between an extra 2,000 years of life for certain and a one-in-ten chance of an extra 30,000 years of life, I think most people would select the former (even under the stipulation that each life year would be of equal quality).

In reality, the prudential case for favoring a wide distribution of gains is presumably subject-relative and situation-dependent. Yet, on the whole, people would be more likely to get (almost all of) what they want if a way is found to achieve a wide distribution—and this holds even before taking into account that a commitment to a wider distribution would tend to foster collaboration and thereby increase the chances of avoiding existential catastrophe. Favoring a broad distribution, therefore, appears to be not only morally mandated but also prudentially advisable.

There is a further set of consequences to collaboration that should be given at least some shrift: the possibility that pre-transition collaboration influences the level of post-transition collaboration. Assume humanity solves the control problem. (If the control problem is not solved, it may scarcely matter how much collaboration there is post transition.) There are two cases to consider. The first is that the intelligence explosion does not create a winner-takes-all dynamic (presumably because the takeoff is relatively slow). In this case it is plausible that if pre-transition collaboration has any systematic effect on post-transition collaboration, it has a positive effect, tending to promote subsequent collaboration. The original collaborative relationships may endure and continue beyond the transition; also, pre-transition collaboration may offer more opportunity for people to steer developments in desirable (and, presumably, more collaborative) post-transition directions.

The second case is that the nature of the intelligence explosion does encourage a winner-takes-all dynamic (presumably because the takeoff is relatively fast). In this case, if there is no extensive collaboration before the takeoff, a singleton is likely to emerge—a single project would undergo the transition alone, at some point obtaining a decisive strategic advantage combined with superintelligence. A singleton, by definition, is a highly collaborative social order.44 The absence of extensive collaboration pre-transition would thus lead to an extreme degree of collaboration post-transition. By contrast, a somewhat higher level of collaboration in the run-up to the intelligence explosion opens up a wider variety of possible outcomes. Collaborating projects could synchronize their ascent to ensure they transition in tandem without any of them getting a decisive strategic advantage. Or different sponsor groups might merge their efforts into a single project, while refusing to give that project a mandate to form a singleton. For example, one could imagine a consortium of nations forming a joint scientific project to develop machine superintelligence, yet not authorizing this project to evolve into anything like a supercharged United Nations, electing instead to maintain the factious world order that existed before.

Particularly in the case of a fast takeoff, therefore, the possibility exists that greater pre-transition collaboration would result in less post-transition collaboration. However, to the extent that collaborating entities are able to shape the outcome, they may allow the emergence or continuation of non-collaboration only if they foresee that no catastrophic consequences would follow from posttransition factiousness. Scenarios in which pre-transition collaboration leads to reduced post-transition collaboration may therefore mostly be ones in which reduced post-transition collaboration is innocuous.

In general, greater post-transition collaboration appears desirable. It would reduce the risk of dystopian dynamics in which economic competition and a rapidly expanding population lead to a Malthusian condition, or in which evolutionary selection erodes human values and selects for non-eudaemonic forms, or in which rival powers suffer other coordination failures such as wars and technology races. The last of these issues, the prospect of technology races, may be particularly problematic if the transition is to an intermediary form of machine intelligence (whole brain emulation) since it would create a new race dynamic that would harm the chances of the control problem being solved for the subsequent second transition to a more advanced form of machine intelligence (artificial intelligence).

We described earlier how collaboration can reduce conflict in the run-up to the intelligence explosion, increasing the chances that the control problem will be solved, and improve both the moral legitimacy and the prudential desirability of the resulting resource allocation. To these benefits of collaboration it may thus be possible to add one more: that broader collaboration pre-transition could help with important coordination problems in the post-transition era.

Working together

Collaboration can take different forms depending on the scale of the collaborating entities. At a small scale, individual AI teams who believe themselves to be in competition with one another could choose to pool their efforts.45 Corporations could merge or cross-invest. At a larger scale, states could join in a big international project. There are precedents to large-scale international collaboration in science and technology (such as CERN, the Human Genome Project, and the International Space Station), but an international project to develop safe superintelligence would pose a different order of challenge because of the security implications of the work. It would have to be constituted not as an open academic collaboration but as an extremely tightly controlled joint enterprise. Perhaps the scientists involved would have to be physically isolated and prevented from communicating with the rest of the world for the duration of the project, except through a single carefully vetted communication channel. The required level of security might be nearly unattainable at present, but advances in lie detection and surveillance technology could make it feasible later this century. It is also worth bearing in mind that broad collaboration does not necessarily mean that large numbers of researchers would be involved in the project; it simply means that many people would have a say in the project’s aims. In principle, a project could involve a maximally broad collaboration comprising all of humanity as sponsors (represented, let us say, by the General Assembly of the United Nations), yet employ only a single scientist to carry out the work.

There is a reason for starting collaboration as early as possible, namely to take advantage of the veil of ignorance that hides from our view any specific information about which individual project will get to superintelligence first. The closer to the finishing line we get, the less uncertainty will remain about the relative chances of competing projects; and the harder it may consequently be to make a case based on the self-interest of the frontrunner to join a collaborative project that would distribute the benefits to all of humanity. On the other hand, it also looks hard to establish a formal collaboration of worldwide scope before the prospect of superintelligence has become much more widely recognized than it currently is and before there is a clearly visible road leading to the creation of machine superintelligence. Moreover, to the extent that collaboration would promote progress along that road, it may actually be counterproductive in terms of safety, as discussed earlier.

The ideal form of collaboration for the present may therefore be one that does not initially require specific formalized agreements and that does not expedite advances in machine intelligence. One proposal that fits these criteria is that we propound an appropriate moral norm, expressing our commitment to the idea that superintelligence should be for the common good. Such a norm could be formulated as follows:

The common good principle

Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals.

Establishing from an early stage that the immense potential of superintelligence belongs to all of humanity will give more time for such a norm to become entrenched.

The common good principle does not preclude commercial incentives for individuals or firms active in related areas. For example, a firm might satisfy the call for universal sharing of the benefits of superintelligence by adopting a “windfall clause” to the effect that all profits up to some very high ceiling (say, a trillion dollars annually) would be distributed in the ordinary way to the firm’s shareholders and other legal claimants, and that only profits in excess of the threshold would be distributed to all of humanity evenly (or otherwise according to universal moral criteria). Adopting such a windfall clause should be substantially costless, any given firm being extremely unlikely ever to exceed the stratospheric profit threshold (and such low-probability scenarios ordinarily playing no role in the decisions of the firm’s managers and investors). Yet its widespread adoption would give humankind a valuable guarantee (insofar as the commitments could be trusted) that if ever some private enterprise were to hit the jackpot with the intelligence explosion, everybody would share in most of the benefits. The same idea could be applied to entities other than firms. For example, states could agree that if ever any one state’s GDP exceeds some very high fraction (say, 90%) of world GDP, the overshoot should be distributed evenly to all.

The common good principle (and particular instantiations, such as windfall clauses) could be adopted initially as a voluntary moral commitment by responsible individuals and organizations that are active in areas related to machine intelligence. Later, it could be endorsed by a wider set of entities and enacted into law and treaty. A vague formulation, such as the one given here, may serve well as a starting point; but it would ultimately need to be sharpened into a set of specific verifiable requirements.

CHAPTER 15: Crunch time

We find ourselves in a thicket of strategic complexity, surrounded by a dense mist of uncertainty. Though many considerations have been discerned, their details and interrelationships remain unclear and iffy—and there might be other factors we have not even thought of yet. What are we to do in this predicament?

Philosophy with a deadline

A colleague of mine likes to point out that a Fields Medal (the highest honor in mathematics) indicates two things about the recipient: that he was capable of accomplishing something important, and that he didn’t. Though harsh, the remark hints at a truth.

Think of a “discovery” as an act that moves the arrival of information from a later point in time to an earlier time. The discovery’s value does not equal the value of the information discovered but rather the value of having the information available earlier than it otherwise would have been. A scientist or a mathematician may show great skill by being the first to find a solution that has eluded many others; yet if the problem would soon have been solved anyway, then the work probably has not much benefited the world. There are cases in which having a solution even slightly sooner is immensely valuable, but this is most plausible when the solution is immediately put to use, either being deployed for some practical end or serving as a foundation to further theoretical work. And in the latter case, where a solution is immediately used only in the sense of serving as a building block for further theorizing, there is great value in obtaining a solution slightly sooner only if the further work it enables is itself both important and urgent.

The question, then, is not whether the result discovered by the Fields Medalist is in itself “important” (whether instrumentally or for knowledge’s own sake). Rather, the question is whether it was important that the medalist enabled the publication of the result to occur at an earlier date. The value of this temporal transport should be compared to the value that a world-class mathematical mind could have generated by working on something else. At least in some cases, the Fields Medal might indicate a life spent solving the wrong problem—for instance, a problem whose allure consisted primarily in being famously difficult to solve.

Similar barbs could be directed at other fields, such as academic philosophy. Philosophy covers some problems that are relevant to existential risk mitigation —we encountered several in this book. Yet there are also subfields within philosophy that have no apparent link to existential risk or indeed any practical concern. As with pure mathematics, some of the problems that philosophy studies might be regarded as intrinsically important, in the sense that humans have reason to care about them independently of any practical application. The fundamental nature of reality, for instance, might be worth knowing about, for its own sake. The world would arguably be less glorious if nobody studied metaphysics, cosmology, or string theory. However, the dawning prospect of an intelligence explosion shines a new light on this ancient quest for wisdom.

The outlook now suggests that philosophic progress can be maximized via an indirect path rather than by immediate philosophizing. One of the many tasks on which superintelligence (or even just moderately enhanced human intelligence) would outperform the current cast of thinkers is in answering fundamental questions in science and philosophy. This reflection suggests a strategy of deferred gratification. We could postpone work on some of the eternal questions for a little while, delegating that task to our hopefully more competent successors—in order to focus our own attention on a more pressing challenge: increasing the chance that we will actually have competent successors. This would be high-impact philosophy and high-impact mathematics.

What is to be done?

We thus want to focus on problems that are not only important but urgent in the sense that their solutions are needed prior to the intelligence explosion. We should also take heed not to work on problems that are negative-value (such that solving them is harmful). Some technical problems in the field of artificial intelligence, for instance, might be negative-value inasmuch as their solution would speed the development of machine intelligence without doing as much to expedite the development of control methods that could render the machine intelligence revolution survivable and beneficial.

It can be hard to identify problems that are both urgent and important and are such that we can confidently take them to be positive-value. The strategic uncertainty surrounding existential risk mitigation means that we must worry that even well-intentioned interventions may turn out to be not only unproductive but counterproductive. To limit the risk of doing something actively harmful or morally wrong, we should prefer to work on problems that seem robustly positive-value (i.e., whose solution would make a positive contribution across a wide range of scenarios) and to employ means that are robustly justifiable (i.e., acceptable from a wide range of moral views).

There is a further desideratum to consider in selecting which problems to prioritize. We want to work on problems that are elastic to our efforts at solving them. Highly elastic problems are those that can be solved much faster, or solved to a much greater extent, given one extra unit of effort. Encouraging more kindness in the world is an important and urgent problem—one, moreover, that seems quite robustly positive-value: yet absent a breakthrough idea for how to go about it, probably a problem of quite low elasticity. Achieving world peace, similarly, would be highly desirable; but considering the numerous efforts already targeting that problem, and the formidable obstacles arrayed against a quick solution, it seems unlikely that the contributions of a few extra individuals would make a large difference.

To reduce the risks of the machine intelligence revolution, we will propose two objectives that appear to best meet all those desiderata: strategic analysis and capacity-building. We can be relatively confident about the sign of these parameters—more strategic insight and more capacity being better. Furthermore, the parameters are elastic: a small extra investment can make a relatively large difference. Gaining insight and capacity is also urgent because early boosts to these parameters may compound, making subsequent efforts more effective. In addition to these two broad objectives, we will point to a few other potentially worthwhile aims for initiatives.

Seeking the strategic light

Against a backdrop of perplexity and uncertainty, analysis stands out as being of particularly high expected value.3 Illumination of our strategic situation would help us target subsequent interventions more effectively. Strategic analysis is especially needful when we are radically uncertain not just about some detail of some peripheral matter but about the cardinal qualities of the central things. For many key parameters, we are radically uncertain even about their sign—that is, we know not which direction of change would be desirable and which undesirable. Our ignorance might not be irremediable. The field has been little prospected, and glimmering strategic insights could still be awaiting their unearthing just a few feet beneath the surface.

What we mean by “strategic analysis” here is a search for crucial considerations: ideas or arguments with the potential to change our views not merely about the fine-structure of implementation but about the general topology of desirability.4 Even a single missed crucial consideration could vitiate our most valiant efforts or render them as actively harmful as those of a soldier who is fighting on the wrong side. The search for crucial considerations (which must explore normative as well as descriptive issues) will often require crisscrossing the boundaries between different academic disciplines and other fields of knowledge. As there is no established methodology for how to go about this kind of research, difficult original thinking is necessary.

Building good capacity

Another high-value activity, one that shares with strategic analysis the robustness property of being beneficial across a wide range of scenarios, is the development of a well-constituted support base that takes the future seriously. Such a base can immediately provide resources for research and analysis. If and when other priorities become visible, resources can be redirected accordingly. A support base is thus a general-purpose capability whose use can be guided by new insights as they emerge.

One valuable asset would be a donor network comprising individuals devoted to rational philanthropy, informed about existential risk, and discerning about the means of mitigation. It is especially desirable that the early-day funders be astute and altruistic, because they may have opportunities to shape the field’s culture before the usual venal interests take up position and entrench. The focus during these opening gambits should thus be to recruit the right kinds of people into the field. It could be worth foregoing some technical advances in the short term in order to fill the ranks with individuals who genuinely care about safety and who have a truth-seeking orientation (and who are likely to attract more of their own kind).

One important variable is the quality of the “social epistemology” of the AI field and its leading projects. Discovering crucial considerations is valuable, but only if it affects action. This cannot always be taken for granted. Imagine a project that invests millions of dollars and years of toil to develop a prototype AI, and that after surmounting many technical challenges the system is finally beginning to show real progress. There is a chance that with just a bit more work it could turn into something useful and profitable. Now a crucial consideration is discovered, indicating that a completely different approach would be a bit safer. Does the project kill itself off like a dishonored samurai, relinquishing its unsafe design and all the progress that had been made? Or does it react like a worried octopus, puffing out a cloud of motivated skepticism in the hope of eluding the attack? A project that would reliably choose the samurai option in such a dilemma would be a far preferable developer.5 Yet building processes and institutions that are willing to commit seppuku based on uncertain allegations and speculative reasoning is not easy. Another dimension of social epistemology is the management of sensitive information, in particular the ability to avoid leaking information that ought be kept secret. (Information continence may be especially challenging for academic researchers, accustomed as they are to constantly disseminating their results on every available lamppost and tree.)

Particular measures

In addition to the general objectives of strategic light and good capacity, some more specific objectives could also present cost-effective opportunities for action.

One such is progress on the technical challenges of machine intelligence safety. In pursing this objective, care should be taken to manage information hazards. Some work that would be useful for solving the control problem would also be useful for solving the competence problem. Work that burns down the AI fuse could easily be a net negative.

Another specific objective is to promote “best practices” among AI researchers. Whatever progress has been made on the control problem needs to be disseminated. Some forms of computational experimentation, particularly if involving strong recursive self-improvement, may also require the use of capability control to mitigate the risk of an accidental takeoff. While the actual implementation of safety methods is not so relevant today, it will increasingly become so as the state of the art advances. And it is not too soon to call for practitioners to express a commitment to safety, including endorsing the common good principle and promising to ramp up safety if and when the prospect of machine superintelligence begins to look more imminent. Pious words are not sufficient and will not by themselves make a dangerous technology safe: but where the mouth goeth, the mind might gradually follow.

Other opportunities may also occasionally arise to push on some pivotal parameter, for example to mitigate some other existential risk, or to promote biological cognitive enhancement and improvements of our collective wisdom, or even to shift world politics into a more harmonious register.

Will the best in human nature please stand up

Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time. We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.

For a child with an undetonated bomb in its hands, a sensible thing to do would be to put it down gently, quickly back out of the room, and contact the nearest adult. Yet what we have here is not one child but many, each with access to an independent trigger mechanism. The chances that we will all find the sense to put down the dangerous stuff seem almost negligible. Some little idiot is bound to press the ignite button just to see what happens.

Nor can we attain safety by running away, for the blast of an intelligence explosion would bring down the entire firmament. Nor is there a grown-up in sight.

In this situation, any feeling of gee-wiz exhilaration would be out of place. Consternation and fear would be closer to the mark; but the most appropriate attitude may be a bitter determination to be as competent as we can, much as if we were preparing for a difficult exam that will either realize our dreams or obliterate them.

This is not a prescription of fanaticism. The intelligence explosion might still be many decades off in the future. Moreover, the challenge we face is, in part, to hold on to our humanity: to maintain our groundedness, common sense, and good-humored decency even in the teeth of this most unnatural and inhuman problem. We need to bring all our human resourcefulness to bear on its solution.

Yet let us not lose track of what is globally significant. Through the fog of everyday trivialities, we can perceive—if but dimly—the essential task of our age. In this book, we have attempted to discern a little more feature in what is otherwise still a relatively amorphous and negatively defined vision—one that presents as our principal moral priority (at least from an impersonal and secular perspective) the reduction of existential risk and the attainment of a civilizational trajectory that leads to a compassionate and jubilant use of humanity’s cosmic endowment.


/ 016. Daughtery, Paul R. et al. "Reimagining Work in the Age of AI." 2023.


INTRODUCTION: What's Our Role in the Age of AI?

In one corner of the BMW assembly plant in Dingolfing, Germany, a worker and robot are collaborating to build a transmission. The worker prepares a gear casing, while a lightweight robot arm, sensitive to and aware of its surroundings, picks up a twelve-pound gear. The worker moves on to her next task, while the robot precisely puts the gear inside the casing and turns away to pick up another.

In another part of the plant, where LP's song "Lost on You" hun1s across the floor, a different lightweight robot arm evenly applies a thick black adhesive to the edge of small car windows. Between applications, a worker walks over to wipe off the glue nozzle, pop in new glass, and carry away the finished windows, as if robot and human are part of a well choreographed dance.

Thanks to recent advances in artificial intelligence (.AJ), we are now at the cusp of a major transformation in business. It's a new era in which the fundamental rules by which we run our organizations are being rewritten daily. AI systems are not just automating many processes, making them more efficient; they are now enabling people and machines to work collaboratively in novel ways. In doing so, they are changing the very nature of work, requiring us to manage our operations and employees in dramatically different ways.

For decades, robots have typically been large pieces of machinery, usually sectioned off from human workers, that would perform a dedicated task-unloading a stamping press, for example. That specific task was part of a rigid, fixed chain of work that would generally include humans doing other predefined tasks for instance, inspecting the stamped metal parts in order to discard defects.

Contrast that traditional assembly line with a factory where robots are much smaller and more flexible, able to work alongside humans. A factory where those robots and other types of machinery are using embedded sensors and sophisticated AI algorithms. Unlike earlier generations of industrial robotics-which were typically bulky, unintelligent, and somewhat dangerous pieces of machinery-these new types of collaborative robots are equipped with the ability to sense their environment, comprehend, act, and learn, thanks to machine-learning software and other related AI technologies. All this then enables the work processes to be self-adapting, with fixed assembly lines giving way to flexible human-machine teams that can be put together on the fly. Now, in order to fulfill customized orders and handle fluctuations in demand, employees can partner with robots to perform new tasks without having to manually overhaul any processes or manufacturing steps. Those changes are baked into the system and are performed automatically.

The advances are not just in manufacturing. AI systems are being integrated across all departments, everything from sales and marketing to customer service to product R&D.

Take, for instance, a designer at Autodesk who decides to build a drone. Instead of modifying preexisting concepts and adjusting for various constraints like weight and propulsion, she inputs these parameters into the company's AI-enabled software. The software's genetic algorithm produces a vast and dizzying array of new designs that no one has ever seen. Some are more bizarre than others, but all fit the initial constraints. The designer chooses one that will distinguish her drone from the rest and further tweaks the design to fit her aesthetic and engineering goals.

From the Mechanistic to the Organic

The potential power of AI to transform businesses is unprecedented, and yet there is an urgent and growing challenge. Companies are now reaching a crossroad in their use of Al, which we define as systems that extend human capability by sensing, comprehending, acting, and learning. As businesses deploy such systems-spanning from machine learning to computer vision to deep learning-some firms will continue to see modest productivity gains over the short run, but those results will eventually stall out. Other companies will be able to attain breakthrough improvements in performance, often by developing game-changing innovations. What accounts for the difference?

It has to do with understanding the true nature of Al's impact. In the past, executives focused on using machines to automate specific workflow processes. Traditionally, these processes were linear, stepwise, sequential, standardized, repeatable, and measurable, and over the years they've been optimized through various time-and-motion analyses (think of those manufacturing assembly lines). But performance gains from that approach have recently been leveling off, as companies wring the last bits of efficiencies from mechanistic automation.

Now, to continue exploiting the full potential of AI technologies, many leading companies have begun to embrace a new view of business processes as more fluid and adaptive. In essence, they are moving beyond rigid assembly lines toward the idea of organic teams that partner humans with advanced AI systems. This collaboration between workers and smart machines is leading to the reinvention of many traditional processes. As BMW and Mercedes-Benz have experienced, rigid assembly lines are giving way to flexible teams of employees working closely alongside robots. Moreover, these novel types of teams can continuously adapt on the fly to new data and market conditions. They are enabling companies to actually reimagine various work processes.

The Third Wave

The key to understanding Al's current and future impact is its transformation of business processes.

A widespread misconception is that AI systems, including advanced robotics and digital bots, will gradually replace humans in one industry after another. Self-driving vehicles, for example, will one day replace taxi, delivery, and truck drivers. That may be true for certain jobs, but what we've found in our research is that, although AI can be deployed to automate certain functions, the technology's greater power is in complementing and augmenting human capabilities. In claims processing, for example, AI isn't replacing the need for humans; instead, it's doing the tedious grunt work, collecting data and doing a preliminary analysis, freeing human claims processors to focus on resolving complex cases. In essence, machines are doing what they do best: performing repetitive tasks, analyzing huge data sets, and handling routine cases. And humans are doing what they do best: resolving ambiguous information, exercising judgment in difficult cases, and dealing with dissatisfied customers. This kind of emerging symbiosis between man and machine is unlocking what we have called the third wave of business transformation.

To see how we got here, it helps to understand some historical context. The first wave of business transformation involved standardized processes. This era was ushered in by Henry Ford, who deconstructed the manufacture of automobiles so that they could be made on an assembly line. Each of those steps in that overall process could then be measured, optimized, and standardized to achieve considerable gains in efficiencies.

The second wave consisted of automated processes. This era emerged in the 1 970s and peaked in the 1990s with the business process reengineering ( BPR) movement, thanks to advances in information technology (IT) : desktop computers, large databases, and software that automated various back-office tasks. Among other companies, retailers like Walmart rode this wave to become worldwide powerhouses. Other firms were able to reinvent themselves. UPS, for example, transformed itself from a package-delivery service to a global logistics company.

Now, the third wave involves adaptive processes. This third era, which builds on the previous two, will be more dramatic than the earlier revolutions enabled by assembly lines and digital computers, and will usher in entirely new, innovative ways of doing business. As we will see throughout this book, the leading firms in many industries are now reimagining their processes to be more flexible, faster, and adaptable to the behaviors, preferences, and needs of their workers at a given moment. This adaptive capability is being driven by real-time data rather than by an a priori sequence of steps. The paradox is that although these processes are not standardized or routine, they can repeatedly deliver better outcomes. In fact, leading organizations have been able to bring to market individualized products and services (as opposed to the mass-produced goods of the past), as well as deliver profitable outcomes.

Think Like Waze

To help illustrate the profound difference between the old process thinking and the new, consider the history of CPS navigation. The first online maps were largely just a digital version of their paper counterparts. But soon, CPS navigation devices changed how we used maps, giving us directions after entering a destination. Even that, though, was still a fairly static process. Now, mobile map apps like Waze are taking advantage of real-time user data-about drivers' locations and speeds as well as crowdsourced information about traffic jams, accidents, and other obstructions to create the perfect map in real time. All that data enables the system to update directions en route so that, if necessary, it can reroute drivers midcourse to minimize any possible delays. Whereas the old approach with CPS simply digitized a static paper-map route, Waze has combined AI algorithms and real-time data to create living, dynamic, optimized maps that can get people to their destinations as quickly as possible. Business approaches that use AI merely for automating existing static processes are like the early CPS navigation devices, whereas the current era of symbiotic collaborations between humans and machines are like Waze in that those traditional processes are being completely reimagined.

Filling the "Missing Middle"

Unfortunately, popular culture has long promoted a man-versus machine view-think of movies such as 2001 : A Space Odyssey and the Terminator series. The idea of intelligent machines as a potential threat to mankind has a long history and has resulted in many executives adopting a somewhat similar perspective, thinking exclusively of machines as threatening to replace humans. But that view is not only woefully misguided; it's also perniciously shortsighted.

The simple truth is that machines are not taking over the world, nor are they obviating the need for humans in the workplace. In this current era of business process transformation, AI systems are not wholesale replacing us; rather, they are amplifying our skills and collaborating with us to achieve productivity gains that have previously not been possible.

As you shall see in this book, the third wave has created a huge, dynamic, and diverse space in which humans and machines collaborate to attain orders-of-magnitude increases in business performance. We call this the "missing middle"-"missing" because almost no one talks about it, and only a small fraction of companies are working to fill this crucial gap (see figure I-1).

In the missing middle, humans work with smart machines to exploit what each party does best. Humans, for example, are needed to develop, train, and manage various AI applications. In doing so, they are enabling those systems to function as true collaborative partners. For their part, machines in the missing middle are helping people to punch above their weight, providing them with superhuman capabilities, such as the ability to process and analyze copious amounts of data from myriad sources in real time. Machines are augmenting human capabilities.

In the missing middle, humans and machines aren't adversaries, fighting for each other's jobs. Instead, they are symbiotic partners, each pushing the other to higher levels of performance. Moreover, in the missing middle, companies can reimagine their business processes to take advantage of collaborative teams of humans working alongside machines. It's not just digital companies that are exploiting the missing middle. Rio Tinto, the global mining conglomerate, is a case in point. The company is using AI to manage its vast fleet of machinery-autonomous drills, excavators, earth movers, and so on-from a central control facility. This has freed human operators from working in dangerous mining conditions and also enabled Rio Tinto's team of data analysts to analyze information from sensors on the remote equipment to uncover valuable insights for helping the firm manage its fleet more efficiently and safely.

Separating Winners from Losers-- and What's in This Book

As we mentioned earlier, i n the current era of adaptive processes, the fundamental rules by which organizations are run are being rewritten daily. As leaders and managers in all kinds of enterprises begin to reimagine their business processes and rethink the basic relationship between employees and machines, they must understand those rules and execute on them. That's why we wrote this book: to give people who are thinking about their organization, their team, or their career the knowledge they need that will separate winners from losers in the new age of AI.

In part one, we show and explain the current state of AI in companies. We begin on the shop floor, and in subsequent chapters, illustrate how organizations are currently deploying AI in different functions-back office, R&D, marketing, and sales. A key lesson here is that companies can't expect to benefit from human-machine collaborations without first laying the proper groundwork. Again, those companies that are using machines merely to replace humans will eventually stall, whereas those that think of innovative ways for machines to augment humans will become the leaders of their industries.

Chapter 1 describes how human-machine teams are transforming the factory floor, not just at BMW and Mercedes-Benz but also at many other large manufacturers. General Electric, for example, h as been building " digital twins" of its deployed products, like the turbine blades on a jet engine. The company bases these virtual models on the current conditions of real machinery, enabling it to improve operations as well as predict breakdowns before they occur, thus fundamentally changing how it maintains commercial equipment.

The focus is back-office operations in chapter 2 . Here, AI technology can help filter and analyze streams of information from a variety of sources and enable the automation of tedious, repetitive tasks as well as the augmentation of human skills and expertise. At a Canadian insurance and financial service provider, for example, an AI system processes unstructured financial data from news articles, reports, and emails to make specific recommendations, and the system can be trained to extract insights tailored to an individual analyst's preferences.

Chapter 3 describes how companies are using AI in research and development. In each major step of the R&D process-observation, hypothesis generation, experiment design, and results analysisAI technologies can lead to increased efficiencies and markedly improved outcomes. At GNS Healthcare, sophisticated machine learning software finds patterns in patient medical health records and then automatically churns out hypotheses straight from that data. The system took just three months to recreate the results of a two-year study investigating adverse reactions between drug combinations in seniors on Medicare.

We turn to the marketing and sales function and describe how the impact of AI has been just as great-if not more so-as in other functions in chapter 4. Machine-learning technologies like Amazon's Alexa, Apple's Siri, and Microsoft's Cortana are increasingly becoming the digital embodiment of those companies' well-known brands. In other words, AI has become the brand.

In part two, we explore the "missing middle" and provide an executive guide for overhauling and "reimagining" the traditional notions of work. To exploit the full power of AI, companies must fill that gap by considering new employee roles, by establishing novel types of working relationships between humans and machines, by changing traditional concepts of management, and by overhauling their very concept of work itself.

Chapter 5 describes how machine learning, when integrated into processes, will lead to a variety of brand-new jobs. Specifically, employees will be needed to design and train algorithms, to explain the algorithms used, and to do so in a way that sustains the algorithms within a process. One such new role is that of machine relations managers, who will be similar to human resources managers except they will oversee AI systems instead of human workers. They will be responsible for regularly conducting performance reviews of a company's AI systems. They will promote those systems that perform well, replicating variants and deploying them to other parts of the organization. Those systems with poor performance will be demoted and possibly decommissioned.

In chapter 6, we describe how people are achieving huge performance boosts by working with AI technologies that dramatically improve their human capabilities; they amplify, interact, and embody new human potential. (In a sense, this chapter is the flip side of ·chapter 5, in which we discussed how humans are helping machines to extend and amplify their capabilities.) These new types of human-machine relationships are helping people to " punch above their weight" by offloading tedious tasks and by enabling them to perform their work faster and more effectively through the expert guidance, advice, and support from AI systems.

Chapter 7 takes a hard look at managerial challenges introduced by AI that require different, new responses from management and leadership. A huge question here is, what steps must management take to facilitate reimagining processes? Specifically, management must support fi ve crucial activities, including an emphasis on trial-and-error experimentation, building a data supply chain for AI, and others.

Finally, we explore the future of work itself in chapter 8 . Specifically, as human-machine collaborations become increasingly prevalent, companies will need to hire for and develop eight new " fusion skills": intelligent interrogation (knowing how best to ask an AI agent questions, across levels of abstraction, to get the insights you need), bot-based empowerment (collaborating with intelligent agents to punch above your weight at work), reciprocal apprenticing (teaching AI agents new skill s while also undergoing on-the-j ob training to work well within AI-enhanced processes), holistic melding (developing mental models of AI agents that improve collaborative outcomes), rehumanizing time (reimagining business process es to amplify the time available for distinctly human tasks and learning), responsible normalizing (shaping the purpose and perception of human machine collaborations as it relates to individuals, businesses, and society), judgment integration (choosing a course of action amid machine uncertainty), and relentless reimagining (thinking of novel ways to overhaul work, processes , and business models to obtain exponential increases in improvement).

Five Crucial Principles

In our research, we have found that the leading companies in various industries-9 percent of our surveyed sample of more than fifteen hundred practitioners-are already riding the third wave. They have maximized automation and are now developing the next generation of processes and skills to capitalize on h u man-machine collaborations. They are thinking as Waze does, re-imagining processes as living and adaptive by using human and crowd inputs and real-time data. They are moving beyond the traditional thinking of simply digitalizing the old static maps.

Mindset: assuming a radically different approach toward business by reimagining work around the missing middle, wherein people improve AI and, in turn, smart machines give humans superpowers. Previously, the focus was on using machines to automate specific steps in a given work flow. Now, the potential collaboration between humans and machines is leading to the reinvention of many traditional processes. Rigid assembly lines are giving w ay to flexible teams of augmented humans and smart machines . Moreover, these teams are continuously adapting on the fly to new data and to different human contributions. They are truly organic, with the associated work processes a k i n to living and breathing organisms. We foresee that A I technologies will be critical in helping companies bring work closer to the markets they serve, improving responsiveness to consumer demand. To achieve that, however, executives must embrace a distinctive, action-oriented mindset to reimagine their operations, as we will discuss throughout the book. That said, executives must also understand that they need to lay a foundation first instead of rushing to fill the missing middle. Specifically, they initially should focus on developing the full potential of their employees by applying automation to routine work; then they can proceed to concentrate on human-machine collaborations.

Experimentation: actively observing for spots in processes to test AI and to learn and scale a reimagined process from the perspective of the missing middle. The age of standard business processes is coming to a n end, and companies will no longer be able to rely on a strategy of replicating best-in -class practices from leading firms. And this i s why experimentation is crucial. Executives must continually conduct tests to derive business processes that will work best for their unique set of conditions. A large part of that effort will require trial and error to determine what work should be done by humans, and what work would best be done by a collaboration between humans and machine (the missing middle).

Leadership: making a commitment to the responsible use of A I from the start. Executives must always consider the ethical, moral, and legal implications of the AI technologies they deploy, and the systems must generate explainable results, promote algorithmic accountability, and eliminate biases. Firms also need to pay close attention to ensure that the employees working with AI systems don't lose a sense of agency and that those individuals develop a heightened sense of empowerment in decision making. In addition, companies must provide the employee training and retraining required so that people will be prepared and ready to assume any new roles in the missing middle. In fact, investing in people must be a core part of any company's AI strategy

Data: building a data "supply chain " to fuel intelligent systems. Al requires extensive amounts of data, both in volume and variety. This includes "exhaust data"-data created as a byproduct of another process (for example, cookies from customer web browsing). Accumulating and preparing such information for use is one of the biggest challenges for organizations that deploy AI systems. Moreover, an organization's data should be able to flow freely, unencumbered by departmental silos. A company can then take full advantage of that information by applying it and other data to support, sustain, and improve AI and human performance in the missing middle.

Skills: actively developing the eight ''fusion skills " necessary for reimagining processes in the missing middle. The growing power of AI is fundamentally transforming the human-machine relationship. In the second wave, machines were generally being used to replace humans-think of how automation has decimated the ranks of factory workers, administrative assistants, bookkeepers, bank tellers, travel agents, and so on. But humans are needed now more than ever in the third wave. Humans are taking center stage i n this current era of business process improvement. Specifically, the era of adaptive processes requires humans in the loop, not only to design, develop, and train AI systems, but also to collaborate with them to fill the missing middle and achieve step-level increases in performance.

The AI revolution is not coming; it is already here, and it is about reimagining your processes, across all functions of the company, to get the nost benefit from this technology's power to augment human capability. This book is your road map for understanding and navigating the new landscape. Let's get started.

PART ONE : Imagining a Future of Humans + Machines... Today

1. The Self-Aware Factory Floor: AI in Production, Supply Chain, and Distribution

For centuries, factories have been the paragon of automation. And the people who work in factories, as a result, have often been measured by the standards of machines. So it's no surprise that the relationship between people and machines in industry has been fraught, with human workers feeling as if they've been dealt a losing hand. There's ample reason for that feeling. Since 2000, the United States has lost five million manufacturing jobs, roughly half of that through efficiency gains and automation.

But things aren't as clear cut as they might first seem. As we discussed in the previous chapter, the second wave of business transformation was all about automating existing processes, and it was during this era that many humans were replaced by machines. In contrast, the third wave relies on adaptive processes that are reimagined from scratch, and the goal here is humans + machines. I n this current era, thanks to AI, factories are acquiring a little more humanity: jobs on manufacturing lines, for instance, have changed in nature and are increasing in number. And it's not just manufacturing. A I is boosting the value of engineers and managers, too. The emergence of AI is also creating brand-new roles and new opportunities for people up and down the industrial value chain.

In this era of reimagining processes with AI, the great irony is that some of the most-automated environments-the factory and other industrial settings-are experiencing a renaissance of human labor. From the assembly-line worker and maintenance specialist to robot engineer and operations manager, A I is rejiggering the concept of what it means to work in an industrial setting. In many cases, A I is freeing up time, creativity, and human capital, essentially letting people work more like humans and less like robots. One implication of the fact that people can work differently and better with the help of A I is that companies are gaining efficiencies and saving money. But perhaps more importantly in the long term is that companies are also starting to rethink their business processes. And as they do, they uncover the need for new kinds of jobs for people, and wholly new ways of doing business, which is our focus in part two of this book.

But let's not get ahead of ourselves. This is a complex journey. ( For some historical perspective, see the sidebar "A Brief History of AI" at the end of this chapter.) Before we rewrite business processes, job descriptions, and business models, we need to answer these questions: what tasks do humans do best, and what do machines do best? There are jobs and tasks that ,vill continue to shift to robots based on their comparative advantages in handling repetition and data processing power. But as we'll see, the transfer of jobs is not simply one way. In this chapter, we survey a number of companies that have already answered the human-machine question for manufacturing, maintenance, warehouses, and agriculture. These early movers have put people and AI-enhanced machines into play, in the roles that they're best suited to, and they are reaping the benefits.

The Arm That Learns

The third shift in a Tokyo factory is showtime for an emerging class of robotic arms that can learn new tricks overnight. Coupled with a camera and machine-learning software, these hinged and swiveling appendages can, on their own, figure out the most efficient ways to pick up parts and pieces and set them down somewhere else. No explicit programming is necessary.

Robotic arms are used in factories to apply hot glue to widgets, to install windshields, and to smooth jagged metal edges, among other tasks. But, traditionally, engineers have preprogrammed them. Then, when robots' jobs change, engineers must reprogram them. In contrast, the new robotic arms, developed by Fanuc in partnership with software-maker Preferred Networks (both based in Japan), adapt on their own. They do it with an AI technique called deep reinforcement learning, in which the robot is given a picture of the successful outcome and then uses trial and error to figure out its own solution.

According to Shohei Hido, chief research officer at Preferred Networks, the arms take eight hours to become at least 90 percent accurate for this kind of task. This is virtually the same time and accuracy of an expert programming it, but because the arm is now autodidactic, the human expert is now freed to do other more complex tasks, especially · those that require human judgment. What's more, once one robot learns a task, it can share its knowledge with other robots in the network. This means that eight arms working together for one hour can learn as much as one working on a problem for eight hours. Hido, who calls this process " distributed learning," says, "You can imagine hundreds of factory robots sharing information."

Now, imagine people working alongside these robots. Mechanical arms are great for highly repetitive tasks and heavy lifting, but in any factory, there will always be a subset of tasks that are just too complex to hand off to a robot-tasks like positioning numerous small wires or handling awkward or dynamic objects. A human is still needed in the loop.

So how do robot arms and humans ·work together? Historically, not so well. Robots, with their fast, decisive movements have been helpful and efficient, but also dangerous to people. They've often been cordoned off behind protective barriers. But that standard segregation is beginning to change. So called collaborative robots from companies like Rethink Robotics, founded by robotics and A I pioneer Rodney Brooks, come equipped with sensors that allow them to recognize a range of objects and avoid knocking people around. When robots aren't so clumsy, they can work well with people. Factories that use Rethink Robotics products often divide the, vork between the robot and the human worker, working side by side, performing tasks best suited to their abilities. ( For further examples of embodied AI, see the sidebar "AI in the Factory." )

Kindler, Gentler Robots

During the second AI "winter," Rodney Brooks challenged one of the fundamental ideas that had driven previous Al research-namely, the reliance on predetermined symbols and relationships between symbols to help computers make sense of the world (see the sidebar "Two AI Winters" ). He claimed a much more robust approach: instead of cataloging the world in advance and representing it with symbols, why not survey it with sensors instead? "The world is its own best model," he wrote in a famous 1990 paper called "Elephants Don't Play Chess." ( Brooks would later found i Robot, maker of the robotic vacuum Roomba, as well as Rethink Robotics. To date, iRobot has deployed the largest fleet of autonomous robots in the world: between 2002 and 2013, more than 10 million were sold. )

Now, Brooks's AI philosophy is alive and well in both research and industry. Rethink Robotics, in particular, demonstrates the power of an arm e quipped with embedded sensors and algorithms for motion control that allow it to " feel" its way and adjust as it goes. The arm features elastic actuators and back-drivable joints, which means it can flex on contact to absorb energy. Consequently, even if it does knock into something (or someone), it wouldn't have nearly the force of a traditional robotic arm.

What's possible when robot arms can learn on their own, as with Fanuc's products? Or when an arm operates in a kinder, gentler way, as with Rethink's products?

On the assembly line, workers can collaborate with a self-aware robot arm. Say a worker is putting together a car and needs to put an interior panel on one of its doors. The robot can lift the panel and position it into place, while the worker performs fine adjustments and fastening without fear that a clunky machine will clock him in the head. AI helps both robots and people play to their strengths, and in the process, the assembly line changes shape.

One way that assembly lines can be reconfigured is through AI itself. Engineers at the Fraunhofer Institute of Material Flow and Logistics (IML) have been testing embedded sensors to create self adapting assembly lines in car plants. Essentially, the l i ne itself can modify the steps in its process to fit the demands of various features and add-ons for highly customizable cars. Thus, instead of engineers designing an assembly line to make one kind of car at a time, these lines can adapt as needed. What's more, says Andreas Nettstrater, who coordinates strategic initiatives at IML, " If one station has a failure or is broken down, the others could also do what should have been done in this assembly station."

This means that assembly-line workers are doing tasks that are less robotic (saving those tasks for the robot) and more nuanced, while process engineers don't need to reconfigure the line every time there's a change in demand or breakdown of a machine. They can spend their time working on more creative tasks to eke out further efficiencies, for instance.

Follow the Data

What starts with smart arms can extend to an entire factory line and beyond : AI-enabled processes throughout manufacturing and industrial environments are freeing up hum a n potential in a variety of contexts. Maintenance work, for instance, has been forever upended by AI. Sophisticated AI systems predict machine breakdowns before they occur, which means that maintenance workers can spend less time running routine checks and diagnostics and more time fixing a company's assets. ( For other applications, see the sidebars "AI for Faster Machine Onboarding" and "AI in the Field-Unmanned Vehicles.")

AI for Warehouse and Logistics

These robots are often sophisticated enough to see where they're going and understand what they're doing. But they have their limitations. Say a case of Cheerios is damaged, making it bulkier on one side. Most robots can't adapt. They'd need to skip it and move along to the next case. But robots from a company called Symbotic have the advantage of machine vision algorithms that allow them to assess an oddly shaped package and pick it up anyway. Even better, the robots can quickly measure shelf space to confirm that a box will fit. If it won't, the robot alerts a central control system, which automatically redirects that box to a shelf where it will fit. The bots zip around the warehouse floor at twenty-five miles per hour, carrying, sensing, and adapting as they go.

The difference between a traditional warehouse and one with Symbotic's robots is stark. Usually, trucks unload pallets of products at the dock; there's an area where pallets are stored until people can unpack them, and conveyor belts move cases of goods to various parts of the warehouse. But because the Symbotic robots immediately remove products from pallets and put then1 on shelves, there's no need to reserve space for storing pallets. And there's no need for conveyor belts either. Thus, a Symbotic-equipped warehouse can reclaim space for shelves. The ramifications are significant: in the best-case scenarios, either a warehouse can store twice as many goods as before, says Joe Caracappa, Symbotic vice president of business development, or it can operate in an area about half the size. Moreover, smaller warehouses can more easily fit into existing neighborhoods, and perishable items can be stored closer to their point of sale.

Because the only human interaction with the goods stored at a warehouse is ·when they're loaded on and off the trucks, we must ask the question: What happens to the human workers at the warehouse? Caracappa says Symbotic currently retrains many of them. Those who performed maintenance on conveyor belts, for instance, are trained to fix robots. And there are new roles, too. Caracappa says system operators monitor the entire flow of robots. "Those roles are typically not in the warehouse before automation comes in," he explains, "but we'll hire them locally and the client will be part of the process."1 0 (In part two of this book, we will explore these new types of jobs in depth when we discuss the missing middle in detail.)

Supply Chinas That Think

Smarter warehouses are Just the beginning. AI technologies are now enabling entire supply chains to become increasingly intelligent, similar to the kinds of advances they've enabled on the factory floor. Of course, companies want to minimize any upstream disruptions to their supply chains, which can come from a number of sources-manufacturing quality problems at a supplier, political instability of a region, labor strikes, adverse weather events, and so on. To that end, AI can help collect and analyze data about suppliers, provide a better understanding of the variables in a supply chain, anticipate future scenarios, and so on. And firms also want to minimize downstream uncertainties. Here, AI can enable companies to optimize their demand planning, forecast more accurately, and better control their inventories. The result is more-agile supply chains capable of anticipating and dealing with the ups and downs of dynamic business conditions.

Consider just one part of the process: demand planning. Getting demand planning right is a pain point for many companies, but the use of neural networks, machine-learning algorithms, and other AI technologies can help lessen that pain. A leading health-food company, for example, leveraged machine-learning capabilities to analyze its demand variations and trends during promotions. The analysis led to a reliable, detailed model that could highlight the expected results from a trade promotion. The gains included a 20 percent reduction in forecast error and a 30 percent reduction in lost sales.

Those improvements are the types sought by consumer goods giant Procter & Gamble, whose CEO recently stated his goal of cutting supply-chain costs by a whopping $ 1 billion a year. Part of those savings w ill come from near-term efforts like the use of AI and the internet of things ( IoT) technologies to automate warehouses and distribution centers. And other savings will come from longer-term projects, including the customized automation of product deliveries of up to seven thousand different stock-keeping units. Whether these and other initiatives will enable P&G to save the company $ 1 billion annually in supply chain costs remains to be seen, but it's safe to say that AI w ill be playing a significant role in those efforts.

The Farms That Feed

AI technology is not only having a large impact on supply chains and the manufacturing of consumer goods and industrial machinery, but is also playing a big role in the production of food. The need for improved efficiency in the agricultural industry is huge. According to various statistics, 795 million people don't have enough food and, to keep pace with population growth, more food will be needed in the next fifty years than has been produced in the past ten thousand years combined. Both fresh water and arable soil are resources that have historically been difficult to acquire or maintain for agriculture. Precision agriculture-which leverages AI and fine-grain data about the state of crops-promises to significantly improve yield, reduce the waste of resources like water and fertilizer, and increase overall efficiency.

To be effective, precision agriculture requires a vast network of diverse IoT sensors to collect granular data. This information might include aerial images captured by satellites or drones (to detect crop distress before it's visible at ground level), environmental sensors in the field (to monitor the chemical composition of the soil, for instance), sensors mounted on farm equipment, weather forecast data, and soil databases.

To help make sense of these various data streams, Accenture has developed a new service-the Precision Agriculture Service-that deploys AI to enable better decision making with respect to pest control, fertilizer usage, and so on. The idea is to process IoT-sensor data with a machine-learning engine to provide feedback that can then be used in one of two ways. It can either go directly to a farmer, who can then implement a solution. Or it can be routed directly to a farm's digital work management system that will then deploy the recommendations automatically. In the system, a feedback loop that incorporates up-to-date sensor data and real-time analytics can help establish a kind of self-healing farm. Farmers can be part of the loop when they approve the system's recommendations, but over time, as the system becomes more reliable, they can spend their time managing other tasks that aren't so easily automated.

AI is also enabling entirely new agricultural models, such as the "vertical farm," in which plants can be grown in thirty foot- high stacks of trays in urban settings such as city warehouses. At one such facility in Newark, New Jersey, run by AeroFarms, data is continuously collected on temperature, humidity, carbon-dioxide levels, and other variables, and machine-learning software analyzes that information in real time to grow the crops (including kale, arugula, and mizuna plants) as efficiently as possible. According to the company, the Newark facility is expected to use 95 percent less water and 50 percent less fertilizer than traditional farms and, because the crops are grown indoors, pesticides aren't needed. AeroFarms is predicting that 2 million pounds of produce can be grown annually at the Newark vertical farm, only about fifteen miles from Manhattan.

Precision agriculture is not yet widespread, but some of its technologies-the analysis of satellite data, for instance-have been used for years. The difference now is the pervasiveness of the IoT, which allows sensor data to talk to apps, and apps to talk to machine-learning-enabled systems. The ultimate goal with precision agriculture is that disparate systems can come together to produce recommendations that farmers can then act on in real time. The result is agricultural processes that produce less waste and higher yields. It's no wonder that precision-agriculture services are expected to grow to $4.55 billion by 2020.1 2 As the technology becomes more widely used, the land will benefit, the farmer will benefit, and the hundreds of millions of people who need access to healthy affordable food will benefit. (See the sidebar "AI for Good: Akshaya Patra." )

AI for Good: Akshaya Patra

Akshaya Patra, an India nonprofit with the vision that "no child in India shall be deprived of education because of hunger," combines the power of AI with blockchain ( digital, decentralized public ledger) and IoT technologies. To achieve its vision, the company's midday meal program provides one wholesome lunchtime meal to keep children sufficiently motivated and nourished to pursue their education. Since 2000, when it began by feeding 1,500 children, its operations have expanded to 1.5 million children per year in 2017; it commemorated its two-billionth meal served in 2016. Thus far, the nonprofit has demonstrated a 20 percent efficiency improvement in selected kitchens. Now feedback is digitized where once it was manually input, and blockchain is driving efficiencies in audit, attendance recording, and invoice processing. AI is used to accurately forecast demand, and IoT sensors monitor and sequence cooking processes to minimize waste and ensure consistent food quality. AI in combination with these other technologies will help Akshaya Patra expand its operations efficiently, meaning more children fed and kept in school.

The Third Wave in Manufacturing

In this chapter, we're beginning to see how artificial intelligence can change the nature of business processes. Factories and industrial settings will continue to be highly automated environments for a variety of reasons; safety and efficiency are two primary drivers. And while new automation technologies will replace some human workers, there's still plenty of space for people, as long as executives look beyond jobs displacement and begin to think differently about work. This is the leadership part of our MELDS framework, which we detailed in the introduction, and it calls for executives to focus on reimagining processes and new roles for employees working in the missing middle (as we' ll discuss in detail in· part two). And, as we've seen in this chapter, some skills are becoming more in demand, and entirely new categories of skills will be required. For example, as we will see in chapter 8, GE and the buyers of its equipment will always need maintenance workers, and they' ll need those workers to be able to work well with new systems that can fuse their skills with advanced technologies in novel ways. This is the skills part of MELDS. Workers in these jobs will need to do what people do well: adapt to new situations and find novel, creative solutions to challenges that arise. Let the machines do the heavy lifting, the monitoring, and the monotonous tasks.

In the case of the researchers, engineers, farmers, and others, the data and analysis that AI systems provide can act as a third eye. And this is why the data part of MELDS is so important. Suddenly, very complex industrial or ecological systems become knowable. Engineers and managers can eliminate previously invisible inefficiencies, and they can make changes to certain aspects of a process with confidence. When you honestly assess the strengths of human and machine workers, and what they do well when they collaborate, a whole new world of possibilities emerges for running a business and designing your processes-that is, the important mindset part of MELDS. And by exploring those possibilities, companies can often develop novel businesses, like vertical farms. Indeed, it's through the experimentation part of MELDS that executives will be able to discover game-changing innovations that could potentially transform their company, if not their entire industry.

In the next chapter, we take artificial intelligence into the back office. It's where "second wave" automation is entrenched, and "third wave" AI has come as a welcome relief to many who have been working with awkward IT tools or inefficient processes. Here, too, we will see how AI and people's imaginations have been transforming seemingly mundane processes, opening up new possibilities for both efficiency and growth through human-machine collaborations.

A Brief History of AI

The driving technology behind the current era of adaptive processes is AI, which has been evolving over decades. A brief history of the technology gives some context to its current state of advanced capabilities.

The field of artificial intelligence was officially born in 1956, when a small group of computer and research scientists, organized by John McCarthy, including Claude Shannon, Marvin Minsky, and others, gathered at Dartmouth College for the first-ever conference to debate the possibility that machine intelligence could imitate human smarts.

The conference, essentially an extended brainstorming session, was based on the assumption that every aspect of learning and creativity could be described so precisely that it could be mathematically modeled and therefore replicated by machines. The goals were lofty; from the event proposal: "An attempt will be made to find how to make machines use language, form, abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves." Of course, this was just the beginning.

The conference succeeded almost immediately in defining the field and unifying many of the mathematical ideas swirling around the concept of artificial intelligence. It also inspired entirely new areas of research in the decades that followed. For instance, Minsky, with Seymour Papert, wrote what was considered the foundational book on scope and limitations of neural networks, a kind of AI that uses biological neurons as its model. Other ideas like expert systems-- wherein a computer contained deep stores of "knowledge" for specific domains like architecture or medical diagnosis-- and natural language processing, computer vision, and mobile robotics can also be traced back to the event.

One conference participant was Arthur Samuel, an engineer at IBM who was building a computer program to play checkers. His program would assess the current state of checkers board and calculate the probability that a given position could lead to a win. In 1959, Samuel coined the term "machine learning": the field of study that gives computers the ability to learn without being explicitly programmed. In 1961, his learning program was used to defeat the fourth-ranked checkers player in the United States. But because Samuel was modest and din't enjoy the politics of self-promotion, it was not until after his retirement from IBM in 1966 that the significance of his machine-learning work became more widely known.

In the decades after the conference, machine learning remained obscure, as other kinds of AI took center stage. In particular, researchers in the 1970s and 1980s focused on a concept of intelligence that was based on physical symbols and manipulated by logical rule. These symbolic systems, however, found no practical success at the time, and their failures led to a period known as n "AI winter."

In the 1990s, however, machine learning began to flourish as its practitioners integrated statistics and probability theory into their approaches. At the same time, the personal computing revolution began. Over the next decade, digital systems, sensors, the internet, and mobile phones would become common, providing all kinds of data for machine-learning experts to use when training these adaptive systems.

Today we think of a machine-learning application as one that builds models based on data sets that engineers or specialists use to train the system. It's a sharp contrast with traditional computer programming. Standard algorithms would follow a predetermined path set into motion by programmers' static instructions or code. A machine-learning system, conversely, can learn as it goes. With each new data set, it updates its models and the way it "sees" the world. In an era in which machines can learn and change based on their experiences and data, programmers have become less like rule makers and dictators, and more like teachers and trainers.

Now AI systems that deploy machine learning are everywhere. Banks use them for fraud detection; dating websites use them to suggest potential matches; marketers use them to try to predict who will respond favorably to an ad' and photo sharing sites use them for automatic face recognition. We've come a long way since checkers. In 2016, Google's AlphaGo demonstrated a significant machine-learning advance. For the first time, a computer beat a human champion of Go, a game far more complex than checkers or chess. In a sign of the times, AlphaGo exhibited moves that were so unexpected that some observers deemed them to actually be creative and even "beautiful."

The growth of AI and machine learning has been intermittent over the decades, but the way that they've crept into products and business operations in recent years shows that they're more than ready for prime time. According to Danny Lange, former head of machine learning at Uber, the technology has finally broken out of the research lab and is fast becoming "the cornerstone of business disruption."

2: Accounting for Robots: AI in Corporate Functions

Money laundering is a major concern for financial institutions, which can face heavy fines and stiff regulatory restrictions for any infractions. At one large global bank, up to ten thousand staffers were engaged in identifying suspicious transactions and accounts that might indicate money laundering, terrorist financing, and other illegal activities. Aggressive monitoring was required to meet the rigid expectations of the US Department of Justice, and the incurred costs were high, with an excessive number of false positives that the bank had to investigate.

In response, the bank implemented a full suite of advanced analytics tools for anti-money-laundering (AML) detection, including machine-learning algorithms to better segment transactions and accounts, and to set the optimal thresholds for alerting investigators to any suspicious activity. All this is done dynamically to incorporate the most recent data and latest results. Moreover, the use of network analysis is helping to uncover valuable new patterns-for example, the closeness of a business relationship between t\vo of the bank's customers can help determine the likelihood that, if one of them is involved in illicit activity, then the other might also be involved.

Thus far, the results have been impressive. The A M L system has reduced false positive alerts by as much as 3 0 percent, allowing staffers more time to investigate those cases requiring human judgment and compliance expertise. The system has also helped reduce the time required to investigate each alert, resulting in a dramatic 40 percent decrease in costs.

Allowing Humans to Be More Human

People rarely revel in performing repetitive or robotic tasks day in and day out. Talk to someone who's worked in a process with a lot of routine steps, and you'll learn how they relish an unusual situation that breaks up a workday or workweek. And if it gives them a chance to solve a hard problem, they feel as if they've made a difference to the organization or maybe even in someone's life. Research by Jordan Etkin of Duke University and Cassie Mogilner of the Wharton School suggests that some variety throughout a workday leads to increased happiness, likely tied to a greater sense of stimulation and productivity.1 So, the question becomes, why continue to train people to work like robots? Why not let workers be more human? Or, as that global bank we discussed discovered, why not let staffers focus on higher-value tasks, requiring their judgment, experience, and expertise?

Our research has confirmed that in many cases, AI allows human workers to be more human. The rote nature of some administrative jobs like invoicing, bookkeeping, accounting, complaints, forms processing, and scheduling arose initially from the use of standard I T technologies that required humans to adjust to the machine limitations of the 1990s and 2000s. Human resources, IT security, and banking compliance departments all use processes that are often made up of well-defined, repetitive tasks. This was the "second wave" of business process improvement.

This chapter examines innovative improvements in enterprise processes-a trend that's been building for years but, thanks to technological advances, has only recently become viable for most organizations. We give examples that address basic questions that anyone interested in deploying AI for enterprise-wide processes should be asking. What will such work look like in this new era of business process transformation? Which tasks are best suited for humans and which are best for machines? While it's true that many organizations can immediately see significant gains when they use AI in conjunction with their existing workforce, what happens if you completely rethink your processes around ultra smart systems? What kind of growth, services, and products become possible?

Your Office Robot

To answer those questions, let's start with a familiar process: categorizing and resolving complaints. In the past, much of the process work around sorting through customer complaints was done manually, and the tedium of many of those tasks detracted from people's job satisfaction. At Virgin Trains, a train-operating company in the U K , for example, a team of customer service reps would manually read, sort, and route complaints. These repetitive activities diverted employees' time and attention, and it fatigued them more than other work they did, like directly talking to customers.

Because the read-sort-route process is clearly defined, it is in some ways an excellent example of a process ripe for automation. But because the incoming information is text-based and is considered "unstructured" in the eyes of software systems, parsing could have been difficult for a less advanced system. Enter AI. Virgin Trains has now installed a machine-learning platform, inSTREAM, with natural-language processing capabilities that can recognize patterns in unstructured data by analyzing a corpus of similar examples-in this case, complaints-and by tracking how customer service representatives interact with incoming text.

Now when a complaint arrives at Virgin Trains, it's automatically read, sorted, and packaged into a case ready file that an employee can quickly review and process. The most common complaints get appropriate, automated responses. If the software isn't fully confident in its assessment of a complaint, it flags it as an exception for a human worker to review; the worker's response effectively updates the software's model. Over time, this kind of feedback improves the algorithm's confidence for an increasing array of scenarios. The system can handle complaints that are terse or long-winded, generic or specific, or in English or other languages.

Thanks to this new technology, Virgin Train's complaints department has decreased its manual work by 85 percent. It also increased correspondence by 20 percent because the new capabilities prompted the company to further open itself to customer interactions. Previously, the company only accepted complaints through its website. Now, it can process inquiries of any type, including email, fax, snail mail, and social media. 2 (Virgin Trains is one of many companies adding some automated intelligence to the back office; see the sidebar ''AI in Business Processes" for more examples.)

Moving Well Beyond RPA

The Virgin Trains system is a relatively advanced form of back office automation because it can analyze and adapt to unstructured data as well as the sudden influx of data. Such applications are called "rob otic process automation" (RPA). Simply put, R PA is software that performs digital office tasks that are administrative, repetitive, and mostly transactional within a workflow. In other words, it automates existing processes. But in order to reimagine processes, firms must utilize more advanced technologies namely, AL (See the sidebar ''AI Technologies and Applications: H ow Does This All Fit Together?" at the end of this chapter.)

Now we're talking about systems that deploy AI techniques such as computer vision, or machine-learning tools to analyze unstructured or complex information. It might be able to read various styles of invoices, contracts, or purchase orders, for instance. It can process these documents-no matter the format-and put the correct values into forms and databases for further action. And then there are even more advanced systems that deploy sophisticated machine-learning algorithms not just to perform the tasks they've been programmed to do, b ut also to assess tasks and processes and adjust as needed. They can learn from observation by "watching over the shoulders" of human employees i n order to improve their performance over time. In other words, they are exactly the kind of technology that is enabling the third wave of business process improvement adaptable processes-that we discussed in the introduction chapter. These applications are more transformational and typically require human employees to actively participate, applying a kind of tacit knowledge or expertise that is difficult to explain or model. Think of the global bank's anti-money-laundering system that we discussed earlier. A complicated financial transaction is processed; an automated system flags it as being suspicious; and a human expert exercises the judgment to decide whether it warrants further investigation. This type of human machine collaboration is also typical of the third wave of business process transformation.

Companies can deploy a range of these technologies, sometimes even for the same application. Case in point: Unilever's process for hiring employees. Say you're looking for a job and through LinkedIn you find a position at Unilever that might be suitable. For the first round of the application process, you'd be asked to play twelve online games based on tests from the field of cognitive neuroscience. The games help assess certain traits, like your risk aversion and ability to read emotional versus contextual cues. According to Unilever, there are no right or wrong answers to the games because, for example, an appetite for risk might be suitable for one type of job while an aversion to risk might be better for another type of position. For this round of the application process, advanced AI isn't required and a relatively basic technology like R PA would suffice.

But if you, as an applicant, made it to the next round, you'd then be asked to submit a video-recorded interview through your computer or smartphone in which you'd answer a set of questions designed for the specific position you were interested in. And here's where sophisticated AI technologies come into play : Your answers would then by analyzed by HireVue, an AI application that not only notes the words you use but also your body language and tone. The best candidates for the position are then invited to the company offices, where they can then be assessed by humans who would make the final hiring decision.

Not only does this Unilever example show how different technologies can be used for different parts of the same application; it also demonstrates the power of human-machine collaboration. Within 90 days of the new system going live, job applications had doubled to 30,000 from the same time period a year ago. In addition, the average time for someone to be hired plunged from four months to just four weeks, and the time that recruiters spent reviewing applications plummeted by 75 percent. Moreover, the company reports that, since installing the syste1n, it has hired its most diverse class to date. For one thing, there was a dramatic increase in the universities represented, from 8 40 to 2,600.

How Do You Know Which Processes to Change?

Repetition. Replication. Redundancy. A well-outlined process. If these elements show up in your business operations, it's a clue that tasks or processes are ready to be changed.

Roger Dickey, a developer and founder of the fast-growing startup Gigster, recognized replication and redundancy in the code of most software applications. At the same time, each new software project-regardless of how similar it was to others that had come before-was incredibly complex to build, complete with bugs and pitfalls that slowed production. Could AI be used to help reimagine the business processes required to build software

The answer, as Gigster has found, is yes. The company typically uses AI to assess the needs of any given software project and automatically assembles an ad hoc team of crack developers to build it. If you are a small firm that needs an app or some other software product but don't have the time or resources to hire a team of developers yourself, you turn to Gigster. If you are a large corporation that doesn't ,vant to divert resources away from established projects, you turn to Gigster.

Gigster effectively takes aim at multiple areas of the enterprise: H R (developer teams are assembled using AI), procurement (quotes are generated using AI), and IT (members of development work and are managed with AI-enabled assistance).

H ow does Gigster upend procurement and H R? Suppose you' d like to build an app that can help patients consolidate their medical records to share with their doctors. Where do you begin? First, you give Gigster a short docuument that explains the app's core function and how you envision a person will use it. On Gigster's end, the project description is cross-referenced with others in Gigster's portfolio of " data structures," which is essentially a catalog of software features. Dickey says his company has mapped the "software genome" and understands five hundred different features a product might have. Next, Gigster takes into account about twenty other customer requirements for how the user interface should look or how quickly the job needs to be completed, and so on. From the customer's mockup, description, and requirements, Gigster's AI quote generator leverages previous projects with simi lar constraints to estimate a price and a timeline.

If you agree to the price and timeline, the next set of Gigster's AI features goes to work. The company deploys its "team builder" in which it matches the demands of your app to members of software development teams who can meet your needs. A typical team will consist of three to five people: a project manager, a designer or two, and a developer or two, all high-performing recruits who are monitored closely by Gigster's online system, which allows the company to guarantee quality and on-time products. This initial setup takes one to three days.

Because software developers work in a digital realm, everything they do can be recorded relatively easily and analyzed . "We believe work is measurable and that data has patterns and those patterns can be exploited to find new efficiencies in work," says Dickey. This means that Gigster knows what processes go into making a software project a success-based on hundreds of other projects just like it-and an AI tool can use that information to spot potential production hiccups before they spiral out of control. Moreover, whenever developers are having trouble with any particular pieces of code, an AI assistant can automatically put them in touch with someone who's recently solved or is grappling with a similar problem. It's "an AI assistant who knows where you are in the project," says Dickey, "and can match you with other people in the world doing the same thing."4 That type of employee augmentation is one of the keys of the third wave of human-machine collaboration.

How Do You Know How Much to Change?

By the very nature of its business-software-Gigster has been able to deploy AI to a range of IT and business processes. Other firms, however, might be better off applying AI to just a few processes. For those organizations, executives need to make judicious decisions about how best to augment their existing employees. And they also must have a plan for scaling up their use of AI in processes.

Those issues were key concerns for SE B, a major Swedish bank, which has been busy installing a virtual assistant called Amelia. Built by IPsoft, Amelia (later renamed Aida in SE B's application) now interacts directly with SE B's I million customers. Within the first three weeks in this role, the software held more than four thousand conversations with seven hundred people and was able to resolve the majority of the issues, says Rasmus Jarborg, SE B's chief strategy officer. The decision to move Aida to a customer facing role came only after the bank had tested the software internally as a virtual IT help-desk agent, assisting its fifteen thousand employees.

Aida is adept at handling natural-language conversations, and the technology is even able to monitor a caller's tone of voice as a way to offer better service in the future. The software adapts, learning new skills by monitoring human customer service agents. This means its capabilities improve and increase over time ; new tasks and processes within the customer service department can become automated with little direct effort by the people who work there.

SEB is the first bank to use Amelia for interactions with customers, and IPsoft has help ed to set up an in-house talent pool of individuals who can mentor the software. These human mentors supervise learning and performance and identify new ways to apply the technology for customer service. 6 We discuss this type of human-machine collaboration in greater detail in chapter 5.

Aida is showing that automated natural-language customer communications are possible in large and complex business environments. As natural-language techniques improve and interfaces advance, they will continue spreading throughout different business functions in various industries. In chapter 4 we'll discuss how various natural-language processing chatbots like Amazon's Alexa are becoming the new front-office faces of companies.

Redefining an Entire Industry

As AI becomes increasingly capable of adding intelligence to middle- and back-office processes, the technology could potentially redefine entire industries. In IT security, for instance, a growing number of security firms are combining machine learning approaches to build ultra-smart, continually evolving defenses against malicious software. These systems can unearth harmful viruses and malware before they cause damage, and they can predict vulnerabilities before they become liabilities that let hackers take over entire systems. In some cases, the IT security process is a closed, automated loop; humans can step away from the day-to-day controls and spend time researching threats or creating new simulations to further test and train bots. (See the sidebar "When Bots Collide." )

In traditional cybersecurity, a company might perform analytics on existing data, gathering signatures of threats and using them to protect against future threats. It's a static operation, and not able to adapt in real time. In contrast, AI-based approaches are able to recognize anomalous patterns as they arise. They do so by calibrating models based on network traffic behavior and scoring anomalies according to the extent to which they deviate from the norm. What's more, AI-based analytics improve as each alert is eventually resolved-either by machine or human-effectively incorporating each new insight into the system as it runs.

Various security companies h ave their own app roaches to the problem. SparkCognition, for instance, offers a product called D eep Armor, which uses a combination of AI techniques including neural networks, heuristics, data science, and natural-language processing to detect threats never seen before and remove malicious files. Another company called Darktrace offers a product called Antigena, which is modeled on the human immune system, identifying and neutralizing bugs as they're encountered. Behavioral analysis of network traffic is key to another company called Vectra. Its AI-enabled software learns the signatures of malicious network behavior and can deploy automated actions to squelch the attack, or pass it off to a team of security experts, who can also decide h ow to handle attacks.

Reimagining Processes Around People

AI technology promises to offload dull and tedious office tasks to software robots, resulting in a work environment that can offer greater satisfaction to human workers. This is the "missing middle" of human-machine symbiotic collaboration that we described in our introductory chapter. It's where companies can generate the most value-more so than with automation-from investments in advanced digital technologies.

In this chapter, we saw the leadership part of M E LDS in action as one global bank was able to reimagine its process for detecting money laundering by using machine-learning algorithms to reduce the number of false positives so that human experts could focus on the more complex, suspicious cases. That type of business process relies heavily on good data, and many companies have been discovering the power of tapping into multiple sources. In the past, Virgin Trains could handle complaints only through its website, but now the company has invested in the data part of MELDS, enabling it to deploy a natural-language processing application that can accept customer inquiries from a variety of channels, including social media. As such systems are deployed, though, employees will need to adjust how they work, and companies must devote resources to the skills part of MELDS. At Gigster, for instance, we saw how AI assistants can automatically put a program developer in touch with others who are grappling with a similar problem, which thus places a premium on the collaborative capabilities of employees. Another lesson from this chapter was that the path toward filling the missing middle takes time as, for example, companies move from R PA to advanced Al, and that transition requires experimentation. The Swedish bank SEB paid proper attention to the experimentation part of MELDS by conducting extensive tests of its virtual assistant Aida on its fifteen thousand employees before launching the system to its one million customers. And lastly, we learned about the importance of the mindset part of MELDS as we saw the potential power of AI to transform an entire industry like IT security, with automated systems helping to uncover malware and identify vulnerabilities so that they can be addressed before a system is breached.

In chapter 3, we'll see how that middle zone can also be extended to the R&D process itself. There, as with the factory floor and office, savvy companies have been reaping the rewards of smart, complementary collaborations between man and machine.

3: THE ULTIMATE INNOVATION MACHINE: AI IN R&D AND BUSINESS INNOVATION

Automaker Tesla has been breaking ground in many ways. Obviously, the company is well known for its snazzy (and pricey) automobiles-including the Tesla Roadster, the first electric sports car-which have attracted not only customers but investors as well. In spring 2017, Tesla's market capitalization topped $50 billion, slightly edging the value of General Motors.1 But it's not just the company's stylish, electric vehicles and soaring stock price that are noteworthy; it's also the clever ways in which the firm conducts its research and development.

In 2016, Tesla announced that every new vehicle would be equipped with all the hardware it needs to drive autonomously, including a bevy of sensors and an onboard computer running a neural network. 2 The kicker: the autonomous AI software won't be fully deployed. As it turns out, Tesla will test drivers against software simulations running in the background on the car's computer. Only when the background program consistently simulates moves more safely than the driver does will the autonomous software be ready for prime time. At that point, Tesla will release the program through remote software updates. What this all means is that Tesla drivers will, in aggregate, be teaching the fleet of cars how to drive.

Tesla is training its AI platform in a distributed test bed with the best data around-its own drivers in real-life conditions. In this case, people's driving skills-at scale-are crucial in the training of the system. AI has allowed Tesla to rethink its fundamental R & D processes and, along the ,vay, speed up the development of its system. This reconsideration of how it conducts R & D is positioning Tesla to be a leader in autonomous cars.

Tesla isn't the only one using AI to rethink its R & D processes, using both machines and people in new, innovative ways. This chapter explores the way that AI enables experimentation within companies and how it's shaking up business processes, especially those that involve customers, medical patients, and others who provide useful data.

You'll see how AI is boosting R&D in the pharmaceutical and life sciences industries, augmenting researchers' intuition and ability to test theories, and speeding up the product-design cycle by orders of magnitude. Thanks to the glut of customer and patient data, traditional processes for researching and developing products and services are changing. Where finding a mass-market hit was once a company's primary driver, ultracustomization is becoming an increasingly critical-and economically feasible-approach.

The Augmented Researcher: How Science Works in the Age of AI

First, ·we need to step back to understand some basic principles. The scientific method is perhaps the most well-understood, widely deployed process in the world. Over centuries, it has come to be defined as a series of discrete, reproducible steps. First, ask questions and make observations. Next, devise hypotheses. After that, design an experiment to test predictions based on the hypotheses. Then, of course, run tests and collect data. Finally, develop generalized theories. As a process, the scientific method can be visualized cyclically. All that data and those general theories lead to more observation and further research, getting the ball rolling all over again.

Because the scientific method's steps are so clear-cut, it's no surprise that there are opportunities for artificial intelligence to change the shape of the process itself. While, so far, research institutions and companies involved in science have not fully overhauled the way science is done, some have successfully compressed or exploded particular scientific steps. The following sections look at A I disruptions at each stage of the scientific process-considering which tasks are best for people, which are best for machines, and how both work together.

Opportunities in Observation

Isaac Asimov contended that "the most exciting phrase to hear in science, the one that heralds new discoveries, is not ' Eureka!,' but 'that's funny . . ."' 3 The observation phase of the scientific process is full of twists and turns and unexpected paths as scientists pore over the latest research, stumble upon a surprise chemical reaction, or through a serendipitous conversation with a colleague, arrive at a new research question.

But now think about how challenging scientific observation is in the modern era; there are already so many studies to consider and so much data to sort. Research from the University of Ottawa in 2009 noted that the total number of science papers published since 1665 exceeds 50 million, and more than 2.5 million new scientific papers are published annually.4 And that's just the papers. What about all the raw data-structured, unstructured, cataloged, cleaned, sifted, and analyzed? Our digital lives produce staggering amounts of data each day. What kind of observations could be done with that? How might we even arrive at the point where something seems "funny" or worth exploring further? (See the sidebar "Learning from Failure." )

While human researchers are very good at creative insights, machines are unequivocally better at data organization and presentation, especially when data volume becomes unwieldy. One company, called Quid, is using AI to reimagine the search-andfind part of the researchers' process. Quid's platform uses natural language processing to create data visualizations from large bodies of text data-from patents to news reports-and sorts the text into networks of ideas. The company's interface, best experienced on a touchscreen, reveals concepts, clusters, and clumps of similarity as well as strong and tenuous connections between ideas.

Shivon Zilis, an investor at Bloomberg Beta, uses Quid in a number of aspects of her work. Zilis might spend her day developing a thesis for the next emerging tech trend, sourcing deals, or helping guide the development of companies she invests in.

Without a tool like Quid, Zilis would have to rely on a hodgepodge of tactics to do her research-searching Google using a variety of search terms or reading news from a necessarily limited number of sources, for instance. But with Quid, Zilis can visualize trends that emerge from analysis of a more complete set of news sources. She can notice connections between technologies that might have been obscured otherwise. Moreover, Zilis's finely tuned research intuition is amplified, which gives her more opportunities to ask additional questions and follow unusual threads as she zooms in and out of the visualized network of ideas. What Quid gives an investor, essentially, is a platform for a next-level observation-questions come quicker and they are more nuanced and incisive-and that can open the gates to unexpected avenues of inquiry, leading to a smarter hypothesis.

The Smart, Automated Hypothesis

After observation, scientists form hypotheses, essentially a proposed-and testable-explanation for a phenomenon. What happens to the scientific process when hypotheses can be generated automatically? The precision medicine company CNS Healthcare is exploring that possibility. Its powerful machine learning and simulation software, Reverse Engineering and Forward Simulation ( RE FS), can churn out hypotheses straight from the data to find relationships in patients' medical health records. In one instance, CNS was able to recreate in three months the results of a two-year-long study on drug interactions.

The study involved finding adverse reactions between drug combinations by seniors using Medicare. Bad drug combinations are a major problem without a standard solution: drugs aren't tested in combination in US Food and Drug Administration trials, so there's no easy way to see which ones are dangerous when combined. Historically, researchers might have relied on scientific i ntuition to notice that drugs that operate via similar enzymatic pathways might interact adversely with other drugs in a similar way. Next, researchers might have developed the hypothesis-for example, drug A plus drug B is leading to adverse event C-and then, of course, they'd test it. Using such an approach, researchers would have discovered that two common drugs for seniors interact poorly, but the study took two years to develop and confirmed only the limited hypothesis about interaction between just those two drugs.

In a test of the REFS system, GNS assessed anonymous data from about 200,000 patients and a wide array of drugs on the market. The data itself was encrypted, says GNS's chairman, cofounder, and CEO Colin Hill. "We had no knowledge of what the drugs were. There was no way we could cheat."6 The machine-learning platform churned through approximately 45 quadrillion hypotheses, and after only three months, the combination of drugs most likely to lead to troublesome interactions emerged as a final result.

Hill says that the people on his team didn't know if their results were right or not; they simply handed them over to researchers studying drug interactions. As it turned out, REFS did indeed uncover the drug interaction that had taken two years to confirm. But it did something else as well: it also uncovered a drug interaction that had only been discussed among patients, but wasn't formally studied. The researchers were able to examine their own observational data on those drugs from a year prior and then look at the records to see what the interactions were a year later. There, in their own records, was validation of a causal connection, hidden in plain sight. "It was the first time that I know of that machines discovered new medical knowledge," says Hill. "Straight from the data. There was no human involved in this discovery."

GNS Healthcare is showing that it's possible, when AI is injected into the hypothesis phase of the scientific method, to find previously hidden correlations and causations. Moreover, use of the technology can result in dramatic cost savings. In one recent success, GNS was able to reverse-engineer-without using a hypothesis or preexisting assumptions-PCSK9, a class of drug that reduces bad cholesterol in the bloodstream. It took seventy years to discover PCSK9 and tens of billions of dollars over decades. But using the same starting data only, GNS's machine-learning models were able to recreate all the known LDL biology in less than ten months for less than $ 1 million.

Exploding the Design Space

After the hypothesis comes tests. For many companies, this stage is intimately tied to product design. Here, firms can use AI and big data to scope out myriad alternatives and then narrow the scope of their experiment to select for the most likely candidates. It's a recurring story: AI helps organizations refocus resources-and most fundamentally, their human resources-into higher-value activities.

Consider Nike, which recently turned to AI for the specific problem of making better spikes for sprinters. The challenge: a stiff spike plate is better-giving runners something solid to push off against-but the most common way to achieve stiffness is to use heavier materials. Unfortunately, heavy materials weigh a sprinter down.

With the help of the company's algorithmic design software, Nike designers optimized for both stiffness and lightness, conceiving a shoe from scratch. Human designers alone would have likely started from existing shoes and iterated until they found something that was satisfactory, but not nearly as optimized. In the end, the company was able to 3 -D print various prototypes and test them, and repeat the cycle until it found the premier design. The ultimate shoe can shave a tenth of a second off a sprinter's time, which can be the difference between first and fourth place. Nike's rapid prototyping of shoe designs points to the way AI disrupts the next phase of the scientific process. Intelligent algorithms are compressing the time it takes to test.8 (For additional examples of AI in product design, see the sidebar ''AI in Product and Service Design.")

Faster Testers

Many researchers find that their least favorite part of what they do is actually running an experiment and collecting the data. The other parts of the R&D process-discovery and asking questions-provide them the most satisfaction. It's a relief, then, to realize what a boon artificial intelligence is to experimental testing. Here we see another recurring story : AI can resolve the tedium and allow practitioners to bypass drudgery, to spend more time conceiving new experiments, for instance, or asking unexpected questions. The business benefit is clear: bringing more high-quality products to market faster.

While today's experimental environments look, in some fundamental ways, similar to labs from decades ago-mice in cages, petri dishes in incubators, titration systems, and so on-many areas of science are moving more fully into silicon. That is, they can simulate experiments inside computers. In chapter I , we described GE's software Predix, which runs virtual experiments with digitized versions of factory machines. But the reality is, you don't need Predix to create a model of your processes and run tests. You just need a solid understanding of the steps in your process and clean data that you can use to develop a model.

Everything from financial services and insurance products to beer brewing and shaving-cream chemistry can be described digitally. Once it is, it can be optimized. Traditionally, optimization algorithms have been confined to academia or used only by experts. One startup called SigOpt, however, realized that machine learning could turn any digital model into an easily solved optimization problem, essentially democratizing this powerful computational tool for the masses.

"You don't want to have to be an expert in Bayesian optimization in order to apply these types of techniques," says Scott Clark, CEO of SigOpt. The company's goal is to free subject matter experts from having to spend time tweaking digital systems, trying to find best-case scenarios. Instead, it wants to empower them to experiment more.

"A chemist who is actually brewing stuff up on their lab bench can just have their laptop open or have [SigOpt's interface] on their phone," explains Clark. The software would prompt the chemist: "This is the next experiment to try." Or it might note that a certain experiment did particularly well. "It's guiding them through that as easily as possible, so they don't have to have any internal knowledge of the system" says Clark. "They just get the best results out."9 I n other words, one of the scientist's main tasks-to test ideas-is augmented by SigOpt's tools.

Personalized Delivery: Theory and Practice

After testing, scientists develop their general theories and then start the process, beginning with observation, all over again. I n business, after the tests and optimization come marketing and product delivery.

Many trends-including the increasing availability of customer data-are leading to a new level of product customization and delivery. As we saw in chapter 1 , AI is making it more economical to make personalized consumables like cars. From chapter 2 , we saw how AI can turn routine back-office interactions into more personalized services that improve customer experience. AI is also operating in the R&D departments responsible for these changes in mass customization. ( For a brief discussion of the balance between personalization and privacy, see side bar "Responsible AI: Ethics as a Precursor to Discovery." )

Take, for example, the health-care industry. AI is now enabling the era of "personalized medicine" based on genetic testing. In the past, it was virtually impossible to analyze and manage all the combinations of possible treatments for each patient by hand. Today, intelligent systems are taking over that job. Decades from now (or sooner), it will seem absurd that doctors prescribed the same treatment to a wide swath of their patients. Everyone's treatments will be personalized.

Along these lines, GNS, the analytics firm, has been crunching huge amounts of data to match specific drugs and nondrug interventions to individual patients. By better matching drugs to different individuals, the company can improve outcomes, lower costs, and save hundreds of billions of dollars, according to cofounder Hill. Now that so much data about patients' individual genomes and responsiveness to various chemical compounds is available, it simply doesn't make sense to deploy one-size-fits-all treatments. Individualized treatments could solve an especially critical problem in clinical trials, of which more than 8 0 percent fail due to some level of mismatching between patient and drug.

The R+D Risk Factor

The use of AI in the different stages of R&D-observations, hypothesis generation, experiment design, and so on-is producing remarkable gains at all levels and in a variety of fields. Discoveries made over the course of a decade are being replicated, without any guidance, within a matter of months, resulting in dramatic time and cost savings. This has led to a fundamental rethinking of how companies manage their R&D activities.

In the past, at many firms, most R&D projects didn't pan out, meaning losses of tens of millions of dollars or more every year. The result was that companies tended to be risk averse, less likely to fund blue-sky research projects. But when you add AI into the R&D pipeline, you speed up the discovery process for some projects and improve the success rate of others. More money can then be freed up for riskier-and potentially the most lucrative or groundbreaking-research initiatives

The pharmaceutical industry is a case in point. Traditionally, drug discovery has started with the medicinal chemist, people who are good at looking at a pharmaceutical problem and finding molecules to match. "Unfortunately, they can only ever test maybe 1 percent of those ideas," explains Brandon Allgood, chief technology officer and cofounder of Numerate. "They have to go through a triage system of winnowing down ideas they think they can try. A lot of that is subjective. A lot of that is based on rules of thumb."

Numerate deploys machine learning to identify compounds with the highest probability of success against specific disease targets. Using that technology, researchers were able to develop a better HIV drug in six months, instead of the ten years and $20 million to develop the current HIV drug. "What our machine learning allows them to do is encode all of these really good ideas to help them be able to search a billion molecules and only make a hundred or two hundred," Allgood says. "It allows them to explore many ideas that they wouldn't even get to-what I would call ' kooky' ideas-and be able to test those now because they can . . . It allows them to be more creative and allows them to think more broadly and try out different ideas."1 2 ( For additional examples of AI used in health-care R&D, see the sidebar ''AI in Health Care and Life Sciences." )

4: Say Hello to Your New Front-Office Bots: AI in Customer Service, Sales, and Marketing

Coca-Cola, the consumer beverage giant, operates a vast army of 16 million cooler cabinets keeping its soft drinks refrigerated at retail outlets worldwide.1 This army requires thousands of employees to visit those sites and manually take stock of the Coca-Cola products at those locations. Recently, the company has been piloting a proof-of-concept project using AI to manage the coolers. The project calls for the deployment of a new AI capability, called Einstein, from customer relationship management (CRM) vendor Salesforce, which uses computer vision, deep learning, and natural-language-processing technologies.

Using an Einstein-powered app, in pilot with select CocaCola retailers, an employee on-site could take a cellphone photo of the cooler cabinet, and Einstein's image-recognition services will analyze the photo to identify and count the different Coca-Cola bottles in it. Einstein would then predict and recommend a restocking order, using C R M data and other information, including weather forecasts, promotional offers, inventory levels, and historical data to account for seasonal fluctuations and various other factors. The automation of the count and restocking order could save employees paper work and time, and the added intelligence of the system has the potential to improve sales and increase customer satisfaction.

In the front office, AI is poised to help companies like CocaCola improve the experiences and outcomes for every critical customer interaction, including interactions in three key functions: sales, marketing, and customer service. In those areas, AI has been both automating employee tasks and augmenting workers' own capabilities. We have seen, for example, how AI agents like Amazon's Alexa and other systems that automate customer interactions now allow workers to handle more complicated tasks, and let businesses shift workers to where people skills are more important.

Such shifts are also significantly affecting the customer relationship with corporations and brands. In many cases, it can save a customer time and effort and can help provide customizable experiences and products-cutting back on a lot of (wasted) advertising-an increasingly important trend in retail. In other cases, such as digital lending-which uses AI to analyze vast and varied data troves-customers who might have been overlooked with a traditional credit check could suddenly have much easier access to credit and loans.

And finally, these changes are bound to affect the relationships that customers and brands have with the products themselves.

As consumable items produce more data about their performance and send this data back to their manufacturer, companies can start to think differently about product support and the products themselves. For instance, Philips smart lighting uses AI to predict when bulbs will lose their efficiency, which ties into the company's recycling and replacement service. In short, sensor data and AI are now allowing the company to sell "light as a service" instead of just bulbs.

Heady times, for sure. But as AI enters the front office, new questions about best practices arise. How do AI and new modes of human-machine interaction change the way companies deliver goods and services, and how are these interactions shaping the future of work? How do the new user interfaces like Alexa change the relationships between companies' brands and their customers? What design choices can make or break a natural-language bot? And what happens when logos and mascots-traditional brand ambassadors-gain intelligence? These questions are at the heart of this chapter.

Customer-Aware Shops

To begin answering those questions, let's return to the retail floor. While Coca-Cola has been piloting the use of AI to autom ate its product-ordering process, other companies are focusing more on improving the customer experience by augmenting the work of floor sales staff. Take, for example, the global fashion company Ralph Lauren, which has partnered with Oak Labs, a San Francisco-based startup, to develop an integrated commerce experience for shoppers. 3 A key part of the technology is the connected fitting room, equipped with a smart mirror that relies on R FI D to automatically recognize the items that a shopper brings into the room.

The mirror, which can translate six languages, can then display details about an item. It can also change the lighting (bright natural light, sunset, club setting, and so on) so shoppers can see how they look in different settings. And the mirror can indicate whether items are available in additional colors or sizes, which a sales associate delivers to the dressing room. This last feature is the kind of personalized customer service that a sales associate, harried by tending to many customers at once, usually wouldn't be able to provide.

Of course, the smart mirror also collects data about the customer- the length of a fitting-room session, the conversion ratio (items bought versus those tried on), and other information-that a store can then analyze in aggregate to gain valuable insights. For example, customers might bring a certain clothing item into the fitting room frequently, but then seldom purchase it, prompting a store to revise its future buying. And further down the line, this kind of customer data and other information such as customer movement could be used to design stores in new ways. Imagine being able to run a variety of customer-data models through design software, optimizing store layout for customer satisfaction, return visits, or the purchase of particular items.

A retailer can also use AI to address operational issues like staffing. One global Japanese clothing retailer has been working to optimize its team of sales associates on the floor. In clothing or shoe stores, salespeople are key : around 70 percent of surveyed customers report wanting in-store recommendations. Thus, to make better staffing decisions, this retailer decided to use a system from an AI company called Percolata.

The software developed an optimal schedule, in fifteen-minute increments, for the stores, and suggested the most effective combination of sales associates at any one time. The automation eliminated managers' sometimes inadvertent biases in scheduling "favorites" more often, even if they didn't contribute to the overall success of a sales team. In a rollout of the system in twenty US locations, the company discovered that its stores were overstaffed 53 percent of the time and understaffed -33 percent of the time, and Percolata's scheduling recommendations have boosted the Japanese retailer's sales by 10 to 30 percent.5 In addition, Percolata's system frees managers from an estimated three hours daily that they had previously used for fiddling with schedules, and it allows sales workers more flexibility in their hours.

An innovation from Europe is pushing the retail envelope in other ways. Almax, an Italian company, has developed a mannequin with computer vision and facial-recognition technology.6 The AI system can identify you by your gender, approximate age, and race. Boutique shops and fashion brands like Benetton have been deploying the high-tech mannequins to learn more about their customers. One retail outlet, for example, discovered that men who shopped during the first few days of a sale tended to spend more than women, prompting that business to modify its window displays accordingly. Another store reportedly learned that Chinese shoppers made up one-third of customers using a particular entrance after 4 p.m., so Chinese-speaking staffers were located there during these hours.

In the future, retailers could use AI technology to provide customer personalization-a mannequin or mirror that recognizes you can pull up your buying history and help a human salesclerk suggest articles of clothing that you might like. Such advances would typify the human-mach ine collaboration and augmentation described in the introductory chapter, with AI technology doing what it does best (sifting through and processing copious data to recommend certain actions) and humans doing what they do best (exercising their judgment and social skills to help customers purchase products that better fit their needs). Moreover, as AI systems become more advanced, they will be able to analyze a customer's facial expression and voice tone to determine that person's emotional state, and then respond in the appropriate way. In chapter 5, we' ll see how some advanced AI applications are already being trained to be more empathetic. ( For additional examples of how retailers are currently taking advantage of AI to personalize the shopping experience online and in stores, see the sidebar "AI in Retail Sales.")

H owever, as such technological advances continue pushing the retail envelope, they'll likely raise privacy and ethical concerns. Almax, for instance, h as been working to extend its mannequins' hearing capability, raising fears they can potentially eavesdrop to capture customers' reactions to the clothing displayed. In chapter 5, we'll discuss how companies that deploy such cutting-edge technologies will need human employees to assess and address the various ethical concerns likely to arise.

AI for Super Salespeople

AI is not only helping salespeople on the retail floor; it's also empowering them wherever and whenever they're interacting with customers. From automatically sending perfectly composed emails via a digital assistant to cleverly and quickly organizing sales data, AI is offloading some of the major time sinks of sales teams. What's more, as sales and marketing have become more digital, they have lost some of the personal touch that attracted many to the field in the first place. AI is giving sale_speople and marketers the time and insights to cut through the high volume and opacity of digital interactions, and letting them be more human.

A startup called 6sense, for example, offers software that crunches huge amounts of data to help a salesperson send an email to a potential customer at just the right time. By analyzing customers who visit a client's site-as well as third-party data from a variety of publicly available sources, including social media- 6sense can paint a more complete picture of interest and assess if and when a customer might be ready to buy and even preempt objections in the sales process. Whereas, in the past, a salesperson might glean a sales opportunity based on physical or social cues over the phone or in person, 6sense is returning to salespeople some of the skills that more socially opaque online interactions, like the extensive use of email, had blunted.

From the Playbooks of Digital Giants

More and more, traditional companies are now deploying data analytics tricks for marketing and sales that we more commonly associate with companies like Amazon, eBay, and Google. This means that even a company like Coca-Cola can be a leader in AI.

Earlier we described how the soft-drink giant was developing a smart cooler cabinet for its millions of retail locations around the world. The company has also deployed AI in its social media marketing. What's special about Coca-Cola's AI application is that it can effectively gauge the emotions behind trending news events like David Bowie's death or the Super Bowl and develop creative marketing that will better resonate with customers.

In tests of the system, creative content that leveraged AIenabled insights on customers' moods during the 2016 Summer Olympics resulted in a 26 percent bump in how much more likely people were to view or share that content. Such increases could potentially lead to a significant impact on the bottom line.

Other AI applications in sales and marketing might be less flashy, but the work they do is no less valuable. Campbell Soup Company, for instance, has worked with Ditto Labs to deploy AI for making sense of the consumer chatter on social media. The application sifts through and analyzes enormous amounts of visual data. So far, the company has tested the technology on its VS brand, and according to Uman Shah, global director of digital marketing and innovation at Campbell, the feedback from the unprompted and authentic data has led to valuable consumer insights.1 2 (Across the sales process-from selling to customers, business, advertising, pricing, and marketing-AI is helping deliver results; for more examples, see the sidebar "AI in the Sales and Marketing Process." )

The Shape of Jobs to Come

this chapter, we learned about companies at the forefront of reimagining their processes for front-office and customer interactions. Coca-Cola, for one, has been conducting a pilot project that could transform the way customers order beverage products for 16 million cooler cabinets around the world, using A I reimagine that process. That reimagining is the mindset part of our MELDS framework. Meanwhile, Ralph Lauren has been developing a "smart mirror" to improve the consumer experience for buying clothes . The mirror not only assists shoppers example, informing them if a particular item might be available in different color or size; it also continually collects information. Here we saw the data part of MELDS in action, as Ralph Lauren can then analyze that information to obtain valuable insights, such as the specific kinds of products that consumers might try on but rarely buy. But the use of increasingly smart mirrors, mannequins that eavesdrop, and other similar devices could eventually raise privacy and ethical concerns that companies will need to address . And this is why firms should never neglect the important leadership part of MELDS. Moreover, AI systems like smart mirrors become more advanced, they will require increasing levels of training. For instance, bots like Siri and Alexa already need considerable training by humans to display the appropriate amount of empathy when a customer is frustrated, angry, or anxious . And that's why executives need to pay attention to the skills part of MELDS, to ensure that they have the employees they need to perform that training. Moreover, firms also must devote the proper resources to the experimentation part of MELDS in order to find, for example, j just the right emotion for bots like Siri and Alexa.

This chapter also hinted at how new kinds of jobs may emerge in the front office. As bots become critical components of the customer-service infrastructure, for instance, their personalities will need to be designed, updated, and managed . Experts in unexpected disciplines such as human conversation, dialogue, humor, poetry, and empathy will need to lead the charge. Moreover, in the new world of augmented and automated work, user interface and experience designers will have utmost importance, as the interface between people-whether an organization's customers or its employees-will have a disproportionate impact on whether an A I-based product or service survives and thrives, or if it fails. In part two of this book (and especially in chapter 5), we will discuss these new types of roles and their important implications for organizations.

PART TWO: The Missing Middle: Reimagining Processes with AI

The previous chapters described how businesses are currently using AI. Across industries, companies are reaping the benefits of effective human-machine teams. Human strengths like creativity, improvisation, dexterity, judging, and social and leadership abilities are still relevant and important, as are machine strengths like speed, accuracy, repetition, predictive capabilities, and scalability. When businesses recognize the relative strengths of each, they can improve the effectiveness and motivation of their employees at the same time that they boost their top and bottom lines.

But what's coming tomorrow? Part two is our prognostication. In the following chapters, we move deeper into the human machine dynamic and look at what you can do to reimagine business processes around such a dynamic.

While certain job tasks might always be the exclusive province of either human or machine, our research shows that many old jobs are transforming and new jobs are emerging around human-machine teams. The novel jobs that grow from the human-machine partnerships are happening in what we call the missing middle-new ways of working that are largely missing from today's economic research and reporting on jobs. The traditional approach has been to view humans and machines as rivals, each side fighting for the other's jobs. But that binary perspective is overly simplified and neglects the powerful collaborations that have been occurring in the missing middle between the two sides.

The simple truth is that companies can achieve the largest boosts in performance when humans and machines work together as allies, not adversaries, in order to take advantage of each other's complementary strengths. What's easy for us (folding a towel, for instance) can be devilishly tricky for machines. And what's easy for machines (spotting hidden patterns in huge datasets, for instance) can be extremely difficult for us to do. Indeed, humans can thrive in situations where there is little or no data, whereas machines excel in situations where there is lots of data. Business requires both kinds of capabilities, and it's in the missing middle where that type of collaborative teamwork occurs. Moreover, machine learning and other AI technologies can often function like " black boxes," resulting in decisions that might not be explainable. That might be acceptable for certain types of systems but other applications (for example, those in the medical and legal fields) generally require humans in the loop.

In the past, when digital tools were used mainly to automate existing processes, companies had no missing middle to fill. But now, with increasingly sophisticated AI technologies that enable human-machine collaborations, developing the missing middle has become one of the key components to reimagining business processes. Doing so starts with the approach that many of the companies described in part one of the book are already taking. They think of AI as an investment in human talent first and technology second. They value workers who are adaptable, entrepreneurial, and open to retraining. Then these companies provide support to ensure that their workers and AI systems can succeed together. In doing so, they lay the foundation for adaptable, robust business processes capable of withstanding economic shocks and increasing the -rate of technological change.

Leveraging the missing middle is one of the main components needed to reimagine business processes, but another key component is revamping the concept of the process itself. Businesses need to shift from seeing processes as collections of sequential tasks. I n the age of AI, processes become more dynamic and adaptable. Instead of visualizing a process as a collection of nodes along a straight line, say, it might help to see it as a sprawling network of movable, reconnectable nodes or perhaps something with a hub and spokes. The linear model for process no longer cuts it.

In addition to developing the missing middle and rethinking process fundamentals, businesses need to have management address the challenges of reimagining process with an awareness of responsible AI . It's important that executives not only provide the training required for people to make valuable contributions in the missing middle; they must also consider the various ethical, moral, and legal issues associated with the AI systems that their organizations deploy. Key questions include:

As a publicly traded company, what obligations do we have to our shareholders, employees, and larger society to ensure that we deploy AI for good and not harm?

If we use AI in a new process, how can we do it in compliance with laws and regulations like General Data Protection Regulation (GDPR)?

How can we ensure that we have thought through Al's possible unintended consequences that can create brand and public relations issues for the company?

It's still the early days, but organizations across a variety of industries are showing remarkable creativity in the ways they're enlisting AI and their employees to responsibly revamp, modify, and reimagine business processes. Along the way, they're shining a light on a future that is increasingly applicable not just to digital behemoths like Facebook and Amazon, but to every organization entering the third era of enterprise transformation.

Consider Rio Tinto, the diversified global mining company. 1 AI technology is enabling the firm to control its large fleet of machinery from a central operations facility. The fleet includes autonomous drills, excavators, earth movers, driverless trucks, and other equipment deployed at mines that might be thousands of miles away. Data from sensors on various machines is continuously fed into a large database, and AI technology is deployed to analyze that information for valuable insights. Data on the braking patterns of dump trucks, for example, might help predict maintenance problems.

But this is hardly an example of pure automation that has replaced humans. The Rio Tinto command center employs a host of data analysts, engineers, and skilled remote operators who work together to manage the large fleet. ]'he data analysts, for instance, analyze information in the database in order to make recommendations to the remote operators. One of the many benefits of the centralized location is that it brings together those operators who might never have met face-to-face. Because these individuals work alongside one another, remotely control ling their equipment through display screens, they can better coordinate their efforts to deal with changing conditions like adverse weather and equipment breakdowns. Rio Tinto's massive investments in AI have certainly not been without their fair share of setbacks-for instance, a driverless train system for hauling ore across Australia has h it a major delay.2 But what's noteworthy here is the powerful combination of humans and machines that has the potential for overall better decision making and continual improvements in the company's vast operations.

Many might not consider Rio Tinto a digital-first company, yet it has reconfigured its workforce to work effectively alongside AI systems. In doing so, it has reimagined some of its processes to look more like something out of NASA's mission control in Houston, an atypical yet effective approach for a mining company.

Now, what happens if you build your company with human machine teams in mind from the start? The six-year-old company Stitch Fix is a prime example of the missing middle and process reimagination in action. Its main service is personal shopping, but with a twist: the company picks out new clothes and sends them straight to your door, based on data you provide, such as a style survey, measurements, and a Pinterest board. If you don't like any of the items, you just send them back. Gone are the days of spending hours at a store and trying on dozens of outfits, only to find (if you're lucky) a few that work.

Stitch Fix wouldn't be possible without machine learning. But the company also knows a human touch is crucial to its success. Because Stitch Fix lives and dies by the quality of its clothing suggestions, its recommendation system-composed of both people and machines-is at the core of its service. The structured data, like surveys, measurements, and brand preferences, is managed by machines. Human stylists pay more attention to the unstructured data, such as images from Pinterest boards and notes from customers about why they're looking for new clothes.

When it's time to assemble a shipment, machine-learning algorithms reduce the potential options-in terms of style, size, brand, and other factors-and provide a stylist with a manageable number of choices; a machine augments the worker. The stylist then uses his or her expertise to finalize the package and possibly include a personalized note. Both the human and machine are constantly learning and updating their decision making. The client's decision to keep an item of clothing or not is the yes-or-no information that's used to train the algorithm so that it suggests more relevant items in the future. The stylist also improves based on this information as well as hunches and notes from the customer.

What is it like to work at Stitch Fix? Its more than twenty-eight hundred stylists log in at their own computers, which become a digital console of sorts, and then click around an interface that's designed to help them make quick, relevant styling decisions. Options are automatically sorted so they waste no time searching through wrong-sized items. The interface also provides client information like risk tolerance and feedback history. Interestingly, the interface is designed to help stylists overcome biases; it can vary the information they see to test for and nudge them out of recommendation ruts.

Even with constant monitoring and algorithms that guide decision making, according to internal surveys, Stitch Fix stylists are mostly satisfied with the work. And this type of work, built around augmented creativity and flexible schedules, will indeed play an important role in the workforce of the future. By offering health insurance and other W-2 benefits for stylists who work a certain number of hours each week, Stitch Fix is also distinguishing itself as a company that understands critical human-management elements of the emerging on-demand work environment.

Fast-Forward

Both Rio Tinto and Stitch Fix have taken their own approaches to fleshing out the missing middle and reimagining processes in their industry. We designed the examples to help you recognize your own opportunities to build the missing middle, transform processes, and take specific steps toward reimagining the future of work.

The human + machine revolution has already begun, but there are still many questions to answer and paths to forge. That's the goal of the remaining chapters, so let's continue our Journey.

5: Rearing Your Algorithms Right: Three Roles Humans Play in Developing and Deploying Responsible AI

Melissa Cefkin has an interesting job. As a principal scientist at Nissan's research center in Silicon Valley, she works alongside traditional car designers in developing the next generation of self-driving vehicles. Her role is to ensure a smooth collaboration between human and machine (that is, between driver and automobile), and that's why she has a background in anthropology. " You need to understand humans if you want to provide them with an automated partner," she contends.

Cefkin's role at Nissan is to think about things that most car designers might not consider. Take, for example, driving rules and conventions, most of which are pretty cut-and-dried (for instance, not crossing a double line), yet people will often break them in certain conditions (crossing a double line to avoid a collision) . How, then, should autonomous cars be programmed to handle exactly when and where to break a rule? Working along with programmers, electronic engineers, and AI experts, Cefkin is hoping to imbue AI self-driving algorithms with specific human traits, such as the flexibility to break rules for a greater good.

As a "vehicle design anthropologist," Cefkin is one of a growing number of professionals whose jobs didn't exist until relatively recently. O ver the years, AI systems have quickly become a part of everyday business, recommending products to customers, helping factories operate more efficiently, and diagnosing and fixing problems with IT systems. That transformation has led to considerable discussion about the potential for the disappearance of whole categories of jobs over the coming years. (Think about the scores of warehouse workers that Amazon currently employs.) But what's often overlooked in the discussion is that many jobs like Cefkin's will also be created. A large number of these jobs will focus on humans training the machines and, in order to develop AI systems capable of complex interactions with people, the training process will increasingly look like a child 's development path.

In our global study of more than fifteen hundred companies now using or testing AI and machine-learning systems, we found the emergence of entire categories of different jobs that will become increasingly prominent.

These new jobs are not simply replacing old ones. They are entirely novel positions, requiring skills and training never needed before. Specifically, sophisticated AI systems are necessitating new business and technology roles that train , explain , and sustain AI behavior, which fall on the left side of the human + machine hybrid activities in figure 5-1. Such work complements the tasks that AI machines perform. Symbiotic with AI, the new roles draw on distinctively human skills. Where in your organization might you find these new jobs? How do they fit into existing and reimagined processes? In this chapter, we answer those questions and provide examples to help you start thinking about your own applications for trainers, explainers, and susta1ners.

Trainers

In the past, people had to adapt to how computers worked. Now, the reverse is happening-AI systems are learning how to adapt to us. To do so, though, those systems need extensive training, and figure 5-2 lists the types of jobs required to teach AI systems how they should perform certain tasks or how they should, well, act a little more human. Generally speaking, we tend to react positively to AI with human-like behavior because it allows us to interact more naturally with machines. (Yet we can also be disturbed by any imperfections in human-like robots, a phenomenon called the "uncanny valley," which will be discussed later in this chapter.)

In manufacturing, for instance, the newer, lightweight, flexible robotic systems that work alongside humans need to be programmed and trained to handle different tasks. This requires employees with the right skills. For automakers, highly automated factories incur steep financial costs for equipment breakdowns. An unscheduled six-hour shut down at an automated assembly line that manufactures a $50,000 vehicle every minute would incur a cost of around $18 million. That's one of the reasons why, over the past decade, Fanuc, a leading robotic manufacturer, has trained forty-seven thousand people to use its equipment. Even so, a shortage of 2 million qualified employees for manufacturing-related jobs in the coming years is projected.

Physical robots aren't the only AI systems that need training. So does AI software, and this is where training machines to become more human-li ke becomes important. Training requires a multi­tude of roles and jobs. At the simple end of the spectrum, trainers help natural-language processors and language translators make fewer errors. At the complex end, AI algorithms must be trained to mimic human behaviors. Customer service chatbots, for exam­ple, need to be tweaked to detect the complexities and subtleties of human communication. At Yahoo!, human trainers are trying to teach the company's language-processing system  that  people do not always literally mean  what they say. Thus far, they have developed an algorithm that can detect sarcasm on social media and websites with an accuracy of at least 80 percent.

As Al creeps across industries, more businesses will need train­ers for their physical and software-based systems. As a first step, consider using expert employees who already work closely with AI or with the systems that will integrate AI as initial trainers. Their tacit knowledge can often make the difference in a system that works well and one that is prone to failure. Then, after a system has learned the fundamentals, consider next-level training,  which  can  provide  further nuance and resilience-as demonstrated in the following examples.

An empathy trainer is an individual who will teach AI systems to display compassion. This may sound far-fetched, but the startup Koko (mentioned in chapter 4), which sprung from the MIT Media Lab, has developed a machine-learning system that can help chatbots like Apple's Siri and Amazon's Alexa respond with sympathy and depth to people's questions. Humans are now training the Koko algorithm to respond more empathetically to people who might, for example, be frustrated that their luggage has been lost, that they've bought a defective product, or that their cable service keeps going on the blink even after repeated attempts to fix it. The goal is for the system to be able to talk people through a problem or difficult situation using the appropriate amount of empathy, compassion, and maybe even humor. Whenever Koko responds inappropriately, a human trainer helps correct that action, and over time, the machine-learning algorithm gets better at determining the best response.

The example highlights the potential of such leading-edge AI. Without Koko, Alexa responds to a user's anxieties by issuing canned, repetitive responses, such as "I'm sorry to hear that" or "Sometimes talking to a friend can help." With Koko, Alexa becomes much more helpful. In a verbatim transcription of Alexa with Koko responding to a person who says he's worried that he'll fail an upcoming exam, Alexa says: "Exams are really stressful but a little anxiety can help us succeed. It sharpens our minds . . . It's your body's way to prepare itself for action. It's actually a really good thing. I wonder if you could think of your nerves as your secret weapon. Easier said than done, I know. But I think you'll do much better than you think." For customer service applications that use AI, a well-trained, empathetic bot could differentiate you from the competition.

In addition to learning empathy, advanced AI systems will also learn to become more humanlike from personality trainers . It might seem almost flippant to suggest that AI systems will need to develop certain personalities, but consider how important a technology like Microsoft's Cortana can be in terms of establishing and maintaining a brand (as discussed in chapter 4). Take, for example, Cortana's confident, competent, and caring demeanor. Thanks to extensive training at Microsoft, Cortana comes across as helpful but not bossy. She might, for instance, learn what times a particular person might be most receptive to receiving suggestions. All this is very much in keeping with the Microsoft brand, which has long espoused user empowerment.

Personality trainers can come from a variety of backgrounds. Consider Robyn Ewing, who used to develop and pitch TV scripts to film studios in Hollywood.4 Now Ewing is deploying her creative talents to help engineers develop the personality of " Sophie," an AI program in the health-care field. Among other tasks, Sophie will remind consumers to take their medication and will regularly check with them to see how they're feeling. Personality trainers like Ewing certainly don't have typical high-tech resumes. At Microsoft, a team that includes a poet, a novelist, and a playwright is responsible for helping to develop Cortana's personality.

The proper training of bots like Cortana will become increasingly important as those applications assume the anthropomorphic faces of many brands. Some marketing experts already foresee the evolution of brands from one-way interactions (brand to consumer) to two-way relationships. In these customer interchanges, AI becomes the new face of your brand, as we detailed in the last chapter.

As chatbots and brands evolve in that way, they will need to be trained with a global perspective, a task that will be the responsibility of worldview and localization trainers. Just as employees who work abroad need to understand the cultural cues and some of the language of their international colleagues, so too do bots need to be sensitive to human variations across the globe. Worldview and localization trainers will help ensure that certain AI systems are imbued with a perspective that takes into account myriad regional differences. In certain countries, for example, people don't have the same anxieties about robots and increasing automation as people in the United States and Western Europe. The Japanese, in particular, seem to have a strong fascination and cultural affinity for robots, potentially easing the path for greater machine-human collaborations. Worldview trainers nee d to be aware of such differences. Giving chatbots cultural awareness can help avoid confusion and embarrassment and foster a sense of trust in the brand.

The training of AI systems to assume humanlike traits and global perspectives can be aided greatly by interaction modelers . These individuals help train the behavior of machines by using expert employees as models. For instance, Julie Shah, a robotics professor at MIT, has been developing robots that can shadow people in their jobs so that they can eventually perform certain tasks. One goal is that the robots will make certain rudimentary decisions-interrupting one job to complete a more crucial task, and then returning to the original job-just as a human worker would.

AI training doesn't necessarily have to be done in-house. Like pay roll, IT, and other functions, the training of AI systems can be crowdsourced or outsourced. One such third-party crowdsourcer called Mighty AI ingeniously uses crowdsourcing techniques to help train systems in vision recognition (for example, identifying lakes, mountains, and roads from photographs) and natural-language processing. The company has amassed copious amounts of training data that it can then deploy for different clients. One client has retained Mighty AI to teach its machine learning platform to extract intent and meaning from human conversations. Previously, Init.ai, another AI company, had attempted to do the training itself by having employees stage dialogue to come up with sample conversations, but that approach was difficult to scale, a limitation that eventually led Init.ai to outsource the work.

Working with Mighty AI, Init.ai created complex tasks from customizable templates by relying on the help of a community of prequalified users. With the appropriate domain knowledge, skills, and specialties, those users could chat with each other in various role-playing scenarios, approximating real-life interactions between customers and company employees. Init.ai could then utilize the resulting data to build its own conversation models, from which the company could then train its machine learning platform.

Clearly, AI systems will only be as good as the data they are trained on. These applications search for patterns in data, and any biases in that information will then be reflected in subsequent analyses. It's like garbage in, garbage out, but the more accurate saying would be biases in, biases out. In an intriguing experiment, computer scientists at DeepMind, a Google owned firm, trained an AI system to play two different games: one that involved hunting and another that focused on fruit gathering. The results were striking. When trained on the hunting game, the AI system later exhibited behavior that could be "highly aggressive." When trained on the fruit-gathering game, it instead later displayed a much greater tendency toward cooperation.

That's why the role of data hygienist is crucial. Not only do the algorithms themselves need to be unbiased, but the data used to train them must also be free from any slanted perspective. In the coming years, data hygienists will become increasingly important as companies use information from a variety of sources: biometrics, satellite images, traffic data, social media, and so on. Much can be data ''exhaust," that is, i n formation created as a by-product of another process . Think of all the daily data that's generated on Facebook.

Leading-edge companies have been quick to explore the potential uses of data exhaust in this new era of big data. The hedge fund BlackRock, for example, has been analyzing satellite i mages of China to better understand the industrial activity in that country. These types of analyses have even led to a new type of financial instrument: "quantamental " funds, which rely on sophisticated machine-learning algorithms to analyze traditional financial information as well as data exhaust in order to predict the value of certain assets in the market.7 Such innovative applications require the expertise of data hygienists, who often working in conjunction with sustainers (detailed later i n the chapter)-must not only convert data exhaust into a form that's suitable for input to an AI system but also ensure that that information is free of any noise or hidden biases.

Explainers

The second category of new jobs needs to bridge the gap between technologists and business leaders . These jobs will become more important as AI systems become increasingly opaque. Many executives have already become uneasy with the black-box nature of sophisticated machine-learning algorithms, especially when those systems recommend actions that may go against the grain of conventional wisdom or that could be controversial. 

Consider ZestFinance, which helps lenders better predict credit risk and expand financing to borrowers who might not ordinarily qualify. The company enables lenders to analyze thousands of data points on an applicant, well beyond the standard FICO scores and credit histories traditionally used, and applies cutting-edge AI technology to arrive at a yes-or-no decision.The average annual income of an applicant is around $30,000, and many have a history of defaults. The loans are typically small, averaging $600, with high interest rates.

Given the nature of its business, ZestFinance's customers need to be able to explain the inner workings of the AI system they use to approve loans. The company has described how it ranks applicants with respect to various categories, such as veracity, stability, and prudence. If someone's reported income is much higher than similar peers, then his veracity score is lowered. If he has moved a dozen times over the past few years, then his stability score takes a hit. And if he doesn't take the time to read all the conditions of the loan before applying for it, then his prudence score suffers. Then, a suite of algorithms, each performing a different analysis, analyzes all that data. One of those algorithms, for example, checks whether certain information might signal larger events, such as a missed payment resulting from a person's illness. All that analysis then results in a ZestFinance score that ranges from zero to a hundred.

Those sophisticated algorithms have enabled ZestFinance to uncover numerous interesting correlations. For example, the company has discovered that, for whatever reason, people who use all capital letters to fill out their loan applications tend to be riskier borrowers. Such results have enabled the firm to continually shave fractions of percentage points off its default rate, enabling it to serve consumers that wouldn't traditional qualify for loans. B ut the point here is that ZestFinance is able to explain how it makes its loan decisions, as it approves roughly one-third of the people who apply.

As companies rely on increasingly advanced A I systems to determine their actions, especially those that affect consumers, they need to be able to explain and justify those decisions. Indeed, governments have already been considering regulations in this area. For example, the European Union's new General Data Protection Regulation, slated to take effect in 2018, will effectively create a "right to explanation," allowing consumers to question and fight any decision that affects them and that was made purely on an algorithmic basis.

Companies that deploy advanced AI systems will need skilled employees who can help explain the inner workings of complex algorithms. One such person will be the algorithm forensics analyst, responsible for holding any algorithm accountable for its results. When a system makes a mistake or when its decisions lead to unintended negative consequences, the forensics analyst must be able to conduct an autopsy to understand the causes of that behavior so that it can be corrected. Certain types, like " falling rule list" algorithms, which deploy a specific ordered list of if-then rules, are relatively straightforward to explain. Others, like deep-learning algorithms, are not that simple. Nevertheless, the forensics analyst needs to have the proper training and skill set to examine in detail all the algorithms deployed by the organization.

Here, techniques like the Local Interpretable Model-Agnostic Explanations ( LI ME) can be extremely useful. LIME doesn't care about the actual AI algorithms used. In fact, it doesn't need to know anything about the inner workings of that system. To perform an autopsy of any result, it makes slight changes to the input variables and observes how they alter that decision. With that information, LIME can highlight the various data that led to a particular conclusion. So, for instance, if an expert HR system has identified the best candidate for a particular R&D job, LIME can identify the variables that led to that conclusion (such as education and deep expertise in a particular narrow field) as well as the evidence against it (such as inexperience in working on collaborative teams). Using such techniques, the forensics analyst can explain why someone was denied credit, or why a manufacturing process was halted, or why a marketing campaign was targeted toward only a subset of consumers.

Even before the need to conduct autopsies, though, companies should have a transparency analyst responsible for classifying the reasons a particular AI algorithm acts as a black box. Different reasons produce different levels of transparency and auditability. For instance, some algorithms are intentionally designed to be black boxes to protect proprietary intellectual property, while others are black boxes due to the complicated nature of the code or the scale of data and decision making the algorithm is managing. 9 A transparency analyst is someone who classifies systems and maintains a database or library of information about a system's accessibility.

That dat abase will be invaluable to the explainability strategist. These individuals are responsible for making important judg­ment calls about which AI  technologies might best be deployed for specific applications. A huge consideration here is accuracy versus "explainability." A deep-learning system, for  example, provides a high level of prediction accuracy, but companies may have difficulty explaining how those results were derived. In contrast, a decision tree  may not lead to results with high prediction accuracy but will enable a significantly greater explainability. So, for instance, an internal system that optimizes a supply chain with small tolerances for scheduling deliveries might best deploy deep-learning technology, whereas a health­ care  or consumer-facing application that will have to stand up to considerable regulatory scrutiny may be better off utilizing falling rule list algorithms.

In addition, the explain ability strategist might also decide that, for a particular application, the company might be better off avoiding the use of AI altogether. Instead, the best option might be a traditional rules engine. To make such decisions, the explainability strategist must take into account not just technological issues but also financial, legal, ethical, and other key considerations.

Sustainers

In 2015, a robot in a Volkswagen plant in Germany grabbed a worker and fatally crushed him. That tragic death highlighted the societal concerns over our growing reliance on automated tools. Ever since computers began to assume increasingly complex tasks, people's fears have heightened about the possibility of the machines running amok. From HAL in 2001 : A Space Odyssey to the cyborgs of The Terminator movie series, popular culture has only stoked the anxieties of the general public. As it turns out, that robot in Germany didn't maliciously turn on the worker and attack him. Initial reports were that a programming error-in other words, a human mistake-was the cause.

While this horrific accident is an extreme example, ensuring the proper use of AI is a primary responsibility of the final category of new jobs-sustainers-who must continually work to ensure that AI systems are functioning properly as tools that exist only to serve us, helping people in their work to make their lives easier. By doing so, sustainers will help allay fears of a dystopian future in which robots become sentient and overtake society.

Obviously, one of the best ways to ensure that sophisticated robots and other AI systems are functioning as intended is to design them right in the first place. This is where companies need the expertise of experienced context designers. When developing a new system, these individuals take into account a variety of contextual factors, including the business environment, the process task, the individual users, cultural issues, and so forth. Even seemingly small details can be important. When General Motors and Fanuc were designing a new flexible manufacturing robot that would work alongside humans, they struggled with what color paint to use. Orange seemed to imply danger, and yellow was interpreted as caution. In the end, the engineers settled on a lime hue that they called "safety green."

"Of course, even well-designed systems can result in problems, and sometimes the issue is that the technology is functioning only too well, resulting in unintended harmful behavior. Years ago, the noted science fiction author Isaac Asimov listed his "Three Laws of Robotics": 

A robot may not injure a human being or, through inaction, allow a human being to come to harm.

A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.

A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.

Introduced in the 1942 short story "Runaround," the three laws are certainly still relevant today, but they are merely a starting point. Should, for example, a driverless vehicle try to protect its occupants by swerving to avoid a chi ld running into the street if that action might lead to a collision with a nearby pedestrian? Such questions are why companies that design and deploy sophisticated AI technologies will require AI safety engineers . These individuals must try to anticipate the unintended consequences of an AI system and also address any harmful occurrences with the appropriate urgency.

In a recent Accenture survey, we found that less than one third of companies have a high degree of confidence in the fairness and auditability of their AI systems, and less than half have similar confidence in the safety of those systems. Moreover, past research has found that about one third of people are fearful of AI, and nearly one-fourth believe the technology will harm society. Clearly, those statistics indicate fundamental issues that need to be resolved for the continued usage of AI technologies. That's where sustainers will play a crucial role.

One of the most important functions is that of ethics compliance manager. These individuals will act as watchdogs and ombudsmen for upholding generally accepted norms of human values and morals. If, for example, an AI system for credit approval is discriminating against people in certain geographic areas, then the ethics compliance manager is responsible for investigating and addressing that ethical-and potentially legal-breach. Other biases might be subtler, for example, a search algorithm that responds with images of only white women when someone queries " loving grandmother." The ethics compliance manager might work with an algorithm forensics analyst to uncover the reasons for those search results and then implement the appropriate fixes.

In the future, AI itself will play an increasingly important role to ensure that advanced systems are operating within human ethical and moral bounds. Mark 0. Riedl and Brent Harrison, researchers at the School of Interactive Computing at Georgia Institute of Technology, have developed an AI prototype named Quixote that can learn about ethics-that, for example, one shouldn't steal-by reading simple stories. According to Riedl and Harrison, the system is able to reverse-engineer human values through stories about how humans interact with one another. Such stories reflect a culture and society, encoding "commonly shared knowledge, social protocols, examples of proper and improper behavior, and strategies for coping with adversity." By reading myriad stories, Quixote has learned that, for instance, striving for efficiency is fine except when it conflicts with other important considerations. Even given such innovations, though, human ethics compliance managers will still need to monitor and help ensure the proper operation of those sophisticated systems.

An AI system could be technically proficient and ethical, but still be detrimental to an organization. That's why companies will need automation ethicists . These individuals will be responsible for evaluating the noneconomic impact of AI systems. One important issue is people's general acceptance for these new technologies. Employees are naturally fearful of losing their jobs to an automated application that performs just as well, if not better, than a human could.

Such emotions can be especially powerful in response to robotic AI systems. Masahiro Mori, a Japanese robotics expert, in a study of how we respond to robots, has discovered an interesting effect. As a robot becomes more lifelike, our affinity and empathy for it increases until a certain point. Then, as the robot becomes more like us, we quickly become repulsed by any slight imperfections. But as those imperfections are fixed and the robot becomes less distinguishable from a human, our positive emotions toward it grow again, eventually approaching an empathy level similar to that of one human toward another. Mori labeled the sudden drop the " uncanny valley," a phenomenon that can impede the success of human-to-robot interactions in the workplace. Automation ethicists need to be aware of such phenomena.

In general, AI systems that perform well should be promoted, with variants replicated and deployed to other parts of the organization. On the other hand, AI systems with poor performance should be demoted and, if they can't be improved, they should be decommissioned. These tasks will be the responsibility of machine relations managers-individuals who will function like H R managers, except that they will oversee AI systems, not human workers. They will work within a "machine relations department" and regularly conduct performance reviews of all the AI systems that the organization deploys. The machine reviews will consider myriad factors, "including the objective performance of the AI systems as well as various soft goals, such as striving to fulfill organizational values like increased   diversity and a commitment toward improving the environment."

When the Challenges Are Human

The issues we've raised in this chapter are a starting point. We out­ lined only a handful of new roles that will emerge as AI extends into more mission-critical business processes. The roles we've described are merely a glimpse of the many new types of jobs that will be created in the future. Indeed, as organizations grow with their human and machine teams, they'll inevitably develop their own specialized versions of trainers, explainers, and sustainers. These emerging jobs-which demonstrate the importance of human skills in the missing middle-require that leaders think differently about the needs of human and machine teams. (This is both the mindset and leadership parts of our MELDS framework.) For instance, the new jobs will require a range of education, training, and experience. Empathy trainers, for example, may not need a traditional college degree. Individuals with a high school education and who are inherently empathetic can be taught the necessary skills through in-house training programs. Many of the new positions may lead to the rise of a "no collar" workforce that slowly evolves out of traditional blue-collar jobs in manufacturing and other professions.

On the other hand, a number of the new jobs, like ethics com­pliance manager, will require advanced degrees and a specialized skill set. For example, earlier in this chapter we described various training roles, and the most advanced companies have already been adapting their training processes by adopting techniques from the child developmental psychology field.

The bottom line is that companies deploying AI systems need to rethink their talent and learn ing strategies in order to better attract, train, manage, and  retain these individuals. And it's clear that the AI will require new capabilities , policies, and pro­cesses-not just in IT, but across the business. We will discuss the corresponding managerial issues in greater detail in chapter 7. After all, as with so many new technologies, the challenges are often more human than technical.

6: Super Results from Everyday People: Three Ways AI Unleashes New Levels of Productivity

If you want to build a chair from scratch, you must first invent the universe. Not so easy, right? We paraphrased this idea from Carl Sagan, who, in his famous quotation, actually referred to baking an apple pie, not building a chair, but his thinking still holds. Sagan simply meant that no seemingly straightforward task would be possible without the laws of nature that underpin it. Put another way, there is a universe of physics and math inside every apple pie and inside every chair. Luckily for bakers and designers alike, their creative process assumes that most of the useful universe is already figured out. The tough parts-the alchemical combination of ingredients or the angles of a chair's legs-are hidden away. They get folded into a trusted recipe, say, or CAD software.

But what if that trusted recipe and standard software are somehow holding back our potential for novel, more interesting, possibly superior pies and chairs? What if we could make tools that can help open the universe again? Help creative people break out of old habits or away from conventional wisdom, but without the burden of truly inventing the universe every time?

These tools do exist today; the Elbo Chair from designers at Autodesk is a prime example of what's become possible. The Elbo Chair is a striking piece of furniture because it's both beautiful and has a provenance unlike any other chair. Its simple frame is made of walnut, which highlights its natural, organic lines. The two front legs seen1 to grow from the bottom, and where they hit the seat, they gently curve back to form smooth, sloping armrests that then meld into the single-panel, horizontal, chair back. The two rear legs also seem to grow from below and then, at the seat, fork up and forward into three thinner branches; two support an armrest, and one on each side holds up the chair back. Adding to the natural look are subtle curves and ripples in the seat and arms near the joints. It's as if a wise, willowy tree were asked to design a chair for a person, and this is what it thought up.

Perhaps even n1ore compelling than the aesthetics, though, 1s the fact that the Elbo Chair's design can1e from designers working in conjunction with AI-enabled software. The team at Autodesk that designed the Elbo Chair used Dreamcatcher software's generative design features to crack open previously unavailable design space-hundreds of potential chair shapes all the while adhering to precise engineering specifications. The seat was to be eighteen inches off the ground, and the structure would need to hold three hundred pounds. It was to be inspired by Hans Wegner's Round Chair and the well-known Lambda Chair. The machine-learning-powered generative design began with a hybrid model of the two chairs and produced a large volume of unexpected shapes that matched the engineering criteria. Along the way, the designs morphed and changed, as if the chair itself was an evolving biological system. And the designers became curators wielding their particular aesthetic tastes and intuitive preferences to choose one of the hundreds of millions of possible chairs they found the most satisfying. In the end, their final selection was the Elbo Chair, a design that required 18 percent less material than the original model the team started with.

Generative design software is a whole new way to approach design, said Jeff Kowalski, Autodesk's chief technology officer. "These technologies are not a threat, they're more like superpowers."

Superpowers, indeed. Suddenly a designer can see design possibilities so different from what she might have considered before-an unprecedented view of the design space and a whole new universe of options. And these computer-generated designs might spur her own ideas even further. But what becomes of the human designer, who now has an extra creative appendage? In this scenario, she gets to be an operator, curator, and mentor to this assistive, AI design agent. Just like that, the design process is reimagined.

Welcome to the right-hand side of the missing middle (see figure 6-1), where machines augment humans. Artificial intelligence tools are empowering workers in a range of fields, from design to medicine to engineering to factory-floor operations. This augmentation comes in a variety of forms-from augmented reality and virtual reality to analytics engines to robot arms and chatbots. But what are the workforce implications of being empowered or augmented by AI? How is introducing AI to a workplace different from the device and technology management that companies already do, things like handing out laptops, software, and log-in information during new-hire orientation? This chapter argues that AI tools don't just automate routine workplace tasks although they can do that, too-but- that they create a symbiotic relationship between people and machines that upends the standard work flow. These new hybrid roles and relationships in the missing middle can offer managers an entirely new lens on their processes and equip people with powerful, new capabilities.

Three Types of Augmentation

AI augmentation and its reshaping of business processes is happening now, across the three categories of human-machine interaction: amplification, interaction, and embodiment.

In the case of amplification, AI agents give people extraordinary data-driven insights, often using real-time data. It's like your brain, but better.

The Elbo Chair example highlights some possibilities of amplification: generative design software expands the design space beyond what a person might be able to imagine. Other businesses are using worker-amplification tools for analyzing customers' sentiments when they interact with companies on Facebook and Twitter, for giving narrative writing advice, and for moderating online comments so internet conversations are constructive and civil. Drug companies are using amplification to monitor the quality control of pharmaceutical drugs after they've been released to the general population. And radiologists are getting help from software that learns the way they hang X-rays and provides patient health data in an easy-to-see format to speed up and improve the accuracy of diagnoses. These workers are all using AI to enhance the effectiveness of their activities and humans' decision-making process.

In the case of interaction, Al agents employ advanced interfaces such as voice-driven natural-language processing to facilitate interactions between people or on behalf of people. These AI agents are often designed to have a personality, and they can function at scale-that is, they can assist many people at once. You see them in personal assistant roles and in customer service. IPsoft's help-desk agent Amelia (called "Aida" in SEB's application, which is described in chapter 2 ) is an example of an AI agent that operates in the interaction domain.

Embodiment is the third category. While both amplification and interaction are mostly in the software realm, using interfaces that can, in some scenarios, seem almost invisible, embodiment is in tangible, physical spaces. It's AI in combination with sensors, motors, and actuators that allow robots to share workspace with humans and engage in physically collaborative work. These robots are on factory floors and in warehouses with people. They come in the form of robot appendages, package-carrying autonomous carts, and drones that deliver medicine.

Car companies, in particular, are using the concept of embodiment in their state-of-the-art manufacturing lines. Thanks to lightweight, context-aware robot arms and "cobots" designed to work closely with people on the line, manufacturers are able to reimagine previously static processes. Meanwhile, workers take on new roles when they collaborate with these smart machines, and businesses can make more varied, adaptable choices about the kinds of products they offer their customers.

In all three types of missing-middle interactions amplification, interaction, and embodiment-companies are gaining not only super-powered employees but also a whole new way of thinking about the ways they run their businesses. AI augmentation allows workers to perform more human, less robotic activities. As certain tasks shift from human to machine, and humans, often working with AI assistants, are able to perform different kinds of work, companies are prompted to reimagine their business processes around entirely new human-machine capabilities. What's more, new augmentation-based relationships demand new kinds of human-computer interfaces. What user interfaces (UI ) ,vill dominate in the missing middle? Is AI the new UI? How might augmentation affect your industry? This chapter provides examples of companies that have reimagined their processes around machine-enabled superpowers and addresses some of these questions along the way.

AI Agents That Amplify

Autodesk's Dreamcatcher software uses genetic algorithms to iterate through possible designs. It offers a great example of the way a process changes when a smart agent is at a worker's side. Traditionally, when a designer wants to create a new object-a chair, a bike stem, an airplane partition-she starts by conducting research, sketching ideas, and moving between sketches, computer models, and physical prototypes. There are many iterations, during which the designer is making mental calculations, the equivalent of qualitative hunches, that nudge designs in one direction or another.

With AI, these mental calculations are offloaded to software, which enables the design process to be reimagined so that it can more fully focus on human creativity and aesthetics. Here, the designer begins by setting parameters, and the software proceeds to move through iterations at a quick pace. As the software produces its designs, the designer can further refine her parameters to see where it might lead the outcome. She's essentially steering the design, ultimately deciding on the final design. The process changes, then, from something that's somewhat clunky, slow, and limited ( depending on other resources available to the designer), to one in which the designer is more often enacting her most valuable skill-her judgment and design sense. This adaptive and organic approach contrasts with the traditional design process, which was governed by iterations of predetermined steps.

Of course, it's not just designers whose work and processes are getting a boost fi􀍪om AL Philips has a software tool for radiologists called Illumeo. One of the software's features is that it includes contextual information about a patient alongside the images, so a radiologist doesn't have to hunt for laboratory results or prior radiology reports, for instance. B ut perhaps most impressively, the software is context aware in a number of areas. For instance, it can recognize the anatomy of the radiological images and automatically suggest the correct tool set-such as one that can measure and analyze blood vessels. The software is also able to learn how the radiologist prefers to look at the images, what's known as a radiologist's hanging protocol. Illumeo is a good example of how AI agents can slide into a preexisting interface-silently observing and learning the preferences of the person using a software tool, for instance, and incorporating that personalized information into the user experience. Illumeo is leveraging AI in its UI so that the relationship between worker and machine is adaptable and improves over time.

So far, we've focused on amplification in office jobs, but workers in the field are also benefiting from amplification, thanks to AI-enhanced user interfaces. In particular, AI tools like smart glasses that provide an experience of augmented reality are overhauling maintenance work and in-field training: the glasses overlay digital information or instructions on a worker's field of view.

At a global industrial services company, the typical process for wiring a wind turbine's control box requires a technician to move between the control box and the hard copy of an instruction manual. But with an augmented-reality-enabled (AR), hands-free, heads-up display, the instructions can be visually projected on top of a technician's workspace. In a side-by-side comparison with the traditional instruction manual method, the AR headset was found to improve the worker's performance by 34 percent on first use. With no need to spend time ramping up or training to use the new technology, efficiency gains are immediate. A similar example from Boeing demonstrates efficiency improvements of 25 percent, and other cases show an average productivity improvement of 32 percent.

Agents of Interaction

We first described Aida in chapter 2, where it was a part of the virtual help desk at the Swedish bank SEB. Over time, SEB trained and tested Aida to the point where the bank was confident enough in the system to allow it to be part of the processes that natural-language AI system that's used for various applications from banking and insurance to travel and health. At Accenture, for instance, Alice is an agent that uses Watson to answer common requests, and Colette is another Watson-based agent that answers home buyer's questions about mortgages.

Interaction agents needn't simply be a voice on the phone or an icon on a computer screen. Some agents have physical forms as well. By now, most people are familiar with consumer-facing, natural-language gadgets like Amazon's Alexa, Google Home, and Apple's Siri. But there are others; BQ Zowi, a squat, tabletop robot originally designed for children, has open architecture and re-programmability that makes it ripe for various other uses. It's now able to answer questions for some banking customers via chatbots. Similarly, the small humanoid robot Nao uses IBM Watson's Q&A service to answer questions about banking, travel, and health, and technical queries about the company's systems, applications, and products.

In these examples of interaction, the software or robotic agent has access to vast stores of data and uses a natural-language interface to quickly access and disseminate that information. Companies that serve the needs of many customers at once can benefit from interaction modality in the missing middle. When interaction is well understood, it can revamp the customer service process, not only in customer service centers, but at points of sale and inside consumers' homes. Interaction can also alleviate previously tedious, repetitive tasks for workers. Once these tasks are gone, management and leadership can reimagine workers' processes around unusual, interesting, more nuanced customer service situations.

Rubbing Elbows with Robots

Amplification and interaction are missing-middle categories that mostly augment the mind. Embodiment, in contrast, deals with physical augmentation. Examples arc often found in manufacturing, such as in the Mercedes-Benz plant in southwest Germany. This facility processes fifteen hundred tons of steel a day, pumping out more than four hundred thousand vehicles a year. With these numbers, you might expect a well oiled, robot-dominated assembly line operating with as few people as possible. But Mercedes is ditching some of its robots and redesign ing its processes to center them around people. The automotive assembly line is changing.

The driver of this change is the rise of customizable cars. You can now go online and choose from an expansive array of features on your next car. Gone are the days of identical Model Ts coming off an assembly line. Even the trend to make cars in three standard trims-common in the United States-is falling out of favor.

With so much variation in car manufacturing, the only way to assemble cars fast enough is to bring people back. "We're moving away from trying to maximize automation, with people taking a bigger part in industrial processes again," says Markus Schaefer, head of production planning at the Mercedes. "When we have people and machines cooperate, such as a person guiding a part automatic robot, we're much more flexible and can produce many more products on one production line. The variety is too much to take on for the machines."

Instead of dividing manufacturing plants into a heavy-lifting robot section, usually fenced off from people for safety reasons, and another area for people to dexterously fiddle with wires and perform more delicate tasks, a new breed of robots is enabling humans and robots to work side by side or in collaboration. These cobots are built with smart software that learns over time and sensors that allow it to adapt to the situation at hand and be responsive to people. In practice, this means that the cobot takes on repetitive and precision tasks as well as the heavy lifting, while a person brings the brains and dexterity to the operation. Cobots, in this way, are literally extending the workers' physical capabilities.

Research from MIT corroborates that this kind of human-machine collaboration is effective for business. In a study with Mercedes' competitor BMW, researchers determined that human-robot interactions in the car plant were about 85 percent more productive than either humans or robots on their own.

At the Mercedes plant, a worker grabs a console with buttons and a visual display, which he uses to guide a robot arm to pick up a heavy wedge of steel that will make up the floor of a car's trunk. The robotic system is equipped with sensors to see its environment and software that can quickly deliver instructions to the robot's actuators if, for instance, a person steps in the way or the car isn't perfectly positioned. This type of cobot system has the worker in control, guiding the building of each car. Industrial work becomes decidedly less manual and more like being a pilot, where the robot becomes an extension of the worker's body.

The cobot arrangement is good for manufacturing because it allows for flexibility and adaptability in processes, and it appears to be good for people, too. One worker who collaborates with cobots at SEW-Eurodrive, a manufacturer of motors, describes the work environment: "This is more satisfying because I am making the whole system. I only did one part of the process on the old line."

Cobots are good for human ergonomics, too. At a BMW plant in South Carolina, engineers analyzed their car-making processes to see how a lightweight robot arm could fit alongside its peopled production line. They found that the act of fixing the door panel, which safeguards the electrical wiring, is best done by a cobot on the line. Previously performed by humans, it was a task that had produced wrist strain. What's more, people seemed to be less consistent at the task than they were at others.

Now a person loosely fits the door panel and then the door moves along the line to the nearby cobot to finish the job. The cobot is outfitted with cameras and other sensors so it can tell when a person is close. Unlike traditional industrial robots that perform set movements over and over without awareness of their surroundings, the cobot in the BMW plant deftly avoids knocking into people or getting in the ·way. Furthermore, these robots can be reprogrammed by nonprogrammers using a tablet. N o coding skills are needed. And because they are lightweight, they can be moved anywhere in a warehouse and perform various tasks, depending on the need.

The human-robot system then acts as an extender of people's ability to work; they're now less likely to get fatigued or injured. Suddenly a factory job isn't only for workers in their physical prime. Embodiment, as demonstrated in the human-robot systems that many manufacturers are using, is opening up more job opportunities: some people who may have ruled out manual labor as a work option-because of age or physical condition-may be able to do the work with the aid of cobots.

A similar collaborative dynamic is on display at warehouses. At Amazon fulfillment centers, shelves full of merchandise seem to independently glide down the warehouse aisles toward a human worker who's awaiting the delivery. The shelves are carried by squat, rolling robots destined to bring the goods to the worker, who plucks the items off the shelves and puts them in a box to ship. Computer vision helps the robots know where they are in the warehouse, sensors keep them from running into each other, and machine-learning algorithms help them determine the best paths and right-of-ways on a warehouse floor full of other robots. The human worker no longer needs to walk miles a day to retrieve goods for packaging.

In another example of embodiment, drones are being tested to deliver health care, on demand, to remote parts of Rwanda, out of reach of traditional medical options. A company called Zipline is pioneering the technology, targeting one of the leading causes of death-postpartum hemorrhage-with a delivery of blood for transfusion.

Drones have become a particularly interesting application of AI: computer vision and smart algorithms process video in real time, thereby allowing people to extend their vision and delivery capabilities up into the air and over miles of potentially impassable terrain.

In a project similar to Zipline, Doctors Without Borders has experimented with using a small quadcopter drone from a company called Matternet. 1'he drone takes lab samples of patients with suspected tuberculosis from remote health centers in Papua New Guinea to a major hospital for testing. Another organization called Wings for Aid is using unmanned drones to drop off supplies in difficult-to-reach areas when a natural disaster has struck.

In the near term, at least, it looks as if some of the most effective ways that robots can be deployed are in conjunction with people. Robots are skilled at heavy lifting and are superb at repetitive tasks. People are good at adapting on the fly, making judgment calls, and using their hands to manipulate wires, textiles, or other tricky materials. The collaboration between the two is changing the way industries think about their people and their processes.

From Task Replacement to Process Change

In all three categories on the right side of the missing middle amplification, interaction, and embodiment-we see that AI offers significant improvements to the way people work, giving them new superpowers. Combine this with the three categories on the left side of the missing middle-training, explaining, and sustaining, which highlight the ways that workers improve the effectiveness of AI-and we start t o see the coming shift

To tap the full potential of a human-machine workplace, organizations need to recognize that these six new ways of working demand a complete reimagination of business processes. Specifically, when a designer can choose from thousands of unexpected and unusual chair designs-all retaining important structural requirements-AI has opened up a universe of creative options that was previously closed. When a carmaker can reimagine its factory floor so that people and robots are working in concert, AI has not only facilitated the creation of highly customized cars at scale, requiring a designer that must have the training and skills to take advantage of that freedom-it has essentially transformed the assembly line into a collaborative robot-human workspace.

In some situations, as with the technicians who wear heads up displays while wiring wind-turbine boxes, these innovations can reduce the time it takes to get the job done by a third. But in others, as with Stitch Fix, mentioned in the introduction to part two, entirely new business models are emerging on the backs of AI technologies. When AI augments workers, we don't just see small, incremental revenue bumps or efficiency gains. We see safer, more engaged workers who can do the work well that they do best. In organizations, AI augmentation opens up the possibility to fundamentally rethink business processes to uncover hidden gains, embolden workers, and discover brand-new business models for this new age. But what are the managerial implications for companies deploying such innovations? How do you train and educate people for the new kinds of workplace interactions? What new skills are needed to work well with AI? The next two chapters look at these and other questions through the lens of MELDS. Chapter 7 will focus on mindset, experimentation, leadership, and data; chapter 8 will concentrate on skills

7: A Leader's Guide to Reimagining Process: Five Steps to Getting Started

8: Extending Human + Machine Collaboration: Eight New Fusion Skills for an AI Workplace

In our research, we see a strong recognition and en1brace of the notion that work is changing in profound new ways. In our Accenture Research global survey on the Future of Work, done in collaboration with the World Economic Forum, we found that 64 percent of workers recognize that the pace of change is accelerating as a result of new technologies like A I. And while nearly all (92 percent) believe the next generation of workplace skills w ill look radically different, most (87 percent) believe new technologies like A I w ill improve their work experience within the next five years. Moreover, 85 percent of workers are willing to invest their free time over the next few years to learn new skills, while another 69 percent place a premium on finding on-the job training opportunities where the training is relevant to the future digital needs of the enterprise.

FUSION SKILL #1: Rehumanizing Time
Definition: The ability to increase the time available for distinctly human tasks like interpersonal interactions, creativity, and decision making in a reimagined business process.

FUSION SKILL #2: Responsible Normalizing
Definition: The act of responsibly shaping the purpose and perception of human-machine interaction as it relates to individuals, businesses, and society.

FUSION SKILL #3: Judgement Integration
Definition: The judgment-based ability to decide a course of action when a machine is uncertain about what to do.

FUSION SKILL #4: Intelligent Interrogation
Definition: Knowing how best to ask questions of AI, across levels of abstraction, to get the insights you need .

FUSION  SKILL  #5 : Bot-based Empowerment
Definition: Working well with AI agents to extend your capabilities, and create superpowers in business processes and professional careers.

FUSION  SKILL  #5  Holistic Melding
Definition: The ability to develop robust mental models of AI agents to improve process outcomes.

FUSION  SKILL  #7 : Reciprocal Apprenticing
Definition: (1) Performing tasks alongside AI agents so they can learn new skills; (2) on-the-job training for people so they can work well within AI-enhanced
processes.

FUSION  SKILL  #8: Relentless Reimagining
Definition: The rigorous discipline of creating new processes and business models from scratch, rather than simply automating old processes.

Opportunities in Neural Opportunism

The concept of fusion skills-abilities for combining the relative strengths of a human and machine to create a better outcome than either could alone-aligns well with research from cognitive science. Neural opportunism-the idea that people naturally use technologies to augment themselves-as well as ideas of extended intelligence and embodied intelligence are relevant here. Humans, as research has shown, incorporate tools and technologies as part of our own cognition. 24 From eyeglasses to bicycles to fighter jets, these tools, when we use them often enough and at an expert level, can feel like extensions of our bodies and minds. Artificial intelligence brings another dimension to this kind of bio-technical symbiosis: smart machines are themselves neurally opportunistic with regard to their own unique strengths. By design, they collect information about their surroundings and incorporate it into their own cognition. So these eight skills, the "S" part of MELDS, put emphasis on a new kind of relational competence that's rarely if ever discussed in today's economic analyses or corporate talent development programs. Fusion skills require a novel way of thinking about human expertise and, by extension, a very different approach to educating and retraining the workforce.

CONCLUSION: Creating Your Future in the Human + Machine Era

When it comes to discussions about artificial intelligence, much of the conversation tends to focus on job displacement and fear that computers will one day take over the world. The underlying assumption is that humans and machines are competitors, and that AI systems, with their superior speed, processing power, and stamina in so many contexts, will directly replace us in companies-and perhaps even outside the workplace as well.

A number of quantitative economic studies have only heightened those fears. As one study concluded, "absent appropriate fiscal policy that redistributes from winners to losers, smart machines can mean long-term misery for all."1 But such quantitative studies have typically focused on broad industry trends, and in doing so they have missed what's been happening behind the scenes in everyday processes and practices.

From our own research-including observational and case analysis across 450 organizations within our sample of 1,500- we were able to identify a number of important phenomena that the quantitative surveys have failed to capture. One is the concept of "fusion skills": humans and machines coming together to form new kinds of jobs and work experiences. This forms the "missing middle" that has been absent in much of today's polarizing jobs debate that has pitted humans on one side and machines on the other. And it's within this missing middle that leading edge companies have been reimagining their work processes, achieving outsize improvements in performance. To obtain such results, though, executives must lead their organizations through the transformation by making the necessary investments, including retraining workers to fill those missing-middle roles.

Doing Different Things, and Doing Things Differently

To extrapolate how companies will be making the transition to a new era of human + machine, we first needed to understand how executives are deploying AI in their businesses today. In part one of this book we described various applications in manufacturing and supply chain (chapter 1), back-office operations (chapter 2 ), R&D and business innovation (chapter 3 ), and marketing, sales, and customer service (chapter 4). Studying those applications gave us a clear view of the future, enabling us to identify the different ways in which companies were filling the missing middle by creating new, enhanced jobs that have cracked open novel economic and employment opportunities.

Specifically, our research discovered just how different these new jobs were from traditional kinds of work. Already, today, 6 1% of activities in the missing middle require employees to do different things and to do things differently-hence the crucial need for companies to reimagine their processes and reskill their employees. As we discussed in chapter 5, these different things include training data models, or explaining and responsibly sustaining an AI system's performance. And, as we described in chapter 6, these new jobs also include employees doing things differently using amplification, interaction, and embodiment to get a job done with superhuman performance. Rather than relying on economic data far removed from organizational practice, you must observe these differences directly to understand and appreciate them.

So far, though, only a small number of companies we've surveyed have begun to capture the potential of fusion skills, and in doing so they have been able to reimagine their businesses, operating models, and processes in innovative ways. T'hese firmsGeneral Electric, Microsoft, BMW, Google, Amazon, and others recognize that AI isn't your typical capital investment; its value actually increases over time and it, in turn, in1proves the value of people as well. Indeed, when humans and machines are allowed to do what each does best, the result is a virtuous cycle of enhanced work that leads to productivity boosts, increased worker satisfaction, and greater innovation. As such, these companies are leading their industries with redesigned job descriptions and learning and retraining programs, all sustained by a new set of leadership practices (described in chapter 7 ). Their early successes have proven that they're on the right path.

The pressing issue, though, is that most organizations have been slow in filling the missing middle, and the results have begun to show. In the United States, 6 million jobs remain open, and within this group, more than 350,000 manufacturing jobs go unfilled each month due to a lack of qualified workers. 2 Globally, across the twelve largest economies by GDP, 38 percent of employers report difficulty filling jobs. 3 The problem right now isn't so much that robots are replacing jobs; it's that workers aren't prepared with the right skills necessary for jobs that are evolving fast due to new technologies, such as AI. And the challenge will only grow as companies apply AI and reimagine work in other areas. For example, the largest one hundred global employers report that more than one-third of skill sets that aren't yet crucial today will be by 2020.

There is a skills gap on the digital side of manufacturing, too. As factories become increasingly high tech, they require more workers with software smarts. Siemens, for example, has recognized this and plans to hire seven thousand more people by 2020 in positions related to training and using coll aborative robotics, software engineering, and computer science. But these sorts of positions wouldn't be included in traditional reports of the job outlook in the AI era, as AI has blurred the lines between blue and white collar, new and old jobs. "People may not count those jobs in IT and software development as manufacturing jobs," says Eric Spiegel, the company's US C EO, "but they really are related to manufacturing."

Our research has found that the real issue isn't simply that humans will be replaced by machines; it's that humans need to be better prepared to fill the growing number of jobs in the missing middle. In chapter 8, we described in detail the kinds of fusion skills that have become increasingly important in this new era of human-machine collaboration. And we discussed the importance of "neural opportunism," as people will increasingly need to incorporate AI tools to extend the capabilities of their bodies and minds. In chapter 8, we described in detail the kinds of fusion skills that have become increasingly important in this new era of human-machine collaboration. And we discussed the importance of "neural opportunism," as people will increasingly need to incorporate AI tools to extend the capabilities of their bodies and minds.

Unfortunately, there is not much evidence that business or political leaders are making the necessary investments in these areas. In the United States, the 2016 White House report "Artificial Intelligence, Automation, and the Economy" notes that the nation spends only around 0.1 percent of its GDP on programs that help people adjust to workplace changes. This number has fallen over the last thirty years, and the federal readjustment programs that exist-mostly used to help people deal with coal mines or military bases that close-are not designed to help people whose jobs are lost or changed by automation.6 Results are mixed in other countries. Japan and China are among those that stand out by making significant commitments to AI education and workforce training as the core piece of long-term national AI strategies. For instance, China's State Council has the stated goal of making the nation equal among leading AI countries by 2020 and the world's "premier artificial intelligence innovation center by 2030." This development plan includes major investments in retraining workers for an economy where "collaboration between humans and machines will become a mainstream production and service mode.

A Call to Action: Reimagining Business

AI is rapidly making inroads in business. Its swift adoption means that questions on both the opportunities and risks are at the forefront of most discussions. Leaders are faced with decisions today that have profound consequences. It's here, in the practical application of AI to business, where we hope this book can offer the most help.

For years, the dream of many researchers was to create an artificial intelligence that could rival that of people. However, we're seeing that AI is instead becoming a tool t o extend our own human capabilities. In t urn, we're guiding AI systems to evolve into better tools that further extend our capabilities. Never before in history have our tools been so responsive to us, and we to our tools. As we see with fusion skills and the missing middle, the real opportunity is to make work more human, reimagine business with a more human approach, and equip people with superhuman capabilities to perform more effectively.

We believe our perspective, grounded i n people and machines working together, requires a new approach and calls for the reimagination of businesses and our business processes. AI is enabling business leaders to understand better than ever what their customers and employees need. Through AI and processes built around hybrid human-machine capabilities, organizations can take these needs into account, realizing solutions that benefit both business and people.

Our primary goal i n Human + Machine is to provide leaders, managers, and workers with the necessary tools to prepare for this coming third wave of process transformation. As we've discussed throughout these chapters, this era calls for humans and machines to fill the missing middle by working closely together i n new types of roles with a new kind of collaborative partnership. To enable that, we have described how executives must implement organizational changes to support a culture that encourages the reimagining of work process, all while investing in learning platforms and the continual retraining of employees. This obviously applies to the underlying fusion skills necessary to develop, maintain, and manage AI-enabled business capabilities, but it's also pertinent to other softer skills, such as those required to make difficult ethical decisions regarding the technology.

As our M ELDS approach shows, successful implementations of AI require more than attention to the technology itself The fundamental focus of the leadership component of MELDS is to always keep people at the center of any AI initiative, taking into account employees and customers, as well as other human stakeholders. For example, as AI is introduced into the workplace, executives need to assess the various implications: How will job requirements need to evolve, and how will any labor displacements be balanced with broader workforce considerations? What new investments in talent are needed to retain industry expertise, and which employees may need to be counseled and retrained?

There are also government regulations, ethical design standards (such as those proposed by the IEEE), and prevailing public sentiment to consider. As we've discussed, it's incumbent on companies to ensure that the AI systems they deploy aren't biased, and they need to be able to understand and explain why they made certain decisions. Executives and managers must also know which decisions are being delegated solely to machines (versus which decisions require human intervention), and there must be accountability for that process. In certain cases, transparency of the entire decision-making process might be necessary.

Lastly, firms must proactively keep maturing AI technology in line with new laws and regional policies, such as the General Data Protection Regulation (GDPR) in Europe. In particular, personal data will require special attention in different ways in different regions, especially as AI systems become increasingly capable of deriving unprecedented types of insights beyond basic demographics.

The above issues are of crucial importance, and the fate of many people, firms, industries, and countries will depend on the solutions that are chosen. As we reimagine business and our organizations through AI, there is tremendous potential to create a better future and improve the way the world works and lives; to not only increase business performance, but to implement more sustainable solutions that better utilize critical resources on the planet, and drive powerful new services and forms of interaction with consumers and workers.

In our research, we found that companies that use AI to augment their human talent while reimagining their business processes achieve step gains in performance, propelling themselves to the forefront of their industries. Firms that continue deploying AI merely to automate in a traditional fashion may see some performance benefits, but those improvements will eventually stall. We predict that within the next decade, there will emerge a vast difference between the winners and losers, a difference determined not on whether an organization has implemented AI, but on how it's done it.

And that's where the human element truly comes in. As we've shown , AI gives people powerful tools to do more, in essence to perform with superhuman capability. I n doing so, AI has the potential to rehumanize work, giving us more time to be human , rather than using our time to work like machines

We're at the cusp of a new era of business transformation , the age of AI , and our actions today have great bearing o n how the future unfolds. We hope our book has provided you with a better lens to understand the opportunities and challenges ahead, and a road map for applying AI in the work that you do. Through the responsible application of AI and the relentless reimagining of work, people can and should reap the benefits of intelligent machines. As we take these steps, it's time to discard dusty old notions of humans versus machines, and instead embrace an exciting new world of human and machine.

POSTSCRIPT FROM THE AUTHORS: Our Commitment to Skills for the Age of AI

Our goal with Human + Machine is to help people navigate the changes that AI is bringing to business, government, and the economy. We strongly believe that artificial intelligence, guided by the right management approach, will yield innovations that truly improve the way the world works and lives. This will also generate a wealth of new types of jobs in the missing middle.

However, we also recognize that AI will bring dislocation, disruption, and challenges to many. It is essential that we provide all people with the education, training, and support they need to take on the many jobs required in the missing middle. In support of this imperative, we are donating our net royalties from the sale of this book to fund education and retraining programs focused on helping people develop the fusion skills they need to be a part of the age of AI.


/ 017. Crawford, Kate. Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence, 2021.

What is AI? Neither Artificial nor Intelligent

Let’s ask the deceptively simple question, What is artificial intelligence? If you ask someone in the street, they might mention Apple’s Siri, Amazon’s cloud service, Tesla’s cars, or Google’s search algorithm. If you ask experts in deep learning, they might give you a technical response about how neural nets are organized into dozens of layers that receive labeled data, are assigned weights and thresholds, and can classify data in ways that cannot yet be fully explained.16 In 1978, when discussing expert systems, Professor Donald Michie described AI as knowledge refining, where “a reliability and competence of codification can be produced which far surpasses the highest level that the unaided human expert has ever, perhaps even could ever, attain.”17 In one of the most popular textbooks on the subject, Stuart Russell and Peter Norvig state that AI is the attempt to understand and build intelligent entities. “Intelligence is concerned mainly with rational action,” they claim. “Ideally, an intelligent agent takes the best possible action in a situation.”

"Each way of defining artificial intelligence is doing work, setting a frame for how it will be understood, measured, valued, and governed. If AI is defined by consumer brands for corporate infrastructure, then marketing and advertising have" "predetermined the horizon. If AI systems are seen as more re- liable or rational than any human expert, able to take the “best possible action,” then it suggests that they should be trusted to make high-stakes decisions in health, education, and criminal justice. When specific algorithmic techniques are the sole focus, it suggests that only continual technical progress matters, with no consideration of the computational cost of those approaches and their far-reaching impacts on a planet under strain."

"In contrast, in this book I argue that AI is neither artificial nor intelligent. Rather, artificial intelligence is both embodied and material, made from natural resources, fuel, human labor, infrastructures, logistics, histories, and classifications. AI systems are not autonomous, rational, or able to discern anything without extensive, computationally intensive training with large datasets or predefined rules and rewards. In fact, artificial intelligence as we know it depends entirely on a much wider set of political and social structures. And due to the capital required to build AI at scale and the ways of seeing that it optimizes AI systems are ultimately designed to serve existing dominant interests. In this sense, artificial intelligence is a registry of power."

"In this book we’ll explore how artificial intelligence is made, in the widest sense, and the economic, political, cultural, and historical forces that shape it. Once we connect AI within these broader structures and social systems, we can es- cape the notion that artificial intelligence is a purely technical domain. At a fundamental level, AI is technical and social practices, institutions and infrastructures, politics and culture. Computational reason and embodied work are deeply interlinked: AI systems both reflect and produce social relations and understandings of the world."

It’s worth noting that the term “artificial intelligence” can create discomfort in the computer science community. The phrase has moved in and out of fashion over the decades and is used more in marketing than by researchers. “Machine learning” is more commonly used in the technical literature. Yet the nomenclature of AI is often embraced during funding application season, when venture capitalists come bearing checkbooks, or when researchers are seeking press attention for a new scientific result. As a result, the term is both used and rejected in ways that keep its meaning in flux. For my purposes, I use AI to talk about the massive industrial formation that includes politics, labor, culture, and capital. When I refer to machine learning, I’m speaking of a range of technical approaches (which are, in fact, social and infrastructural as well, although rarely spoken about as such).

But there are significant reasons why the field has been focused so much on the technical—algorithmic breakthroughs, incremental product improvements, and greater convenience. The structures of power at the intersection of technology, capital, and governance are well served by this narrow, abstracted analysis. To understand how AI is fundamentally political, we need to go beyond neural nets and statistical pattern recognition to instead ask what is being optimized, and for whom, and who gets to decide. Then we can trace the implications of those choices.

Seeing AI Like an Atlas

How can an atlas help us to understand how artificial intelligence is made? An atlas is an unusual type of book. It is a collection of disparate parts, with maps that vary in resolution from a satellite view of the planet to a zoomed-in detail of an archipelago. When you open an atlas, you may be seeking specific information about a particular place—or perhaps you are wandering, following your curiosity, and finding unexpected pathways and new perspectives. As historian of science Lorraine Daston observes, all scientific atlases seek to school the eye, to focus the observer’s attention on particular telling details and significant characteristics.19 An atlas presents you with a particular viewpoint of the world, with the imprimatur of science—scales and ratios, latitudes and longitudes—and a sense of form and consistency.

Yet an atlas is as much an act of creativity—a subjective, political, and aesthetic intervention—as it is a scientific collection. The French philosopher Georges Didi-Huberman thinks of the atlas as something that inhabits the aesthetic paradigm of the visual and the epistemic paradigm of knowledge. By implicating both, it undermines the idea that science and art are ever completely separate.20 Instead, an atlas offers us the possibility of rereading the world, linking disparate pieces differently and “reediting and piecing it together again without thinking we are summarizing or exhausting it.”

Perhaps my favorite account of how a cartographic approach can be helpful comes from the physicist and technology critic Ursula Franklin: “Maps represent purposeful endeavors: they are meant to be useful, to assist the traveler and bridge the gap between the known and the as yet unknown; they are testaments of collective knowledge and insight.”

Maps, at their best, offer us a compendium of open pathways— shared ways of knowing—that can be mixed and combined to make new interconnections. But there are also maps of domination, those national maps where territory is carved along the fault lines of power: from the direct interventions of drawing borders across contested spaces to revealing the colonial paths of empires. By invoking an atlas, I’m suggesting that we need new ways to understand the empires of artificial intelligence. We need a theory of AI that accounts for the states and corporations that drive and dominate it, the extractive mining that leaves an imprint on the planet, the mass capture of data, and the profoundly unequal and increasingly exploitative labor practices that sustain it. These are the shifting tectonics of power in AI. A topographical approach offers different perspectives and scales, beyond the abstract promises of artificial intelligence or the latest machine learning models. The aim is to understand AI in a wider context by walking through the many different landscapes of computation and seeing how they connect.

There’s another way in which atlases are relevant here. The field of AI is explicitly attempting to capture the planet in a computationally legible form. This is not a metaphor so much as the industry’s direct ambition. The AI industry is making and normalizing its own proprietary maps, as a centralized God’s-eye view of human movement, communication, and labor. Some AI scientists have stated their desire to capture the world and to supersede other forms of knowing. AI professor Fei-Fei Li describes her ImageNet project as aiming to “map out the entire world of objects.”24 In their textbook, Russell and Norvig describe artificial intelligence as “relevant to any intellectual task; it is truly a universal field.”25 One of the founders of artificial intelligence and early experimenter in facial recognition, Woody Bledsoe, put it most bluntly: “in the long run, AI is the only science.”26 This is a desire not to create an atlas of the world but to be the atlas—the dominant way of seeing. This colonizing impulse centralizes power in the AI field: it determines how the world is measured and defined while simultaneously denying that this is an inherently political activity.

Instead of claiming universality, this book is a partial account, and by bringing you along on my investigations, I hope to show you how my views were formed. We will encounter well-visited and lesser-known landscapes of computation: the pits of mines, the long corridors of energy-devouring data centers, skull archives, image databases, and the fluorescent-lit hangars of delivery warehouses. These sites are included not just to illustrate the material construction of AI and its ideologies but also to “illuminate the unavoidably subjective and political aspects of mapping, and to provide alternatives to hegemonic, authoritative—and often naturalized and reified— approaches,” as media scholar Shannon Mattern writes.

Models for understanding and holding systems accountable have long rested on ideals of transparency. As I’ve written with the media scholar Mike Ananny, being able to see a system is sometimes equated with being able to know how it works and how to govern it.28 But this tendency has serious limitations. In the case of AI, there is no singular black box to open, no secret to expose, but a multitude of interlaced systems of power. Complete transparency, then, is an impossible goal. Rather, we gain a better understanding of AI’s role in the world by engaging with its material architectures, contextual environments, and prevailing politics and by tracing how they are connected.

My thinking in this book has been informed by the disciplines of science and technology studies, law, and political philosophy and from my experience working in both academia and an industrial AI research lab for almost a decade. Over those years, many generous colleagues and communities have changed the way I see the world: mapping is always a collective exercise, and this is no exception.29 I’m grateful to the scholars who created new ways to understand sociotechnical systems, including Geoffrey Bowker, Benjamin Bratton, Wendy Chun, Lorraine Daston, Peter Galison, Ian Hacking, Stuart Hall, Donald MacKenzie, Achille Mbembé, Alondra Nelson, Susan Leigh Star, and Lucy Suchman, among many others. This book benefited from many in-person conversations and reading the recent work by authors studying the politics of technology, including Mark Andrejevic, Ruha Benjamin, Meredith Broussard, Simone Browne, Julie Cohen, Sasha Costanza-Chock, Virginia Eubanks, Tarleton Gillespie, Mar Hicks, Tung-Hui Hu, Yuk Hui, Safiya Umoja Noble, and Astra Taylor.

As with any book, this one emerges from a specific lived experience that imposes limitations. As someone who has lived and worked in the United States for the past decade, my focus skews toward the AI industry in Western centers of power. But my aim is not to create a complete global atlas—the very idea invokes capture and colonial control. Instead, any author’s view can be only partial, based on local observations and interpretations, in what environmental geographer Samantha Saville calls a “humble geography” that acknowledges one’s specific perspectives rather than claiming objectivity or mastery.

Just as there are many ways to make an atlas, so there are many possible futures for how AI will be used in the world. The expanding reach of AI systems may seem inevitable, but this is contestable and incomplete. The underlying visions of the AI field do not come into being autonomously but instead have been constructed from a particular set of beliefs and perspectives. The chief designers of the contemporary atlas of AI are a small and homogenous group of people, based in a handful of cities, working in an industry that is currently the wealthiest in the world. Like medieval European mappae mundi, which illustrated religious and classical concepts as much as coordinates, the maps made by the AI industry are political interventions, as opposed to neutral reflections of the world. This book is made against the spirit of colonial mapping logics, and it embraces different stories, locations, and knowledge bases to better understand the role of AI in the world.

Topographies of Computation

How, at this moment in the twenty-first century, is AI conceptualized and constructed? What is at stake in the turn to artificial intelligence, and what kinds of politics are contained in the way these systems map and interpret the world? What are the social and material consequences of including AI and related algorithmic systems into the decision-making systems of social institutions like education and health care, finance, government operations, workplace interactions and hiring, communication systems, and the justice system? This book is not a story about code and algorithms or the latest thinking in computer vision or natural language processing or reinforcement learning. Many other books do that. Neither is it an ethnographic account of a single community and the effects of AI on their experience of work or housing or medicine—although we certainly need more of those.

Instead, this is an expanded view of artificial intelligence as an extractive industry. The creation of contemporary AI systems depends on exploiting energy and mineral resources from the planet, cheap labor, and data at scale. To observe this in action, we will go on a series of journeys to places that reveal the makings of AI.

In chapter 1, we begin in the lithium mines of Nevada, one of the many sites of mineral extraction needed to power contemporary computation. Mining is where we see the extractive politics of AI at their most literal. The tech sector’s demand for rare earth minerals, oil, and coal is vast, but the true costs of this extraction is never borne by the industry itself. On the software side, building models for natural language processing and computer vision is enormously energy hungry, and the competition to produce faster and more efficient models has driven computationally greedy methods that expand AI’s carbon footprint. From the last trees in Malaysia that were harvested to produce latex for the first transatlantic undersea cables to the giant artificial lake of toxic residues in Inner Mongolia, we trace the environmental and human birthplaces of planetary computation networks and see how they continue to terraform the planet.

Chapter 2 shows how artificial intelligence is made of human labor. We look at the digital pieceworkers paid pennies on the dollar clicking on microtasks so that data systems can seem more intelligent than they are.31 Our journey will take us inside the Amazon warehouses where employees must keep in time with the algorithmic cadences of a vast logistical empire, and we will visit the Chicago meat laborers on the disassembly lines where animal carcasses are vivisected and prepared for consumption. And we’ll hear from the workers who are protesting against the way that AI systems are increasing surveillance and control for their bosses.

Labor is also a story about time. Coordinating the actions of humans with the repetitive motions of robots and line machinery has always involved a controlling of bodies in space and time.32 From the invention of the stopwatch to Google’s TrueTime, the process of time coordination is at the heart of workplace management. AI technologies both require and create the conditions for ever more granular and precise mechanisms of temporal management. Coordinating time demands increasingly detailed information about what people are doing and how and when they do it.

Chapter 3 focuses on the role of data. All publicly accessible digital material—including data that is personal or potentially damaging—is open to being harvested for training datasets that are used to produce AI models. There are gigantic datasets full of people’s selfies, of hand gestures, of people driving cars, of babies crying, of newsgroup conversations from the 1990s, all to improve algorithms that perform such functions as facial recognition, language prediction, and object detection. When these collections of data are no longer seen as people’s personal material but merely as infrastructure, the specific meaning or context of an image or a video is assumed to be irrelevant. Beyond the serious issues of privacy and ongoing surveillance capitalism, the current practices of working with data in AI raise profound ethical, methodological, and epistemological concerns.

And how is all this data used? In chapter 4, we look at the practices of classification in artificial intelligence systems, what sociologist Karin Knorr Cetina calls the “epistemic machinery.” 34 We see how contemporary systems use labels to predict human identity, commonly using binary gender, essentialized racial categories, and problematic assessments of character and credit worthiness. A sign will stand in for a system, a proxy will stand for the real, and a toy model will be asked to substitute for the infinite complexity of human subjectivity. By looking at how classifications are made, we see how technical schemas enforce hierarchies and magnify inequity. Machine learning presents us with a regime of normative reasoning that, when in the ascendant, takes shape as a powerful governing rationality.

From here, we travel to the hill towns of Papua New Guinea to explore the history of affect recognition, the idea that facial expressions hold the key to revealing a person’s inner emotional state. Chapter 5 considers the claim of the psychologist Paul Ekman that there are a small set of universal emotional states which can be read directly from the face. Tech companies are now deploying this idea in affect recognition systems, as part of an industry predicted to be worth more than seventeen billion dollars.35 But there is considerable scientific controversy around emotion detection, which is at best incomplete and at worst misleading. Despite the unstable premise, these tools are being rapidly implemented into hiring, education, and policing systems.

In chapter 6 we look at the ways in which AI systems are used as a tool of state power. The military past and present of artificial intelligence have shaped the practices of surveillance, data extraction, and risk assessment we see today. The deep interconnections between the tech sector and the military are now being reined in to fit a strong nationalist agenda. Meanwhile, extralegal tools used by the intelligence community have now dispersed, moving from the military world into the commercial technology sector, to be used in classrooms, police stations, workplaces, and unemployment offices. The military logics that have shaped AI systems are now part of the workings of municipal government, and they are further skewing the relation between states and subjects.

The concluding chapter assesses how artificial intelligence functions as a structure of power that combines infrastructure, capital, and labor. From the Uber driver being nudged to the undocumented immigrant being tracked to the public housing tenants contending with facial recognition systems in their homes, AI systems are built with the logics of capital, policing, and militarization—and this combination further widens the existing asymmetries of power. These ways of seeing depend on the twin moves of abstraction and extraction: abstracting away the material conditions of their making while extracting more information and resources from those least able to resist.

But these logics can be challenged, just as systems that perpetuate oppression can be rejected. As conditions on Earth change, calls for data protection, labor rights, climate justice, and racial equity should be heard together. When these interconnected movements for justice inform how we understand artificial intelligence, different conceptions of planetary politics become possible.

Extraction, Power, and Politics

Artificial intelligence, then, is an idea, an infrastructure, an industry, a form of exercising power, and a way of seeing; it’s also a manifestation of highly organized capital backed by vast systems of extraction and logistics, with supply chains that wrap around the entire planet. All these things are part of what artificial intelligence is—a two-word phrase onto which is mapped a complex set of expectations, ideologies, desires, and fears.

AI can seem like a spectral force—as disembodied computation— but these systems are anything but abstract. They are physical infrastructures that are reshaping the Earth, while simultaneously shifting how the world is seen and understood.

It’s important for us to contend with these many aspects of artificial intelligence—its malleability, its messiness, and its spatial and temporal reach. The promiscuity of AI as a term, its openness to being reconfigured, also means that it can be put to use in a range of ways: it can refer to everything from consumer devices like the Amazon Echo to nameless back-end processing systems, from narrow technical papers to the biggest industrial companies in the world. But this has its usefulness, too. The breadth of the term “artificial intelligence” gives us license to consider all these elements and how they are deeply imbricated: from the politics of intelligence to the mass harvesting of data; from the industrial concentration of the tech sector to geopolitical military power; from the deracinated environment to ongoing forms of discrimination.

The task is to remain sensitive to the terrain and to watch the shifting and plastic meanings of the term “artificial intelligence”— like a container into which various things are placed and then removed—because that, too, is part of the story.

Simply put, artificial intelligence is now a player in the shaping of knowledge, communication, and power. These reconfigurations are occurring at the level of epistemology, principles of justice, social organization, political expression, culture, understandings of human bodies, subjectivities, and identities: what we are and what we can be. But we can go further. Artificial intelligence, in the process of remapping and intervening in the world, is politics by other means—although rarely acknowledged as such. These politics are driven by the Great Houses of AI, which consist of the half-dozen or so companies that dominate large-scale planetary computation.

Many social institutions are now influenced by these tools and methods, which shape what they value and how decisions are made while creating a complex series of downstream effects. The intensification of technocratic power has been under way for a long time, but the process has now accelerated. In part this is due to the concentration of industrial capital at a time of economic austerity and outsourcing, including the defunding of social welfare systems and institutions that once acted as a check on market power. This is why we must contend with AI as a political, economic, cultural, and scientific force. As Alondra Nelson, Thuy Linh Tu, and Alicia Headlam Hines observe, “Contests around technology are always linked to larger struggles for economic mobility, political maneuvering, and community building.”'

We are at a critical juncture, one that requires us to ask hard questions about the way AI is produced and adopted. We need to ask: What is AI? What forms of politics does it propagate? Whose interests does it serve, and who bears the greatest risk of harm? And where should the use of AI be constrained? These questions will not have easy answers. But neither is this an irresolvable situation or a point of no return—dystopian forms of thinking can paralyze us from taking action and prevent urgently needed interventions.37 As Ursula Franklin writes, “The viability of technology, like democracy, depends in the end on the practice of justice and on the enforcement of limits to power.”

This book argues that addressing the foundational problems of AI and planetary computation requires connecting issues of power and justice: from epistemology to labor rights, resource extraction to data protections, racial inequity to climate change. To do that, we need to expand our understanding of what is under way in the empires of AI, to see what is at stake, and to make better collective decisions about what should come next.

1 Earth

The Myth of Clean Tech

Minerals are the backbone of AI, but its lifeblood is still electrical energy. Advanced computation is rarely considered in terms of carbon footprints, fossil fuels, and pollution; metaphors like “the cloud” imply something floating and delicate within a natural, green industry.50 Servers are hidden in nondescript data centers, and their polluting qualities are far less visible than the billowing smokestacks of coal-fired power stations. The tech sector heavily publicizes its environmental policies, sustainability initiatives, and plans to address climate-related problems using AI as a problem-solving tool. It is all part of a highly produced public image of a sustainable tech industry with no carbon emissions. In reality, it takes a gargantuan amount of energy to run the computational infrastructures of Amazon Web Services or Microsoft’s Azure, and the carbon footprint of the AI systems that run on those platforms is growing.

As Tung-Hui Hu writes in A Prehistory of the Cloud, “The cloud is a resource-intensive, extractive technology that converts water and electricity into computational power, leaving a sizable amount of environmental damage that it then displaces from sight.” Addressing this energy-intensive infrastructure has become a major concern. Certainly, the industry has made significant efforts to make data centers more energy efficient and to increase their use of renewable energy. But already, the carbon footprint of the world’s computational infrastructure has matched that of the aviation industry at its height, and it is increasing at a faster rate.53 Estimates vary, with researchers like Lotfi Belkhir and Ahmed Elmeligi estimating that the tech sector will contribute 14 percent of global greenhouse emissions by 2040, while a team in Sweden predicts that the electricity demands of data centers alone will increase about fifteenfold by 2030.

By looking closely at the computational capacity needed to build AI models, we can see how the desire for exponential increases in speed and accuracy is coming at a high cost to the planet. The processing demands of training AI models, and thus their energy consumption, is still an emerging area of investigation. One of the early papers in this field came from AI researcher Emma Strubell and her team at the University of Massachusetts Amherst in 2019. With a focus on trying to understand the carbon footprint of natural language processing (NLP) models, they began to sketch out potential estimates by running AI models over hundreds of thousands of computational hours.55 The initial numbers were striking. Strubell’s team found that running only a single NLP model produced more than 660,000 pounds of carbon dioxide emissions, the equivalent of five gas-powered cars over their total lifetime (including their manufacturing) or 125 round-trip flights from New York to Beijing.

Worse, the researchers noted that this modeling is, at minimum, a baseline optimistic estimate. It does not reflect the true commercial scale at which companies like Apple and Amazon operate, scraping internet-wide datasets and feeding their own NLP models to make AI systems like Siri and Alexa sound more human. But the exact amount of energy consumption produced by the tech sector’s AI models is unknown; that information is kept as highly guarded corporate secrets. Here, too, the data economy is premised on maintaining environmental ignorance

In the AI field, it is standard practice to maximize computational cycles to improve performance, in accordance with a belief that bigger is better. As Rich Sutton of DeepMind describes it: “Methods that leverage computation are ultimately the most effective, and by a large margin.”57 The computational technique of brute-force testing in AI training runs, or systematically gathering more data and using more computational cycles until a better result is achieved, has driven a steep increase in energy consumption. OpenAI estimated that since 2012, the amount of compute used to train a single AI model has increased by a factor of ten every year. That’s due to developers “repeatedly finding ways to use more chips in parallel, and being willing to pay the economic cost of doing so.”58 Thinking only in terms of economic cost narrows the view on the wider local and environmental price of burning computation cycles as a way to create incremental efficiencies. The tendency toward “compute maximalism” has profound ecological impacts.

Data centers are among the world’s largest consumers of electricity.59 Powering this multilevel machine requires grid electricity in the form of coal, gas, nuclear, or renewable energy. Some corporations are responding to growing alarm about the energy consumption of large-scale computation, with Apple and Google claiming to be carbon neutral (which means they offset their carbon emissions by purchasing credits) and Microsoft promising to become carbon negative by 2030. But workers within the companies have pushed for reductions in emissions across the board, rather than what they see as buying indulgences out of environmental guilt.60 Moreover, Microsoft, Google, and Amazon all license their AI platforms, engineering workforces, and infrastructures to fossil fuel companies to help them locate and extract fuel from the ground, which further drives the industry most responsible for anthropogenic climate change.

Beyond the United States, more clouds of carbon dioxide are rising. China’s data center industry draws 73 percent of its power from coal, emitting about 99 million tons of CO2 in 2018.61 And electricity consumption from China’s data center infrastructure is expected to increase by two-thirds by 2023.62 Greenpeace has raised the alarm about the colossal energy demands of China’s biggest technology companies, arguing that “China’s leading tech companies, including Alibaba, Tencent, and GDS, must dramatically scale up clean energy procurement and disclose energy use data.”63 But the lasting impacts of coal-fired power are everywhere, exceeding any national boundaries. The planetary nature of resource extraction and its consequences goes well beyond what the nation-state was designed to address.

Water tells another story of computation’s true cost. The history of water use in the United States is full of battles and secret deals, and as with computation, the deals made over water are kept close. One of the biggest U.S. data centers belongs to the National Security Agency (NSA) in Bluffdale, Utah. Open since late 2013, the Intelligence Community Comprehensive National Cybersecurity Initiative Data Center is impossible to visit directly. But by driving up through the adjacent suburbs, I found a cul-de- sac on a hill thick with sagebrush, and from there I was afforded a closer view of the sprawling 1.2-million- square- foot facility. The site has a kind of symbolic power of the next era of government data capture, having been featured in films like Citizenfour and pictured in thousands of news stories about the NSA. In person, though, it looks nondescript and prosaic, a giant storage container combined with a government office block.

The struggle over water began even before the data center was officially open, given its location in drought-parched Utah.64 Local journalists wanted to confirm whether the estimated consumption of 1.7 million gallons of water per day was accurate, but the NSA initially refused to share usage data, redacted all details from public records, and claimed that its water use was a matter of national security. Antisurveillance activists created handbooks encouraging the end of material support of water and energy to surveillance, and they strategized that legal controls over water usage could help shut down the facility.65 But the city of Bluffdale had already made a multiyear deal with the NSA, in which the city would sell water at rates well below the average in return for the promise of economic growth the facility might bring to the region.66 The geopolitics of water are now deeply combined with the mechanisms and politics of data centers, computation, and power—in every sense. From the dry hillside that overlooks the NSA’s data repository, all the contestation and obfuscation about water makes sense: this is a landscape with a limit, and water that is used to cool servers is being taken away from communities and habitats that rely on it to live.

Just as the dirty work of the mining sector was far removed from the companies and city dwellers who profited most, so the majority of data centers are far removed from major population hubs, whether in the desert or in semi-industrial exurbs. This contributes to our sense of the cloud being out of sight and abstracted away, when in fact it is material, affecting the environment and climate in ways that are far from being fully recognized and accounted for. The cloud is of the earth, and to keep it growing requires expanding resources and layers of logistics and transport that are in constant motion.

The Logistical Layer

So far, we have considered the material stuff of AI, from rare earth elements to energy. By grounding our analysis in the specific materialities of AI—the things, places, and people— we can better see how the parts are operating within broader systems of power. Take, for example, the global logistical machines that move minerals, fuel, hardware, workers, and consumer AI devices around the planet.67 The dizzying spectacle of logistics and production displayed by companies like Amazon would not be possible without the development and widespread acceptance of a standardized metal object: the cargo container. Like submarine cables, cargo containers bind the industries of global communication, transport, and capital, a material exercise of what mathematicians call “optimal transport”— in this case, as an optimization of space and resources across the trade routes of the world.

Standardized cargo containers (themselves built from the basic earth elements of carbon and iron forged as steel) enabled the explosion of the modern shipping industry, which in turn made it possible to envision and model the planet as a single massive factory. The cargo container is the single unit of value—like a piece of Lego—that can travel thousands of miles before meeting its final destination as a modular part of a greater system of delivery. In 2017, the capacity of container ships in seaborne trade reached nearly 250 million deadweight tons of cargo, dominated by giant shipping companies including Maersk of Denmark, the Mediterranean Shipping Company of Switzerland, and France’s CMA CGM Group, each owning hundreds of container vessels.68 For these commercial ventures, cargo shipping is a relatively cheap way to navigate the vascular system of the global factory, yet it disguises far larger external costs. Just as they tend to neglect the physical realities and costs of AI infrastructure, popular culture and media rarely cover the shipping industry. The author Rose George calls this condition “sea blindness.”

In recent years, shipping vessels produced 3.1 percent of yearly global carbon dioxide emissions, more than the total produced by Germany.70 In order to minimize their internal costs, most container shipping companies use low-grade fuel in enormous quantities, which leads to increased amounts of airborne sulfur and other toxic substances. One container ship is estimated to emit as much pollution as fifty million cars, and sixty thousand deaths every year are attributed indirectly to cargo-ship- industry pollution.

Even industry-friendly sources like the World Shipping Council admit that thousands of containers are lost each year, sinking to the ocean floor or drifting loose.72 Some carry toxic substances that leak into the oceans; others release thousands of yellow rubber ducks that wash ashore around the world over decades.73 Typically, workers spend almost six months at sea, often with long working shifts and without access to external communications.

Here, too, the most severe costs of global logistics are borne by the Earth’s atmosphere, the oceanic ecosystem and low-paid workers. The corporate imaginaries of AI fail to depict the lasting costs and long histories of the materials needed to build computational infrastructures or the energy required to power them. The rapid growth of cloud-based computation, portrayed as environmentally friendly, has paradoxically driven an expansion of the frontiers of resource extraction. It is only by factoring in these hidden costs, these wider collections of actors and systems, that we can understand what the shift toward increasing automation will mean. This requires working against the grain of how the technological imaginary usually works, which is completely untethered from earthly matters. Like running an image search of “AI,” which returns dozens of pictures of glowing brains and blue-tinted binary code floating in space, there is a powerful resistance to engaging with the materialities of these technologies. Instead, we begin with the earth, with extraction, and with the histories of industrial power and then consider how these patterns are repeated in systems of labor and data.

AI as Megamachine

In the late 1960s, the historian and philosopher of technology Lewis Mumford developed the concept of the megamachine to illustrate how all systems, no matter how immense, consist of the work of many individual human actors.74 For Mumford, the Manhattan Project was the defining modern megamachine whose intricacies were kept not only from the public but even from the thousands of people who worked on it at discrete, secured sites across the United States. A total of 130,000 workers operated in complete secrecy under the direction of the military, developing a weapon that would kill (by conservative estimates) 237,000 people when it hit Hiroshima and Nagasaki in 1945. The atomic bomb depended on a complex, secret chain of supply, logistics, and human labor.

Artificial intelligence is another kind of megamachine, a set of technological approaches that depend on industrial infrastructures, supply chains, and human labor that stretch around the globe but are kept opaque. We have seen how AI is much more than databases and algorithms, machine learning models and linear algebra. It is metamorphic: relying on manufacturing, transportation, and physical work; data centers and the undersea cables that trace lines between the continents; personal devices and their raw components; transmission signals passing through the air; datasets produced by scraping the internet; and continual computational cycles. These all come at a cost.

We have looked at the relations between cities and mines, companies and supply chains, and the topographies of extraction that connect them. The fundamentally intertwined nature of production, manufacturing, and logistics reminds us that the mines that drive AI are everywhere: not only sited in discrete locations but diffuse and scattered across the geography of the earth, in what Mazen Labban has called the “planetary mine.”75 This is not to deny the many specific locations where technologically driven mining is taking place. Rather, Labban observes that the planetary mine expands and reconstitutes extraction into novel arrangements, extending the practices of mines into new spaces and interactions around the world.

Finding fresh methods for understanding the deep material and human roots of AI systems is vital at this moment in history, when the impacts of anthropogenic climate change are already well under way. But that’s easier said than done. In part, that’s because many industries that make up the AI system chain conceal the ongoing costs of what they do. Furthermore, the scale required to build artificial intelligence systems is too complex, too obscured by intellectual property law, and too mired in logistical and technical complexity for us to see into it all. But the aim here is not to try and make these complex assemblages transparent: rather than trying to see inside them, we will be connecting across multiple systems to understand how they work in relation to each other.76 Thus, our path will follow the stories about the environmental and labor costs of AI and place them in context with the practices of extraction and classification braided throughout everyday life. It is by thinking about these issues together that we can work toward greater justice.

2 Labor

Rather than debating whether humans will be replaced by robots, in this chapter I focus on how the experience of work is shifting in relation to increased surveillance, algorithmic assessment, and the modulation of time. Put another way, instead of asking whether robots will replace humans, I’m interested in how humans are increasingly treated like robots and what this means for the role of labor. Many forms of work are shrouded in the term “artificial intelligence,” hiding the fact that people are often performing rote tasks to shore up the impression that machines can do the work. But large-scale computation is deeply rooted in and running on the exploitation of human bodies.

If we want to understand the future of work in the context of artificial intelligence, we need to begin by understanding the past and present experience of workers. Approaches to maximizing the extraction of value from workers vary from reworkings of the classical techniques used in Henry Ford’s factories to a range of machine learning–assisted tools designed to increase the granularity of tracking, nudging, and assessment. This chapter maps geographies of labor past and present, from Samuel Bentham’s inspection houses to Charles Babbage’s theories of time management and to Frederick Winslow Taylor’s micromanagement of human bodies. Along the way, we will see how AI is built on the very human efforts of (among other things) crowdwork, the privatization of time, and the seemingly never-ending reaching, lifting, and toiling of putting boxes into order. From the lineage of the mechanized factory, a model emerges that values increased conformity, standardization, and interoperability—for products, processes, and humans alike.

Prehistories of Workplace AI

Workplace automation, though often told as a story of the future, is already a long-established experience of contemporary work. The manufacturing assembly line, with its emphasis on consistent and standardized units of production, has analogues in the service industries, from retail to restaurants. Secretarial labor has been increasingly automated since the 1980s and now is emulated by highly feminized AI assistants such as Siri, Cortana, and Alexa.2 So-called knowledge workers, those white-collar employees assumed to be less threatened by the forces driving automation, find themselves increasingly subjected to workplace surveillance, process automation, and collapse between the distinction of work and leisure time (although women have rarely experienced such clear distinctions, as feminist theorists of work like Silvia Federici and Melissa Gregg have shown).3 Work of all stripes has had to significantly adapt itself in order to be interpretable and understood by software-based systems.

The common refrain for the expansion of AI systems and process automation is that we are living in a time of beneficial human-AI collaboration. But this collaboration is not fairly negotiated. The terms are based on a significant power asymmetry— is there ever a choice not to collaborate with algorithmic systems? When a company introduces a new AI platform, workers are rarely allowed to opt out. This is less of a collaboration than a forced engagement, where workers are expected to re-skill, keep up, and unquestioningly accept each new technical development.

Rather than representing a radical shift from established forms of work, the encroachment of AI into the workplace should properly be understood as a return to older practices of industrial labor exploitation that were well established in the 1890s and the early twentieth century. That was a time when factory labor was already seen in relation to machines and work tasks were increasingly subdivided into smaller actions requiring minimal skill but maximum exertion. Indeed, the current expansion of labor automation continues the broader historical dynamics inherent in industrial capitalism. Since the appearance of the earliest factories, workers have encountered ever more powerful tools, machines, and electronic systems that play a role in changing how labor is managed while transferring more value to their employers. We are witnessing new refrains on an old theme. The crucial difference is that employers now observe, assess, and modulate intimate parts of the work cycle and bodily data—down to the last micromovement— that were previously off-limits to them.

There are many prehistories of workplace AI; one is the Industrial Revolution’s widespread automation of common productive activities. In his Wealth of Nations, the eighteenth-century political economist Adam Smith first pointed to the division and subdivision of manufacturing tasks as the basis of both improved productivity and increasing mechanization.5 He observed that by identifying and analyzing the various steps involved in manufacturing any given item, it was possible to divide them into ever-smaller steps, so that a product once made entirely by expert craftspeople could now be built by a team of lower-skill workers equipped with tools purpose-built for a particular task. Thus, a factory’s output could be scaled up significantly without an equivalent increase in labor cost.

Developments in mechanization were important, but it was only when combined with a growing abundance of energy derived from fossil fuels that they could drive a massive increase in the productive capacities of industrial societies. This increase in production occurred in tandem with a major transformation of the role of labor vis-à- vis machinery in the workplace. Initially conceived as labor-saving devices, factory machines were meant to assist workers with their daily activities but quickly became the center of productive activity, shaping the speed and character of work. Steam engines powered by coal and oil could drive continuous mechanical actions that influenced the pace of work in the factory. Work ceased to be primarily a product of human labor and took on an increasingly machinelike character, with workers adapting to the needs of the machine and its particular rhythms and cadences. Building on Smith, Karl Marx noted as early as 1848 that automation abstracts labor from the production of finished objects and turns a worker into “an appendage of the machine.”

The integration of workers’ bodies with machines was sufficiently thorough that early industrialists could view their employees as a raw material to be managed and controlled like any other resource. Factory owners, using both their local political clout and paid muscle, sought to direct and restrict how their workers moved around within factory towns, sometimes even preventing workers from emigrating to less mechanized regions of the world.

This also meant increasing control over time. The historian E. P. Thompson’s formative essay explores how the Industrial Revolution demanded greater synchronization of work and stricter time disciplines.8 The transition to industrial capitalism came with new divisions of labor, oversight, clocks, fines, and time sheets—technologies that also influenced the way people experienced time. Culture was also a powerful tool. During the eighteenth and nineteenth centuries, the propaganda about hard work came in the forms of pamphlets and essays on the importance of discipline and sermons on the virtues of early rising and working diligently for as long as possible.9 The use of time came to be seen in both moral and economic terms: understood as a currency, time could be well spent or squandered away. But as more rigid time disciplines were imposed in workshops and factories, the more workers began to push back—campaigning over time itself. By the 1800s, labor movements were strongly advocating for reducing the working day, which could run as long as sixteen hours. Time itself became a key site for struggle.

Maintaining an efficient and disciplined workforce in the early factory necessitated new systems of surveillance and control. One such invention from the early years of industrial manufacturing was the inspection house, a circular arrangement that placed all of a factory’s workers within sight of their supervisors, who worked from an office placed on a raised platform at the center of the building. Developed in the 1780s in Russia by the English naval engineer Samuel Bentham while under the employ of Prince Potemkin, this arrangement allowed expert supervisors to keep an eye on their untrained subordinates—mostly Russian peasants loaned to Bentham by Potemkin—for signs of poor working habits. It also allowed Bentham himself to keep an eye on the supervisors for signs of ill-discipline. The supervisors, mostly master shipbuilders recruited from England, caused Bentham great annoyance due to their tendency to drink and get into petty disagreements with one another. “Morning after morning I am taken up chiefly with disputes amongst my Officers,” Bentham complained.10 As his frustrations grew, he embarked on a redesign that would maximize his ability to keep a watchful eye on them, and on the system as a whole. With a visit from his elder brother, the utilitarian philosopher Jeremy Bentham, Samuel’s inspection house became the inspiration for the famous panopticon, a design for a model prison featuring a central watchtower from which guards could supervise the prisoners in their cells.

Now the role of oversight in the modern workplace is primarily deputized to surveillance technologies. The managerial class employs a wide range of technologies to surveil employees, including tracking their movements with apps, analyzing their social media feeds, comparing the patterns of replying to emails and booking meetings, and nudging them with suggestions to make them work faster and more efficiently. Employee data is used to make predictions about who is most likely to succeed (according to narrow, quantifiable parameters), who might be diverging from company goals, and who might be organizing other workers. Some use the techniques of machine learning, and others are more simplistic algorithmic systems. As workplace AI becomes more prevalent, many of the more basic monitoring and tracking systems are being expanded with new predictive capacities to become increasingly invasive mechanisms of worker management, asset control, and value extraction.

Potemkin AI and the Mechanical Turks

One of the less recognized facts of artificial intelligence is how many underpaid workers are required to help build, maintain, and test AI systems. This unseen labor takes many forms— supply-chain work, on-demand crowdwork, and traditional service-industry jobs. Exploitative forms of work exist at all stages of the AI pipeline, from the mining sector, where resources are extracted and transported to create the core infrastructure of AI systems, to the software side, where distributed workforces are paid pennies per microtask. Mary Gray and Sid Suri refer to such hidden labor as “ghost work.”16 Lilly Irani calls it “human-fueled automation.”17 These scholars have drawn attention to the experiences of crowdworkers or microworkers who perform the repetitive digital tasks that underlie AI systems, such as labeling thousands of hours of training data and reviewing suspicious or harmful content. Workers do the repetitive tasks that backstop claims of AI magic—but they rarely receive credit for making the systems function.

Although this labor is essential to sustaining AI systems, it is usually very poorly compensated. A study from the United Nations International Labour Organization surveyed 3,500 crowdworkers from seventy-five countries who routinely offered their labor on popular task platforms like Amazon Mechanical Turk, Figure Eight, Microworkers, and Clickworker. The report found that a substantial number of people earned below their local minimum wage even though the majority of respondents were highly educated, often with specializations in science and technology.19 Likewise, those who do content moderation work—assessing violent videos, hate speech, and forms of online cruelty for deletion—are also paid poorly. As media scholars such as Sarah Roberts and Tarleton Gillespie have shown, this kind of work can leave lasting forms of psychological trauma.

But without this kind of work, AI systems won’t function. The technical AI research community relies on cheap, crowd-sourced labor for many tasks that can’t be done by machines. Between 2008 and 2016, the term “crowdsourcing” went from appearing in fewer than a thousand scientific articles to more than twenty thousand—which makes sense, given that Mechanical Turk launched in 2005. But during the same time frame, there was far too little debate about what ethical questions might be posed by relying on a workforce that is commonly paid far below the minimum wage.

Of course, there are strong incentives to ignore the dependency on underpaid labor from around the world. All the work they do—from tagging images for computer-vision systems to testing whether an algorithm is producing the right results—refines AI systems much more quickly and cheaply, particularly when compared to paying students to do these tasks (as was the earlier tradition). So the issue has generally been ignored, and as one crowdwork research team observed, clients using these platforms “expect cheap, ‘frictionless’ completion of work without oversight, as if the platform were not an interface to human workers but a vast computer without living expenses.”22 In other words, clients treat human employees as little more than machines, because to recognize their work and compensate it fairly would make AI more expensive and less “efficient.”

Sometimes workers are directly asked to pretend to be an AI system. The digital personal assistant start-up x.ai claimed that its AI agent, called Amy, could “magically schedule meetings” and handle many mundane daily tasks. But a detailed Bloomberg investigation by journalist Ellen Huet revealed that it wasn’t artificial intelligence at all. “Amy” was carefully being checked and rewritten by a team of contract workers pulling long shifts. Similarly, Facebook’s personal assistant, M, was relying on regular human intervention by a group of workers paid to review and edit every message.

Faking AI is an exhausting job. The workers at x.ai were sometimes putting in fourteen-hour shifts of annotating emails in order to sustain the illusion that the service was automated and functioning 24/7. They couldn’t leave at the end of the night until the queues of emails were finished. “I left feeling totally numb and absent of any sort of emotion,” one employee told Huet.

We could think of this as a kind of Potemkin AI—little more than facades, designed to demonstrate to investors and a credulous media what an automated system would look like while actually relying on human labor in the background.25 In a charitable reading, these facades are an illustration of what the system might be capable of when fully realized, or a “minimum viable product” designed to demonstrate a concept. In a less charitable reading, Potemkin AI systems are a form of deception perpetrated by technology vendors eager to stake a claim in the lucrative tech space. But until there is another way to create large-scale AI that doesn’t use extensive behind-the- curtain work by humans, this is a core logic of how AI works.

The writer Astra Taylor has described the kind of overselling of high-tech systems that aren’t actually automated as “fauxtomation.”26 Automated systems appear to do work previously performed by humans, but in fact the system merely coordinates human work in the background. Taylor cites the examples of self-service kiosks in fast-food restaurants and self-checkout systems in supermarkets as places where an employee’s labor appears to have been replaced by an automated system but where in fact the data-entry labor has simply been relocated from a paid employee to the customer. Meanwhile, many online systems that provide seemingly automated decisions, such as removing duplicated entries or deleting offensive content, are actually powered by humans working from home on endless queues of mundane tasks.27 Much like Potemkin’s decorated villages and model workshops, many valuable automated systems feature a combination of underpaid digital pieceworkers and consumers taking on unpaid tasks to make systems function. Meanwhile, companies seek to convince investors and the general public that intelligent machines are doing the work.


What is at stake in this artifice? The true labor costs of AI are being consistently downplayed and glossed over, but the forces driving this performance run deeper than merely marketing trickery. It is part of a tradition of exploitation and deskilling, where people must do more tedious and repetitive work to back-fill for automated systems, for a result that may be less effective or reliable than what it replaced. But this approach can scale—producing cost reductions and profit increases while obscuring how much it depends on remote workers being paid subsistence wages and off-loading additional tasks of maintenance or error-checking to consumers.

Fauxtomation does not directly replace human labor; rather, it relocates and disperses it in space and time. In so doing it increases the disconnection between labor and value and thereby performs an ideological function. Workers, having been alienated from the results of their work as well as disconnected from other workers doing the same job, are liable to be more easily exploited by their employers. This is evident from the extremely low rates of compensation crowdworkers receive around the world.28 They and other kinds of fauxtomation laborers face the very real fact that their labor is interchangeable with any of the thousands of other workers who compete with them for work on platforms. At any point they could be replaced by another crowdworker, or possibly by a more automated system.

These examples of Potemkin AI are all around. Some are directly visible: when we see one of the current crop of self-driving cars on the streets, we also see a human operator in the driver’s seat, ready to take control of the vehicle at the first sign of trouble. Others are less visible, as when we interact with a web-based chat interface. We engage only with the facades that obscure their inner workings, designed to hide the various combinations of machine and human labor in each interaction. We aren’t informed whether we are receiving a response from the system itself or from a human operator paid to respond on its behalf.

If there is growing uncertainty about whether we are engaging with an AI system or not, the feeling is mutual. In a paradox that many of us have experienced, and ostensibly in order to prove true human identity when reading a website, we are required to convince Google’s reCAPTCHA of our humanity. So we dutifully select multiple boxes containing street numbers, or cars, or houses. We are training Google’s image recognition algorithms for free. Again, the myth of AI as affordable and efficient depends on layers of exploitation, including the extraction of mass unpaid labor to fine-tune the AI systems of the richest companies on earth.

Contemporary forms of artificial intelligence are neither artificial nor intelligent. We can—and should—speak instead of the hard physical labor of mine workers, the repetitive factory labor on the assembly line, the cybernetic labor in the cognitive sweatshops of outsourced programmers, the poorly paid crowdsourced labor of Mechanical Turk workers, and the unpaid immaterial work of everyday users. These are the places where we can see how planetary computation depends on the exploitation of human labor, all along all the supply chain of extraction.

Visions of Disassembly and Workplace Automation: Babbage, Ford, and Taylor

Charles Babbage is well known as the inventor of the first mechanical computer. In the 1820s, he developed the idea for the Difference Engine, a mechanical calculating machine designed to generate mathematical and astronomical tables in a fraction of the time required to calculate them by hand. By the 1830s, he had a viable conceptual design for the Analytical Engine, a programmable general-purpose mechanical computer, complete with a system of punch cards for providing it with instructions.

Babbage also had a strong interest in liberal social theory and wrote extensively on the nature of labor—the combination of his interests in computation and worker automation. Following Adam Smith, he noted the division of labor as a means of streamlining factory work and generating efficiencies. He went further, however, arguing that the industrial corporation could be understood as an analogue to a computational system. Just like a computer, it included multiple specialized units performing particular tasks, all coordinated to produce a given body of work, but with the labor content of the finished product rendered largely invisible by the process as a whole.

In Babbage’s more speculative writing, he imagined perfect flows of work through the system that could be visualized as data tables and monitored by pedometers and repeating clocks.35 Through a combination of computation, surveillance, and labor discipline, he argued, it would be possible to enforce ever-higher degrees of efficiency and quality control.36 It was a strangely prophetic vision. Only in very recent years, with the adoption of artificial intelligence in the workplace, has Babbage’s unusual twin goals of computation and worker automation become possible at scale.

Babbage’s economic thought extended outward from Smith’s but diverged in one important way. For Smith, the economic value of an object was understood in relation to the cost of the labor required to produce it. In Babbage’s rendering, however, value in a factory was derived from investment in the design of the manufacturing process rather than from the labor force of its employees. The real innovation was the logistical process, while workers simply enacted the tasks defined for them and operated the machines as instructed.

For Babbage, labor’s role in the value production chain was largely negative: workers might fail to perform their tasks in the timely manner prescribed by the precision machines they operated, whether through poor discipline, injury, absenteeism, or acts of resistance. As noted by historian Simon Schaffer, “Under Babbage’s gaze, factories looked like perfect engines and calculating machines like perfect computers. The workforce might be a source of trouble—it could make tables err or factories fail—but it could not be seen as a source of value.”37 The factory is conceived as a rational calculating machine with only one weakness: its frail and untrustworthy human labor force.

Babbage’s theory was, of course, heavily inflected with a kind of financial liberalism, causing him to view labor as a problem that needed to be contained by automation. There was little consideration of the human costs of this automation or of how automation might be put to use to improve the working lives of factory employees. Instead, Babbage’s idealized machinery aimed primarily to maximize financial returns to the plant owners and their investors. In a similar vein, today’s proponents of workplace AI present a vision of production that prioritizes efficiency, cost-cutting, and higher profits instead of, say, assisting their employees by replacing repetitive drudge work. As Astra Taylor argues, “The kind of efficiency to which techno-evangelists aspire emphasizes standardization, simplification, and speed, not diversity, complexity, and interdependence.” 38 This should not surprise us: it is a necessary outcome of the standard business model of for-profit companies where the highest responsibility is to shareholder value. We are living the result of a system in which companies must extract as much value as possible. Meanwhile, 94 percent of all new American jobs created between 2005 and 2015 were for “alternative work”—jobs that fall outside of full-time, salaried employment.39 As companies reap the benefits of increasing automation, people are, on average, working longer hours, in more jobs, for less pay, in insecure positions.

The Meat Market

Among the first industries to implement the type of mechanized production line Babbage envisioned was the Chicago meat-packing industry in the 1870s. Trains brought livestock to the stockyard gates; the animals were funneled toward their slaughter in adjacent plants; and the carcasses were transported to various butchering and processing stations by means of a mechanized overhead trolley system, forming what came to be known as the disassembly line. The finished products could be shipped to faraway markets in specially designed refrigerated rail cars.40 Labor historian Harry Braverman noted that the Chicago stockyards realized Babbage’s vision of automation and division of labor so completely that the human techniques required at any point on the disassembly line could be performed by just about anyone.41 Low-skill laborers could be paid the bare minimum and replaced at the first sign of trouble, themselves becoming as thoroughly commoditized as the packaged meats they produced.

When Upton Sinclair wrote The Jungle, his harrowing novel about working-class poverty, he set it in the meat-packing plants of Chicago. Although his intended point was to highlight the hardships of working immigrants in support of a socialist political vision, the book had an entirely different effect. The depictions of diseased and rotting meat prompted a public outcry over food safety and resulted in the passing of the Meat Inspection Act in 1906. But the focus on workers was lost. Powerful institutions from the meat-packing industry to Congress were prepared to intervene to improve the methods of production, but addressing the more fundamental exploitative labor dynamics that propped up the entire system was off limits. The persistence of this pattern underscores how power responds to critique: whether the product is cow carcasses or facial recognition, the response is to accept regulation at the margins but to leave untouched the underlying logics of production.

Two other figures loom large in the history of workplace automation: Henry Ford, whose moving assembly line from the early twentieth century was inspired by Chicago’s disassembly lines, and Frederick Winslow Taylor, the founder of scientific management. Taylor forged his career in the latter years of the nineteenth century developing a systematic approach to workplace management, one that focused on the minute movements of workers’ bodies. Whereas Smith’s and Babbage’s notion of the division of labor was intended to provide a way to distribute work between people and tools, Taylor narrowed his focus to include microscopic subdivisions in the actions of each worker.

As the latest technology for precisely tracking time, the stopwatch was to become a key instrument of workplace surveillance for shop-floor supervisors and production engineers alike. Taylor used stopwatches to perform studies of workers that included detailed breakdowns of the time taken to perform the discrete physical motions involved in any given task. His Principles of Scientific Management established a system to quantify the movements of workers’ bodies, with a view to deriving an optimally efficient layout of tools and working processes. The aim was maximum output at minimal cost.42 It exemplified Marx’s description of the domination of clock time, “Time is everything, man is nothing; he is, at most, time’s carcass.”

Foxconn, the largest electronics manufacturing company in the world, which makes Apple iPhones and iPads, is a vivid example of how workers are reduced to animal bodies performing tightly controlled tasks. Foxconn became notorious for its rigid and militaristic management protocols after a spate of suicides in 2010.44 Just two years later, the company’s chairman, Terry Gou, described his more than one million employees this way: “As human beings are also animals, to manage one million animals gives me a headache.”

Controlling time becomes another way to manage bodies. In service and fast-food industries, time is measured down to the second. Assembly line workers cooking burgers at McDonald’s are assessed for meeting such targets as five seconds to process screen-based orders, twenty-two seconds to assemble a sandwich, and fourteen seconds to wrap the food.46 Strict adherence to the clock removes margin for error from the system. The slightest delay (a customer taking too long to order, a coffee machine failing, an employee calling in sick) can result in a cascading ripple of delays, warning sounds, and management notifications.

Even before McDonald’s workers join the assembly line, their time is being managed and tracked. An algorithmic scheduling system incorporating historical data analysis and demand-prediction models determines workers’ shift allocations, resulting in work schedules that can vary from week to week and even day to day. A 2014 class action lawsuit against McDonald’s restaurants in California noted that franchisees are led by software that gives algorithmic predictions regarding employee-to- sales ratios and instructs managers to reduce staff quickly when demand drops.47 Employees reported being told to delay clocking in to their shifts and instead to hang around nearby, ready to return to work if the restaurant started getting busy again. Because employees are paid only for time clocked in, the suit alleged that this amounted to significant wage theft on the part of the company and its franchisees.

Algorithmically determined time allocations will vary from extremely short shifts of an hour or less to very long ones during busy times—whatever is most profitable. The algorithm doesn’t factor in the human costs of waiting or getting to work only to be sent home or being unable to predict one’s schedule and plan one’s life. This time theft helps the efficiency of the company, but it comes at the direct cost of the employees.

Managing Time, Privatizing Time

Fast-food entrepreneur Ray Kroc, who helped turn McDonald’s into a global franchise, joined the lineage of Smith, Babbage, Taylor, and Ford when he designed the standard sandwich assembly line and made his employees follow it unthinkingly. Surveillance, standardization, and the reduction of individual craft were central to Kroc’s method. As labor researchers Clare Mayhew and Michael Quinlan argue with regard to the McDonald’s standardized process, “The Fordist management system documented work and production tasks in minuscule detail. It required on-going documented participation and entailed detailed control of each individual’s work process. There was an almost total removal of all conceptual work from execution of tasks.”

Minimizing the time spent at each station, or cycle time, became an object of intense scrutiny within the Fordist factory, with engineers dividing work tasks into ever-smaller pieces so they could be optimized and automated, and with supervisors disciplining workers whenever they fell behind. Supervisors, even Henry Ford himself, could often be seen walking the length of the factory, stopwatch in hand, recording cycle times and noting any discrepancies in a station’s productivity.

Now employers can passively surveil their workforce without walking out onto the factory floor. Instead, workers clock in to their shifts by swiping access badges or by presenting their fingerprints to readers attached to electronic time clocks. They work in front of timing devices that indicate the minutes or seconds left to perform the current task before a manager is notified. They sit at workstations fitted with sensors that continuously report on their body temperature, their physical distance from colleagues, the amount of time they spend browsing websites instead of performing assigned tasks, and so on. WeWork, the coworking behemoth that burned itself out over the course of 2019, quietly fitted its work spaces with surveillance devices in an effort to create new forms of data monetization. Its 2019 acquisition of the spatial analytics startup Euclid raised concerns, with the suggestion that it planned to track its paying members as they moved through their facilities.51 Domino’s Pizza has added to its kitchens machine-vision systems that inspect a finished pizza to ensure the staff made it according to prescribed standards.52 Surveillance apparatuses are justified for producing inputs for algorithmic scheduling systems that further modulate work time, or to glean behavioral signals that may correlate with signs of high or low performance, or merely sold to data brokers as a form of insight.

In her essay “How Silicon Valley Sets Time,” sociology professor Judy Wajcman argues that the aims of time-tracking tools and the demographic makeup of Silicon Valley are no coincidence. 53 Silicon Valley’s elite workforce “is even younger, more masculine and more fully committed to working all hours,” while also creating productivity tools that are premised on a kind of ruthless, winner-takes- all race to maximal efficiency. 54 This means that young, mostly male engineers, often unencumbered by time-consuming familial or community responsibilities, are building the tools that will police very different workplaces, quantifying the productivity and desirability of employees. The workaholism and round-the- clock hours often glorified by tech start-ups become an implicit benchmark against which other workers are measured, producing a vision of a standard worker that is masculinized, narrow, and reliant on the unpaid or underpaid care work of others.

Private Time

The coordination of time has become ever more granular in the technological forms of workplace management. For example, General Motors’ Manufacturing Automation Protocol (MAP) was an early attempt to provide standard solutions to common manufacturing robot coordination problems, including clock synchronization.55 In due course, other, more generic time synchronization protocols that could be delivered over ethernet and TCP/IP networks emerged, including the Network Time Protocol (NTP), and, later, the Precision Time Protocol (PTP), each of which spawned a variety of competing implementations across various operating systems. Both NTP and PTP function by establishing a hierarchy of clocks across a network, with a “master” clock driving the “slave” clocks.

The master-slave metaphor is riddled throughout engineering and computation. One of the earliest uses of this racist metaphor dates back to 1904 describing astronomical clocks in a Cape Town observatory.56 But it wasn’t until 1960s that the master-slave terminology spread, particularly after it was used in computing, starting with the Dartmouth timesharing system. Mathematicians John Kemeny and Thomas Kurtz developed a time-sharing program for access to computing resources after a suggestion by one of the early founders of AI, John McCarthy. As they wrote in Science in 1968, “First, all computing for users takes place in the slave computer, while the executive program (the ‘brains’ of the system) resides in the master computer. It is thus impossible for an erroneous or runaway user program in the slave computer to ‘damage’ the executive program and thereby bring the whole system to a halt.”57 The problematic implication that control is equivalent to intelligence would continue to shape the AI field for decades. And as Ron Eglash has argued, the phrasing has a strong echo of the pre–Civil War discourse on runaway slaves.

The master-slave terminology has been seen as offensive by many and has been removed from Python, a coding language common in machine learning, and Github, a software development platform. But it persists in one of the most expansive computational infrastructures in the world. Google’s Spanner—named as such because it spans the entire planet— is a massive, globally distributed, synchronously replicated database. It is the infrastructure that supports Gmail, Google search, advertising, and all of Google’s distributed services.

At this scale, functioning across the globe, Spanner synchronizes time across millions of servers in hundreds of data centers. Every data center has a “time master” unit that is always receiving GPS time. But because servers were polling a variety of master clocks, there was slight network latency and clock drift. How to resolve this uncertainty? The answer was to create a new distributed time protocol—a proprietary form of time—so that all servers could be in sync regardless of where they were across the planet. Google called this new protocol, without irony, TrueTime.

Google’s TrueTime is a distributed time protocol that functions by establishing trust relationships between the local clocks of data centers so they can decide which peers to synchronize with. Benefiting from a sufficiently large number of reliable clocks, including GPS receivers and atomic clocks that provide an extremely high degree of precision, and from sufficiently low levels of network latency, TrueTime allows a distributed set of servers to guarantee that events can occur in a determinate sequence across a wide area network.

What’s most remarkable in this system of privatized Google time is how TrueTime manages uncertainty when there is clock drift on individual servers. “If the uncertainty is large, Spanner slows down to wait out that uncertainty,” Google researchers explain.60 This embodies the fantasy of slowing down time, of moving it at will, and of bringing the planet under a single proprietary time code. If we think of the human experience of time as something shifting and subjective, moving faster or slower depending on where we are and whom we are with, then this is a social experience of time. TrueTime is the ability to create a shifting timescale under the control of a centralized master clock. Just as Isaac Newton imagined an absolute form of time that exists independently of any perceiver, Google has invented its own form of universal time.

Proprietary forms of time have long been used to make machines run smoothly. Railroad magnates in the nineteenth century had their own forms of time. In New England in 1849, for example, all trains were to adopt “true time at Boston as given by William Bond & Son, No. 26 Congress Street.”61 As Peter Galison has documented, railroad executives weren’t fond of having to switch to other times depending on which state their trains traveled to, and the general manager of the New York & New England Railroad Company called switching to other times “a nuisance and great inconvenience and no use to anybody I can see.”62 But after a head-on train collision killed fourteen people in 1853, there was immense pressure to coordinate all of the clocks using the new technology of the telegraph.

Like artificial intelligence, the telegraph was hailed as a unifying technology that would expand the capabilities of human beings. In 1889 Lord Salisbury boasted that the telegraph had “assembled all mankind upon one great plane.”63 Businesses, governments, and the military used the telegraph to compile time into a coherent grid, erasing more local forms of timekeeping. And the telegraph was dominated by one of the first great industrial monopolies, Western Union. In addition to altering the temporal and spatial boundaries of human interaction, communications theorist James Carey argues that the telegraph also enabled a new form of monopoly capitalism: “a new body of law, economic theory, political arrangements, management techniques, organizational structures, and scientific rationales with which to justify and make effective the development of a privately owned and controlled monopolistic corporation.”64 While this interpretation implies a kind of technological determinism in what was a complex series of developments, it is fair to say that the telegraph—paired with the transatlantic cable—enabled imperial powers to maintain more centralized control over their colonies.

The telegraph made time a central focus for commerce. Rather than traders exploiting the difference in prices between regions by buying low and selling high in varying locations, now they traded between time zones: in Carey’s terms, a shift from space to time, from arbitrage to futures.65 The privatized time zones of data centers are just the latest example. The infrastructural ordering of time acts as a kind of “macrophysics of power,” determining new logics of information at a planetary level.66 Such power is necessarily centralizing, creating orders of meaning that are extremely difficult to see, let alone disrupt.

Defiance of centralized time is a vital part of this history. In the 1930s, when Ford wanted more control over his global supply chain, he set up a rubber plantation and processing facility deep in the Brazilian rain forest, in a town he named Fordlandia. He employed local workers to process rubber for shipping back to Detroit, but his attempts to impose his tightly controlled manufacturing process on the local population backfired. Rioting workers tore apart the factory’s time clocks, smashing the devices used to track the entry and exit of each worker in the plant.

Other forms of insurgence have centered on adding friction to the work process. The French anarchist Émile Pouget used the term “sabotage” to mean the equivalent of a “go slow” on the factory floor, when workers intentionally reduce their pace of work.67 The objective was to withdraw efficiency, to reduce the value of time as a currency. Although there will always be ways to resist the imposed temporality of work, with forms of algorithmic and video monitoring, this becomes much harder—as the relation between work and time is observed at ever closer range.

From the fine modulations of time within factories to the big modulations of time at the scale of planetary computation networks, defining time is an established strategy for centralizing power. Artificial intelligence systems have allowed for greater exploitation of distributed labor around the world to take advantage of uneven economic topologies. Simultaneously, the tech sector is creating for itself a smooth global terrain of time to strengthen and speed its business objectives. Controlling time—whether via the clocks for churches, trains, or data centers—has always been a function of controlling the political order. But this battle for control has never been smooth, and it is a far-reaching conflict. Workers have found ways to intervene and resist, even when technological developments were forced on them or presented as desirable improvements, particularly if the only refinements were to increase surveillance and company control.

Setting the Rate

Amazon goes to great lengths to control what members of the public can see when they enter a fulfillment center. We are told about the fifteen-dollar- an- hour minimum wage and the perks for employees who can last longer than a year, and we are shown brightly lit break rooms that have Orwellian corporate slogans painted on the walls: “Frugality,” “Earn trust of others,” and “Bias for action.” The official Amazon guide cheerily explains what is happening at predetermined stops with rehearsed vignettes. Any questions about labor conditions are carefully answered to paint the most positive picture. But there are signs of unhappiness and dysfunction that are much harder to manage.

Out on the picking floor, where associates must pick up gray containers (known as “totes”) full of purchases to ship, whiteboards bear the marks of recent meetings. One had multiple complaints that the totes were stacked too high and that constantly reaching up to grab them was causing considerable pain and injuries. When asked about this, the Amazon guide quickly responded that this concern was being addressed by lowering the height of the conveyor belt in key sections. This was seen as a success: a complaint had been registered and action would be taken. The guide took this opportunity to explain for the second time that this was why unions were unnecessary here, because “associates have many opportunities to interface with their managers,” and unionization only interferes with communication.

But on the way out of the facility, I walked past a live feed of messages from workers on a large flat screen, with a sign above it that read, “The Voice of the Associates.” This was far less varnished. Messages scrolled rapidly past with complaints about arbitrary scheduling changes, the inability to book vacation time near holidays, and missing family occasions and birthdays. Pat responses from management seemed to be multiple variations on the theme of “We value your feedback.”

“Enough is enough. Amazon, we want you to treat us like humans, and not like robots.”69 These are the words of Abdi Muse, executive director of the Awood Center in Minneapolis, a community organization that advocates for the working conditions of Minnesota’s East African populations. Muse is a soft-spoken defender of Amazon warehouse workers who are pushing for better working conditions. Many workers in his Minnesota community have been hired by Amazon, which actively recruited them and added sweeteners to the deal, such as free busing to work.

What Amazon didn’t advertise was “the rate”—the worker productivity metric driving the fulfillment centers that quickly became unsustainable and, according to Muse, inhumane. Workers began suffering high stress, injuries, and illness. Muse explained that if their rate went down three times they would be fired, no matter how long they had worked at the warehouse. Workers talked about having to skip bathroom breaks for fear that they would underperform.

But the day we met, Muse was optimistic. Even though Amazon explicitly discourages unions, informal groups of workers were springing up across the United States and staging protests. He smiled widely as he reported that the organizing was starting to have an impact. “Something incredible is happening,” he told me. “Tomorrow a group of Amazon workers will be walking off the job. It’s such a courageous group of women, and they are the real heroes.”70 Indeed, that night, approximately sixty warehouse workers walked out of a delivery center in Eagan, Minnesota, wearing their mandated yellow vests. They were mostly women of Somali descent, and they held up signs in the rain, demanding such improvements as increased wages for night shifts and weight restrictions on boxes.71 Only a few days earlier, Amazon workers in Sacramento, California, had protested the firing of an employee who had gone one hour over her bereavement leave after a family member died. Two weeks before that, more than a thousand Amazon workers staged the first ever white-collar walkout in the company’s history over its massive carbon footprint.

Eventually, Amazon’s representatives in Minnesota came to the table. They were happy to discuss many issues but never “the rate.” “They said forget about ‘the rate,’” recounted Muse. “We can talk about other issues, but the rate is our business model. We cannot change that.”72 The workers threatened to walk away from the table, and still Amazon would not budge. For both sides, “the rate” was the core issue, but it was also the hardest to alter. Unlike other local labor disputes where the on-the- ground supervisors might have been able to make concessions, the rate was set based on what the executives and tech workers in Seattle—far removed from the warehouse floor— had decided and had programmed Amazon’s computational distribution infrastructure to optimize for. If the local warehouses were out of sync, Amazon’s ordering of time was threatened. Workers and organizers started to see this as the real issue. They are shifting their focus accordingly toward building a movement across different factories and sectors of Amazon’s workforce to address the core issues of power and centralization represented by the relentless rhythm of “the rate” itself.

These fights for time sovereignty, as we’ve seen, have a history. AI and algorithmic monitoring are simply the latest technologies in the long historical development of factories, timepieces, and surveillance architectures. Now many more sectors—from Uber drivers to Amazon warehouse workers to highly paid Google engineers—perceive themselves in this shared fight. This was strongly articulated by the executive director of the New York Taxi Workers Alliance, Bhairavi Desai, who put it this way: “Workers always know. They are out there building solidarity with each other, at red lights or in restaurants or in hotel queues, because they know that in order to prosper they have to band together.”73 Technologically driven forms of worker exploitation are a widespread problem in many industries. Workers are fighting against the logics of production and the order of time they must work within. The structures of time are never completely inhumane, but they are maintained right at the outer limit of what most people can tolerate.

Cross-sector solidarity in labor organizing is nothing new. Many movements, such as those led by traditional labor unions, have connected workers in different industries to win the victories of paid overtime, workplace safety, parental leave, and weekends. But as powerful business lobbies and neoliberal governments have chipped away at labor rights and protections over the past several decades and limited the avenues for worker organizing and communications, cross-sector support has become more difficult.74 Now AI-driven systems of extraction and surveillance have become a shared locus for labor organizers to fight as a unified front.

“We are all tech workers” has become a common sign at tech-related protests, carried by programmers, janitors, cafeteria workers, and engineers alike.76 It can be read in multiple ways: it demands that the tech sector recognize the wide labor force it draws on to make its products, infrastructures, and workplaces function. It also reminds us that so many workers use laptops and mobile devices for work, engage on platforms like Facebook or Slack, and are subject to forms of workplace AI systems for standardization, tracking, and assessment. This has set the stage for a form of solidarity built around tech work. But there are risks in centering tech workers and technology in what are more generalized and long-standing labor struggles. All kinds of workers are subject to the extractive technical infrastructures that seek to control and analyze time to its finest grain—many of whom have no identification with the technology sector or tech work at all. The histories of labor and automation remind us that what is at stake is producing more just conditions for every worker, and this broader goal should not depend on expanding the definition of tech work in order to gain legitimacy. We all have a collective stake in what the future of work looks like.

3 Data

I’ve looked at hundreds of datasets over years of research into how AI systems are built, but the NIST mug shot databases are particularly disturbing because they represent the model of what was to come. It’s not just the overwhelming pathos of the images themselves. Nor is it solely the invasion of privacy they represent, since suspects and prisoners have no right to refuse being photographed. It’s that the NIST databases foreshadow the emergence of a logic that has now thoroughly pervaded the tech sector: the unswerving belief that everything is data and is there for the taking. It doesn’t matter where a photograph was taken or whether it reflects a moment of vulnerability or pain or if it represents a form of shaming the subject. It has become so normalized across the industry to take and use whatever is available that few stop to question the underlying politics.

Mug shots, in this sense, are the urtext of the current approach to making AI. The context—and exertion of power— that these images represent is considered irrelevant because they no longer exist as distinct things unto themselves. They are not seen to carry meanings or ethical weight as images of individual people or as representations of structural power in the carceral system. The personal, the social, and the political meanings are all imagined to be neutralized. I argue this represents a shift from image to infrastructure, where the meaning or care that might be given to the image of an individual person, or the context behind a scene, is presumed to be erased at the moment it becomes part of an aggregate mass that will drive a broader system. It is all treated as data to be run through functions, material to be ingested to improve technical performance. This is a core premise in the ideology of data extraction.

Machine learning systems are trained on images like these every day—images that were taken from the internet or from state institutions without context and without consent. They are anything but neutral. They represent personal histories, structural inequities, and all the injustices that have accompanied the legacies of policing and prison systems in the United States. But the presumption that somehow these images can serve as apolitical, inert material influences how and what a machine learning tool “sees.” A computer vision system can detect a face or a building but not why a person was inside a police station or any of the social and historical context surrounding that moment. Ultimately, the specific instances of data—a picture of a face, for example—aren’t considered to matter for training an AI model. All that matters is a sufficiently varied aggregate. Any individual image could easily be substituted for another and the system would work the same. According to this worldview, there is always more data to capture from the constantly growing and globally distributed treasure chest of the internet and social media platforms.

A person standing in front of a camera in an orange jumpsuit, then, is dehumanized as just more data. The history of these images, how they were acquired, and their institutional, personal, and political contexts are not considered relevant. The mug shot collections are used like any other practical resource of free, well-lit images of faces, a benchmark to make tools like facial recognition function. And like a tightening ratchet, the faces of deceased persons, suspects, and prisoners are harvested to sharpen the police and border surveillance facial recognition systems that are then used to monitor and detain more people.

The last decade has seen a dramatic capture of digital material for AI production. This data is the basis for sensemaking in AI, not as classical representations of the world with individual meaning, but as a mass collection of data for machine abstractions and operations. This large-scale capture has become so fundamental to the AI field that it is unquestioned. So how did we get here? What ways of conceiving data have facilitated this stripping of context, meaning, and specificity? How is training data acquired, understood, and used in machine learning? In what ways does training data limit what and how AI interprets the world? What forms of power do these approaches enhance and enable?

In this chapter I show how data has become a driving force in the success of AI and its mythos and how everything that can be readily captured is being acquired. But the deeper implications of this standard approach are rarely addressed, even as it propels further asymmetries of power. The AI industry has fostered a kind of ruthless pragmatism, with minimal context, caution, or consent-driven data practices while promoting the idea that the mass harvesting of data is necessary and justified for creating systems of profitable computational “intelligence.” This has resulted in a profound metamorphosis, where all forms of image, text, sound, and video are just raw data for AI systems and the ends are thought to justify the means. But we should ask: Who has benefited most from this transformation, and why have these dominant narratives of data persisted? And as we saw in the previous chapters, the logic of extraction that has shaped the relationship to the earth and to human labor is also a defining feature of how data is used and understood in AI. By looking closely at training data as a central example in the ensemble of machine learning, we can begin to see what is at stake in this transformation.

Training Machines to See

It’s useful to consider why machine learning systems currently demand massive amounts of data. One example of the problem in action is computer vision, the subfield of artificial intelligence concerned with teaching machines to detect and interpret images. For reasons that are rarely acknowledged in the field of computer science, the project of interpreting images is a profoundly complex and relational endeavor. Images are remarkably slippery things, laden with multiple potential meanings, irresolvable questions, and contradictions. Yet now it’s common practice for the first steps of creating a computer vision system to scrape thousands—or even millions—of images from the internet, create and order them into a series of classifications, and use this as a foundation for how the system will perceive observable reality. These vast collections are called training datasets, and they constitute what AI developers often refer to as “ground truth.”13 Truth, then, is less about a factual representation or an agreed-upon reality and more commonly about a jumble of images scraped from whatever various online sources were available.

For supervised machine learning, human engineers supply labeled training data to a computer. Two distinct types of algorithms then come into play: learners and classifiers. The learner is the algorithm that is trained on these labeled data examples; it then informs the classifier how best to analyze the relation between the new inputs and the desired target output (or prediction). It might be predicting whether a face is contained in an image or whether an email is spam. The more examples of correctly labeled data there are, the better the algorithm will be at producing accurate predictions. There are many kinds of machine learning models, including neural networks, logistic regression, and decision trees. Engineers will This content choose a model based on what they are building—be it a facial recognition system or a means of detecting sentiment on social media—and fit it to their computational resources.

Consider the task of building a machine learning system that can detect the difference between pictures of apples and oranges. First, a developer has to collect, label, and train a neural network on thousands of labeled images of apples and oranges. On the software side, the algorithms conduct a statistical survey of the images and develop a model to recognize the difference between the two classes. If all goes according to plan, the trained model will be able to distinguish the difference between images of apples and oranges that it has never encountered before.

But if, in our example, all of the training images of apples are red and none are green, then a machine learning system might deduce that “all apples are red.” This is what is known as an inductive inference, an open hypothesis based on available data, rather than a deductive inference, which follows logically from a premise.14 Given how this system was trained, a green apple wouldn’t be recognized as an apple at all. Training datasets, then, are at the core of how most machine learning systems make inferences. They serve as the primary source material that AI systems use to form the basis of their predictions.

Training data also defines more than just the features of machine learning algorithms. It is used to assess how they perform over time. Like prized thoroughbreds, machine learning algorithms are constantly raced against one another in competitions all over the world to see which ones perform the best with a given dataset. These benchmark datasets become the alphabet on which a lingua franca is based, with many labs from multiple countries converging around canonical sets to try to outperform one another. One of the best-known competitions is the ImageNet Challenge, where researchers compete to see whose methods can most accurately classify and detect objects and scenes.

Once training sets have been established as useful benchmarks, they are commonly adapted, built upon, and expanded. As we will see in the next chapter, a type of genealogy of training sets emerges—they inherit learned logic from earlier examples and then give rise to subsequent ones. For example, ImageNet draws on the taxonomy of words inherited from the influential 1980s lexical database known as WordNet; and WordNet inherits from many sources, including the Brown Corpus of one million words, published in 1961. Training datasets stand on the shoulders of older classifications and collections. Like an expanding encyclopedia, the older forms remain and new items are added over decades.

Training data, then, is the foundation on which contemporary machine learning systems are built.16 These datasets shape the epistemic boundaries governing how AI operates and, in that sense, create the limits of how AI can “see” the world. But training data is a brittle form of ground truth—and even the largest troves of data cannot escape the fundamental slippages that occur when an infinitely complex world is simplified and sliced into categories.

A Brief History of the Demand for Data

“The world has arrived at an age of cheap complex devices of great reliability; and something is bound to come of it.” So said Vannevar Bush, the inventor and administrator who oversaw the Manhattan Project as director of the Office of Scientific Research and Development and later was integral to the creation of the National Science Foundation. It was July 1945; the bombs were yet to drop on Hiroshima and Nagasaki, and Bush had a theory about a new kind of data-connecting system that was yet to be born. He envisaged the “advanced arithmetical machines of the future” that would perform at extremely fast speed and “select their own data and manipulate it in accordance with the instructions.” But the machines would need monumental amounts of data: “Such machines will have enormous appetites. One of them will take instructions and data from a whole roomful of girls armed with simple key board punches, and will deliver sheets of computed results every few minutes. There will always be plenty of things to compute in the detailed affairs of millions of people doing complicated things.”

The “roomful of girls” Bush referred to were the keypunch operators doing the day-to- day work of computation. As historians Jennifer Light and Mar Hicks have shown, these women were often dismissed as input devices for intelligible data records. In fact, their role was just as important to crafting data and making systems work as that of the engineers who designed the wartime-era digital computers.18 But the relationship between data and processing machinery was already being imagined as one of endless consumption. The machines would be data-hungry, and there would surely be a wide horizon of material to extract from millions of people.

In the 1970s, artificial intelligence researchers were mainly exploring what’s called an expert systems approach: rules-based programming that aims to reduce the field of possible actions by articulating forms of logical reasoning. But it quickly became evident that this approach was fragile and impractical in real-world settings, where a rule set was rarely able to handle uncertainty and complexity.19 New approaches were needed. By the mid-1980s, research labs were turning toward probabilistic or brute force approaches. In short, they were using lots of computing cycles to calculate as many options as possible to find the optimal result.

One significant example was the speech recognition group at IBM Research. The problem of speech recognition had primarily been dealt with using linguistic methods, but then information theorists Fred Jelinek and Lalit Bahl formed a new group, which included Peter Brown and Robert Mercer (long before Mercer became a billionaire, associated with funding Cambridge Analytica, Breitbart News, and Donald Trump’s 2016 presidential campaign). They tried something different. Their techniques ultimately produced precursors for the speech recognition systems underlying Siri and Dragon Dictate, as well as machine translation systems like Google Translate and Microsoft Translator.

They started using statistical methods that focused more on how often words appeared in relation to one another, rather than trying to teach computers a rules-based approach using grammatical principles or linguistic features. Making this statistical approach work required an enormous amount of real speech and text data, or training data. The result, as media scholar Xiaochang Li writes, was that it required “a radical reduction of speech to merely data, which could be modeled and interpreted in the absence of linguistic knowledge or understanding. Speech as such ceased to matter.” This shift was incredibly significant, and it would become a pattern repeated for decades: the reduction from context to data, from meaning to statistical pattern recognition. Li explains:

The reliance on data over linguistic principles, however, presented a new set of challenges, for it meant that the statistical models were necessarily determined by the characteristics of training data. As a result, the size of the dataset became a central concern. . . . Larger datasets of observed outcomes not only improved the probability estimates for a random process, but also increased the chance that the data would capture more rarely-occurring outcomes. Training data size, in fact, was so central to IBM’s approach that in 1985, Robert Mercer explained the group’s outlook by simply proclaiming, “There’s no data like more data.”

For several decades, that data was remarkably hard to come by. As Lalit Bahl describes in an interview with Li, “Back in those days . . . you couldn’t even find a million words in computer-readable text very easily. And we looked all over the place for text.”21 They tried IBM technical manuals, children’s novels, patents of laser technology, books for the blind, and even the typed correspondence of IBM Fellow Dick Garwin, who created the first hydrogen bomb design.22 Their method strangely echoed a short story by the science fiction author Stanislaw Lem, in which a man called Trurl decides to build a machine that would write poetry. He starts with “eight hundred and twenty tons of books on cybernetics and twelve thousand tons of the finest poetry.” But Trurl realizes that to program an autonomous poetry machine, one needs “to repeat the entire Universe from the beginning—or at least a good piece of it.”

Ultimately, the IBM Continuous Speech Recognition group found their “good piece” of the universe from an unlikely source. A major federal antitrust lawsuit was filed against IBM in 1969; the proceedings lasted for thirteen years, and almost a thousand witnesses were called. IBM employed a large staff just to digitize all of the deposition transcripts onto Hollerith punch cards. This ended up creating a corpus of a hundred million words by the mid-1980s. The notoriously antigovernment Mercer called this a “case of utility accidentally created by the government in spite of itself.”

IBM wasn’t the only group starting to gather words by the ton. From 1989 to 1992, a team of linguists and computer scientists at the University of Pennsylvania worked on the Penn Treebank Project, an annotated database of text. They collected four and a half million words of American English for the purpose of training natural language processing systems. Their sources included Department of Energy abstracts, Dow Jones newswire articles, and Federal News Service reports of “terrorist activity” in South America.26 The emerging text collections borrowed from earlier collections and then contributed new sources. Genealogies of data collections began to emerge, each building on the last—and often importing the same peculiarities, issues, or omissions wholesale.

Another classic corpus of text came from the fraud investigations of Enron Corporation after it declared the largest bankruptcy in American history. The Federal Energy Regulatory Commission seized the emails of 158 employees for the purposes of legal discovery.27 It also decided to release these emails online because “the public’s right to disclosure outweighs the individual’s right to privacy.”28 This became an extraordinary collection. Over half a million exchanges in everyday speech could now be used as a linguistic mine: one that nonetheless represented the gender, race, and professional skews of those 158 workers. The Enron corpus has been cited in thousands of academic papers. Despite its popularity, it is rarely looked at closely: the New Yorker described it as “a canonic research text that no one has actually read.”29 This construction of and reliance on training data anticipated a new way of doing things. It transformed the field of natural language processing and laid the foundations of what would become normal practice in machine learning.

The seeds of later problems were planted here. Text archives were seen as neutral collections of language, as though there was a general equivalence between the words in a technical manual and how people write to colleagues via email. All text was repurposable and swappable, so long as there was enough of it that it could train a language model to predict with high levels of success what word might follow another. Like images, text corpuses work on the assumption that all training data is interchangeable. But language isn’t an inert substance that works the same way regardless of where it is found. Sentences taken from Reddit will be different from those composed by executives at Enron. Skews, gaps, and biases in the collected text are built into the bigger system, and if a language model is based on the kinds of words that are clustered together, it matters where those words come from. There is no neutral ground for language, and all text collections are also accounts of time, place, culture, and politics. Further, languages that have less available data are not served by these approaches and so are often left behind.

Clearly there are many histories and contexts that combine within IBM’s training data, the Enron archive, or the Penn Treebank. How do we unpack what is and is not meaningful to understand these datasets? How does one communicate warnings like, “This dataset likely reflects skews related to its reliance on news stories about South American terrorists in the 1980s”? The origins of the underlying data in a system can be incredibly significant, and yet there are still, thirty years later, no standardized practices to note where all this data came from or how it was acquired—let alone what biases or classificatory politics these datasets contain that will influence all the systems that come to rely on them.

Capturing the Face

While computer-readable text was becoming highly valued for speech recognition, the human face was the core concern for building systems of facial recognition. One central example emerged in the last decade of the twentieth century, funded by the Department of Defense CounterDrug Technology Development Program Office. It sponsored the Face Recognition Technology (FERET) program to develop automatic face recognition for intelligence and law enforcement. Before FERET, little training data of human faces was available, only a few collections of fifty or so faces here and there—not enough to do facial recognition at scale. The U.S. Army Research Laboratory led the technical project of creating a training set of portraits of more than a thousand people, in multiple poses, to make a grand total of 14,126 images. Like NIST’s mug shot collections, FERET became a standard benchmark—a shared measuring tool to compare approaches for detecting faces.

The tasks that the FERET infrastructure was created to support included, once again, automated searching of mug shots, as well as monitoring airports and border crossings and searching driver’s license databases for “fraud detection” (multiple welfare claims was a particular example mentioned in FERET research papers).32 But there were two primary testing scenarios. In the first, an electronic mug book of known individuals would be presented to an algorithm, which then had to locate the closest matches from a large gallery. The second scenario focused on border and airport control: identifying a known individual—“smugglers, terrorists, or other criminals”— from a large population of unknown people.

These photographs are machine-readable by design, and not meant for human eyes, yet they make for remarkable viewing. The images are surprisingly beautiful—high-resolution photographs captured in the style of formal portraiture. Taken with 35 mm cameras at George Mason University, the tightly framed headshots depict a wide range of people, some of whom seem to have dressed for the occasion with carefully styled hair, jewelry, and makeup. The first set of photographs, taken between 1993 and 1994, are like a time capsule of early nineties haircuts and fashion. The subjects were asked to turn their heads to multiple positions; flicking through the images, you can see profile shots, frontal images, varying levels of illumination, and sometimes different outfits. Some subjects were photographed over several years, in order to begin to study how to track people as they age. Each subject was briefed about the project and signed a release form that had been approved by the university’s ethics review board. Subjects knew what they were participating in and gave full consent.33 This level of consent would become a rarity in later years.

FERET was the high-water mark of a formal style of “making data,” before the internet began offering mass extraction without any permissions or careful camera work. Even at this early stage, though, there were problems with the lack of diversity of the faces collected. The FERET research paper from 1996 admits that “some questions were raised about the age, racial, and sexual distribution of the database” but that “at this stage of the program, the key issue was algorithm performance on a database of a large number of individuals.”34 Indeed, FERET was extraordinarily useful for this. As the interest in terrorist detection intensified and funding for facial recognition dramatically increased after 9/11, FERET became the most commonly used benchmark. From that point onward, biometric tracking and automated vision systems would rapidly expand in scale and ambition.

From the Internet to ImageNet

The internet, in so many ways, changed everything; it came to be seen in the AI research field as something akin to a natural resource, there for the taking. As more people began to upload their images to websites, to photo-sharing services, and ultimately to social media platforms, the pillaging began in earnest. Suddenly, training sets could reach a size that scientists in the 1980s could never have imagined. Gone was the need to stage photo shoots using multiple lighting conditions, controlled parameters, and devices to position the face. Now there were millions of selfies in every possible lighting condition, position, and depth of field. People began to share their baby photos, family snaps, and images of how they looked a decade ago, an ideal resource for tracking genetic similarity and face aging. Trillions of lines of text, containing both formal and informal forms of speech, were published every day. It was all grist for the mills of machine learning. And it was vast. As an example, on an average day in 2019, approximately 350 million photographs were uploaded to Facebook and 500 million tweets were sent.35 And that’s just two platforms based in the United States. Anything and everything online was primed to become a training set for AI.

The tech industry titans were now in a powerful position: they had a pipeline of endlessly refreshing images and text, and the more people shared their content, the more the tech industry’s power grew. People would happily label their photographs with names and locations, free of charge, and that unpaid labor resulted in having more accurate, labeled data for machine vision and language models. Within the industry, these collections are highly valuable. They are proprietary troves that are rarely shared, given both the privacy issues and the competitive advantage they represent. But those outside the industry, such as the leading computer science labs in academia, wanted the same advantages. How could they afford to harvest people’s data and have it hand-labeled by willing human participants? That’s when new ideas began to emerge: combining images and text extracted from the internet with the labor of low-paid crowdworkers.

One of the most significant training sets in AI is Image- Net. It was first conceptualized in 2006, when Professor Fei-Fei Li decided to build an enormous dataset for object recognition. “We decided we wanted to do something that was completely historically unprecedented,” Li said. “We’re going to map out the entire world of objects.”36 The breakthrough research poster was published by the ImageNet team at a computer vision conference in 2009. It opened with this description:

The digital era has brought with it an enormous explosion of data. The latest estimations put a number of more than 3 billion photos on Flickr, a similar number of video clips on YouTube and an even larger number for images in the Google Image Search database. More sophisticated and robust models and algorithms can be proposed by exploiting these images, resulting in better applications for users to index, retrieve, organize and interact with these data.

From the outset, data was characterized as something voluminous, disorganized, impersonal, and ready to be exploited. According to the authors, “Exactly how such data can be utilized and organized is a problem yet to be solved.” By extracting millions of images from the internet, primarily from search engines using the image-search option, the team produced a “large-scale ontology of images” that was meant to serve as a resource for “providing critical training and benchmarking data” for object and image recognition algorithms. Using this approach, ImageNet grew enormous. The team mass-harvested more than fourteen million images from the internet to be organized into more than twenty thousand categories. Ethical concerns about taking people’s data were not mentioned in any of the team’s research papers, even though many thousands of the images were of a highly personal and compromising nature.

Once the images had been scraped from the internet, a major concern arose: Who would label them all and put them into intelligible categories? As Li describes it, the team’s first plan was to hire undergraduate students for ten dollars an hour to find images manually and add them to the dataset.38 But she realized that with their budget, it would take more than ninety years to complete the project. The answer came when a student told Li about a new service: Amazon Mechanical Turk. As we saw in chapter 2, this distributed platform meant that it was suddenly possible to access a distributed labor force to do online tasks, like labeling and sorting images, at scale and at low cost. “He showed me the website, and I can tell you literally that day I knew the ImageNet project was going to happen,” Li said. “Suddenly we found a tool that could scale, that we could not possibly dream of by hiring Princeton undergrads.”39 Unsurprisingly, the undergraduates did not get the job.

Instead, ImageNet would become, for a time, the world’s largest academic user of Amazon’s Mechanical Turk, deploying an army of piecemeal workers to sort an average of fifty images a minute into thousands of categories.40 There were categories for apples and airplanes, scuba divers and sumo wrestlers. But there were cruel, offensive, and racist labels, too: photographs of people were classified into categories like “alcoholic,” “ape-man,” “crazy,” “hooker,” and “slant eye.” All of these terms were imported from WordNet’s lexical database and given to crowdworkers to pair with images. Over the course of a decade, ImageNet grew into a colossus of object recognition for machine learning and a powerfully important benchmark for the field. The approach of mass data extraction without consent and labeling by underpaid crowdworkers would become standard practice, and hundreds of new training datasets would follow ImageNet’s lead. As we will see in the next chapter, these practices—and the labeled data they generated— eventually came back to haunt the project.

The End of Consent

The early years of the twenty-first century marked a shift away from consent-driven data collection. In addition to dispensing with the need for staged photo shoots, those responsible for assembling datasets presumed that the contents of the internet were theirs for the taking, beyond the need for agreements, signed releases, and ethics reviews. Now even more troubling practices of extraction began to emerge. For example, at the Colorado Springs campus of the University of Colorado, a professor installed a camera on the main walkway of the campus and secretly captured photos of more than seventeen hundred students and faculty—all to train a facial recognition system of his own.41 A similar project at Duke University harvested footage of more than two thousand students without their knowledge as they went between their classes and then published the results on the internet. The dataset, called DukeMTMC (for multitarget, multicamera facial recognition), was funded by the U.S. Army Research Office and the National Science Foundation.

The DukeMTMC project was roundly criticized after an investigative project by artists and researchers Adam Harvey and Jules LaPlace showed that the Chinese government was using the images to train systems for the surveillance of ethnic minorities. This spurred an investigation by Duke’s institutional review board, which determined that this was a “significant deviation” from acceptable practices. The dataset was removed from the internet.

But what happened at the University of Colorado and Duke were by no means isolated cases. At Stanford University, researchers commandeered a webcam from a popular café in San Francisco to extract almost twelve thousand images of “everyday life of a busy downtown café” without anyone’s consent. 44 Over and over, data extracted without permission or consent would be uploaded for machine learning researchers, who would then use it as an infrastructure for automated imaging systems.

Another example is Microsoft’s landmark training dataset MS-Celeb, which scraped approximately ten million photos of a hundred thousand celebrities from the internet in 2016. At the time, it was the largest public facial recognition dataset in the world, and the people included were not just famous actors and politicians but also journalists, activists, policymakers, academics, and artists.45 Ironically, several of the people who had been included in the set without consent are known for their work critiquing surveillance and facial recognition itself, including documentary filmmaker Laura Poitras; digital rights activist Jillian York; critic Evgeny Morozov; and the author of Surveillance Capitalism, Shoshana Zuboff.

Even when datasets are scrubbed of personal information and released with great caution, people have been reidentified or highly sensitive details about them have been revealed. In 2013, for example, the New York City Taxi and Limousine Commission released a dataset of 173 million individual cab rides, and it included pickup and drop-off times, locations, fares, and tip amounts. The taxi drivers’ medallion numbers were anonymized, but this was quickly undone, enabling researchers to infer sensitive information like annual incomes and home addresses.47 Once combined with public information from sources like celebrity blogs, some actors and politicians were identified, and it was possible to deduce the addresses of people who visited strip clubs.48 But beyond individual harms, such datasets also generate “predictive privacy harms” for whole groups or communities.49 For instance, the same New York City taxi dataset was used to suggest which taxi drivers were devout Muslims by observing when they stopped at prayer times.

From any seemingly innocuous and anonymized dataset can come many unexpected and highly personal forms of information, but this fact has not hampered the collection of images and text. As success in machine learning has come to rely on ever-larger datasets, more people are seeking to acquire them. But why does the wider AI field accept this practice, despite the ethical, political, and epistemological problems and potential harms? What beliefs, justifications, and economic incentives normalized this mass extraction and general equivalence of data?

Myths and Metaphors of Data

The oft-cited history of artificial intelligence written by AI professor Nils Nilsson outlines several of the founding myths about data in machine learning. He neatly illustrates how data is typically described in the technical disciplines: “The great volume of raw data calls for efficient ‘data-mining’ techniques for classifying, quantifying, and extracting useful information. Machine learning methods are playing an increasingly important role in data analysis because they can deal with massive amounts of data. In fact, the more data the better."

Echoing Robert Mercer from decades earlier, Nilsson perceived that data was everywhere for the taking, and all the better for mass classification by machine learning algorithms. It was such a common belief as to have become axiomatic: data is there to be acquired, refined, and made valuable.

But vested interests carefully manufactured and supported this belief over time. As sociologists Marion Fourcade and Kieran Healy note, the injunction always to collect data came not only from the data professions but also from their institutions and the technologies they deploy:

The institutional command coming from technology is the most potent of all: we do these things because we can. . . . Professionals recommend, the institutional environment demands, and technology enables organizations to sweep up as much individual data as possible. It does not matter that the amounts collected may vastly exceed a firm’s imaginative reach or analytic grasp. The assumption is that it will eventually be useful, i.e. valuable. . . . Contemporary organizations are both culturally impelled by the data imperative and powerfully equipped with new tools to enact it.

This produced a kind of moral imperative to collect data in order to make systems better, regardless of the negative impacts the data collection might cause at any future point. Behind the questionable belief that “more is better” is the idea that individuals can be completely knowable, once enough disparate pieces of data are collected. But what counts as data? Historian Lisa Gitelman notes that every discipline and institution “has its own norms and standards for the imagination of data.” Data, in the twenty-first century, became whatever could be captured.

Terms like “data mining” and phrases like “data is the new oil” were part of a rhetorical move that shifted the notion of data away from something personal, intimate, or subject to individual ownership and control toward something more inert and nonhuman. Data began to be described as a resource to be consumed, a flow to be controlled, or an investment to be harnessed.56 The expression “data as oil” became commonplace, and although it suggested a picture of data as a crude material for extraction, it was rarely used to emphasize the costs of the oil and mining industries: indentured labor, geopolitical conflicts, depletion of resources, and consequences stretching beyond human timescales.

Ultimately, “data” has become a bloodless word; it disguises both its material origins and its ends. And if data is seen as abstract and immaterial, then it more easily falls outside of traditional understandings and responsibilities of care, consent, or risk. As researchers Luke Stark and Anna Lauren Hoffman argue, metaphors of data as a “natural resource” just lying in wait to be discovered are a well-established rhetorical trick used for centuries by colonial powers.57 Extraction is justified if it comes from a primitive and “unrefined” source.58 If data is framed as oil, just waiting to be extracted, then machine learning has come to be seen as its necessary refinement process.

Data also started to be viewed as capital, in keeping with the broader neoliberal visions of markets as the primary forms of organizing value. Once human activities are expressed through digital traces and then tallied up and ranked within scoring metrics, they function as a way to extract value. As Fourcade and Healy observe, those who have the right data signals gain advantages like discounted insurance and higher standing across markets.59 High achievers in the mainstream economy tend to do well in a data-scoring economy, too, while those who are poorest become targets of the most harmful forms of data surveillance and extraction. When data is considered as a form of capital, then everything is justified if it means collecting more. The sociologist Jathan Sadowski similarly argues that data now operates as a form of capital. He suggests that once everything is understood as data, it justifies a cycle of ever-increasing data extraction: “Data collection is thus driven by the perpetual cycle of capital accumulation, which in turn drives capital to construct and rely upon a world in which everything is made of data. The supposed universality of data reframes everything as falling under the domain of data capitalism. All spaces must be subjected to datafication. If the universe is conceived of as a potentially infinite reserve of data, then that means the accumulation and circulation of data can be sustained forever.”

This drive to accumulate and circulate is the powerful underlying ideology of data. Mass data extraction is the “new frontier of accumulation and next step in capitalism,” Sadowski suggests, and it is the foundational layer that makes AI function.61 Thus, there are entire industries, institutions, and individuals who don’t want this frontier—where data is there for the taking—to be questioned or destabilized.

Machine learning models require ongoing flows of data to become more accurate. But machines are asymptotic, never reaching full precision, which propels the justification for more extraction from as many people as possible to fuel the refineries of AI. This has created a shift away from ideas like “human subjects”—a concept that emerged from the ethics debates of the twentieth century—to the creation of “data subjects,” agglomerations of data points without subjectivity or context or clearly defined rights.

Ethics at Arm's Length

The great majority of university-based AI research is done without any ethical review process. But if machine learning techniques are being used to inform decisions in sensitive domains like education and health care, then why are they not subject to greater review? To understand that, we need to look at the precursor disciplines of artificial intelligence. Before the emergence of machine learning and data science, the fields of applied mathematics, statistics, and computer science had not historically been considered forms of research on human subjects.

In the early decades of AI, research using human data was usually seen to be a minimal risk.62 Even though datasets in machine learning often come from and represent people and their lives, the research that used those datasets was seen more as a form of applied math with few consequences for human subjects. The infrastructures of ethics protections, like university-based institutional review boards (IRBs), had accepted this position for years.63 This initially made sense; IRBs had been overwhelmingly focused on the methods common to biomedical and psychological experimentation in which interventions carry clear risks to individual subjects. Computer science was seen as far more abstract.

Once AI moved out of the laboratory contexts of the 1980s and 1990s and into real-world situations—such as attempting to predict which criminals will reoffend or who should receive welfare benefits—the potential harms expanded. Further, those harms affect entire communities as well as individuals. But there is still a strong presumption that publicly available datasets pose minimal risks and therefore should be exempt from ethics review.64 This idea is the product of an earlier era, when it was harder to move data between locations and very expensive to store it for long periods. Those earlier assumptions are out of step with what is currently going on in machine learning. Now datasets are more easily connectable, indefinitely repurposable, continuously updatable, and frequently removed from the context of collection.

The risk profile of AI is rapidly changing as its tools become more invasive and as researchers are increasingly able to access data without interacting with their subjects. For example, a group of machine learning researchers published a paper in which they claimed to have developed an “automatic system for classifying crimes.”65 In particular, their focus was on whether a violent crime was gang-related, which they claimed their neural network could predict with only four pieces of information: the weapon, the number of suspects, the neighborhood, and the location. They did this using a crime dataset from the Los Angeles Police Department, which included thousands of crimes that had been labeled by police as gang-related.

Gang data is notoriously skewed and riddled with errors, yet researchers use this database and others like it as a definitive source for training predictive AI systems. The CalGang database, for example, which is widely used by police in California, has been shown to have major inaccuracies. The state auditor discovered that 23 percent of the hundreds of records it reviewed lacked adequate support for inclusion. The database also contained forty-two infants, twenty-eight of whom were listed for having “admitting to being gang members.”66 Most of the adults on the list had never been charged, but once they were included in the database, there was no way to have their name removed. Reasons for being included might be as simple as chatting with a neighbor while wearing a red shirt; using these trifling justifications, Black and Latinx people have been disproportionately added to the list.

When the researchers presented their gang-crime prediction project at a conference, some attendees were troubled. As reported by Science, questions from the audience included, “How could the team be sure the training data were not biased to begin with?” and “What happens when someone is mislabeled as a gang member?” Hau Chan, a computer scientist now at Harvard University who presented the work, responded that he couldn’t know how the new tool would be used. “[These are the] sort of ethical questions that I don’t know how to answer appropriately,” he said, being just “a researcher.” An audience member replied by quoting a lyric from Tom Lehrer’s satiric song about the wartime rocket scientist Wernher von Braun: “Once the rockets are up, who cares where they come down?"

This separation of ethical questions away from the technical reflects a wider problem in the field, where the responsibility for harm is either not recognized or seen as beyond the scope of the research. As Anna Lauren Hoffman writes: “The problem here isn’t only one of biased datasets or unfair algorithms and of unintended consequences. It’s also indicative of a more persistent problem of researchers actively reproducing ideas that damage vulnerable communities and reinforce current injustices. Even if the Harvard team’s proposed system for identifying gang violence is never implemented, hasn’t a kind of damage already been done? Wasn’t their project an act of cultural violence in itself?”69 Sidelining issues of ethics is harmful in itself, and it perpetuates the false idea that scientific research happens in a vacuum, with no responsibility for the ideas it propagates.

The reproduction of harmful ideas is particularly dangerous now that AI has moved from being an experimental discipline used only in laboratories to being tested at scale on millions of people. Technical approaches can move rapidly from conference papers to being deployed in production systems, where harmful assumptions can become ingrained and hard to reverse.

Machine learning and data-science methods can create an abstract relationship between researchers and subjects, where work is being done at a distance, removed from the communities and individuals at risk of harm. This arm’s-length relationship of AI researchers to the people whose lives are reflected in datasets is a long-established practice. Back in 1976, when AI scientist Joseph Weizenbaum wrote his scathing critique of the field, he observed that computer science was already seeking to circumvent all human contexts.70 He argued that data systems allowed scientists during wartime to operate at a psychological distance from the people “who would be maimed and killed by the weapons systems that would result from the ideas they communicated.”71 The answer, in Weizenbaum’s view, was to directly contend with what data actually represents: “The lesson, therefore, is that the scientist and technologist must, by acts of will and of the imagination, actively strive to reduce such psychological distances, to counter the forces that tend to remove him from the consequences of his actions. He must—it is as simple as this—think of what he is actually doing.”

Weizenbaum hoped that scientists and technologists would think more deeply about the consequences of their work—and of who might be at risk. But this would not become the standard of the AI field. Instead, data is more commonly seen as something to be taken at will, used without restriction, and interpreted without context. There is a rapacious international culture of data harvesting that can be exploitative and invasive and can produce lasting forms of harm.73 And there are many industries, institutions, and individuals who are strongly incentivized to maintain this colonizing attitude—where data is there for the taking—and they do not want it questioned or regulated.

The Capture of the Commons

The current widespread culture of data extraction continues to grow despite concerns about privacy, ethics, and safety. By researching the thousands of datasets that are freely available for AI development, I got a glimpse into what technical systems are built to recognize, of how the world is rendered for computers in ways that humans rarely see. There are gigantic datasets full of people’s selfies, tattoos, parents walking with their children, hand gestures, people driving their cars, people committing crimes on CCTV, and hundreds of everyday human actions like sitting down, waving, raising a glass, or crying. Every form of biodata—including forensic, biometric, sociometric, and psychometric—is being captured and logged into databases for AI systems to find patterns and make assessments.

Training sets raise complex questions from ethical, methodological, and epistemological perspectives. Many were made without people’s knowledge or consent and were harvested from online sources like Flickr, Google image search, and You- Tube or were donated by government agencies like the FBI. This data is now used to expand facial recognition systems, modulate health insurance rates, penalize distracted drivers, and fuel predictive policing tools. But the practices of data extraction are extending even deeper into areas of human life that were once off-limits or too expensive to reach. Tech companies have drawn on a range of approaches to gain new ground. Voice data is gathered from devices that sit on kitchen counters or bedroom nightstands; physical data comes from watches on wrists and phones in pockets; data about what books and newspapers are read comes from tablets and laptops; gestures and facial expressions are compiled and assessed in workplaces and classrooms.

The collection of people’s data to build AI systems raises clear privacy concerns. Take, for example, the deal that Britain’s Royal Free National Health Service Foundation Trust made with Google’s subsidiary DeepMind to share the patient data records of 1.6 million people. The National Health Service in Britain is a revered institution, entrusted to provide health care that is primarily free to all while keeping patient data secure. But when the agreement with DeepMind was investigated, the company was found to have violated data protection laws by not sufficiently informing patients.74 In her findings, the information commissioner observed that “the price of innovation does not need to be the erosion of fundamental privacy rights.”

Yet there are other serious issues that receive less attention than privacy. The practices of data extraction and training dataset construction are premised on a commercialized capture of what was previously part of the commons. This particular form of erosion is a privatization by stealth, an extraction of knowledge value from public goods. A dataset may still be publicly available, but the metavalue of the data—the model created by it—is privately held. Certainly, many good things can be done with public data. But there has been a social and, to some degree, a technical expectation that the value of data shared via public institutions and public spaces online should come back to the public good in other forms of the commons. Instead, we see a handful of privately owned companies that now have enormous power to extract insights and profits from those sources. The new AI gold rush consists of enclosing different fields of human knowing, feeling, and action—every type of available data—all caught in an expansionist logic of never-ending collection. It has become a pillaging of public space.

Fundamentally, the practices of data accumulation over many years have contributed to a powerful extractive logic, a logic that is now a core feature of how the AI field works. This logic has enriched the tech companies with the largest data pipelines, while the spaces free from data collection have dramatically diminished. As Vannevar Bush foresaw, machines have enormous appetites. But how and what they are fed has an enormous impact on how they will interpret the world, and the priorities of their masters will always shape how that vision is monetized. By looking at the layers of training data that shape and inform AI models and algorithms, we can see that gathering and labeling data about the world is a social and political intervention, even as it masquerades as a purely technical one.

The way data is understood, captured, classified, and named is fundamentally an act of world-making and containment. It has enormous ramifications for the way artificial intelligence works in the world and which communities are most affected. The myth of data collection as a benevolent practice in computer science has obscured its operations of power, protecting those who profit most while avoiding responsibility for its consequences. 

4 Classification

The tendency to focus on the issue of bias in artificial intelligence has drawn us away from assessing the core practices of classification in AI, along with their attendant politics. To see that in action, in this chapter we’ll explore some of the training datasets of the twenty-first century and observe how their schemas of social ordering naturalize hierarchies and magnify inequalities. We will also look at the limits of the bias debates in AI, where mathematical parity is frequently proposed to produce “fairer systems” instead of contending with underlying social, political, and economic structures. In short, we will consider how artificial intelligence uses classification to encode power.

Systems of Circular Logic

A decade ago, the suggestion that there could be a problem of bias in artificial intelligence was unorthodox. But now examples of discriminatory AI systems are legion, from gender bias in Apple’s creditworthiness algorithms to racism in the COMPAS criminal risk assessment software and to age bias in Facebook’s ad targeting.17 Image recognition tools miscategorize Black faces, chatbots adopt racist and misogynistic language, voice recognition software fails to recognize female-sounding voices, and social media platforms show more highly paid job advertisements to men than to women.18 As scholars like Ruha Benjamin and Safiya Noble have shown, there are hundreds of examples throughout the tech ecosystem.19 Many more have never been detected or publicly admitted.

The typical structure of an episode in the ongoing AI bias narrative begins with an investigative journalist or whistleblower revealing how an AI system is producing discriminatory results. The story is widely shared, and the company in question promises to address the issue. Then either the system is superseded by something new, or technical interventions are made in the attempt to produce results with greater parity. Those results and technical fixes remain proprietary and secret, and the public is told to rest assured that the malady of bias has been “cured.”20 It is much rarer to have a public debate about why these forms of bias and discrimination frequently recur and whether more fundamental problems are at work than simply an inadequate underlying dataset or a poorly designed algorithm.

One of the more vivid examples of bias in action comes from an insider account at Amazon. In 2014, the company decided to experiment with automating the process of recommending and hiring workers. If automation had worked to drive profits in product recommendation and warehouse organization, it could, the logic went, make hiring more efficient. In the words of one engineer, “They literally wanted it to be an engine where I’m going to give you 100 resumes, it will spit out the top five, and we’ll hire those.”21 The machine learning system was designed to rank people on a scale of one to five, mirroring Amazon’s system of product ratings. To build the underlying model, Amazon’s engineers used a dataset of ten years’ worth of résumés from fellow employees and then trained a statistical model on fifty thousand terms that appeared in those résumés. Quickly, the system began to assign less importance to commonly used engineering terms, like programming languages, because everyone listed them in their job histories. Instead, the models began valuing more subtle cues that recurred on successful applications. A strong preference emerged for particular verbs. The examples the engineers mentioned were “executed” and “captured.”

Recruiters starting using the system as a supplement to their usual practices.23 Soon enough, a serious problem emerged: the system wasn’t recommending women. It was actively downgrading résumés from candidates who attended women’s colleges, along with any résumés that even included the word “women.” Even after editing the system to remove the influence of explicit references to gender, the biases remained. Proxies for hegemonic masculinity continued to emerge in the gendered use of language itself. The model was biased against women not just as a category but against commonly gendered forms of speech.

Inadvertently, Amazon had created a diagnostic tool. The vast majority of engineers hired by Amazon over ten years had been men, so the models they created, which were trained on the successful résumés of men, had learned to recommend men for future hiring. The employment practices of the past and present were shaping the hiring tools for the future. Amazon’s system unexpectedly revealed the ways bias already existed, from the way masculinity is encoded in language, in résumés, and in the company itself. The tool was an intensification of the existing dynamics of Amazon and highlighted the lack of diversity across the AI industry past and present.

Amazon ultimately shut down its hiring experiment. But the scale of the bias problem goes much deeper than a single system or failed approach. The AI industry has traditionally understood the problem of bias as though it is a bug to be fixed rather than a feature of classification itself. The result has been a focus on adjusting technical systems to produce greater quantitative parity across disparate groups, which, as we’ll see, has created its own problems.

Understanding the relation between bias and classification requires going beyond an analysis of the production of knowledge—such as determining whether a dataset is biased or unbiased—and, instead, looking at the mechanics of knowledge construction itself, what sociologist Karin Knorr Cetina calls the “epistemic machinery.”25 To see that requires observing how patterns of inequality across history shape access to resources and opportunities, which in turn shape data. That data is then extracted for use in technical systems for classification and pattern recognition, which produces results that are perceived to be somehow objective. The result is a statistical ouroboros: a self-reinforcing discrimination machine that amplifies social inequalities under the guise of technical neutrality.

The Limits of Debasing Systems

To better understand the limitations of analyzing AI bias, we can look to the attempts to fix it. In 2019, IBM tried to respond to concerns about bias in its AI systems by creating what the company described as a more “inclusive” dataset called Diversity in Faces (DiF).26 DiF was part of an industry response to the groundbreaking work released a year earlier by researchers Joy Buolamwini and Timnit Gebru that had demonstrated that several facial recognition systems—including those by IBM, Microsoft, and Amazon—had far greater error rates for people with darker skin, particularly women. As a result, efforts were ongoing inside all three companies to show progress on rectifying the problem.

“We expect face recognition to work accurately for each of us,” the IBM researchers wrote, but the only way that the “challenge of diversity could be solved” would be to build “a data set comprised from the face of every person in the world.”28 IBM’s researchers decided to draw on a preexisting dataset of a hundred million images taken from Flickr, the largest publicly available collection on the internet at the time.29 They then used one million photos as a small sample and measured the craniofacial distances between landmarks in each face: eyes, nasal width, lip height, brow height, and so on. Like Morton measuring skulls, the IBM researchers sought to assign cranial measures and create categories of difference.

The IBM team claimed that their goal was to increase diversity of facial recognition data. Though well intentioned, the classifications they used reveal the politics of what diversity meant in this context. For example, to label the gender and age of a face, the team tasked crowdworkers to make subjective annotations, using the restrictive model of binary gender. Anyone who seemed to fall outside of this binary was removed from the dataset. IBM’s vision of diversity emphasized the expansive options for cranial orbit height and nose bridges but discounted the existence of trans or gender nonbinary people. “Fairness” was reduced to meaning higher accuracy rates for machine-led facial recognition, and “diversity” referred to a wider range of faces to train the model. Craniometric analysis functions like a bait and switch, ultimately depoliticizing the idea of diversity and replacing it with a focus on variation. Designers get to decide what the variables are and how people are allocated to categories. Again, the practice of classification is centralizing power: the power to decide which differences make a difference.

IBM’s researchers go on to state an even more problematic conclusion: “Aspects of our heritage—including race, ethnicity, culture, geography—and our individual identity—age, gender and visible forms of self-expression— are reflected in our faces.”30 This claim goes against decades of research that has challenged the idea that race, gender, and identity are biological categories at all but are better understood as politically, culturally, and socially constructed.31 Embedding identity claims in technical systems as though they are facts observable from the face is an example of what Simone Browne calls “digital epidermalization,” the imposition of race on the body. Browne defines this as the exercise of power when the disembodied gaze of surveillance technologies “do the work of alienating the subject by producing a ‘truth’ about the body and one’s identity (or identities) despite the subject’s claims.”

The foundational problems with IBM’s approach to classifying diversity grow out of this kind of centralized production of identity, led by the machine learning techniques that were available to the team. Skin color detection is done because it can be, not because it says anything about race or produces a deeper cultural understanding. Similarly, the use of cranial measurement is done because it is a method that can be done with machine learning. The affordances of the tools become the horizon of truth. The capacity to deploy cranial measurements and digital epidermalization at scale drives a desire to find meaning in these approaches, even if this method has nothing to do with culture, heritage, or diversity. They are used to increase a problematic understanding of accuracy. Technical claims about accuracy and performance are commonly shot through with political choices about categories and norms but are rarely acknowledged as such.33 These approaches are grounded in an ideological premise of biology as destiny, where our faces become our fate.

The Many Definitions of Bias

Since antiquity, the act of classification has been aligned with power. In theology, the ability to name and divide things was a divine act of God. The word “category” comes from the Ancient Greek katēgoríā, formed from two roots: kata (against) and agoreuo (speaking in public). In Greek, the word can be either a logical assertion or an accusation in a trial—alluding to both scientific and legal methods of categorization.

The historical lineage of “bias” as a term is much more recent. It first appears in fourteenth-century geometry, where it refers to an oblique or diagonal line. By the sixteenth century, it had acquired something like its current popular meaning, of “undue prejudice.” By the 1900s, “bias” had developed a more technical meaning in statistics, where it refers to systematic differences between a sample and population, when the sample is not truly reflective of the whole.34 It is from this statistical tradition that the machine learning field draws its understanding of bias, where it relates to a set of other concepts: generalization, classification, and variance.

Machine learning systems are designed to be able to generalize from a large training set of examples and to correctly classify new observations not included in the training datasets. 35 In other words, machine learning systems can perform a type of induction, learning from specific examples (such as past résumés of job applicants) in order to decide which data points to look for in new examples (such as word groupings in résumés from new applicants). In such cases, the term “bias” refers to a type of error that can occur during this predictive process of generalization—namely, a systematic or consistently reproduced classification error that the system exhibits when presented with new examples. This type of bias is often contrasted with another type of generalization error, variance, which refers to an algorithm’s sensitivity to differences in training data. A model with high bias and low variance may be underfitting the data—failing to capture all of its significant features or signals. Alternatively, a model with high variance and low bias may be overfitting the data—building a model too close to the training data so that it potentially captures “noise” in addition to the data’s significant features.

Outside of machine learning, “bias” has many other meanings. For instance, in law, bias refers to a preconceived notion or opinion, a judgment based on prejudices, as opposed to a decision come to from the impartial evaluation of the facts of a case.37 In psychology, Amos Tversky and Daniel Kahneman study “cognitive biases,” or the ways in which human judgments deviate systematically from probabilistic expectations. 38 More recent research on implicit biases emphasizes the ways that unconscious attitudes and stereotypes “produce behaviors that diverge from a person’s avowed or endorsed beliefs or principles.”39 Here bias is not simply a type of technical error; it also opens onto human beliefs, stereotypes, or forms of discrimination. These definitional distinctions limit the utility of “bias” as a term, especially when used by practitioners from different disciplines.

Technical designs can certainly be improved to better account for how their systems produce skews and discriminatory results. But the harder questions of why AI systems perpetuate forms of inequity are commonly skipped over in the rush to arrive at narrow technical solutions of statistical bias as though that is a sufficient remedy for deeper structural problems. There has been a general failure to address the ways in which the instruments of knowledge in AI reflect and serve the incentives of a wider extractive economy. What remains is a persistent asymmetry of power, where technical systems maintain and extend structural inequality, regardless of the intention of the designers.

Every dataset used to train machine learning systems, whether in the context of supervised or unsupervised machine learning, whether seen to be technically biased or not, contains a worldview. To create a training set is to take an almost infinitely complex and varied world and fix it into taxonomies composed of discrete classifications of individual data points, a process that requires inherently political, cultural, and social choices. By paying attention to these classifications, we can glimpse the various forms of power that are built into the architectures of AI world-building.

Training Sets as Classification Engines: The Case of ImageNet

In the last chapter we looked at the history of ImageNet and how this benchmark training set has influenced computer vision research since its creation in 2009. By taking a closer look at ImageNet’s structure, we can begin to see how the dataset is ordered and its underlying logic for mapping the world of objects. ImageNet’s structure is labyrinthine, vast, and filled with curiosities. The underlying semantic structure of Image- Net was imported from WordNet, a database of word classifications first developed at Princeton University’s Cognitive Science Laboratory in 1985 and funded by the U.S. Office of Naval Research.40 WordNet was conceived as a machine-readable dictionary, where users would search on the basis of semantic rather than alphabetic similarity. It became a vital source for the fields of computational linguistics and natural language processing. The WordNet team collected as many words as they could, starting with the Brown Corpus, a collection of one million words compiled in the 1960s.41 The words in the Brown Corpus came from newspapers and a ramshackle collection of books including New Methods of Parapsychology, The Family Fallout Shelter, and Who Rules the Marriage Bed?

WordNet attempts to organize the entire English language into synonym sets, or synsets. The ImageNet researchers selected only nouns, with the idea that nouns are things that pictures can represent—and that would be sufficient to train machines to automatically recognize objects. So ImageNet’s taxonomy is organized according to a nested hierarchy derived from WordNet, in which each synset represents a distinct concept, with synonyms grouped together (for example, “auto” and “car” are treated as belonging to the same set). The hierarchy moves from more general concepts to more specific ones. For example, the concept “chair” is found under artifact → furnishing → furniture → seat → chair. This classification system unsurprisingly evokes many prior taxonomical ranks, from the Linnaean system of biological classification to the ordering of books in libraries.

But the first indication of the true strangeness of Image- Net’s worldview is its nine top-level categories that it drew from WordNet: plant, geological formation, natural object, sport, artifact, fungus, person, animal, and miscellaneous. These are curious categories into which all else must be ordered. Below that, it spawns into thousands of strange and specific nested classes, into which millions of images are housed like Russian dolls. There are categories for apples, apple butter, apple dumplings, apple geraniums, apple jelly, apple juice, apple maggots, apple rust, apple trees, apple turnovers, apple carts, and applesauce. There are pictures of hot lines, hot pants, hot plates, hot pots, hot rods, hot sauce, hot springs, hot toddies, hot tubs, hot-air balloons, hot fudge sauce, and hot water bottles. It is a riot of words, ordered into strange categories like those from Jorge Luis Borges’s mythical encyclopedia.43 At the level of images, it looks like madness. Some images are high-resolution stock photography, others are blurry phone photographs in poor lighting. Some are photos of children. Others are stills from pornography. Some are cartoons. There are pin-ups, religious icons, famous politicians, Hollywood celebrities, and Italian comedians. It veers wildly from the professional to the amateur, the sacred to the profane.

Human classifications are a good place to see these politics of classification at work. In ImageNet the category “human body” falls under the branch Natural Object → Body → Human Body. Its subcategories include “male body,” “person,” “juvenile body,” “adult body,” and “female body.” The “adult body” category contains the subclasses “adult female body” and “adult male body.” There is an implicit assumption here that only “male” and “female” bodies are recognized as “natural.” There is an ImageNet category for the term “Hermaphrodite,” but it is situated within the branch Person → Sensualist → Bisexual alongside the categories “Pseudohermaphrodite” and “Switch Hitter.”

Even before we look at the more controversial categories within ImageNet, we can see the politics of this classificatory scheme. The decisions to classify gender in this way are also naturalizing gender as a biological construct, which is binary, and transgender or gender nonbinary people are either nonexistent or placed under categories of sexuality.45 Of course, this is not a novel approach. The classification hierarchy of gender and sexuality in ImageNet recalls earlier harmful forms of categorization, such as the classification of homosexuality as a mental disorder in the Diagnostic and Statistical Manual.46 This deeply damaging categorization was used to justify subjecting people to repressive so-called therapies, and it took years of activism before the American Psychiatric Association removed it in 1973.

Reducing humans into binary gender categories and rendering transgender people invisible or “deviant” are common features of classification schemes in machine learning. Os Keyes’s study of automatic gender detection in facial recognition shows that almost 95 percent of papers in the field treat gender as binary, with the majority describing gender as immutable and physiological.48 While some might respond that this can be easily remedied by creating more categories, this fails to address the deeper harm of allocating people into gender or race categories without their input or consent. This practice has a long history. Administrative systems for centuries have sought to make humans legible by applying fixed labels and definite properties. The work of essentializing and ordering on the basis of biology or culture has long been used to justify forms of violence and oppression.

While these classifying logics are treated as though they are natural and fixed, they are moving targets: not only do they affect the people being classified, but how they impact people in turn changes the classifications themselves. Hacking calls this the “looping effect,” produced when the sciences engage in “making up people.”49 Bowker and Star also underscore that once classifications of people are constructed, they can stabilize a contested political category in ways that are difficult to see.50 They become taken for granted unless they are actively resisted. We see this phenomenon in the AI field when highly influential infrastructures and training datasets pass as purely technical, whereas in fact they contain political interventions within their taxonomies: they naturalize a particular ordering of the world which produces effects that are seen to justify their original ordering.

The Power to Define "Person"

To impose order onto an undifferentiated mass, to ascribe phenomena to a category—that is, to name a thing—is in turn a means of reifying the existence of that category.

In the case of the 21,841 categories that were originally in the ImageNet hierarchy, noun classes such as “apple” or “apple butter” might seem reasonably uncontroversial, but not all nouns are created equal. To borrow an idea from linguist George Lakoff, the concept of an “apple” is a more nouny noun than the concept of “light,” which in turn is more nouny than a concept such as “health.”51 Nouns occupy various places on an axis from the concrete to the abstract, from the descriptive to the judgmental. These gradients have been erased in the logic of ImageNet. Everything is flattened out and pinned to a label, like taxidermy butterflies in a display case. While this approach has the aesthetics of objectivity, it is nonetheless a profoundly ideological exercise.

For a decade, ImageNet contained 2,832 subcategories under the top-level category “Person.” The subcategory with the most associated pictures was “gal” (with 1,664 images) followed by “grandfather” (1,662), “dad” (1,643), and chief executive officer (1,614—most of them male). With these highly populated categories, we can already begin to see the outlines of a worldview. ImageNet contains a profusion of classificatory categories, including ones for race, age, nationality, profession, economic status, behavior, character, and even morality.

There are many problems with the way ImageNet’s taxonomy purports to classify photos of people with the logics of object recognition. Even though its creators removed some explicitly offensive synsets in 2009, categories remained for racial and national identities including Alaska Native, Anglo-American, Black, Black African, Black Woman (but not White Woman), Latin American, Mexican American, Nicaraguan, Pakistani, South American Indian, Spanish American, Texan, Uzbek, White, and Zulu. To present these as logical categories of organizing people is already troubling, even before they are used to classify people based on their appearance. Other people are labeled by careers or hobbies: there are Boy Scouts, cheerleaders, cognitive neuroscientists, hairdressers, intelligence analysts, mythologists, retailers, retirees, and so on. The existence of these categories suggests that people can be visually ordered according to their profession, in a way that seems reminiscent of such children’s books as Richard Scarry’s What Do People Do All Day? ImageNet also contains categories that make no sense whatsoever for image classification such as Debtor, Boss, Acquaintance, Brother, and Color-Blind Person. These are all nonvisual concepts that describe a relationship, be it to other people, to a financial system, or to the visual field itself. The dataset reifies these categories and connects them to images, so that similar images can be “recognized” by future systems.

Many truly offensive and harmful categories hid in the depths of ImageNet’s Person categories. Some classifications were misogynist, racist, ageist, and ableist. The list includes Bad Person, Call Girl, Closet Queen, Codger, Convict, Crazy, Deadeye, Drug Addict, Failure, Flop, Fucker, Hypocrite, Jezebel, Kleptomaniac, Loser, Melancholic, Nonperson, Pervert, Prima Donna, Schizophrenic, Second-Rater, Slut, Spastic, Spinster, Streetwalker, Stud, Tosser, Unskilled Person, Wanton, Waverer, and Wimp. Insults, racist slurs, and moral judgments abound.

These offensive terms remained in ImageNet for ten years. Because ImageNet was typically used for object recognition— with “object” broadly defined—the specific Person category was rarely discussed at technical conferences, nor did it receive much public attention until the ImageNet Roulette project went viral in 2019: led by the artist Trevor Paglen, the project included an app that allowed people to upload images to see how they would be classified based on ImageNet’s Person categories. 52 This focused considerable media attention on the influential collection’s longtime inclusion of racist and sexist terms. The creators of ImageNet published a paper shortly afterward titled “Toward Fairer Datasets” that sought to “remove unsafe synsets.” They asked twelve graduate students to flag any categories that seemed unsafe because they were either “inherently offensive” (for example, containing profanity or “racial or gender slurs”) or “sensitive” (not inherently offensive but terms that “may cause offense when applied inappropriately, such as the classification of people based on sexual orientation and religion”).53 While this project sought to assess the offensiveness of ImageNet’s categories by asking graduate students, the authors nonetheless continue to support the automated classification of people based on photographs despite the notable problems.

The ImageNet team ultimately removed 1,593 of 2,832 of the People categories—roughly 56 percent—deeming them “unsafe,” along with the associated 600,040 images. The remaining half-million images were “temporarily deemed safe.”54 But what constitutes safe when it comes to classifying people? The focus on the hateful categories is not wrong, but it avoids addressing questions about the workings of the larger system. The entire taxonomy of ImageNet reveals the complexities and dangers of human classification. While terms like “microeconomist” or “basketball player” may initially seem less concerning than the use of labels like “spastic,” “unskilled person,” “mulatto,” or “redneck,” when we look at the people who are labeled in these categories we see many assumptions and stereotypes, including race, gender, age, and ability. In the metaphysics of ImageNet, there are separate image categories for “assistant professor” and “associate professor”—as though once someone gets a promotion, her or his biometric profile would reflect the change in rank.

In fact, there are no neutral categories in ImageNet, because the selection of images always interacts with the meaning of words. The politics are baked into the classificatory logic, even when the words aren’t offensive. ImageNet is a lesson, in this sense, of what happens when people are categorized like objects. But this practice has only become more common in recent years, often inside the tech companies. The classification schemes used in companies like Facebook are much harder to investigate and criticize: proprietary systems offer few ways for outsiders to probe or audit how images are ordered or interpreted.

Then there is the issue of where the images in ImageNet’s Person categories come from. As we saw in the last chapter, ImageNet’s creators harvested images en masse from image search engines like Google, extracted people’s selfies and vacation photos without their knowledge, and then paid Mechanical Turk workers to label and repackage them. All the skews and biases in how search engines return results are then informing the subsequent technical systems that scrape and label them. Low-paid crowdworkers are given the impossible task of making sense of the images at the rate of fifty per minute and fitting them into categories based on WordNet sysnets and Wikipedia definitions.55 Perhaps it is no surprise that when we investigate the bedrock layer of these labeled images, we find that they are beset with stereotypes, errors, and absurdities. A woman lying on a beach towel is a “kleptomaniac,” a teenager in a sports jersey is labeled a “loser,” and an image of the actor Sigourney Weaver appears, classified as a “hermaphrodite.”

Images—like all forms of data—are laden with all sorts of potential meanings, irresolvable questions, and contradictions. In trying to resolve these ambiguities, ImageNet’s labels compress and simplify complexity. The focus on making training sets “fairer” by deleting offensive terms fails to contend with the power dynamics of classification and precludes a more thorough assessment of the underlying logics. Even if the worst examples are fixed, the approach is still fundamentally built on an extractive relationship with data that is divorced from the people and places from whence it came. Then it is rendered through a technical worldview that seeks to fuse together a form of singular objectivity from what are complex and varied cultural materials. The worldview of ImageNet is not unusual in this sense. In fact, it is typical of many AI training datasets, and it reveals many of the problems of top-down schemes that flatten complex social, cultural, political, and historical relations into quantifiable entities. This phenomenon is perhaps most obvious and insidious when it comes to the widespread efforts to classify people by race and gender in technical systems.

Constructing Race and Gender

By focusing on classification in AI, we can trace the ways that gender, race, and sexuality are falsely assumed to be natural, fixed, and detectable biological categories. Surveillance scholar Simone Browne observes, “There is a certain assumption with these technologies that categories of gender identity and race are clear cut, that a machine can be programmed to assign gender categories or determine what bodies and body parts should signify.”56 Indeed, the idea that race and gender can be automatically detectable in machine learning is treated as an assumed fact and rarely questioned by the technical disciplines, despite the profound political problems this presents.

The UTKFace dataset (produced by a group at the University of Tennessee at Knoxville), for example, consists of more than twenty thousand images of faces with annotations for age, gender, and race.58 The dataset’s authors state that the dataset can be used for a variety of tasks, including automated face detection, age estimation, and age progression. The annotations for each image include an estimated age for each person, expressed in years from zero to 116. Gender is a forced binary: either zero for male or one for female. Second, race is categorized into five classes: White, Black, Asian, Indian, and Others. The politics of gender and race here are as obvious as they are harmful. Yet these kinds of dangerously reductive categorizations are widely used across many human-classifying training sets and have been part of the AI production pipelines for years.

UTKFace’s narrow classificatory schema echoes the problematic racial classifications of the twentieth century, such as South Africa’s apartheid system. As Bowker and Star have detailed, the South African government passed legislation in the 1950s that created a crude racial classification scheme to divide citizens into the categories of “Europeans, Asiatics, persons of mixed race or coloureds, and ‘natives’ or pure-blooded individuals of the Bantu race.”59 This racist legal regime governed people’s lives, overwhelmingly those of Black South Africans whose movements were restricted and who were forcibly removed from their land. The politics of racial classification extended into the most intimate parts of people’s lives. Interracial sexuality was forbidden, leading to more than 11,500 convictions by 1980, mostly of nonwhite women.60 The complex centralized database for these classifications was designed and maintained by IBM, but the firm often had to rearrange the system and reclassify people, because in practice there were no singular pure racial categories.

Above all, these systems of classification have caused enormous harm to people, and the concept of a pure “race” signifier has always been in dispute. In her writing about race, Donna Haraway observes, “In these taxonomies, which are, after all, little machines for clarifying and separating categories, the entity that always eluded the classifier was simple: race itself. The pure Type, which animated dreams, sciences, and terrors, kept slipping through, and endlessly multiplying, all the typological taxonomies.” Yet in dataset taxonomies, and in the machine learning systems that train on them, the myth of the pure type has emerged once more, claiming the authority of science. In an article on the dangers of facial recognition, media scholar Luke Stark notes that “by introducing a variety of classifying logics that either reify existing racial categories or produce new ones, the automated pattern-generating logics of facial recognition systems both reproduce systemic inequality and exacerbate it.”

Some machine learning methods go beyond predicting age, gender, and race. There have been highly publicized efforts to detect sexuality from photographs on dating sites and criminality based on headshots from drivers’ licenses.64 These approaches are deeply problematic for many reasons, not least of which is that characteristics such as “criminality”—like race and gender—are profoundly relational, socially determined categories. These are not inherent features that are fixed; they are contextual and shifting depending on time and place. To make such predictions, machine learning systems are seeking to classify entirely relational things into fixed categories and are rightly critiqued as scientifically and ethically problematic.

Machine learning systems are, in a very real way, constructing race and gender: they are defining the world within the terms they have set, and this has long-lasting ramifications for the people who are classified. When such systems are hailed as scientific innovations for predicting identities and future actions, this erases the technical frailties of how the systems were built, the priorities of why they were designed, and the many political processes of categorization that shape them. Disability scholars have long pointed to the ways in which so-called normal bodies are classified and how that has worked to stigmatize difference.66 As one report notes, the history of disability itself is a “story of the ways in which various systems of classification (i.e., medical, scientific, legal) interface with social institutions and their articulations of power and knowledge.” 67 At multiple levels, the act of defining categories and ideas of normalcy creates an outside: forms of abnormality, difference, and otherness. Technical systems are making political and normative interventions when they give names to something as dynamic and relational as personal identity, and they commonly do so using a reductive set of possibilities of what it is to be human. That restricts the range of how people are understood and can represent themselves, and it narrows the horizon of recognizable identities.

As Ian Hacking observes, classifying people is an imperial imperative: subjects were classified by empires when they were conquered, and then they were ordered into “a kind of people” by institutions and experts.68 These acts of naming were assertions of power and colonial control, and the negative effects of those classifications can outlast the empires themselves. Classifications are technologies that produce and limit ways of knowing, and they are built into the logics of AI.

The Limits of Measurement

So what is to be done? If so much of the classificatory strata in training data and technical systems are forms of power and politics represented as objective measurement, how should we go about redressing this? How should system designers account for, in some cases, slavery, oppression, and hundreds of years of discrimination against some groups to the benefit of others? In other words, how should AI systems make representations of the social?

Making these choices about which information feeds AI systems to produce new classifications is a powerful moment of decision making: but who gets to choose and on what basis? The problem for computer science is that justice in AI systems will never be something that can be coded or computed. It requires a shift to assessing systems beyond optimization metrics and statistical parity and an understanding of where the frameworks of mathematics and engineering are causing the problems. This also means understanding how AI systems interact with data, workers, the environment, and the individuals whose lives will be affected by its use and deciding where AI should not be used.

Bowker and Star conclude that the sheer density of the collisions of classification schemes calls for a new kind of approach, a sensitivity to the “topography of things such as the distribution of ambiguity; the fluid dynamics of how classification systems meet up—a plate tectonics rather than static geology.”69 But it also requires attending to the uneven allocations of advantage and suffering, for “how these choices are made, and how we may think about that invisible matching process, is at the core of the ethical project.”70 Nonconsensual classifications present serious risks, as do normative assumptions about identity, yet these practices have become standard. That must change.

In this chapter we’ve seen how classificatory infrastructures contain gaps and contradictions: they necessarily reduce complexity, and they remove significant context, in order to make the world more computable. But they also proliferate in machine learning platforms in what Umberto Eco called “chaotic enumeration.”71 At a certain level of granularity, like and unlike things become sufficiently commensurate so that their similarities and differences are machine readable, yet in actuality their characteristics are uncontainable. Here, the issues go far beyond whether something is classified wrong or classified right. We are seeing strange, unpredictable twists as machine categories and people interact and change each other, as they try to find legibility in the shifting terrain, to fit the right categories and be spiked into the most lucrative feeds. In a machine learning landscape, these questions are no less urgent because they are hard to see. What is at stake is not just a historical curiosity or the odd feeling of a mismatch between the dotted-outline profiles we may glimpse in our platforms and feeds. Each and every classification has its consequence.

The histories of classification show us that the most harmful forms of human categorization—from the Apartheid system to the pathologization of homosexuality—did not simply fade away under the light of scientific research and ethical critique. Rather, change also required political organizing, sustained protest, and public campaigning over many years. Classificatory schemas enact and support the structures of power that formed them, and these do not shift without considerable effort. In Frederick Douglass’s words, “Power concedes nothing without a demand. It never did and it never will.”72 Within the invisible regimes of classification in machine learning, it is harder to make demands and oppose their internal logics.

The training sets that are made public—such as Image- Net, UTKFace, and DiF—give us some insight into the kinds of categorizations that are propagating across industrial AI systems and research practices. But the truly massive engines of classification are the ones being operated at a global scale by private technology companies, including Facebook, Google, TikTok, and Baidu. These companies operate with little oversight into how they categorize and target users, and they fail to offer meaningful avenues for public contestation. When the matching processes of AI are truly hidden and people are kept unaware of why or how they receive forms of advantage or disadvantage, a collective political response is needed—even as it becomes more difficult.

5 Affect

In a remote outpost in the mountainous highlands of Papua New Guinea, a young American psychologist named Paul Ekman arrived with a collection of flashcards and a new theory.1 It was 1967, and Ekman had heard that the Fore people of Okapa were so isolated from the wider world that they would be his ideal test subjects. Like many Western researchers before him, Ekman had come to Papua New Guinea to extract data from the indigenous community. He was gathering evidence to bolster a controversial hypothesis: that all humans exhibit a small number of universal emotions or affects that are natural, innate, cross-cultural, and the same all over the world. Although that claim remains tenuous, it has had far-reaching consequences: Ekman’s presuppositions about emotions have grown into an expanding industry worth well over seventeen billion dollars.2 This is the story of how affect recognition came to be part of artificial intelligence and the problems this presents. 

In the tropics of Okapa, guided by medical researcher D. Carleton Gajdusek and anthropologist E. Richard Sorenson, Ekman hoped to run experiments that would assess how the Fore recognized emotions conveyed by facial expressions. Because the Fore had minimal contact with Westerners or mass media, Ekman theorized that their recognition and display of core expressions would prove that such expressions were universal. His methods were simple. He would show them flashcards of facial expressions and see if they described the emotion as he did. In Ekman’s own words, “All I was doing was showing funny pictures.”

But Ekman had no training in Fore history, language, culture, or politics. His attempts to conduct his flashcard experiments using translators floundered; he and his subjects were exhausted by the process, which he described as like pulling teeth.4 Ekman left Papua New Guinea, frustrated by his first attempt at cross-cultural research on emotional expression. But this would just be the beginning.

Today affect recognition tools can be found in national security systems and at airports, in education and hiring start-ups, from systems that purport to detect psychiatric illness to policing programs that claim to predict violence. By looking at the history of how computer-based emotion detection came to be, we can understand how its methods have raised both ethical concerns and scientific doubts. As we will see, the claim that a person’s interior state of feeling can be accurately assessed by analyzing their face is premised on shaky evidence.5 In fact, a comprehensive review of the available scientific literature on inferring emotions from facial movements published in 2019 was definitive: there is no reliable evidence that you can accurately predict someone’s emotional state from their face.

How did this collection of contested claims and experimental methodologies resolve into an approach that drives many parts of the affect AI industry? Why did the idea that there is a small set of universal emotions, readily interpreted from the face, become so accepted in the AI field, despite considerable evidence to the contrary? To understand that requires tracing how these ideas developed, long before AI emotion detection tools were built into the infrastructure of everyday life.

Ekman is just one of many people who have contributed to the theories behind affect recognition. But the rich and surprising history of Ekman’s research illuminates some of the complex forces driving the field. His work is connected to U.S. intelligence funding of the human sciences during the Cold War through foundational work in the field of computer vision to the post-9/ 11 security programs employed to identify terrorists and right up to the current fashion for AI-based emotion recognition. It is a chronicle that combines ideology, economic policy, fear-based politics, and the desire to extract more information about people than they are willing to give.

Emotion Prophets: When Feelings Pay

For the world’s militaries, corporations, intelligence agencies, and police forces, the idea of automated affect recognition is as compelling as it is lucrative. It holds the promise of reliably filtering friend from foe, distinguishing lies from truths, and using the instruments of science to see into interior worlds.

Technology companies have captured immense volumes of surface-level imagery of human expressions—including billions of Instagram selfies, Pinterest portraits, TikTok videos, and Flickr photos. One of the many things made possible by this profusion of images is the attempt to extract the so-called hidden truth of interior emotional states using machine learning. Affect recognition is being built into several facial recognition platforms, from the biggest tech companies to small start-ups. Whereas facial recognition attempts to identify a particular individual, affect detection aims to detect and classify emotions by analyzing any face. These systems may not be doing what they purport to do, but they can nonetheless be powerful agents in influencing behavior and training people to perform in recognizable ways. These systems are already playing a role in shaping how people behave and how social institutions operate, despite a lack of substantial scientific evidence that they work.

Automated affect detection systems are now widely deployed, particularly in hiring. A startup in London called Human uses emotion recognition to analyze video interviews of job candidates. According to a report in the Financial Times, “The company claims it can spot the emotional expressions of prospective candidates and match them with personality traits”; the company then scores subjects on such personality traits as honesty or passion for a job.7 The AI hiring company HireVue, which lists among its clients Goldman Sachs, Intel and Unilever, uses machine learning to assess facial cues to infer people’s suitability for a job. In 2014, the company launched its AI system to extract microexpressions, tone of voice, and other variables from video job interviews, which they used to compare job applicants against the company’s top performers.

In January 2016, Apple acquired the startup Emotient, which claimed to have produced software capable of detecting emotions from images of faces.9 Emotient grew out of academic research conducted at the University of California San Diego and is one of a number of startups working in this area.10 Perhaps the largest of these is Affectiva, a company based in Boston that emerged from academic work done at Massachusetts Institute of Technology. At MIT, Rosalind Picard and her colleagues were part of an emergent wider field known as affective computing, which describes computing that “relates to, arises from, or deliberately influences emotion or other affective phenomena.”

Affectiva codes a variety of emotion-related applications, primarily using deep learning techniques. These range from detecting distracted and “risky” drivers on roads to measuring the emotional responses of consumers to advertising. The company has built what they call the world’s largest emotion database, made up of over ten million people’s expressions from eighty-seven countries.12 Their monumental collection of videos of people emoting was hand labeled by crowdworkers based primarily in Cairo.13 Many more companies have now licensed Affectiva’s products to develop everything from applications that assess job candidates to analyzing whether students are engaged in class, all by capturing and analyzing their facial expressions and body language.

Beyond the start-up sector, AI giants like Amazon, Microsoft, and IBM have all designed systems for affect and emotion detection. Microsoft offers emotion detection in its Face API, which claims to detect what an individual is feeling across the emotions of “anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise” and asserts that “these emotions are understood to be cross-culturally and universally communicated with particular facial expressions.”15 Amazon’s Rekognition tool similarly claims that it can identify “all seven emotions” and “measure how these things change over time, such as constructing a timeline of the emotions of an actor.”

But how do these technologies work? Emotion recognition systems grew from the interstices between AI technologies, military priorities, and the behavioral sciences—psychology in particular. They share a similar set of blueprints and founding assumptions: that there is a small number of distinct and universal emotional categories, that we involuntarily reveal these emotions on our faces, and that they can be detected by machines. These articles of faith are so accepted in some fields that it can seem strange even to notice them, let alone question them. They are so ingrained that they have come to constitute “the common view.”17 But if we look at how emotions came to be taxonomized—neatly ordered and labeled—we see that questions are lying in wait at every corner. And a leading figure behind this approach is Paul Ekman.

The Politics of Faces

Instead of trying to build more systems that can group expressions into machine-readable categories, we should question the origins of those categories themselves, as well as their social and political consequences. Already, affect recognition tools are being deployed in political attacks. For example, a conservative blog claimed to create a “virtual polygraph system” to assess videos of Congresswoman Ilhan Abdullahi Omar.104 By using face and speech analytics from Amazon’s Rekognition, XRVision Sentinel AI, and IBM Watson, the blogger claimed that Omar’s analytic lie score exceeded her “truth baseline” and that she was registering high on stress, contempt, and nervousness. Several conservative media outlets ran with the story, claiming that Omar is a “pathological liar” and a security threat to the nation.

It’s known that these systems flag the speech affects of women differently from men, particularly Black women. As we saw in chapter 3, the construction of the “average” from unrepresentative training data is epistemologically suspect from the outset, with clear racial biases. A study conducted at the University of Maryland has shown that some facial recognition software interprets Black faces as having more negative emotions than white faces, particularly registering them as angrier and more contemptuous, even controlling for their degree of smiling.

This is the danger of affect recognition tools. As we’ve seen, they take us back to the phrenological past, where spurious claims were made, allowed to stand, and deployed to support existing systems of power. The decades of scientific controversies around the idea of inferring distinct emotions from human faces underscores a central point: the one-size- fits- all recognition model is not the right metaphor for identifying emotional states. Emotions are complex, and they develop and change in relation to our families, friends, cultures, and histories, all the manifold contexts that live outside of the AI frame. In many cases, emotion detection systems do not do what they claim. Rather than directly measuring people’s interior mental states, they merely statistically optimize correlations of certain physical characteristics among facial images. The scientific foundations of automated emotion detection are in question, yet a new generation of affect tools is already making inferences across a growing range of high-stakes contexts from policing to hiring.

Even though evidence now points to the unreliability of affect detection, companies continue to seek out new sources to mine for facial imagery, vying for the leading market share of a sector that promises billions in profits. Barrett’s systemic review of the research behind inferring emotion from people’s faces concludes on a damning note: “More generally, tech companies may well be asking a question that is fundamentally wrong. Efforts to simply ‘read out’ people’s internal states from an analysis of their facial movements alone, without considering various aspects of context, are at best incomplete and at worst entirely lack validity, no matter how sophisticated the computational algorithms. . . . It is premature to use this technology to reach conclusions about what people feel on the basis of their facial movements.”

Until we resist the desire to automate affect recognition, we run the risk of job applicants being judged unfairly because their microexpressions do not match other employees, students receiving poorer grades than their peers because their faces indicate a lack of enthusiasm, and customers being detained because an AI system flagged them as likely shoplifters based on their facial cues.108 These are the people who will bear the costs of systems that are not just technically imperfect but based on questionable methodologies.

The areas of life in which these systems are operating are expanding as rapidly as labs and corporations can create new markets for them. Yet they all rely on a narrow understanding of emotions—grown from Ekman’s initial set of anger, happiness, surprise, disgust, sadness, and fear—to stand in for the infinite universe of human feeling and expression across space and time. This takes us back to the profound limitations of capturing the complexities of the world in a single classificatory schema. It returns us to the same problem we have seen repeated: the desire to oversimplify what is stubbornly complex so that it can be easily computed, and packaged for the market. AI systems are seeking to extract the mutable, private, divergent experiences of our corporeal selves, but the result is a cartoon sketch that cannot capture the nuances of emotional experience in the world

6 State

I’m sitting in front of an air-gapped laptop on the tenth floor of a warehouse building in New York. On the screen is a software program normally used for digital forensics, a tool for investigating evidence and validating information held on hard drives. I’m here to research an archive that contains some of the most specific details about how machine learning began to be used in the intelligence sector, as led by some of the wealthiest governments in the world. This is the Snowden archive: all the documents, PowerPoint presentations, internal memos, newsletters, and technical manuals that former NSA contractor and whistleblower Edward Snowden leaked in 2013. Each page is marked with a header noting different forms of classification. top secret // si // orcon // noforn. Each is a warning and a designation.

The filmmaker Laura Poitras first gave me access to this archive in 2014. It was overwhelming to read: the archive held well over a decade of intelligence thinking and communication, including internal documents of the National Security Agency in the United States and the Government Communication Headquarters in the United Kingdom, and the international network of the Five Eyes.2 This knowledge was strictly off-limits to those without high-level clearance. It was part of the “classified empire” of information, once estimated to be growing five times faster than publicly accessible knowledge but now is anyone’s guess.3 The Snowden archive captures the years when the collection of data metastasized: when phones, browsers, social media platforms, and email all became data sources for the state. The documents reveal how the intelligence community contributed to the development of many of the techniques we now refer to as artificial intelligence.

The Snowden archive reveals a parallel AI sector, one developed in secrecy. The methods share many similarities, but there are striking differences in terms of the reach, the objectives, and the result. Gone are any rhetorical constructs justifying extraction and capture: every software system is simply described as something to be owned, to be defeated; all data platforms are fair game, and very little is designated as protected. One NSA PowerPoint deck outlines treas uremap, a program designed to build a near real-time, interactive map of the internet.4 It claims to track the location and owner of any connected computer, mobile device, or router: “Map the entire internet—any device, anywhere, all the time,” the slide boasts. A few slides on “treas uremap as an Enabler” offers up a layer-cake image of signals analysis. Above the geographical layer and the network layer is the “cyber persona layer”— quaintly represented on the slide by jellybean-era iMacs and Nokia feature phones—and then the “persona layer” of personal connections. This is meant to depict all people who use connected devices around the world, in a “300,000-foot view of the internet.” It also looks remarkably like the work of social network mapping and manipulation companies like Cambridge Analytica.

The Snowden documents were released in 2013, but they still read like the AI marketing brochures of today. If “aggressively pursue legal authorities and a policy framework mapped more fully to the information age.”8 In other words, change the laws to fit the tools, not the other way around.

The U.S. intelligence agencies are the old guards of big data. Along with the Defense Advanced Research Projects Agency, they have been major drivers of AI research since the 1950s. As the historian of science Paul Edwards describes in The Closed World, military research agencies actively shaped the emerging field that would come to be known as AI from its earliest days.9 The Office of Naval Research, for example, partly funded the first Summer Research Project on Artificial Intelligence at Dartmouth College in 1956.10 The field of AI has always been strongly guided by military support and often military priorities, long before it was clear that AI could be practical at scale. As Edwards notes:

As the project with the least immediate utility and the farthest-reaching ambitions, AI came to rely unusually heavily on ARPA funding. As a result, ARPA became the primary patron for the first twenty years of AI research. Former director Robert Sproull proudly concluded that “a whole generation of computer experts got their start from DARPA funding” and that “all the ideas that are going into the fifth-generation [advanced computing] project [of the mid-1980s]— artificial intelligence, parallel computing, speech understanding, natural-languages programming—ultimately started from DARPA-funded research.”

The military priorities of command and control, automation, and surveillance profoundly shaped what AI was to become. The tools and approaches that came out of DARPA funding have marked the field, including computer vision, automatic translation, and autonomous vehicles. But these technical methods have deeper implications. Infused into the overall logics of AI are certain kinds of classificatory thinking—from explicitly battlefield-oriented notions such as target, asset, and anomaly detection to subtler categories of high, medium, and low risk. Concepts of constant situational awareness and targeting would drive AI research for decades, creating epistemological frameworks that would inform both industry and academia.

From the point of view of the state, the turn to big data and machine learning expanded the modes of information extraction and informed a social theory of how people can be tracked and understood: you shall know them by their metadata. Who is texted, which locations are visited, what is read, when devices spring into action and for what reason—these molecular actions became a vision of threat identification and assessment, guilt or innocence. Harvesting and measuring large aggregates of data at a distance became the preferred way to develop alleged insights into groups and communities as well as assessments of potential targets for killing. The NSA and GCHQ are not unique—China, Russia, Israel, Syria, and many other countries have similar agencies. There are many systems of sovereign surveillance and control, a multitude of war machines that never wind down. The Snowden archive underscores how state and corporate actors collaborate in order to produce what Achille Mbembe calls “infrastructural warfare.”

But the relationship between national militaries and the AI industry has expanded beyond security contexts. Technologies once only available to intelligence agencies—that were extralegal by design—have filtered down to the state’s municipal arms: government and law enforcement agencies. While the NSA has been a focus for privacy concerns, less attention is given to the growing commercial surveillance sector, which aggressively markets its tools and platforms to police departments and public agencies. The AI industry is simultaneously challenging and reshaping the traditional role of states while also being used to shore up and expand older forms of geopolitical power. Algorithmic governance is both part of and exceeds traditional state governance. To paraphrase the theorist Benjamin Bratton, the state is taking on the armature of a machine because the machines have already taken on the roles and register of the state.

Making the Third Offset

The story of the internet’s creation has been centered around U.S. military and academic innovation and dominance.14 But in the space of AI, we see that there is no pure national system. Instead, AI systems operate within a complex interwoven network of multinational and multilateral tools, infrastructures, and labor. Take, for example, a facial recognition system that was rolled out in the streets of Belgrade.15 The director of police ordered the installation of two thousand cameras in eight hundred locations around the city to capture faces and license plates. The Serbian government signed an agreement with Chinese telecommunications giant Huawei to provide the video surveillance, 4G network support, and unified data and command centers. Such deals are common. Local systems are often hybrids, with infrastructure from China, India, the United States, and elsewhere, with porous boundaries, different security protocols, and potential data backdoors.

But the rhetoric around artificial intelligence is much starker: we are repeatedly told that we are in an AI war. The dominant objects of concern are the supernational efforts of the United States and China, with regular reminders that China has stated its commitment to be the global leader in AI.16 The data practices of China’s leading tech companies, including Alibaba, Huawei, Tencent, and ByteDance, are often framed as direct Chinese state policy and thus seen as inherently more threatening than U.S. private actors such as Amazon and Facebook, even though the lines between state and corporate imperatives and incentives are complexly intertwined. Yet the language of war is more than just the usual articulation of xenophobia, mutual suspicion, international espionage, and network hacking. As media scholars such as Wendy Chun and Tung-Hui Hu have noted, the liberal vision of global digital citizens engaging as equals in the abstract space of networks has shifted toward a paranoid vision of defending a national cloud against the racialized enemy.17 The specter of the foreign threat works to assert a kind of sovereign power over AI and to redraw the locus of power of tech companies (which are transnational in infrastructure and influence) back within the bounds of the nation-state.

Yet the nationalized race for technological superiority is both rhetorical and real at the same time, creating the dynamics for geopolitical competition across and within commercial and military sectors, increasingly blurring the lines between the two. The dual use of AI applications in both civilian and military domains has also produced strong incentives for close collaboration and funding.18 In the United States, we can see how this became an explicit strategy: to seek national control and international dominance of AI in order to secure military and corporate advantage.

The latest iteration of this strategy emerged under Ash Carter, who served as U.S. secretary of defense from 2015 to 2017. Carter played a significant role in bringing Silicon Valley into closer relationship to the military, convincing tech companies that national security and foreign policy depended on American dominance of AI.19 He called this the Third Offset strategy. An offset is generally understood as a way of compensating for an underlying military disadvantage by changing the conditions, or as former secretary of defense Harold Brown stated in 1981, “Technology can be a force multiplier, a resource that can be used to help offset numerical advantages of an adversary. Superior technology is one very effective way to balance military capabilities other than by matching an adversary tank-for- tank or soldier-for- soldier.”

The First Offset is commonly understood as the use of nuclear weapons in the 1950s.21 The Second was the expansion of covert, logistical, and conventional weapons in the 1970s and 1980s. The Third, according to Carter, should be a combination of AI, computational warfare, and robots.22 But unlike the NSA, which already had robust surveillance capabilities, the U.S. military lacked the AI resources, expertise, and infrastructure of America’s leading technology companies.23 In 2014, Deputy Defense Secretary Robert Work outlined the Third Offset as an attempt to “exploit all the advances in artificial intelligence and autonomy.”

To build AI war machines, the Department of Defense would need gigantic extractive infrastructures. Yet in order to gain access to highly paid engineering labor and sophisticated development platforms, partnering with industry was necessary. The NSA had paved the way with systems like pris m, both working with and secretly infiltrating telecommunications and technology companies.25 But these more covert approaches faced renewed political pushback after the Snowden disclosures. Congress passed the USA Freedom Act in 2015, which introduced some limitations on the NSA’s access to real-time data from Silicon Valley. Yet the possibility for a larger military-industrial complex around data and AI remained tantalizingly close. Silicon Valley had already built and monetized the logics and infrastructures of AI required to drive a new offset. But first the tech sector had to be convinced that partnering on creating the infrastructure of warfare would be worth it without alienating their employees and deepening public mistrust.

Enter Project Maven

In April 2017, the Department of Defense published a memo announcing the Algorithmic Warfare Cross-Functional Team, code-named Project Maven.26 “The Department of Defense must integrate artificial intelligence and machine learning more effectively across operations to maintain advantages over increasingly capable adversaries and competitors,” wrote the deputy defense secretary.27 The goal of the program was to get the best possible algorithmic systems into the battlefield quickly, even when they were just 80 percent complete.28 It was part of a much bigger plan, the Joint Enterprise Defense Infrastructure cloud project—or JEDI—an enormous redesign of the entire IT infrastructure of the Defense Department, from the Pentagon to field-level support. Project Maven was a small piece of this larger picture, and the aim was to create an AI system that would allow analysts to select a target and then see every existing clip of drone footage that featured the same person or vehicle.29 Ultimately, the Defense Department wanted an automated search engine of drone videos to detect and track enemy combatants.

The technical platforms and machine learning skills needed for Project Maven were centered in the commercial tech sector. The Defense Department decided to pay tech companies to analyze military data collected from satellites and battlefield drones in places where U.S. domestic privacy laws did not apply. This would align military and U.S. tech sector financial interests around AI without directly triggering constitutional privacy tripwires, as the National Security Agency had done. A bidding war began among the technology companies that wanted the Maven contract, including Amazon, Microsoft, and Google.

The first Project Maven contract went to Google. Under the agreement, the Pentagon would use Google’s TensorFlow AI infrastructure to comb through drone footage and detect objects and individuals as they moved between locations.30 Fei-Fei Li, then chief scientist of AI/ML at Google, was already an expert in building object recognition datasets, given her experience creating ImageNet and using satellite data to detect and analyze cars.31 But she was adamant that the project should be kept secret. “Avoid at all cos ts any mention or implication of AI,” Li wrote in an email to Google colleagues that was later leaked. “Weaponized AI is probably one of the most sensitized topics of AI—if not the most. This is red meat to the media to find all ways to damage Google.”

But in 2018, Google employees discovered the extent of the company’s role in the project. They were furious that their work was being used for warfare purposes, especially after it became known that Project Maven’s image identification goals included objects such as vehicles, buildings, and humans.33 More than 3,100 employees signed a letter of protest stating that Google should not be in the business of war and demanded that the contract be canceled.34 Under increasing pressure, Google officially ended its work on Project Maven and withdrew from the competition for the Pentagon’s ten-billion- dollar JEDI contract. In October that year, Microsoft’s president, Brad Smith, announced in a blog post that “we believe in the strong defense of the United States and we want the people who defend it to have access to the nation’s best technology, including from Microsoft.” The contract ultimately went to Microsoft, which outbid Amazon.

Shortly after the internal uprising, Google released its Artificial Intelligence Principles, which included a section on “AI applications we will not pursue.”37 These included making “weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people,” as well as “technologies that gather or use information for surveillance violating internationally accepted norms.”38 While the turn to AI ethics quelled some internal and external concerns, the enforceability and parameters of ethical restraint were left unclear.

In response, former Google CEO Eric Schmidt characterized the pushback over Project Maven as “a general concern in the tech community of somehow the military-industrial complex using our stuff to kill people incorrectly, if you will.”40 This shift, from the debate over whether to use AI in warfare at all to a debate over whether AI could help to “kill people correctly,” was quite strategic.41 It moved the focus away from the foundational ethics of AI as a military technology toward questions of precision and technical accuracy. But Lucy Suchman argues that the problems with automated warfare go far beyond whether the killing was accurate or “correct.”42 Particularly in the case of object detection, Suchman asks, who is building the training sets and using what data, and how are things labeled as an imminent threat? What kinds of classificatory taxonomies are used to decide what constitutes sufficiently abnormal activity to trigger a legal drone attack? And why should we condone attaching life or death consequences to these unstable and inherently political classifications?

The Maven episode, as well as the AI principles that emerge, points to the deep schisms in the AI industry about the relationship between the military and civilian spheres. The AI war, both real and imagined, instills a politics of fear and insecurity that creates a climate that is used to stifle internal dissent and promote unquestioning support for a nationalist agenda.44 After the fallout from Maven faded, Google’s chief legal officer, Kent Walker, said that the company was pursuing higher security certifications in order to work more closely with the Defense Department. “I want to be clear,” he said. “We are a proud American company.”45 Articulating patriotism as policy, tech companies are increasingly expressing strong alignment with the interests of the nation-state, even as their platforms and capacities exceed traditional state governance.

The Outsourced State

The relationship between the state and the AI industry goes well beyond national militaries. The technologies once reserved for war zones and espionage are now used at the local level of government, from welfare agencies to law enforcement. This shift has been propelled by outsourcing key functions of the state to technology contractors. On the surface, this does not seem very different than the usual outsourcing of government functions to the private sector through companies such as Lockheed Martin or Halliburton. But now militarized forms of pattern detection and threat assessment are moving at scale into municipal-level services and institutions.46 A significant example of this phenomenon is the company named after the magical seeing stones in Lord of the Rings: Palantir.

Palantir was established in 2004, cofounded by PayPal billionaire Peter Thiel, who was also an adviser and financial supporter of President Trump. Thiel would later argue in an opinion piece that AI is first and foremost a military technology: “Forget the sci-fi fantasy; what is powerful about actually existing AI is its application to relatively mundane tasks like computer vision and data analysis. Though less uncanny than Frankenstein’s monster, these tools are nevertheless valuable to any army—to gain an intelligence advantage, for example. . . . No doubt machine learning tools have civilian uses, too.”

While Thiel recognizes the nonmilitary uses of machine learning, he particularly believes in the in-between space: where commercial companies produce military-styled tools to be provided to anyone who would like to gain an intelligence advantage and is willing to pay for it. Both he and Palantir’s CEO, Alex Karp, describe Palantir as “patriotic,” with Karp accusing other technology companies that refuse to work with the military agencies as “borderline craven.”48 In an insightful essay, the writer Moira Weigel studied Karp’s university dissertation, which reveals his early intellectual interest in aggression and a belief that “the desire to commit violence is a constant founding fact of human life.”49 Karp’s thesis was titled “Aggression in the Life World.”

Palantir’s original clients were federal military and intelligence agencies, including the Defense Department, National Security Agency, FBI, and CIA.50 As revealed in an investigation by Mijente, after Trump took the presidency, Palantir’s contracts with U.S. agencies totaled more than a billion dollars.51 But Palantir did not style itself as a typical defense contractor in the mold of Lockheed Martin. It adopted the character of the Silicon Valley start-up, based in Palo Alto and predominantly staffed by young engineers, and it was backed by In-Q- Tel, the venture capital firm funded by the CIA. Beyond its initial intelligence agency clients, Palantir began to work with hedge funds, banks, and corporations like Walmart. 52 But its DNA was shaped working for, and within, the defense community. It deployed the same approaches seen in the Snowden documents, including extracting data across devices and infiltrating networks in order to track and evaluate people and assets. Palantir quickly became a preferred outsourced surveillance provider, including designing the databases and management software to drive the mechanics of deportation for Immigration and Customs Enforcement (ICE).

Palantir’s business model is based on a mix of data analysis and pattern detection using machine learning, combined with more generic consulting. Palantir sends engineers into a company, who extract a wide variety of data—emails, call logs, social media, when employees enter and leave buildings, when they book plane tickets, everything the company is prepared to share—then look for patterns and give advice on what to do next. One common approach is to search for current or potential so-called bad actors, disgruntled employees who may leak information or defraud the company. The underlying worldview built into Palantir’s tools is reminiscent of the NSA: collect everything, then look for anomalies in the data. However, while the NSA’s tools are built to surveil and target enemies of the state, in either conventional or covert warfare, Palantir’s approach has been directed against civilians. As described in a major investigation by Bloomberg in 2018, Palantir is “an intelligence platform designed for the global War on Terror” that is now “weaponized against ordinary Americans at home”: “Palantir cut its teeth working for the Pentagon and the CIA in Afghanistan and Iraq. . . . The U.S. Department of Health and Human Services uses Palantir to detect Medicare fraud. The FBI uses it in criminal probes. The Department of Homeland Security deploys it to screen air travelers and keep tabs on immigrants.”

Soon, keeping tabs on undocumented workers evolved into capturing and deporting people at schools and places of work. In furtherance of this objective, Palantir produced a phone app called falcon , which functions as a vast dragnet, gathering data from multiple law enforcement and public databases that list people’s immigration histories, family relationships, employment information, and school details. In 2018, ICE agents used FALCON to guide their raid of almost a hundred 7-Elevens across the United States in what was called “the largest operation against a single employer in the Trump era.”

Despite Palantir’s efforts to maintain secrecy about what it builds or how its systems work, its patent applications give us some insight into the company’s approach to AI for deportation. In an application innocuously entitled Database systems and user interfaces for dynamic and interactive mobile image analysis and identification, Palantir brags about the app’s ability to photograph people in short-time- frame encounters and, regardless of whether they are under suspicion or not, to run their image against all available databases. In essence, the system uses facial recognition and back-end processing to create a framework on which to base any arrest or deportation.

While Palantir’s systems have structural similarities to those at the NSA, they have devolved to a local community level, to be sold to supermarket chains and local law enforcement alike. This represents a shift away from traditional policing toward the goals more associated with military intelligence infrastructures. As law professor Andrew Ferguson explains, “We are moving to a state where prosecutors and police are going to say ‘the algorithm told me to do it, so I did, I had no idea what I was doing.’ And this will be happening at a widespread level with very little oversight.”

The sociologist Sarah Brayne was one of the first scholars to observe directly how Palantir’s data platforms are used in situ, specifically by the Los Angeles Police Department. After more than two years of riding along with police on patrols, watching them at their desks, and conducting multiple interviews, Brayne concluded that in some domains these tools merely amplify prior police practices but that in other ways they are transforming the process of surveillance entirely. In short, police are turning into intelligence agents:

The shift from traditional to big data surveillance is associated with a migration of law enforcement operations toward intelligence activities. The basic distinction between law enforcement and intelligence is as follows: law enforcement typically becomes involved once a criminal incident has occurred. Legally, the police cannot undertake a search and gather personal information until there is probable cause. Intelligence, by contrast, is fundamentally predictive. Intelligence activities involve gathering data; identifying suspicious patterns, locations, activity, and individuals; and preemptively intervening based on the intelligence acquired.

Although everyone is subject to these types of surveillance, some people are more likely to be subjected to it than others: immigrants, the undocumented, the poor, and communities of color. As Brayne observed in her study, the use of Palantir’s software reproduces inequality, making those in predominantly poor, Black, and Latinx neighborhoods subject to even greater surveillance. Palantir’s point system lends an aura of objectivity: it’s “just math,” in the words of one police officer. But it creates a reinforcing loop of logic. Brayne writes:

Despite the stated intent of the point system to avoid legally contestable bias in police practices, it hides both intentional and unintentional bias in policing and creates a self-perpetuating cycle: if individuals have a high point value, they are under heightened surveillance and therefore have a greater likelihood of being stopped, further increasing their point value. Such practices hinder the ability of individuals already in the criminal justice system from being further drawn into the surveillance net, while obscuring the role of enforcement in shaping risk scores.

The machine learning approaches of Palantir and its ilk can lead to a feedback loop, where those included in a criminal justice database are more likely to be surveilled and thus more likely to have more information about them included, which justifies further police scrutiny.60 Inequity is not only deepened but tech-washed, justified by the systems that appear immune to error yet are, in fact, intensifying the problems of overpolicing and racially biased surveillance.61 The intelligence models that began in national government agencies have now become part of the policing of local neighborhoods. The NSA-ification of police departments exacerbates historical inequality and radically transforms and expands the practices of police work.

Despite the massive expansion of government contracts for AI systems, little attention has been given to the question of whether private vendors of these technologies should be legally accountable for the harms produced when governments use their systems. Given how often governments are turning to contractors to provide the algorithmic architectures for state decision-making, be it policing or welfare systems, there is a case that technology contractors like Palantir should be liable for discrimination and other violations. Currently most states attempt to disclaim any responsibility for problems created by the AI systems they procure, with the argument that “we cannot be responsible for something we don’t understand.” This means that commercial algorithmic systems are contributing to the process of government decision making without meaningful mechanisms of accountability. With the legal scholar Jason Schultz, I’ve argued that developers of AI systems that directly influence government decisions should be found to be state actors for purposes of constitutional liability in certain contexts.62 That is, they could be found legally liable for harms in the same way that states can be. Until then, vendors and contractors have little incentive to ensure that their systems aren’t reinforcing historical harms or creating entirely new ones.

Another example of this phenomenon is Vigilant Solutions, established in 2005. The company works on the basis of a single premise: take surveillance tools that might require judicial oversight if operated by governments and turn them into a thriving private enterprise outside constitutional privacy limits. Vigilant began its venture in multiple cities across the United States by installing automatic license-plate recognition (ALPR) cameras, placing them everywhere from cars to light poles, parking lots to apartment buildings. This array of networked cameras photographs every passing car, storing license plate images in a massive perpetual database. Vigilant then sells access to that database to the police, private investigators, banks, insurance companies, and others who want access to it. If police officers want to track a car across the entire state and mark every place it has been, Vigilant can show them. Likewise, if a bank wanted to repossess a car, Vigilant could reveal where it was, for a price.

California-based Vigilant markets itself as “one of those trusted crime fighting tools to help law enforcement develop leads and solve crimes faster,” and it has partnered with a range of governments in Texas, California, and Georgia to provide their police with a suite of ALPR systems to use on patrol, along with access to Vigilant’s database.64 In return, the local governments provide Vigilant with records of outstanding arrest warrants and overdue court fees. Any license plates flagged to match those associated with outstanding fines in the database are fed into police officers’ mobile systems, altering them to pull these drivers over. Drivers are then given two options: pay the outstanding fine on the spot or be arrested. On top of taking a 25 percent surcharge, Vigilant keeps records of every license plate reading, extracting that data to add to its massive databases.

Vigilant signed a significant contract with ICE that gave the agency access to five billion records of license plates gathered by private businesses, as well as 1.5 billion data points contributed by eighty local law enforcement agencies across the United States—including information on where people live and work. That data can stem from informal arrangements between local police and ICE and may already violate state data-sharing laws. ICE’s own privacy policy limits data collection near “sensitive locations” like schools, churches, and protests. But in this case, ICE doesn’t collect the data or maintain the database—the agency simply buys access to Vigilant’s systems, which has far fewer restrictions. This is a de facto privatization of public surveillance, a blurring between private contractors and state entities, and it creates opaque forms of data harvesting that live outside of traditional protective guidelines.

All the money and resources spent on relentless surveillance is part of a fever dream of centralized control that has come at the cost of other visions of social organization. The Snowden disclosures were a watershed moment in revealing how far a culture of extraction can go when the state and the commercial sector collaborate, but the network diagrams and PowerPoint clip art can feel quaint compared to all that has happened since.95 The NSA’s distinctive methods and tools have filtered down to classrooms, police stations, workplaces, and unemployment offices. It is the result of enormous investments, of de facto forms of privatization, and the securitization of risk and fear. The current deep entanglement of different forms of power was the hope of the Third Offset. It has warped far beyond the objective of strategic advantage in battlefield operations to encompass all those parts of everyday life that can be tracked and scored, grounded in normative definitions of how good citizens should communicate, behave, and spend. This shift brings with it a different vision of state sovereignty, modulated by corporate algorithmic governance, and it furthers the profound imbalance of power between agents of the state and the people they are meant to serve.

Conclusion: Power

Artificial intelligence is not an objective, universal, or neutral computational technique that makes determinations without human direction. Its systems are embedded in social, political, cultural, and economic worlds, shaped by humans, institutions, and imperatives that determine what they do and how they do it. They are designed to discriminate, to amplify hierarchies, and to encode narrow classifications. When applied in social contexts such as policing, the court system, health care, and education, they can reproduce, optimize, and amplify existing structural inequalities. This is no accident: AI systems are built to see and intervene in the world in ways that primarily benefit the states, institutions, and corporations that they serve. In this sense, AI systems are expressions of power that emerge from wider economic and political forces, created to increase profits and centralize control for those who wield them. But this is not how the story of artificial intelligence is typically told.

The standard accounts of AI often center on a kind of algorithmic exceptionalism—the idea that because AI systems can perform uncanny feats of computation, they must be smarter and more objective than their flawed human creators. Consider this diagram of AlphaGo Zero, an AI program designed by Google’s DeepMind to play strategy games.1 The image shows how it “learned” to play the Chinese strategy game Go by evaluating more than a thousand options per move. In the paper announcing this development, the authors write: “Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance.”2 DeepMind cofounder Demis Hassabis has described these game engines as akin to an alien intelligence. “It doesn’t play like a human, but it also doesn’t play like computer engines. It plays in a third, almost alien, way. . . . It’s like chess from another dimension.”3 When the next iteration mastered Go within three days, Hassabis described it as “rediscovering three thousand years of human knowledge in 72 hours!”

The Go diagram shows no machines, no human workers, no capital investment, no carbon footprint, just an abstract rules-based system endowed with otherworldly skills. Narratives of magic and mystification recur throughout AI’s history, drawing bright circles around spectacular displays of speed, efficiency, and computational reasoning.5 It’s no coincidence that one of the iconic examples of contemporary AI is a game.

Games without Frontiers

Games have been a preferred testing ground for AI programs since the 1950s.6 Unlike everyday life, games offer a closed world with defined parameters and clear victory conditions. The historical roots of AI in World War II stemmed from military-funded research in signal processing and optimization that sought to simplify the world, rendering it more like a strategy game. A strong emphasis on rationalization and prediction emerged, along with a faith that mathematical formalisms would help us understand humans and society.7 The belief that accurate prediction is fundamentally about reducing the complexity of the world gave rise to an implicit theory of the social: find the signal in the noise and make order from disorder.

This epistemological flattening of complexity into clean signal for the purposes of prediction is now a central logic of machine learning. The historian of technology Alex Campolo and I call this enchanted determinism: AI systems are seen as enchanted, beyond the known world, yet deterministic in that they discover patterns that can be applied with predictive certainty to everyday life.8 In discussions of deep learning systems, where machine learning techniques are extended by layering abstract representations of data on top of each other, enchanted determinism acquires an almost theological quality. That deep learning approaches are often uninterpretable, even to the engineers who created them, gives these systems an aura of being too complex to regulate and too powerful to refuse. As the social anthropologist F. G. Bailey observed, the technique of “obscuring by mystification” is often employed in public settings to argue for a phenomenon’s inevitability.9 We are told to focus on the innovative nature of the method rather than on what is primary: the purpose of the thing itself. Above all, enchanted determinism obscures power and closes off informed public discussion, critical scrutiny, or outright rejection.

Enchanted determinism has two dominant strands, each a mirror image of the other. One is a form of tech utopianism that offers computational interventions as universal solutions applicable to any problem. The other is a tech dystopian perspective that blames algorithms for their negative outcomes as though they are independent agents, without contending with the contexts that shape them and in which they operate. At an extreme, the tech dystopian narrative ends in the singularity, or superintelligence—the theory that a machine intelligence could emerge that will ultimately dominate or destroy humans.10 This view rarely contends with the reality that so many people around the world are already dominated by systems of extractive planetary computation.

These dystopian and utopian discourses are metaphysical twins: one places its faith in AI as a solution to every problem, while the other fears AI as the greatest peril. Each offers a profoundly ahistorical view that locates power solely within technology itself. Whether AI is abstracted as an all-purpose tool or an all-powerful overlord, the result is technological determinism. AI takes the central position in society’s redemption or ruin, permitting us to ignore the systemic forces of unfettered neoliberalism, austerity politics, racial inequality, and widespread labor exploitation. Both the tech utopians and dystopians frame the problem with technology always at the center, inevitably expanding into every part of life, decoupled from the forms of power that it magnifies and serves.

When AlphaGo defeats a human grandmaster, it’s tempting to imagine that some kind of otherworldly intelligence has arrived. But there’s a far simpler and more accurate explanation. AI game engines are designed to play millions of games, run statistical analyses to optimize for winning outcomes, and then play millions more. These programs produce surprising moves uncommon in human games for a straightforward reason: they can play and analyze far more games at a far greater speed than any human can. This is not magic; it is statistical analysis at scale. Yet the tales of preternatural machine intelligence persist.11 Over and over, we see the ideology of Cartesian dualism in AI: the fantasy that AI systems are disembodied brains that absorb and produce knowledge independently from their creators, infrastructures, and the world at large. These illusions distract from the far more relevant questions: Whom do these systems serve? What are the political economies of their construction? And what are the wider planetary consequences?

The Pipelines of AI

Consider a different illustration of AI: the blueprint for Google’s first owned and operated data center, in The Dalles, Oregon. It depicts three 68,680-square- foot buildings, an enormous facility that was estimated in 2008 to use enough energy to power eighty-two thousand homes, or a city the size of Tacoma, Washington.12 The data center now spreads along the shores of the Columbia River, where it draws heavily on some of the cheapest electricity in North America. Google’s lobbyists negotiated for six months with local officials to get a deal that included tax exemptions, guarantees of cheap energy, and use of the city-built fiber-optic ring. Unlike the abstract vision of a Go game, the engineering plan reveals how much of Google’s technical vision depends on public utilities, including gas mains, sewer pipes, and the high-voltage lines through which the discount electricity would flow. In the words of the writer Ginger Strand, “Through city infrastructure, state givebacks, and federally subsidized power, YouTube is bankrolled by us.”

The blueprint reminds us of how much the artificial intelligence industry’s expansion has been publicly subsidized: from defense funding and federal research agencies to public utilities and tax breaks to the data and unpaid labor taken from all who use search engines or post images online. AI began as a major public project of the twentieth century and was relentlessly privatized to produce enormous financial gains for the tiny minority at the top of the extraction pyramid.

These diagrams present two different ways of understanding how AI works. I’ve argued that there is much at stake in how we define AI, what its boundaries are, and who determines them: it shapes what can be seen and contested. The Go diagram speaks to the industry narratives of an abstract computational cloud, far removed from the earthly resources needed to produce it, a paradigm where technical innovation is lionized, regulation is rejected, and true costs are never revealed. The blueprint points us to the physical infrastructure, but it leaves out the full environmental implications and the political deals that made it possible. These partial accounts of AI represent what philosophers Michael Hardt and Antonio Negri call the “dual operation of abstraction and extraction” in information capitalism: abstracting away the material conditions of production while extracting more information and resources.14 The description of AI as fundamentally abstract distances it from the energy, labor, and capital needed to produce it and the many different kinds of mining that enable it.

This book has explored the planetary infrastructure of AI as an extractive industry: from its material genesis to the political economy of its operations to the discourses that support its aura of immateriality and inevitability. We have seen the politics inherent in how AI systems are trained to recognize the world. And we’ve observed the systemic forms of inequity that make AI what it is today. The core issue is the deep entanglement of technology, capital, and power, of which AI is the latest manifestation. Rather than being inscrutable and alien, these systems are products of larger social and economic structures with profound material consequences.

The Map Is Not the Territory

How do we see the full life cycle of artificial intelligence and the dynamics of power that drive it? We have to go beyond the conventional maps of AI to locate it in a wider landscape. Atlases can provoke a shift in scale, to see how spaces are joined in relation to one another. This book proposes that the real stakes of AI are the global interconnected systems of extraction and power, not the technocratic imaginaries of artificiality, abstraction, and automation. To understand AI for what it is, we need to see the structures of power it serves.

AI is born from salt lakes in Bolivia and mines in Congo, constructed from crowdworker-labeled datasets that seek to classify human actions, emotions, and identities. It is used to navigate drones over Yemen, direct immigration police in the United States, and modulate credit scores of human value and risk across the world. A wide-angle, multiscalar perspective on AI is needed to contend with these overlapping regimes.

This book began below the ground, where the extractive politics of artificial intelligence can be seen at their most literal. Rare earth minerals, water, coal, and oil: the tech sector carves out the earth to fuel its highly energy-intensive infrastructures. AI’s carbon footprint is never fully admitted or accounted for by the tech sector, which is simultaneously expanding the networks of data centers while helping the oil and gas industry locate and strip remaining reserves of fossil fuels. The opacity of the larger supply chain for computation in general, and AI in particular, is part of a long-established business model of extracting value from the commons and avoiding restitution for the lasting damage.

Labor represents another form of extraction. In chapter 2, we ventured beyond the highly paid machine learning engineers to consider the other forms of work needed to make artificial intelligence systems function. From the miners extracting tin in Indonesia to crowdworkers in India completing tasks on Amazon Mechanical Turk to iPhone factory workers at Foxconn in China, the labor force of AI is far greater than we normally imagine. Even within the tech companies there is a large shadow workforce of contract laborers, who significantly outnumber full-time employees but have fewer benefits and no job security.

In the logistical nodes of the tech sector, we find humans completing the tasks that machines cannot. Thousands of people are needed to support the illusion of automation: tagging, correcting, evaluating, and editing AI systems to make them appear seamless. Others lift packages, drive for ride-hailing apps, and deliver food. AI systems surveil them all while squeezing the most output from the bare functionality of human bodies: the complex joints of fingers, eyes, and knee sockets are cheaper and easier to acquire than robots. In those spaces, the future of work looks more like the Taylorist factories of the past, but with wristbands that vibrate when workers make errors and penalties given for taking too many bathroom breaks.

The uses of workplace AI further skew power imbalances by placing more control in employers’ hands. Apps are used to track workers, nudge them to work longer hours, and rank them in real time. Amazon provides a canonical example of how a microphysics of power—disciplining bodies and their movement through space—is connected to a macrophysics of power, a logistics of planetary time and information. AI systems exploit differences in time and wages across markets to speed the circuits of capital. Suddenly, everyone in urban centers can have—and expects—same day delivery. And the system speeds up again, with the material consequences hidden behind the cardboard boxes, delivery trucks, and “buy now” buttons.

At the data layer, we can see a different geography of extraction. “We are building a mirror of the real world,” a Google Street View engineer said in 2012. “Anything that you see in the real world needs to be in our databases.”16 Since then, the harvesting of the real world has only intensified to reach into spaces that were previously hard to capture. As we saw in chapter 3, there has been a widespread pillaging of public spaces; the faces of people in the street have been captured to train facial recognition systems; social media feeds have been ingested to build predictive models of language; sites where people keep personal photos or have online debates have been scraped in order to train machine vision and natural language algorithms. This practice has become so common that few in the AI field even question it. In part, that is because so many careers and market valuations depend on it. The collect-it- all mentality, once the remit of intelligence agencies, is not only normalized but moralized—it is seen as wasteful not to collect data wherever possible.

Once data is extracted and ordered into training sets, it becomes the epistemic foundation by which AI systems classify the world. From the benchmark training sets such as ImageNet, MS-Celeb, or NIST’s collections, images are used to represent ideas that are far more relational and contested than the labels may suggest. In chapter 4, we saw how labeling taxonomies allocate people into forced gender binaries, simplistic and offensive racial groupings, and highly normative and stereotypical analyses of character, merit, and emotional state. These classifications, unavoidably value-laden, force a way of seeing onto the world while claiming scientific neutrality.

Datasets in AI are never raw materials to feed algorithms: they are inherently political interventions. The entire practice of harvesting data, categorizing and labeling it, and then using it to train systems is a form of politics. It has brought a shift to what are called operational images—representations of the world made solely for machines. Bias is a symptom of a deeper affliction: a far-ranging and centralizing normative logic that is used to determine how the world should be seen and evaluated.

A central example of this is affect detection, described in chapter 5, which draws on controversial ideas about the relation of faces to emotions and applies them with the reductive logic of a lie detector test. The science remains deeply contested. 19 Institutions have always classified people into identity categories, narrowing personhood and cutting it down into precisely measured boxes. Machine learning allows that to happen at scale. From the hill towns of Papua New Guinea to military labs in Maryland, techniques have been developed to reduce the messiness of feelings, interior states, preferences, and identifications into something quantitative, detectable, and trackable.

What epistemological violence is necessary to make the world readable to a machine learning system? AI seeks to systematize the unsystematizable, formalize the social, and convert an infinitely complex and changing universe into a Linnaean order of machine-readable tables. Many of AI’s achievements have depended on boiling things down to a terse set of formalisms based on proxies: identifying and naming some features while ignoring or obscuring countless others. To adapt a phrase from philosopher Babette Babich, machine learning exploits what it does know to predict what it does not know: a game of repeated approximations. Datasets are also proxies—stand-ins for what they claim to measure. Put simply, this is transmuting difference into computable sameness. This kind of knowledge schema recalls what Friedrich Nietzsche described as “the falsifying of the multifarious and incalculable into the identical, similar, and calculable.”20 AI systems become deterministic when these proxies are taken as ground truth, when fixed labels are applied to a fluid complexity. We saw this in the cases where AI is used to predict gender, race, or sexuality from a photograph of a face.21 These approaches resemble phrenology and physiognomy in their desire to essentialize and impose identities based on external appearances.

The problem of ground truth for AI systems is heightened in the context of state power, as we saw in chapter 6. The intelligence agencies led the way on the mass collection of data, where metadata signatures are sufficient for lethal drone strikes and a cell phone location becomes a proxy for an unknown target. Even here, the bloodless language of metadata and surgical strikes is directly contradicted by the unintended killings from drone missiles.22 As Lucy Suchman has asked, how are “objects” identified as imminent threats? We know that “ISIS pickup truck” is a category based on hand-labeled data, but who chose the categories and identified the vehicles?23 We saw the epistemological confusions and errors of object recognition training sets like ImageNet; military AI systems and drone attacks are built on the same unstable terrain.

The deep interconnections between the tech sector and the military are now framed within a strong nationalist agenda. The rhetoric about the AI war between the United States and China drives the interests of the largest tech companies to operate with greater government support and few restrictions. Meanwhile, the surveillance armory used by agencies like the NSA and the CIA is now deployed domestically at a municipal level in the in-between space of commercial-military contracting by companies like Palantir. Undocumented immigrants are hunted down with logistical systems of total information control and capture that were once reserved for extralegal espionage. Welfare decision-making systems are used to track anomalous data patterns in order to cut people off from unemployment benefits and accuse them of fraud. License plate reader technology is being used by home surveillance systems— a widespread integration of previously separate surveillance networks.

The result is a profound and rapid expansion of surveillance and a blurring between private contractors, law enforcement, and the tech sector, fueled by kickbacks and secret deals. It is a radical redrawing of civic life, where the centers of power are strengthened by tools that see with the logics of capital, policing, and militarization.

Toward Connected Movements for Justice

If AI currently serves the existing structures of power, an obvious question might be: Should we not seek to democratize it? Could there not be an AI for the people that is reoriented toward justice and equality rather than industrial extraction and discrimination? This may seem appealing, but as we have seen throughout this book, the infrastructures and forms of power that enable and are enabled by AI skew strongly toward the centralization of control. To suggest that we democratize AI to reduce asymmetries of power is a little like arguing for democratizing weapons manufacturing in the service of peace. As Audre Lorde reminds us, the master’s tools will never dismantle the master’s house.

A reckoning is due for the technology sector. To date, one common industry response has been to sign AI ethics principles. As European Union parliamentarian Marietje Schaake observed, in 2019 there were 128 frameworks for AI ethics in Europe alone. These documents are often presented as products of a “wider consensus” on AI ethics. But they are overwhelmingly produced by economically developed countries, with little representation from Africa, South and Central America, or Central Asia. The voices of the people most harmed by AI systems are largely missing from the processes that produce them. Further, ethical principles and statements don’t discuss how they should be implemented, and they are rarely enforceable or accountable to a broader public. As Shannon Mattern has noted, the focus is more commonly on the ethical ends for AI, without assessing the ethical means of its application. 28 Unlike medicine or law, AI has no formal professional governance structure or norms—no agreed-upon definitions and goals for the field or standard protocols for enforcing ethical practice.

Self-regulating ethical frameworks allow companies to choose how to deploy technologies and, by extension, to decide what ethical AI means for the rest of the world.30 Tech companies rarely suffer serious financial penalties when their AI systems violate the law and even fewer consequences when their ethical principles are violated. Further, public companies are pressured by shareholders to maximize return on investment over ethical concerns, commonly making ethics secondary to profits. As a result, ethics is necessary but not sufficient to address the fundamental concerns raised in this book.

To understand what is at stake, we must focus less on ethics and more on power. AI is invariably designed to amplify and reproduce the forms of power it has been deployed to optimize. Countering that requires centering the interests of the communities most affected. Instead of glorifying company founders, venture capitalists, and technical visionaries, we should begin with the lived experiences of those who are disempowered, discriminated against, and harmed by AI systems. When someone says, “AI ethics,” we should assess the labor conditions for miners, contractors, and crowdworkers. When we hear “optimization,” we should ask if these are tools for the inhumane treatment of immigrants. When there is applause for “large-scale automation,” we should remember the resulting carbon footprint at a time when the planet is already under extreme stress. What would it mean to work toward justice across all these systems?

In 1986, the political theorist Langdon Winner described a society “committed to making artificial realities” with no concern for the harms it could bring to the conditions of life: “Vast transformations in the structure of our common world have been undertaken with little attention to what those alterations mean. . . . In the technical realm we repeatedly enter into a series of social contracts, the terms of which are only revealed after signing.”

In the four decades since, those transformations are now at a scale that has shifted the chemical composition of the atmosphere, the temperature of Earth’s surface, and the contents of the planet’s crust. The gap between how technology is judged on its release and its lasting consequences has only widened. The social contract, to the extent that there ever was one, has brought a climate crisis, soaring wealth inequality, racial discrimination, and widespread surveillance and labor exploitation. But the idea that these transformations occurred in ignorance of their possible results is part of the problem. The philosopher Achille Mbembé sharply critiques the idea that we could not have foreseen what would become of the knowledge systems of the twenty-first century, as they were always “operations of abstraction that claim to rationalize the world on the basis of corporate logic.” He writes: “It is about extraction, capture, the cult of data, the commodification of human capacity for thought and the dismissal of critical reason in favour of programming. . . . Now more than ever before, what we need is a new critique of technology, of the experience of technical life.”

The next era of critique will also need to find spaces beyond technical life by overturning the dogma of inevitability. When AI’s rapid expansion is seen as unstoppable, it is possible only to patch together legal and technical restraints on systems after the fact: to clean up datasets, strengthen privacy laws, or create ethics boards. But these will always be partial and incomplete responses in which technology is assumed and everything else must adapt. But what happens if we reverse this polarity and begin with the commitment to a more just and sustainable world? How can we intervene to address interdependent issues of social, economic, and climate injustice? Where does technology serve that vision? And are there places where AI should not be used, where it undermines justice?

This is the basis for a renewed politics of refusal—opposing the narratives of technological inevitability that says, “If it can be done, it will be.” Rather than asking where AI will be applied, merely because it can, the emphasis should be on why it ought to be applied. By asking, “Why use artificial intelligence?” we can question the idea that everything should be subject to the logics of statistical prediction and profit accumulation, what Donna Haraway terms the “informatics of domination.”35 We see glimpses of this refusal when populations choose to dismantle predictive policing, ban facial recognition, or protest algorithmic grading. So far these minor victories have been piecemeal and localized, often centered in cities with more resources to organize, such as London, San Francisco, Hong Kong, and Portland, Oregon. But they point to the need for broader national and international movements that refuse technology-first approaches and focus on addressing underlying inequities and injustices. Refusal requires rejecting the idea that the same tools that serve capital, militaries, and police are also fit to transform schools, hospitals, cities, and ecologies, as though they were value neutral calculators that can be applied everywhere.

The calls for labor, climate, and data justice are at their most powerful when they are united. Above all, I see the greatest hope in the growing justice movements that address the interrelatedness of capitalism, computation, and control: bringing together issues of climate justice, labor rights, racial justice, data protection, and the overreach of police and military power. By rejecting systems that further inequity and violence, we challenge the structures of power that AI currently reinforces and create the foundations for a different society.36 As Ruha Benjamin notes, “Derrick Bell said it like this: ‘To see things as they really are, you must imagine them for what they might be.’ We are pattern makers and we must change the content of our existing patterns.”37 To do so will require shaking off the enchantments of tech solutionism and embracing alternative solidarities—what Mbembé calls “a different politics of inhabiting the Earth, of repairing and sharing the planet.”38 There are sustainable collective politics beyond value extraction; there are commons worth keeping, worlds beyond the market, and ways to live beyond discrimination and brutal modes of optimization. Our task is to chart a course there.


/ 018. Russell, Stuart. Human Compatible: Artificial Intelligence and the Problem of Control, 2019.

PREFACE: Why This Book? Why Now?

This book is about the past, present, and future of our attempt to understand and create intelligence. This matters, not because Al is rapidly becoming a pervasive aspect of the present but because it is the dominant technology of the future. The world’s great powers are waking up to this fact, and the world’s largest corporations have known it for some time. We cannot predict exactly how the technology will develop or on what timeline. Nevertheless, we must plan for the possibility that machines will far exceed the human capacity for decision making in the real world. What then?

Everything civilization has to offer is the product of our intelligence; gaining access to considerably greater intelligence would be the biggest event in human history. The purpose of the book is to explain why it might be the last event in human history and how to make sure that it is not.

1. IF WE SUCCEED

How Did We Get Here?

The roots of AI stretch far back into antiquity, but its “official” beginning was in 1956. Two young mathematicians, John McCarthy and Marvin Minsky, had persuaded Claude Shannon, already famous as the inventor of information theory, and Nathaniel Rochester, the designer of IBM’s first commercial computer, to join them in organizing a summer program at Dartmouth College. The goal was stated as follows:

The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.

Needless to say, it took much longer than a summer: we are still working on all these problems.

In the first decade or so after the Dartmouth meeting, AI had several major successes, including Alan Robinson’s algorithm for general purpose logical reasoning2 and Arthur Samuels checker-playing program, which taught itself to beat its creator.3 The first AI bubble burst in the late 1960s, when early efforts at machine learning and machine translation failed to live up to expectations. A report commissioned by the UK government in 1973 concluded, "In no part of the field have the discoveries made so far produced the major impact that was then promised.”4 In other words, the machines just weren’t smart enough.

My eleven-year-old self was, fortunately, unaware of this report. Two years later, when I was given a Sinclair Cambridge Programmable calculator, I just wanted to make it intelligent. With a maximum program size of thirty-six keystrokes, however, the Sinclair was not quite big enough for human-level AI. Undeterred, I gained access to the giant CDC 6600 supercomputer5 at Imperial College London and wrote a chess program—a stack of punched cards two feet high. It wasn’t very good, but it didn’t matter. I knew what I wanted to do.

By the mid-1980s, I had become a professor at Berkeley, and AI was experiencing a huge revival thanks to the commercial potential of so-called expert systems. The second AI bubble burst when these systems proved to be inadequate for many of the tasks to which they were applied. Again, the machines just weren’t smart enough. An AI winter ensued. My own AI course at Berkeley, currently bursting with over nine hundred students, had just twenty-five students in 1990.

The AI community learned its lesson: smarter, obviously, was better, but we would have to do our homework to make that happen. The field became far more mathematical. Connections were made to the long-established disciplines of probability, statistics, and control theory. The seeds of today's progress were sown during that AI winter, including early work on large-scale probabilistic reasoning systems and what later became known as deep learning.

Beginning around 2011, deep learning techniques began to produce dramatic advances in speech recognition, visual object recognition, and machine translation—three of the most important open problems in the field. By some measures, machines now match or exceed human capabilities in these areas. In 2016 and 2017, DeepMind’s AlphaGo defeated Lee Sedol, former world Go champion, and Ke Jie, the current champion—events that some experts predicted wouldn't happen until 2097, if ever.

Now AI generates front-page media coverage almost every day. Thousands of start-up companies have been created, fueled by a flood of venture funding. Millions of students have taken online AI and machine learning courses, and experts in the area command salaries in the millions of dollars. Investments flowing from venture funds, national governments, and major corporations are in the tens of billions of dollars annually—more money in the last five years than in the entire previous history of the field. Advances that are already in the pipeline, such as self-driving cars and intelligent personal assistants, are likely to have a substantial impact on the world over the next decade or so. The potential economic and social benefits of AI are vast, creating enormous momentum in the AI research enterprise.

What Happens Next?

Does this rapid rate of progress mean that we are about to be overtaken by machines? No. There are several breakthroughs that have to happen before we have anything resembling machines with superhuman intelligence.

Scientific breakthroughs are notoriously hard to predict. To get a sense of just how hard, we can look back at the history of another field with civilization-ending potential: nuclear physics.

In the early years of the twentieth century, perhaps no nuclear physicist was more distinguished than Ernest Rutherford, the discoverer of the proton and the “man who split the atom” (figure 2[a]). Like his colleagues, Rutherford had long been aware that atomic nuclei stored immense amounts of energy; yet the prevailing view was that tapping this source of energy was impossible.

On September 11, 1933, the British Association for the Advancement of Science held its annual meeting in Leicester. Lord Rutherford addressed the evening session. As he had done several times before, he poured cold water on the prospects for atomic energy: “Anyone who looks for a source of power in the transformation of the atoms is talking moonshine.” Rutherford s speech was reported in the Times of London the next morning (figure 2[b]).

Leo Szilard (figure 2[c]), a Hungarian physicist who had recently fled from Nazi Germany, was staying at the Imperial Hotel on Russell Square in London. He read the Times’ report at breakfast. Mulling over what he had read, he went for a walk and invented the neutron-induced nuclear chain reaction.7 The problem of liberating nuclear energy went from impossible to essentially solved in less than twenty-four hours. Szilard filed a secret patent for a nuclear reactor the following year. The first patent for a nuclear weapon was issued in France in 1939.

The moral of this story is that betting against human ingenuity is foolhardy, particularly when our future is at stake. Within the AI community, a kind of denialism is emerging, even going as far as denying the possibility of success in achieving the long-term goals of AI. It's as if a bus driver, with all of humanity as passengers, said, “Yes, I am driving as hard as I can towards a cliff, but trust me, we'll run out of gas before we get there!"

I am not saying that success in AI will necessarily happen, and I think it's quite unlikely that it will happen in the next few years. It seems prudent, nonetheless, to prepare for the eventuality. If all goes well, it would herald a golden age for humanity, but we have to face the fact that we are planning to make entities that are far more powerful than humans. How do we ensure that they never, ever have power over us?

To get just an inkling of the fire we're playing with, consider how content-selection algorithms function on social media. They aren't particularly intelligent, but they are in a position to affect the entire world because they directly influence billions of people. Typically, such algorithms are designed to maximize click-through, that is, the probability that the user clicks on presented items. The solution is simply to present items that the user likes to click on, right? Wrong. The solution is to change the user's preferences so that they become more predictable. A more predictable user can be fed items that they are likely to click on, thereby generating more revenue. People with more extreme political views tend to be more predictable in which items they will click on. (Possibly there is a category of articles that die-hard centrists are likely to click on, but it’s not easy to imagine what this category consists of.) Like any rational entity, the algorithm learns how to modify the state of its environment—in this case, the users mind—in order to maximize its own reward.8 The consequences include the resurgence of fascism, the dissolution of the social contract that underpins democracies around the world, and potentially the end of the European Union and NATO. Not bad for a few lines of code, even if it had a helping hand from some humans. Now imagine what a really intelligent algorithm would be able to do.

What Went Wrong?

The history of AI has been driven by a single mantra: “The more intelligent the better.” I am convinced that this is a mistake—not because of some vague fear of being superseded but because of the way we have understood intelligence itself.

The concept of intelligence is central to who we are—that’s why we call ourselves Homo sapiens, or “wise man.” After more than two thousand years of self-examination, we have arrived at a characterization of intelligence that can be boiled down to this:

Humans are intelligent to the extent that our actions can be expected to achieve our objectives.

All those other characteristics of intelligence—perceiving, thinking, learning, inventing, and so on—can be understood through their contributions to our ability to act successfully. From the very beginnings of AI, intelligence in machines has been defined in the same way:

Machines are intelligent to the extent that their actions can be expected to achieve their objectives.

Because machines, unlike humans, have no objectives of their own, we give them objectives to achieve. In other words, we build optimizing machines, we feed objectives into them, and off they go.

This general approach is not unique to AI. It recurs throughout the technological and mathematical underpinnings of our society. In the field of control theory, which designs control systems for everything from jumbo jets to insulin pumps, the job of the system is to minimize a cost function that typically measures some deviation from a desired behavior. In the field of economics, mechanisms and policies are designed to maximize the utility of individuals, the welfare of groups, and the profit of corporations.9 In operations research, which solves complex logistical and manufacturing problems, a solution maximizes an expected sum of rewards over time. Finally, in statistics, learning algorithms are designed to minimize an expected loss function that defines the cost of making prediction errors.

Evidently, this general scheme—which I will call the standard model—is widespread and extremely powerful. Unfortunately, we don't want machines that are intelligent in this sense.

The drawback of the standard model was pointed out in 1960 by Norbert Wiener, a legendary professor at MIT and one of the leading mathematicians of the mid-twentieth century. Wiener had just seen Arthur Samuel’s checker-playing program learn to play checkers far better than its creator. That experience led him to write a prescient but little-known paper, “Some Moral and Technical Consequences of Automation.”10 Here’s how he states the main point:

If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively . . . we had better be quite sure that the purpose put into the machine is the purpose which we really desire.

“The purpose put into the machine” is exactly the objective that machines are optimizing in the standard model. If we put the wrong objective into a machine that is more intelligent than us, it will achieve the objective, and we lose. The social-media meltdown I described earlier is just a foretaste of this, resulting from optimizing the wrong objective on a global scale with fairly unintelligent algorithms. In Chapter 5, I spell out some far worse outcomes.

All this should come as no great surprise. For thousands of years, we have known the perils of getting exactly what you wish for. In every story where someone is granted three wishes, the third wish is always to undo the first two wishes.

In summary, it seems that the march towards superhuman intelligence is unstoppable, but success might be the undoing of the human race. Not all is lost, however. We have to understand where we went wrong and then fix it.

Can We Fix It?

The problem is right there in the basic definition of AI. We say that machines are intelligent to the extent that their actions can be expected to achieve their objectives, but we have no reliable way to make sure that their objectives are the same as our objectives.

What if, instead of allowing machines to pursue their objectives, we insist that they pursue our objectives? Such a machine, if it could be designed, would be not just intelligent but also beneficial to humans. So let’s try this:

Machines are beneficial to the extent that their actions can be expected to achieve our objectives.

This is probably what we should have done all along.

The difficult part, of course, is that our objectives are in us (all eight billion of us, in all our glorious variety] and not in the machines. It is, nonetheless, possible to build machines that are beneficial in exactly this sense. Inevitably, these machines will be uncertain about our objectives—after all, we are uncertain about them ourselves—but it turns out that this is a feature, not a bug (that is, a good thing and not a bad thing). Uncertainty about objectives implies that machines will necessarily defer to humans: they will ask permission, they will accept correction, and they will allow themselves to be switched off.

Removing the assumption that machines should have a definite objective means that we will need to tear out and replace part of the foundations of artificial intelligence—the basic definitions of what we are trying to do. That also means rebuilding a great deal of the superstructure—the accumulation of ideas and methods for actually doing AI. The result will be a new relationship between humans and machines, one that I hope will enable us to navigate the next few decades successfully.

2: INTELLIGENCE IN HUMANS AND MACHINES

When you arrive at a dead end, it’s a good idea to retrace your steps and work out where you took a wrong turn. I have argued that the standard model of AI, wherein machines optimize a fixed objective supplied by humans, is a dead end. The problem is not that we might fail to do a good job of building AI systems; its that we might succeed too well. The very definition of success in AI is wrong.

So let’s retrace our steps, all the way to the beginning. Let’s try to understand how our concept of intelligence came about and how it came to be applied to machines. Then we have a chance of coming up with a better definition of what counts as a good AI system.

Intelligence

How does the universe work? How did life begin? Where are my keys? These are fundamental questions worthy of thought. But who is asking these questions? How am I answering them? How can a handful of matter—the few pounds of pinkish-gray blancmange we call a brain—perceive, understand, predict, and manipulate a world of unimaginable vastness? Before long, the mind turns to examine itself.

We have been trying for thousands of years to understand how our minds work. Initially, the purposes included curiosity, self-management, persuasion, and the rather pragmatic goal of analyzing mathematical arguments. Yet every step towards an explanation of how the mind works is also a step towards the creation of the mind’s capabilities in an artifact—that is, a step towards artificial intelligence.

Before we can understand how to create intelligence, it helps to understand what it is. The answer is not to be found in IQ tests, or even in Turing tests, but in a simple relationship between what we perceive, what we want, and what we do. Roughly speaking, an entity is intelligent to the extent that what it does is likely to achieve what it wants, given what it has perceived.

Computers

Having a reasonable definition of intelligence is the first ingredient in creating intelligent machines. The second ingredient is a machine in which that definition can be realized. For reasons that will soon become obvious, that machine is a computer. It could have been something different—for example, we might have tried to make intelligent machines out of complex chemical reactions or by hijacking biological cells30—but devices built for computation, from the very earliest mechanical calculators onwards, have always seemed to their inventors to be the natural home for intelligence.

We are so used to computers now that we barely notice their utterly incredible powers. If you have a laptop or a desktop or a smart phone, look at it: a small box, with a way to type characters. Just by typing, you can create programs that turn the box into something new, perhaps something that magically synthesizes moving images of oceangoing ships hitting icebergs or alien planets with tall blue people; type some more, and it translates English into Chinese; type some more, and it listens and speaks; type some more, and it defeats the world chess champion.

This ability of a single box to carry out any process that you can imagine is called universality, a concept first introduced by Alan Turing in 1936.31 Universality means that we do not need separate machines for arithmetic, machine translation, chess, speech understanding, or animation: one machine does it all. Your laptop is essentially identical to the vast server farms run by the worlds largest IT companies—even those equipped with fancy, special-purpose tensor processing units for machine learning. It’s also essentially identical to all future computing devices yet to be invented. The laptop can do exactly the same tasks, provided it has enough memory; it just takes a lot longer.

Turing’s paper introducing universality was one of the most important ever written. In it, he described a simple computing device that could accept as input the description of any other computing device, together with that second device’s input, and, by simulating the operation of the second device on its input, produce the same output that the second device would have produced. We now call this first device a universal Turing machine. To prove its universality, Turing introduced precise definitions for two new kinds of mathematical objects: machines and programs. Together, the machine and program define a sequence of events—specifically, a sequence of state changes in the machine and its memory.

In the history of mathematics, new kinds of objects occur quite rarely. Mathematics began with numbers at the dawn of recorded history. Then, around 2000 BCE, ancient Egyptians and Babylonians worked with geometric objects (points, lines, angles, areas, and so on). Chinese mathematicians introduced matrices during the first millennium BCE, while sets as mathematical objects arrived only in the nineteenth century. Turing’s new objects—machines and programs— are perhaps the most powerful mathematical objects ever invented. It is ironic that the field of mathematics largely failed to recognize this, and from the 1940s onwards, computers and computation have been the province of engineering departments in most major universities.

The field that emerged—computer science—exploded over the next seventy years, producing a vast array of new concepts, designs, methods, and applications, as well as seven of the eight most valuable companies in the world.

The central concept in computer science is that of an algorithm, which is a precisely specified method for computing something. Algorithms are, by now, familiar parts of everyday life: a square-root until 2025 or so, although at a slightly slower rate. For some years, speeds have been limited by the large amount of heat generated by the fast switching of silicon transistors; moreover, circuit sizes cannot get much smaller because the wires and connectors are (as of 2019) no more than twenty-five atoms wide and five to ten atoms thick. Beyond 2025, we will need to use more exotic physical phenomena—including negative capacitance devices,32 single-atom transistors, graphene nanotubes, and photonics—to keep Moore’s law (or its successor) going.

Instead of just speeding up general-purpose computers, another possibility is to build special-purpose devices that are customized to perform just one class of computations. For example, Google’s tensor processing units (TPUs) are designed to perform the calculations required for certain machine learning algorithms. One TPU pod (2018 version) performs roughly 1017 calculations per second—nearly as much as the Summit machine—but uses about one hundred times less power and is one hundred times smaller. Even if the underlying chip technology remains roughly constant, these kinds of machines can simply be made larger and larger to provide vast quantities of raw computational power for AI systems.

Quantum computation is a different kettle of fish. It uses the strange properties of quantum-mechanical wave functions to achieve something remarkable: with twice the amount of quantum hardware, you can do more than twice the amount of computation] Very roughly, it works like this:33 Suppose you have a tiny physical device that stores a quantum bit, or qubit. A qubit has two possible states, 0 and 1. Whereas in classical physics the qubit device has to be in one of the two states, in quantum physics the wave function that carries information about the qubit says that it is in both states simultaneously. If you have two qubits, there are four possible joint states: 00, 01, 10, and 11. If the wave function is coherently entangled across the two qubits, meaning that no other physical processes are there to mess it up, then the two qubits are in all four states simultaneously. Moreover, if the two qubits are connected into a quantum circuit that performs some calculation, then the calculation proceeds with all four states simultaneously. With three qubits, you get eight states processed simultaneously, and so on. Now, there are some physical limitations so that the amount of work that gets done is less than exponential in the number of qubits,34 but we know that there are important problems for which quantum computation is provably more efficient than any classical computer.

As of 2019, there are experimental prototypes of small quantum processors in operation with a few tens of qubits, but there are no interesting computing tasks for which a quantum processor is faster than a classical computer. The main difficulty is decoherence— processes such as thermal noise that mess up the coherence of the multi-qubit wave function. Quantum scientists hope to solve the decoherence problem by introducing error correction circuitry, so that any error that occurs in the computation is quickly detected and corrected by a kind of voting process. Unfortunately, error-correcting systems require far more qubits to do the same work: while a quantum machine with a few hundred perfect qubits would be very powerful compared to existing classical computers, we will probably need a fewT million error-correcting qubits to actually realize those computations. Going from a few tens to a few million qubits will take quite a few years. If, eventually, we get there, that would completely change the picture of what we can do by sheer brute-force computation.35 Rather than waiting for real conceptual advances in AI, we might be able to use the raw power of quantum computation to bypass some of the barriers faced by current “unintelligent” algorithms.

The limits of computation

Even in the 1950s, computers were described in the popular press as “super-brains” that were “faster than Einstein.” So can we say now, finally, that computers are as powerful as the human brain? No. Focusing on raw computing power misses the point entirely. Speed alone won’t give us AI. Running a poorly designed algorithm on a faster computer doesn’t make the algorithm better; it just means you get the wrong answer more quickly. (And with more data there are more opportunities for wrong answers!) The principal effect of faster machines has been to make the time for experimentation shorter, so that research can progress more quickly. It’s not hardware that is holding AI back; it’s software. We don’t yet know how to make a machine really intelligent—even if it were the size of the universe.

Suppose, however, that we do manage to develop the right kind of AI software. Are there any limits placed by physics on how powerful a computer can be? Will those limits prevent us from having enough computing power to create real AI? The answers seem to be yes, there are limits, and no, there isn’t a ghost of a chance that the limits will prevent us from creating real AI. MIT physicist Seth Lloyd has estimated the limits for a laptop-sized computer, based on considerations from quantum theory and entropy.36 The numbers would raise even Carl Sagan’s eyebrows: 1051 operations per second and 1030 bytes of memory, or approximately a billion trillion trillion times faster and four trillion times more memory than Summit—which, as noted previously, has more raw power than the human brain. Thus, when one hears suggestions that the human mind represents an upper limit on what is physically achievable in our universe,37 one should at least ask for further clarification.

Besides limits imposed by physics, there are other limits on the abilities of computers that originate in the work of computer scientists. Turing himself proved that some problems are undecidable by any computer: the problem is well defined, there is an answer, but there cannot exist an algorithm that always finds that answer. He gave the example of what became known as the halting problem: Can an algorithm decide if a given program has an “infinite loop” that prevents it from ever finishing?

Turing’s proof that no algorithm can solve the halting problem39 is incredibly important for the foundations of mathematics, but it seems to have no bearing on the issue of whether computers can be intelligent. One reason for this claim is that the same basic limitation seems to apply to the human brain. Once you start asking a human brain to perform an exact simulation of itself simulating itself simulating itself, and so on, you’re bound to run into difficulties. I, for one, have never worried about my inability to do this.

Focusing on decidable problems, then, seems not to place any real restrictions on AI. It turns out, however, that decidable doesn’t mean easy. Computer scientists spend a lot of time thinking about the complexity of problems, that is, the question of how much computation is needed to solve a problem by the most efficient method. Here’s an easy problem: given a list of a thousand numbers, find the biggest number. If it takes one second to check each number, then it takes a thousand seconds to solve this problem by the obvious method of checking each in turn and keeping track of the biggest. Is there a faster method? No, because if a method didn’t check some number in the list, that number might be the biggest, and the method would fail. So, the time to find the largest element is proportional to the size of the list. A computer scientist would say the problem has linear complexity, meaning that it’s very easy; then she would look for something more interesting to work on.

What gets theoretical computer scientists excited is the fact that many problems appear40 to have exponential complexity in the worst case. This means two things: first, all the algorithms we know about require exponential time—that is, an amount of time exponential in the size of the input—to solve at least some problem instances; second, theoretical computer scientists are pretty sure that more efficient algorithms do not exist.

Exponential growth in difficulty means that problems may be solvable in theory (that is, they are certainly decidable) but sometimes unsolvable in practice; we call such problems intractable. An example is the problem of deciding whether a given map can be colored with just three colors, so that no two adjacent regions have the same color.

Does the existence of intractable problems give us any reason to think that computers cannot be as intelligent as humans? No. There is no reason to suppose that humans can solve intractable problems either. Quantum computation helps a bit (whether in machines or brains), but not enough to change the basic conclusion.

Complexity means that the real-world decision problem—the problem of deciding what to do right now, at every instant in ones life—is so difficult that neither humans nor computers will ever come close to finding perfect solutions.

This has two consequences: first, we expect that, most of the time, real-world decisions will be at best halfway decent and certainly far from optimal; second, we expect that a great deal of the mental architecture of humans and computers—the way their decision processes actually operate—will be designed to overcome complexity to the extent possible—that is, to make it possible to find even halfway decent answers despite the overwhelming complexity of the world. Finally, we expect that the first two consequences will remain true no matter how intelligent and powerful some future machine may be. The machine may be far more capable than us, but it will still be far from perfectly rational.

Intelligent Computers

The development of logic by Aristotle and others made available precise rules for rational thought, but we do not know whether Aristotle ever contemplated the possibility of machines that implemented these rules. In the thirteenth century, the influential Catalan philosopher, seducer, and mystic Ramon Llull came much closer: he actually made paper wheels inscribed with symbols, by means of which he could generate logical combinations of assertions. The great seventeenthcentury French mathematician Blaise Pascal was the first to develop a real and practical mechanical calculator. Although it could only add and subtract and was used mainly in his father’s tax-collecting office, it led Pascal to write, “The arithmetical machine produces effects which appear nearer to thought than all the actions of animals.”

Technology took a dramatic leap forward in the nineteenth century when the British mathematician and inventor Charles Babbage designed the Analytical Engine, a programmable universal machine in the sense defined later by Turing. He was helped in his work by Ada, Countess of Lovelace, daughter of the romantic poet and adventurer Lord Byron. Whereas Babbage hoped to use the Analytical Engine to compute accurate mathematical and astronomical tables, Lovelace understood its true potential,41 describing it in 1842 as “a thinking or . . . a reasoning machine” that could reason about “all subjects in the universe.” So, the basic conceptual elements for creating AI were in placel From that point, surely, AI would be just a matter of time. . . .

A long time, unfortunately—the Analytical Engine was never built, and Lovelaces ideas were largely forgotten. With Turing’s theoretical work in 1936 and the subsequent impetus of World War II, universal computing machines were finally realized in the 1940s. Thoughts about creating intelligence followed immediately. Turing’s 1950 paper, “Computing Machinery and Intelligence,”42 is the best known of many early works on the possibility of intelligent machines. Skeptics were already asserting that machines would never be able to do X, for almost any X you could think of, and Turing refuted those assertions. He also proposed an operational test for intelligence, called the imitation game, which subsequently (in simplified form) became known as the Turing test. The test measures the behavior of the machine—specifically, its ability to fool a human interrogator into thinking that it is human.

The imitation game serves a specific role in Turing’s paper—namely as a thought experiment to deflect skeptics who supposed that machines could not think in the right way, for the right reasons, with the right kind of awareness. Turing hoped to redirect the argument towards the issue of whether a machine could behave in a certain way; and if it did—if it was able, say, to discourse sensibly on Shakespeare’s sonnets and their meanings—then skepticism about AI could not really be sustained. Contrary to common interpretations, I doubt that the test was intended as a true definition of intelligence, in the sense that a machine is intelligent if and only if it passes the Turing test. Indeed, Turing wrote, “May not machines carry out something which ought to be described as thinking but which is very different from what a man does?” Another reason not to view the test as a definition for AI is that it’s a terrible definition to work with. And for that reason, mainstream AI researchers have expended almost no effort to pass the Turing test.

The Turing test is not useful for AI because it’s an informal and highly contingent definition: it depends on the enormously complicated and largely unknown characteristics of the human mind, which derive from both biology and culture. There is no way to “unpack” the definition and work back from it to create machines that will provably pass the test. Instead, AI has focused on rational behavior, just as described previously: a machine is intelligent to the extent that what it does is likely to achieve what it wants, given what it has perceived.

Initially, like Aristotle, AI researchers identified “what it wants” with a goal that is either satisfied or not. These goals could be in toy worlds like the 15-puzzle, where the goal is to get all the numbered tiles lined up in order from 1 to 15 in a little (simulated) square tray; or they might be in real, physical environments: in the early 1970s, the Shakey robot at SRI in California was pushing large blocks into desired configurations, and Freddy at the University of Edinburgh was assembling a wooden boat from its component pieces. All this work was done using logical problem-solvers and planning systems to construct and execute guaranteed plans to achieve goals.

By the 1980s, it was clear that logical reasoning alone could not suffice, because, as noted previously, there is no plan that is guaranteed to get you to the airport. Logic requires certainty, and the real world simply doesn’t provide it. Meanwhile, the Israeli-American computer scientist Judea Pearl, who went on to win the 2011 Turing Award, had been working on methods for uncertain reasoning based in probability theory.44 AI researchers gradually accepted Pearl’s ideas; they adopted the tools of probability theory and utility theory and thereby connected AI to other fields such as statistics, control theory, economics, and operations research. This change marked the beginning of what some observers call modern AL

Agents and environments

The central concept of modern AI is the intelligent agent— something that perceives and acts. The agent is a process occurring over time, in the sense that a stream of perceptual inputs is converted into a stream of actions. For example, suppose the agent in question is a self-driving taxi taking me to the airport. Its inputs might include eight RGB cameras operating at thirty frames per second; each frame consists of perhaps 7.5 million pixels, each with an image intensity value in each of three color channels, for a total of more than five gigabytes per second. (The flow of data from the two hundred million photoreceptors in the retina is even larger, which partially explains why vision occupies such a large fraction of the human brain.) The taxi also gets data from an accelerometer one hundred times per second, as well as GPS data. This incredible flood of raw data is transformed by the simply gargantuan computing power of billions of transistors (or neurons) into smooth, competent driving behavior. The taxi’s actions include the electronic signals sent to the steering wheel, brakes, and accelerator, twenty times per second. (For an experienced human driver, most of this maelstrom of activity is unconscious: you may be aware only of making decisions such as “overtake this slow truck” or “stop for gas,” but your eyes, brain, nerves, and muscles are still doing all the other stuff.) For a chess program, the inputs are mostly just the clock ticks, with the occasional notification of the opponent’s move and the new board state, while the actions are mostly doing nothing while the program is thinking, and occasionally choosing a move and notifying the opponent. For a personal digital assistant, or PDA, such as Siri or Cortana, the inputs include not just the acoustic signal from the microphone (sampled forty-eight thousand times per second) and input from the touch screen but also the content of each Web page that it accesses, while the actions include both speaking and displaying material on the screen.

The way we build intelligent agents depends on the nature of the problem we face. This, in turn, depends on three things: first, the nature of the environment the agent will operate in—a chessboard is a very different place from a crowded freeway or a mobile phone; second, the observations and actions that connect the agent to the environment—for example, Siri might or might not have access to the phone’s camera so that it can see; and third, the agent’s objective— teaching the opponent to play better chess is a very different task from winning the game.

To give just one example of how the design of the agent depends on these things: If the objective is to win the game, a chess program need consider only the current board state and does not need any memory of past events.45 The chess tutor, on the other hand, should continually update its model of which aspects of chess the pupil does or does not understand so that it can provide useful advice. In other words, for the chess tutor, the pupil’s mind is a relevant part of the environment. Moreover, unlike the board, it is a part of the environment that is not directly observable.

3: HOW MIGHT AI PROGRESS IN THE FUTURE?

The AI ecosystem

In the beginning, the environment in which most computers operated was essentially formless and void: their only input came from punched cards and their only method of output was to print characters on a line printer. Perhaps for this reason, most researchers viewed intelligent machines as question-answerers; the view of machines as agents perceiving and acting in an environment did not become widespread until the 1980s.

The advent of the World Wide Web in the 1990s opened up a whole new universe for intelligent machines to play in. A new word, softbot, was coined to describe software “robots” that operate entirely in a software environment such as the Web. Softbots, or bots as they later became known, perceive Web pages and act by emitting sequences of characters, URLs, and so on.

AI companies mushroomed during the dot-com boom (19972000), providing core capabilities for search and e-commerce, including link analysis, recommendation systems, reputation systems, comparison shopping, and product categorization.

In the early 2000s, the widespread adoption of mobile phones with microphones, cameras, accelerometers, and GPS provided new access for AI systems to people’s daily lives; “smart speakers” such as the Amazon Echo, Google Home, and Apple HomePod have completed this process.

By around 2008, the number of objects connected to the Internet exceeded the number of people connected to the Internet—a transition that some point to as the beginning of the Internet of Things (IoT). Those things include cars, home appliances, traffic lights, vending machines, thermostats, quadcopters, cameras, environmental sensors, robots, and all kinds of material goods both in the manufacturing process and in the distribution and retail system. This provides AI systems with far greater sensory and control access to the real world.

Finally, improvements in perception have allowed Al-powered robots to move out of the factory, where they relied on rigidly constrained arrangements of objects, and into the real, unstructured, messy world, where their cameras have something interesting to look at.

Intelligence on a global scale

The development of basic capabilities for understanding speech and text will allow intelligent personal assistants to do things that human assistants can already do (but they will be doing it for pennies per month instead of thousands of dollars per month). Basic speech and text understanding also enable machines to do things that no human can do—not because of the depth of understanding but because of its scale. For example, a machine with basic reading capabilities will be able to read everything the human race has ever written by lunchtime, and then it will be looking around for something else to do.22 With speech recognition capabilities, it could listen to every radio and television broadcast before teatime. For comparison, it would take two hundred thousand full-time humans just to keep up with the world’s current level of print publication (let alone all the written material from the past) and another sixty thousand to listen to current broadcasts.

Such a system, if it could extract even simple factual assertions and integrate all this information across all languages, would represent an incredible resource for answering questions and revealing patterns— probably far more powerful than search engines, which are currently valued at around $1 trillion. Its research value for fields such as history and sociology would be inestimable.

Of course, it would also be possible to listen to all the world s phone calls (a job that would require about twenty million people). There are certain clandestine agencies that would find this valuable. Some of them have been doing simple kinds of large-scale machine listening, such as spotting key words in conversations, for many years, and have now made the transition to transcribing entire conversations into searchable text.24 Transcriptions are certainly useful, but not nearly as useful as simultaneous understanding and content integration of all conversations.

Another “superpower” that is available to machines is to see the entire world at once. Roughly speaking, satellites image the entire world every day at an average resolution of around fifty centimeters per pixel. At this resolution, every house, ship, car, cow, and tree on Earth is visible. Well over thirty million full-time employees would be needed to examine all these images;25 so, at present, no human ever sees the vast majority of satellite data. Computer vision algorithms could process all this data to produce a searchable database of the whole world, updated daily, as well as visualizations and predictive models of economic activities, changes in vegetation, migrations of animals and people, the effects of climate change, and so on. Satellite companies such as Planet and DigitalGlobe are busy making this idea a reality.

With the possibility of sensing on a global scale comes the possibility of decision making on a global scale. For example, from global satellite data feeds, it should be possible to create detailed models for managing the global environment, predicting the effects of environmental and economic interventions, and providing the necessary analytical inputs to the UN’s sustainable development goals.26 We are already seeing “smart city” control systems that aim to optimize traffic management, transit, trash collection, road repairs, environmental maintenance, and other functions for the benefit of citizens, and these may be extended to the country level. Until recently, this degree of coordination could be achieved only by huge, inefficient, bureaucratic hierarchies of humans; inevitably, these will be replaced by megaagents that take care of more and more aspects of our collective lives. Along with this, of course, comes the possibility of privacy invasion and social control on a global scale, to which I return in the next chapter.

When Will Superintelligent AI Arrive?

I am often asked to predict when superintelligent AI will arrive, and I usually refuse to answer. There are three reasons for this. First, there is a long history of such predictions going wrong.27 For example, in 1960, the AI pioneer and Nobel Prize-winning economist Herbert Simon wrote, “Technologically . . . machines will be capable, within twenty years, of doing any work a man can do.”28 In 1967, Marvin Minsky, a co-organizer of the 1956 Dartmouth workshop that started the field of AI, wrote, “Within a generation, I am convinced, few compartments of intellect will remain outside the machine's realm—the problem of creating artificial intelligence' will be substantially solved.”

A second reason for declining to provide a date for superintelligent AI is that there is no clear threshold that will be crossed. Machines already exceed human capabilities in some areas. Those areas will broaden and deepen, and it is likely that there will be superhuman general knowledge systems, superhuman biomedical research systems, superhuman dexterous and agile robots, superhuman corporate planning systems, and so on well before we have a completely general superintelligent AI system. These “partially superintelligent” systems will, individually and collectively, begin to pose many of the same issues that a generally intelligent system would.

A third reason for not predicting the arrival of superintelligent AI is that it is inherently unpredictable. It requires “conceptual breakthroughs,” as noted by John McCarthy in a 1977 interview.30 McCarthy went on to say, “What you want is 1.7 Einsteins and 0.3 of the Manhattan Project, and you want the Einsteins first. I believe it’ll take five to 500 years.” In the next section I’ll explain what some of the conceptual breakthroughs are likely to be. Just how unpredictable are they? Probably as unpredictable as Szilard’s invention of the nuclear chain reaction a few hours after Rutherford’s declaration that it was completely impossible.

Once, at a meeting of the World Economic Forum in 2015, I answered the question of when we might see superintelligent AI. The meeting was under Chatham House rules, which means that no remarks may be attributed to anyone present at the meeting. Even so, out of an excess of caution, I prefaced my answer with “Strictly off the record. . . . ” I suggested that, barring intervening catastrophes, it would probably happen in the lifetime of my children—who were still quite young and would probably have much longer lives, thanks to advances in medical science, than many of those at the meeting. Less than two hours later, an article appeared in the Daily Telegraph citing Professor Russell’s remarks, complete with images of rampaging Terminator robots. The headline was ‘SOCIOPATHIC' ROBOTS COULD OVERRUN THE HUMAN RACE WITHIN A GENERATION.

My timeline of, say, eighty years is considerably more conservative than that of the typical AI researcher. Recent surveys31 suggest that most active researchers expect human-level AI to arrive around the middle of this century. Our experience with nuclear physics suggests that it would be prudent to assume that progress could occur quite quickly and to prepare accordingly. If just one conceptual breakthrough were needed, analogous to Szilard’s idea for a neutron-induced nuclear chain reaction, superintelligent AI in some form could arrive quite suddenly. The chances are that we would be unprepared: if we built superintelligent machines with any degree of autonomy, we would soon find ourselves unable to control them. I am, however, fairly confident that we have some breathing space because there are several major breakthroughs needed between here and superintelligence, not just one.

Conceptual Breakthroughs to Come

The problem of creating general-purpose, human-level AI is far from solved. Solving it is not a matter of spending money on more engineers, more data, and bigger computers. Some futurists produce charts that extrapolate the exponential growth of computing power into the future based on Moore's law, showing the dates when machines will become more powerful than insect brains, mouse brains, human brains, all human brains put together, and so on.32 These charts are meaningless because, as I have already said, faster machines just give you the wrong answer more quickly. If one were to collect AI's leading experts into a single team with unlimited resources, with the goal of creating an integrated, human-level intelligent system by combining all our best ideas, the result would be failure. The system would break in the real world. It wouldn’t understand what was going on; it wouldn't be able to predict the consequences of its actions; it wouldn’t understand what people want in any given situation; and so it would do ridiculously stupid things.

By understanding how the system would break, AI researchers are able to identify the problems that have to be solved—the conceptual breakthroughs that are needed—in order to reach human-level AI. I will now describe some of these remaining problems. Once they are solved, there may be more, but not very many more.

Imagining a Superintelligent Machine

The technical community has suffered from a failure of imagination when discussing the nature and impact of superintelligent AI. Often, we see discussions of reduced medical errors,48 safer cars,49 or other advances of an incremental nature. Robots are imagined as individual entities carrying their brains with them, whereas in fact they are likely to be wirelessly connected into a single, global entity that draws on vast stationary computing resources. It’s as if researchers are afraid of examining the real consequences of success in AI.

A general-purpose intelligent system can, by assumption, do what any human can do. For example, some humans did a lot of mathematics, algorithm design, coding, and empirical research to come up with the modern search engine. The results of all this work are very useful and of course very valuable. How valuable? A recent study showed that the median American adult surveyed would need to be paid at least $17,500 to give up using search engines for a year,50 which translates to a global value in the tens of trillions of dollars.

Now imagine that search engines don’t exist yet because the necessary decades of work have not been done, but you have access instead to a superintelligent AI system. Simply by asking the question, you now have access to search engine technology, courtesy of the AI system. Done! Trillions of dollars in value, just for the asking, and not a single line of additional code written by you. The same goes for any other missing invention or series of inventions: if humans could do it, so can the machine.

This last point provides a useful lower bound—a pessimistic estimate—on what a superintelligent machine can do. By assumption, the machine is more capable than an individual human. There are many things an individual human cannot do, but a collection of n humans can do: put an astronaut on the Moon, create a gravitationalwave detector, sequence the human genome, run a country with hundreds of millions of people. So, roughly speaking, we create n software copies of the machine and connect them in the same way—with the same information and control flows—as the n humans. Now we have a machine that can do whatever n humans can do, except better, because each of its n components is superhuman.

This multi-agent cooperation design for an intelligent system is just a lower bound on the possible capabilities of machines because there are other designs that work better. In a collection of n humans, the total available information is kept separately in n brains and communicated very slowly and imperfectly between them. That’s why the n humans spend most of their time in meetings. In the machine, there is no need for this separation, which often prevents connecting the dots. For an example of disconnected dots in scientific discovery, a brief perusal of the long history of penicillin is quite eye-opening.

Another useful method of stretching your imagination is to think about some particular form of sensory input—say, reading—and scale it up. Whereas a human can read and understand one book in a week, a machine could read and understand every book ever written—all 150 million of them—in a few hours. This requires a decent amount of processing power, but the books can be read largely in parallel, meaning that simply adding more chips allows the machine to scale up its reading process. By the same token, the machine can see everything at once through satellites, robots, and hundreds of millions of surveillance cameras; watch all the world’s TV broadcasts; and listen to all the world’s radio stations and phone conversations. Very quickly it would gain a far more detailed and accurate understanding of the world and its inhabitants than any human could possibly hope to acquire.

One can also imagine scaling the machine’s capacity for action. A human has direct control over only one body, while a machine can control thousands or millions. Some automated factories already exhibit this characteristic. Outside the factory, a machine that controls thousands of dexterous robots can, for example, produce vast numbers of houses, each one tailored to its future occupants’ needs and desires. In the lab, existing robotic systems for scientific research could be scaled up to perform millions of experiments simultaneously—perhaps to create complete predictive models of human biology down to the molecular level. Note that the machine’s reasoning capabilities will give it a far greater capacity to detect inconsistencies between scientific theories and between theories and observations. Indeed, it may already be the case that we have enough experimental evidence about biology to devise a cure for cancer: we just haven’t put it together.

In the cyber realm, machines already have access to billions of effectors— namely, the displays on all the phones and computers in the world. This partly explains the ability of IT companies to generate enormous wealth with very few employees; it also points to the severe vulnerability of the human race to manipulation via screens.

Scale of a different kind comes from the machine’s ability to look further into the future, with greater accuracy, than is possible for humans. We have seen this for chess and Go already; with the capacity for generating and analyzing hierarchical plans over long time scales and the ability to identify new abstract actions and high-level descriptive models, machines will transfer this advantage to domains such as mathematics (proving novel, useful theorems) and decision making in the real world. Tasks such as evacuating a large city in the event of an environmental disaster will be relatively straightforward, with the machine able to generate individual guidance for every person and vehicle to minimize the number of casualties.

The machine might work up a slight sweat when devising policy recommendations to prevent global warming. Earth systems modeling requires knowledge of physics (atmosphere, oceans), chemistry (carbon cycle, soils), biology (decomposition, migration), engineering (renewable energy, carbon capture), economics (industry, energy use), human nature (stupidity, greed), and politics (even more stupidity, even more greed). As noted, the machine will have access to vast quantities of evidence to feed all these models. It will be able to suggest or carry out new experiments and expeditions to narrow down the inevitable uncertainties—for example, to discover the true extent of gas hydrates in shallow ocean reservoirs. It will be able to consider a vast range of possible policy recommendations—laws, nudges, markets, inventions, and geoengineering interventions—but of course it will also need to find ways to persuade us to go along with them.

The Limits of Superintelligence

While stretching your imagination, don’t stretch it too far. A common mistake is to attribute godlike powers of omniscience to superintelligent AI systems—complete and perfect knowledge not just of the present but also of the future.52 This is quite implausible because it requires an unphysical ability to determine the exact current state of the world as well as an unrealizable ability to simulate, much faster than real time, the operation of a world that includes the machine itself (not to mention billions of brains, which would still be the secondmost- complex objects in the universe).

This is not to say that it is impossible to predict some aspects of the future with a reasonable degree of certainty—for example, I know what class I’ll be teaching in what room at Berkeley almost a year from now, despite the protestations of chaos theorists about butterfly wings and all that. (Nor do I think that humans are anywhere close to predicting the future as well as the laws of physics allow!) Prediction depends on having the right abstractions—for example, I can predict that “I” will be “on stage in Wheeler Auditorium" on the Berkeley campus on the last Tuesday in April, but I cannot predict my exact location down to the millimeter or which atoms of carbon will have been incorporated into my body by then.

Machines are also subject to certain speed limits imposed by the real world on the rate at which new knowledge of the world can be acquired—one of the valid points made by Kevin Kelly in his article on oversimplified predictions about superhuman AI.53 For example, to determine whether a specific drug cures a certain kind of cancer in an experimental animal, a scientist—human or machine— has two choices: inject the animal with the drug and wait several weeks or run a sufficiently accurate simulation. To run a simulation, however, requires a great deal of empirical knowledge of biology, some of which is currently unavailable; so, more model-building experiments would have to be done first. Undoubtedly, these would take time and must be done in the real world.

On the other hand, a machine scientist could run vast numbers of model-building experiments in parallel, could integrate their outcomes into an internally consistent (albeit very complex) model, and could compare the model’s predictions with the entirety of experimental evidence known to biology. Moreover, simulating the model does not necessarily require a quantum-mechanical simulation of the entire organism down to the level of individual molecular reactions— which, as Kelly points out, would take more time than simply doing the experiment in the real world. Just as I can predict my future location on Tuesdays in April with some certainty, properties of biological systems can be predicted accurately with abstract models. (Among other reasons, this is because biology operates with robust control systems based on aggregate feedback loops, so that small variations in initial conditions usually don’t lead to large variations in outcomes.) Thus, while instantaneous machine discoveries in the empirical sciences are unlikely, we can expect that science will proceed much faster with the help of machines. Indeed, it already is.

A final limitation of machines is that they are not human. This puts them at an intrinsic disadvantage when trying to model and predict one particular class of objects: humans. Our brains are all quite similar, so we can use them to simulate—to experience, if you will—the mental and emotional lives of others. This, for us, comes for free. (If you think about it, machines have an even greater advantage with each other: they can actually run each other’s code!) For example, I don’t need to be an expert on neural sensory systems to know what it feels like when you hit your thumb with a hammer. I can just hit my thumb with a hammer. Machines, on the other hand, have to start almost54 from scratch in their understanding of humans: they have access only to our external behavior, plus all the neuroscience and psychology literature, and have to develop an understanding of how we work on that basis. In principle, they will be able to do this, but it’s reasonable to suppose that acquiring a human-level or superhuman understanding of humans will take them longer than most other capabilities.

How Will AI Benefit Humans?

Our intelligence is responsible for our civilization. With access to greater intelligence we could have a greater—and perhaps far better— civilization. One can speculate about solving major open problems such as extending human life indefinitely or developing faster-thanlight travel, but these staples of science fiction are not yet the driving force for progress in AI. (With superintelligent AI, we’ll probably be able to invent all sorts of quasi-magical technologies, but it’s hard to say now what those might be.) Consider, instead, a far more prosaic goal: raising the living standard of everyone on Earth, in a sustainable way, to a level that would be viewed as quite respectable in a developed country. Choosing (somewhat arbitrarily) respectable to mean the eighty-eighth percentile in the United States, the stated goal represents almost a tenfold increase in global gross domestic product (GDP), from $76 trillion to $750 trillion per year.

To calculate the cash value of such a prize, economists use the net present value of the income stream, which takes into account the discounting of future income relative to the present. The extra income of $674 trillion per year has a net present value of roughly $13,500 trillion, 56 assuming a discount factor of 5 percent. So, in very crude terms, this is a ballpark figure for what human-level AI might be worth if it can deliver a respectable living standard for everyone. With numbers like this, it’s not surprising that companies and countries are investing tens of billions of dollars annually in AI research and development.57 Even so, the sums invested are minuscule compared to the size of the prize.

Of course, these are all made-up numbers unless one has some idea of how human-level AI could achieve the feat of raising living standards. It can do this only by increasing the per-capita production of goods and services. Put another way: the average human can never expect to consume more than the average human produces. The example of self-driving taxis discussed earlier in the chapter illustrates the multiplier effect of AI: with an automated service, it should be possible for (say) ten people to manage a fleet of one thousand vehicles, so each person is producing one hundred times as much transportation as before. The same goes for manufacturing the cars and for extracting the raw materials from which the cars are made. Indeed, some iron-ore mining operations in northern Australia, where temperatures regularly exceed 45 degrees Celsius (113 degrees Fahrenheit), are almost completely automated already.

These present-day applications of AI are special-purpose systems: self-driving cars and self-operating mines have required huge investments in research, mechanical design, software engineering, and testing to develop the necessary algorithms and to make sure that they work as intended. That's just how things are done in all spheres of engineering. That's how things used to be done in personal travel too: if you wanted to travel from Europe to Australia and back in the seventeenth century, it would have involved a huge project costing vast sums of money, requiring years of planning, and carrying a high risk of death. Now we are used to the idea of transportation as a service (TaaS): if you need to be in Melbourne early next week, it just requires a few taps on your phone and a relatively minuscule amount of money.

General-purpose AI would be everything as a service (EaaS). There would be no need to employ armies of specialists in different disciplines, organized into hierarchies of contractors and subcontractors, in order to carry out a project. All embodiments of general-purpose AI would have access to all the knowledge and skills of the human race, and more besides. The only differentiation would be in the physical capabilities: dexterous legged robots for construction or surgery, wheeled robots for large-scale goods transportation, quadcopter robots for aerial inspections, and so on. In principle—politics and economics aside—everyone could have at their disposal an entire organization composed of software agents and physical robots, capable of designing and building bridges, improving crop yields, cooking dinner for a hundred guests, running elections, or doing whatever else needs doing. It’s the generality of general-purpose intelligence that makes this possible.

History has shown, of course, that a tenfold increase in global GDP per capita is possible without AI—it’s just that it took 190 years (from 1820 to 2010) to achieve that increase.59 It required the development of factories, machine tools, automation, railways, steel, cars, airplanes, electricity, oil and gas production, telephones, radio, television, computers, the Internet, satellites, and many other revolutionary inventions. The tenfold increase in GDP posited in the preceding paragraphs is predicated not on further revolutionary technologies but on the ability of AI systems to employ what we already have more effectively and at greater scale.

Of course, there will be effects besides the purely material benefit of raising living standards. For example, personal tutoring is known to be far more effective than classroom teaching, but when done by humans it is simply unaffordable—and always will be—for the vast majority of people. With AI tutors, the potential of each child, no matter how poor, can be realized. The cost per child would be negligible, and that child would live a far richer and more productive life. The pursuit of artistic and intellectual endeavors, whether individually or collectively, would be a normal part of life rather than a rarefied luxury.

In the area of health, AI systems should enable researchers to unravel and master the vast complexities of human biology and thereby gradually banish disease. Greater insights into human psychology and neurochemistry should lead to broad improvements in mental health.

Perhaps more unconventionally, AI could enable far more effective authoring tools for virtual reality (VR) and could populate VR environments with far more interesting entities. This might turn VR into the medium of choice for literary and artistic expression, creating experiences of a richness and depth that is currently unimaginable.

And in the mundane world of daily life, an intelligent assistant and guide would—if well designed and not co-opted by economic and political interests—empower every individual to act effectively on their own behalf in an increasingly complex and sometimes hostile economic and political system. You would, in effect, have a high-powered lawyer, accountant, and political adviser on call at any time. Just as traffic jams are expected to be smoothed out by intermixing even a small percentage of autonomous vehicles, one can only hope that wiser policies and fewer conflicts will emerge from a better-informed and better-advised global citizenry.

These developments taken together could change the dynamic of history—at least that part of history that has been driven by conflicts within and between societies for access to the wherewithal of life. If the pie is essentially infinite, then fighting others for a larger share makes little sense. It would be like fighting over who gets the most digital copies of the newspaper—completely pointless when anyone can make as many digital copies as they want for free.

There are some limits to what AI can provide. The pies of land and raw materials are not infinite, so there cannot be unlimited population growth and not everyone will have a mansion in a private park. (This will eventually necessitate mining elsewhere in the solar system and constructing artificial habitats in space; but I promised not to talk about science fiction.) The pie of pride is also finite: only 1 percent of people can be in the top 1 percent on any given metric. If human happiness requires being in the top 1 percent, then 99 percent of humans are going to be unhappy, even when the bottom 1 percent has an objectively splendid lifestyle.60 It will be important, then, for our cultures to gradually down-weight pride and envy as central elements of perceived self-worth.

As Nick Bostrom puts it at the end of his book Superintelligence, success in AI will yield “a civilizational trajectory that leads to a compassionate and jubilant use of humanity’s cosmic endowment.” If we fail to take advantage of what AI has to offer, we will have only ourselves to blame.

4: Misuses of AI

A compassionate and jubilant use of humanity’s cosmic endowment sounds wonderful, but we also have to reckon with d:he rapid rate of innovation in the malfeasance sector. Ill intentioned people are thinking up new ways to misuse AI so quickly that this chapter is likely to be outdated even before it attains printed form. Think of it not as depressing reading, however, but as a call to act before it is too late.

Surveillance, Persuasion, and Control

The automated Stasi

The Ministerium fur Staatsicherheit of East Germany, more commonly known as the Stasi, is widely regarded as “one of the most effective and repressive intelligence and secret police agencies to have ever existed.”1 It maintained files on the great majority of East German households. It monitored phone calls, read letters, and planted hidden cameras in apartments and hotels. It was ruthlessly effective at identifying and eliminating dissident activity. Its preferred modus operandi was psychological destruction rather than imprisonment or execution. This level of control came at great cost, however: by some estimates, more than a quarter of working-age adults were Stasi informants. Stasi paper records have been estimated at twenty billion pages2 and the task of processing and acting on the huge incoming flows of information began to exceed the capacity of any human organization.

It should come as no surprise, then, that intelligence agencies have spotted the potential for using AI in their work. For many years, they have been applying simple forms of AI technology, including voice recognition and identification of key words and phrases in both speech and text. Increasingly, AI systems are able to understand the content of what people are saying and doing, whether in speech, text, or video surveillance. In regimes where this technology is adopted for the purposes of control, it will be as if every citizen had their own personal Stasi operative watching over them twenty-four hours a day.

Even in the civilian sphere, in relatively free countries, we are subject to increasingly effective surveillance. Corporations collect and sell information about our purchases, Internet and social network usage, electrical appliance usage, calling and texting records, employment, and health. Our locations can be tracked through our cell phones and our Internet-connected cars. Cameras recognize our faces on the street. All this data, and much more, can be pieced together by intelligent information integration systems to produce a fairly complete picture of what each of us is doing, how we live our lives, who we like and dislike, and how we will vote.4 The Stasi will look like amateurs by comparison.

Controlling your behavior

Once surveillance capabilities are in place, the next step is to modify your behavior to suit those who are deploying this technology. One rather crude method is automated, personalized blackmail: a system that understands what you are doing—whether by listening, reading, or watching you—can easily spot things you should not be doing. Once it finds something, it will enter into correspondence with you to extract the largest possible amount of money (or to coerce behavior, if the goal is political control or espionage). The extraction of money works as the perfect reward signal for a reinforcement learning algorithm, so we can expect AI systems to improve rapidly in their ability to identify and profit from misbehavior. Early in 2015, I suggested to a computer security expert that automated blackmail systems, driven by reinforcement learning, might soon become feasible; he laughed and said it was already happening. The first blackmail bot to be widely publicized was Delilah, identified in July 2016.

A more subtle way to change people’s behavior is to modify their information environment so that they believe different things and make different decisions. Of course, advertisers have been doing this for centuries as a way of modifying the purchasing behavior of individuals. Propaganda as a tool of war and political domination has an even longer history.

So what’s different now? First, because AI systems can track an individual’s online reading habits, preferences, and likely state of knowledge, they can tailor specific messages to maximize impact on that individual while minimizing the risk that the information will be disbelieved. Second, the AI system knows whether the individual reads the message, how long they spend reading it, and whether they follow additional links within the message. It then uses these signals as immediate feedback on the success or failure of its attempt to influence each individual; in this way, it quickly learns to become more effective in its work. This is how content selection algorithms on social media have had their insidious effect on political opinions.

Another recent change is that the combination of AI, computer graphics, and speech synthesis is making it possible to generate deepfakes—realistic video and audio content of just about anyone saying or doing just about anything. The technology will require little more than a verbal description of the desired event, making it usable by more or less anyone in the world. Cell phone video of Senator X accepting a bribe from cocaine dealer Y at shady establishment Z? No problem! This kind of content can induce unshakeable beliefs in things that never happened.6 In addition, AI systems can generate millions of false identities—the so-called bot armies—that can pump out billions of comments, tweets, and recommendations daily, swamping the efforts of mere humans to exchange truthful information. Online marketplaces such as eBay, Taobao, and Amazon that rely on reputation systems7 to build trust between buyers and sellers are constantly at war with bot armies designed to corrupt the markets.

Finally, methods of control can be direct if a government is able to implement rewards and punishments based on behavior. Such a system treats people as reinforcement learning algorithms, training them to optimize the objective set by the state. The temptation for a government, particularly one with a top-down, engineering mind-set, is to reason as follows: it would be better if everyone behaved well, had a patriotic attitude, and contributed to the progress of the country; technology enables measurement of individual behavior, attitudes, and contributions; therefore, everyone will be better off if we set up a technology-based system of monitoring and control based on rewards and punishments.

There are several problems with this line of thinking. First, it ignores the psychic cost of living under a system of intrusive monitoring and coercion; outward harmony masking inner misery is hardly an ideal state. Every act of kindness ceases to be an act of kindness and becomes instead an act of personal score maximization and is perceived as such by the recipient. Or worse, the very concept of a voluntary act of kindness gradually becomes just a fading memory of something people used to do. Visiting an ailing friend in hospital will, under such a system, have no more moral significance and emotional value than stopping at a red light. Second, the scheme falls victim to the same failure mode as the standard model of AI, in that it assumes that the stated objective is in fact the true, underlying objective. Inevitably, Goodhart’s law will take over, whereby individuals optimize the official measure of outward behavior, just as universities have learned to optimize the ‘objective” measures of “quality” used by university ranking systems instead of improving their real (but unmeasured) quality.8 Finally, the imposition of a uniform measure of behavioral virtue misses the point that a successful society may comprise a wide variety of individuals, each contributing in their own way.

A right to mental security

One of the great achievements of civilization has been the gradual improvement in physical security for humans. Most of us can expect to conduct our daily lives without constant fear of injury and death. Article 3 of the 1948 Universal Declaration of Human Rights states, “Everyone has the right to life, liberty and security of person.”

I would like to suggest that everyone should also have the right to mental security—the right to live in a largely true information environment. Humans tend to believe the evidence of our eyes and ears. We trust our family, friends, teachers, and (some) media sources to tell us what they believe to be the truth. Even though we do not expect used-car salespersons and politicians to tell us the truth, we have trouble believing that they are lying as brazenly as they sometimes do. We are, therefore, extremely vulnerable to the technology of misinformation.

The right to mental security does not appear to be enshrined in the Universal Declaration. Articles 18 and 19 establish the rights of “freedom of thought” and “freedom of opinion and expression.” Ones thoughts and opinions are, of course, partly formed by one’s information environment, which, in turn, is subject to Article 19’s “right to . . . impart information and ideas through any media and regardless of frontiers.” That is, anyone, anywhere in the world, has the right to impart false information to you. And therein lies the difficulty: democratic nations, particularly the United States, have for the most part been reluctant—or constitutionally unable—to prevent the imparting of false information on matters of public concern because of justifiable fears regarding government control of speech. Rather than pursuing the idea that there is no freedom of thought without access to true information, democracies seem to have placed a naive trust in the idea that the truth will win out in the end, and this trust has left us unprotected. Germany is an exception; it recently passed the Network Enforcement Act, which requires content platforms to remove proscribed hate speech and fake news, but this has come under considerable criticism as being unworkable and undemocratic.

For the time being, then, we can expect our mental security to remain under attack, protected mainly by commercial and volunteer efforts. These efforts include fact-checking sites such as factcheck.org and snopes.com—but of course other “fact-checking” sites are springing up to declare truth as lies and lies as truth.

The major information utilities such as Google and Facebook have come under extreme pressure in Europe and the United States to “do something about it.” They are experimenting with ways to flag or relegate false content—using both AI and human screeners—and to direct users to verified sources that counteract the effects of misinformation. Ultimately, all such efforts rely on circular reputation systems, in the sense that sources are trusted because trusted sources report them to be trustworthy. If enough false information is propagated, these reputation systems can fail: sources that are actually trustworthy can become untrusted and vice versa, as appears to be occurring today with major media sources such as CNN and Fox News in the United States. Aviv Ovadya, a technologist working against misinformation, has called this the “infopocalypse—a catastrophic failure of the marketplace of ideas.”

One way to protect the functioning of reputation systems is to inject sources that are as close as possible to ground truth. A single fact that is certainly true can invalidate any number of sources that are only somewhat trustworthy, if those sources disseminate information contrary to the known fact. In many countries, notaries function as sources of ground truth to maintain the integrity of legal and realestate information; they are usually disinterested third parties in any transaction and are licensed by governments or professional societies. (In the City of London, the Worshipful Company of Scriveners has been doing this since 1373, suggesting that a certain stability inheres in the role of truth telling.) If formal standards, professional qualifications, and licensing procedures emerge for fact-checkers, that would tend to preserve the validity of the information flows on which we depend. Organizations such as the W3C Credible Web group and the Credibility Coalition aim to develop technological and crowdsourcing methods for evaluating information providers, which would then allow users to filter out unreliable sources.

A second way to protect reputation systems is to impose a cost for purveying false information. Thus, some hotel rating sites -accept reviews concerning a particular hotel only from those who have booked and paid for a room at that hotel through the site, while other rating sites accept reviews from anyone. It will come as no surprise that ratings at the former sites are far less biased, because they impose a cost (paying for an unnecessary hotel room) for fraudulent reviews.11 Regulatory penalties are more controversial: no one wants a Ministry of Truth, and Germany’s Network Enforcement Act penalizes only the content platform, not the person posting the fake news. On the other hand, just as many nations and many US states make it illegal to record telephone calls without permission, it ought, at least, to be possible to impose penalties for creating fictitious audio and video recordings of real people.

Finally, there are two other facts that work in our favor. First, almost no one actively wants, knowingly, to be lied to. (This is not to say that parents always inquire vigorously into the truthfulness of those who praise their children’s intelligence and charm; it’s just that they are less likely to seek such approval from someone who is known to lie at every opportunity.) This means that people of all political persuasions have an incentive to adopt tools that help them distinguish truth from lies. Second, no one wants to be known as a liar, least of all news outlets. This means that information providers—at least those for who reputation matters—have an incentive to join industry associations and subscribe to codes of conduct that favor truth telling. In turn, social media platforms can offer users the option of seeing content from only reputable sources that subscribe to these codes and subject themselves to third-party fact-checking.

Lethal Autonomous Weapons

The United Nations defines lethal autonomous weapons systems (AWS for short, because LAWS is quite confusing) as weapons systems that “locate, select, and eliminate human targets without human intervention.” AWS have been described, with good reason, as the “third revolution in warfare,” after gunpowder and nuclear weapons.

You may have read articles in the media about AWS; usually the article will call them killer robots and will be festooned with images from the Terminator movies. This is misleading in at least two ways: first, it suggests that autonomous weapons are a threat because they might take over the world and destroy the human race; second, it suggests that autonomous weapons will be humanoid, conscious, and evil.

The net effect of the media’s portrayal of the issue has been to make it seem like science fiction. Even the German government has been taken in: it recently issued a statement12 asserting that “having the ability to learn and develop self-awareness constitutes an indispensable attribute to be used to define individual functions or weapon systems as autonomous.” (This makes as much sense as asserting that a missile isn’t a missile unless it goes faster than the speed of light.) In fact, autonomous weapons will have the same degree of autonomy as a chess program, which is given the mission of winning the game but decides by itself where to move its pieces and which enemy pieces to eliminate.

AWS are not science fiction. They already exist. Probably the clearest example is Israels Harop (figure 7, left), a loitering munition with a ten-foot wingspan and a fifty-pound warhead. It searches for up to six hours in a given geographical region for any target that meets a given criterion and then destroys it. The criterion could be “emits a radar signal resembling antiaircraft radar” or “looks like a tank.”

By combining recent advances in miniature quadrotor design, miniature cameras, computer vision chips, navigation and mapping algorithms, and methods for detecting and tracking humans, it would be possible in fairly short order to field an antipersonnel weapon like the Slaughterbot13 shown in figure 7 (right). Such a weapon could be tasked with attacking anyone meeting certain visual criteria (age, gender, uniform, skin color, and so on) or even specific individuals based on face recognition. I’m told that the Swiss Defense Department has already built and tested a real Slaughterbot and found that, as expected, the technology is both feasible and lethal.

Since 2014, diplomatic discussions have been underway in Geneva that may lead to a treaty banning AWS. At the same time, some of the major participants in these discussions (the United States, China, Russia, and to some extent Israel and the UK) are engaged in a dangerous competition to develop autonomous weapons. In the United States, for example, the CODE (Collaborative Operations in Denied Environments) program aims to move towards autonomy by enabling drones to function with at best intermittent radio contact. The drones will “hunt in packs, like wolves” according to the program manager.14 In 2016, the US Air Force demonstrated the in-flight deployment of 103 Perdix micro-drones from three F/A-18 fighters. According to the announcement, “Perdix are not pre-programmed synchronized individuals, they are a collective organism, sharing one distributed brain for decisionmaking and adapting to each other like swarms in nature.”

You may think it’s pretty obvious that building machines that can decide to kill humans is a bad idea. But “pretty obvious” is not always persuasive to governments—including some of those listed in the preceding paragraph—who are bent on achieving what they think of as strategic superiority. A more convincing reason to reject autonomous weapons is that they are scalable weapons of mass destruction.

Scalable is a term from computer science; a process is scalable if you can do a million times more of it essentially by buying a million times more hardware. Thus, Google handles roughly five billion search requests per day by having not millions of employees but millions of computers. With autonomous weapons, you can do a million times more killing by buying a million times more weapons, precisely because the weapons are autonomous. Unlike remotely piloted drones or AK-47s, they don’t need individual human supervision to do their work.

As weapons of mass destruction, scalable autonomous weapons have advantages for the attacker compared to nuclear weapons and carpet bombing: they leave property intact and can be applied selectively to eliminate only those who might threaten an occupying force. They could certainly be used to wipe out an entire ethnic group or all the adherents of a particular religion (if adherents have visible indicia). Moreover, whereas the use of nuclear weapons represents a cataclysmic threshold that we have (often by sheer luck) avoided crossing since 1945; there is no such threshold with scalable autonomous weapons. Attacks could escalate smoothly from one hundred casualties to one thousand to ten thousand to one hundred thousand. In addition to actual attacks, the mere threat of attacks by such weapons makes them an effective tool for terror and oppression. Autonomous weapons will greatly reduce human security at all levels: personal, local, national, and international.

This is not to say that autonomous weapons will be the end of the world in the way envisaged in the Terminator movies. They need not be especially intelligent—a self-driving car probably needs to be smarter—and their missions will not be of the “take over the world” variety. The existential risk from AI does not come primarily from simple-minded killer robots. On the other hand, superintelligent machines in conflict with humanity could certainly arm themselves this way, by turning relatively stupid killer robots into physical extensions of a global control system.

Eliminating Work as We Know It

Thousands of media articles and opinion pieces and several books have been written on the topic of robots taking jobs from humans. Research centers are springing up all over the world to understand what is likely to happen.16 The titles of Martin Ford's Rise of the Robots: Technology and the Threat of a Jobless Future17 and Calum Chaces The Economic Singularity: Artificial Intelligence and the Death of Capitalism18 do a pretty good job of summarizing the concern. Although, as will soon become evident, I am by no means qualified to opine on what is essentially a matter for economists,191 suspect that the issue is too important to leave entirely to them.

The issue of technological unemployment was brought to the fore in a famous article, “Economic Possibilities for Our Grandchildren,” by John Maynard Keynes. He wrote the article in 1930, when the Great Depression had created mass unemployment in Britain, but the topic has a much longer history. Aristotle, in Book I of his Politics, presents the main point quite clearly:

For if every instrument could accomplish its own work, obeying or anticipating the will of others . . . if, in like manner, the shuttle would weave and the plectrum touch the lyre without a hand to guide them, chief workmen would not want servants, nor masters slaves.

Everyone agrees with Aristotle’s observation that there is an immediate reduction in employment when an employer finds a mechanical method to perform work previously done by a person. The issue is whether the so-called compensation effects that ensue—and that tend to increase employment—will eventually make up for this reduction. The optimists say yes—and in the current debate, they point to all the new jobs that emerged after previous industrial revolutions. The pessimists say no—and in the current debate, they argue that machines will do all the “new jobs” too. When a machine replaces one’s physical labor, one can sell mental labor. When a machine replaces one’s mental labor, what does one have left to sell?

In Life 3.0, Max Tegmark depicts the debate as a conversation between two horses discussing the rise of the internal combustion engine in 1900. One predicts “new jobs for horses. . . . That’s what’s always happened before, like with the invention of the wheel and the plow.” For most horses, alas, the “new job” was to be pet food.

The debate has persisted for millennia because there are effects in both directions. The actual outcome depends on which effects matter more. Consider, for example, what happens to housepainters as technology improves. For the sake of simplicity, I’ll let the width of the paintbrush stand for the degree of automation:

Thus, the direct effects of technology work both ways: at first, by increasing productivity, technology can increase employment by reducing the price of an activity and thereby increasing demand; subsequently, further increases in technology mean that fewer and fewer humans are required. Figure 8 illustrates these developments.

Many technologies exhibit similar curves. If, in some given sector of the economy, we are to the left of the peak, then improving technology increases employment in that sector; present-day examples might include tasks such as graffiti removal, environmental cleanup, inspection of shipping containers, and housing construction in less developed countries, all of which might become more economically feasible if we have robots to help us. If we are already to the right of the peak, then further automation decreases employment. For example, its not hard to predict that elevator operators will continue to be squeezed out. In the long run, we have to expect that most industries are going to be pushed to the far right on the curve. One recent article, based on a careful econometric study by economists David Autor and Anna Salomons, states that “over the last 40 years, jobs have fallen in every single industry that introduced technologies to enhance productivity."

Economists have tried to measure the size of these effects in various industries experiencing increased automation, but the results are generally inconclusive.

Historically, most mainstream economists have argued from the "big picture" view: automation increases productivity, so, as a whole, humans are better off, in the sense that we enjoy more goods and services for the same amount of work.

Economic theory does not, unfortunately, predict that each human will be better off as a result of automation. Generally, automation increases the share of income going to capital (the owners of the housepainting robots) and decreases the share going to labor (the ex-housepainters). The economists Erik Brynjolfsson and Andrew McAfee, in The Second Machine Age, argue that this has already been happening for several decades. Data for the United States are shown in figure 9. They indicate that between 1947 and 1973, wages and productivity increased together, but after 1973, wages stagnated even while productivity roughly doubled. Brynjolfsson and McAfee call this the Great Decoupling. Other leading economists have also sounded the alarm, including Nobel laureates Robert Shiller, Mike Spence, and Paul Krugman; Klaus Schwab, head of the World Economic I’orum; and Larry Summers, former chief economist of the World Bank and Treasury secretary under President Bill Clinton.

Those arguing against the notion of technological unemployment often point to bank tellers, whose work can be done in part by ATMs, and retail cashiers, whose work is sped up by barcodes and RFID tags on merchandise. It is often claimed that these occupations are growing because of technology. Indeed, the number of tellers in the United States roughly doubled from 1970 to 2010, although it should be noted that the US population grew by 50 percent and the financial sector by over 400 percent in the same period,22 so it is difficult to attribute all, or perhaps any, of the employment growth to ATMs. Unfortunately, between 2010 and 2016 about one hundred thousand tellers lost their jobs, and the US Bureau of Labor Statistics (BLS) predicts another forty thousand job losses by 2026: “Online banking and automation technology are expected to continue replacing more job duties that tellers traditionally performed.”23 The data on retail cashiers are no more encouraging: the number per capita dropped by 5 percent from 1997 to 2015, and the BLS says, “Advances in technology, such as self service checkout stands in retail stores and increasing online sales, will continue to limit the need for cashiers.” Both sectors appear to be on the downslope. The same is true of almost all low-skilled occupations that involve working with machines.

Which occupations are about to decline as new, Al-based technology arrives? The prime example cited in the media is that of driving. In the United States there are about 3.5 million truck drivers; many of these jobs would be vulnerable to automation. Amazon, among other companies, is already using self-driving trucks for freight haulage on interstate freeways, albeit currently with human backup drivers.24 It seems very likely that the long-haul part of each truck journey will soon be autonomous, while humans, for the time being, will handle city traffic, pickup, and delivery. As a consequence of these expected developments, very few young people are interested in trucking as a career; ironically, there is currently a significant shortage of truck drivers in the Unites States, which is only hastening the onset of automation.

White-collar jobs are also at risk. For example, the BLS projects a 13 percent decline in per-capita employment of insurance underwriters from 2016 to 2026: “Automated underwriting software allows workers to process applications more quickly than before, reducing the need for as many underwriters.” If language technology develops as expected, many sales and customer service jobs will also be vulnerable, as well as jobs in the legal profession. (In a 2018 competition, AI software outscored experienced law professors in analyzing standard nondisclosure agreements and completed the task two hundred times faster.25) Routine forms of computer programming—the kind that is often outsourced today—are also likely to be automated. Indeed, almost anything that can be outsourced is a good candidate for automation, because outsourcing involves decomposing jobs into tasks that can be parceled up and distributed in a decontextualized form. The robot process automation industry produces software tools that achieve exactly this effect for clerical tasks performed online.

As AI progresses, it is certainly possible—perhaps even likely— that within the next few decades essentially all routine physical and mental labor will be done more cheaply by machines. Since we ceased to be hunter-gatherers thousands of years ago, our societies have used most people as robots, performing repetitive manual and mental tasks, so it is perhaps not surprising that robots will soon take on these roles. When this happens, it will push wages below the poverty line for those people who are unable to compete for the highly skilled jobs that remain. Larry Summers put it this way: “It may well be that, given the possibilities for substitution [of capital for labor], some categories of labor will not be able to earn a subsistence income.”26 This is precisely what happened to the horses: mechanical transportation became cheaper than the upkeep cost of a horse, so horses became pet food. Faced with the socioeconomic equivalent of becoming pet food, humans will be rather unhappy with their governments.

Faced with potentially unhappy humans, governments around the world are beginning to devote some attention to the issue. Most have already discovered that the idea of retraining everyone as a data scientist or robot engineer is a nonstarter—the world might need five or ten million of these, but nowhere close to the billion or so jobs that are at risk. Data science is a very tiny lifeboat for a giant cruise ship.

Some are working on “transition plans”—but transition to what? We need a plausible destination in order to plan a transition—that is, we need a plausible picture of a desirable future economy where most of what we currently call work is done by machines.

One rapidly emerging picture is that of an economy where far fewer people work because work is unnecessary. Keynes envisaged just such a future in his essay “Economic Possibilities for Our Grandchildren.” He described the high unemployment afflicting Great Britain in 1930 as a “temporary phase of maladjustment” caused by an “increase of technical efficiency” that took place “faster than we can deal with the problem of labour absorption.” He did not, however, imagine that in the long run—after a century of further technological advances—there would be a return to full employment:

Thus for the first time since his creation man will be faced with his real, his permanent problem—how to use his freedom from pressing economic cares, how to occupy the leisure, which science and compound interest will have won for him, to live wisely and agreeably and well.

Such a future requires a radical change in our economic system, because, in many countries, those who do not work face poverty or destitution. Thus, modern proponents of Keynes’s vision usually support some form of universal basic income, or UBI. Funded by valueadded taxes or by taxes on income from capital, UBI would provide a reasonable income to every adult, regardless of circumstance. Those who aspire to a higher standard of living can still work without losing the UBI, while those who do not can spend their time as they see fit. Perhaps surprisingly, UBI has support across the political spectrum, ranging from the Adam Smith Institute28 to the Green Party.

For some, UBI represents a version of paradise.30 For others, it represents an admission of failure—an assertion that most people will have nothing of economic value to contribute to society. They can be fed and housed—mostly by machines—but otherwise left to their own devices. The truth, as always, lies somewhere in between, and it depends largely on how one views human psychology. Keynes, in his essay, made a clear distinction between those who strive and those who enjoy—those “purposive” people for whom “jam is not jam unless it is a case of jam to-morrow and never jam to-day” and those “delightful” people who are “capable of taking direct enjoyment in things.” The UBI proposal assumes that the great majority of people are of the delightful variety.

Keynes suggests that striving is one of the “habits and instincts of the ordinary man, bred into him for countless generations” rather than one of the “real values of life.” He predicts that this instinct will gradually disappear. Against this view, one may suggest that striving is intrinsic to what it means to be truly human. Rather than striving and enjoying being mutually exclusive, they are often inseparable: true enjoyment and lasting fulfillment come from having a purpose and achieving it (or at least trying), usually in the face of obstacles, rather than from passive consumption of immediate pleasure. There is a difference between climbing Everest and being deposited on top by helicopter.

The connection between striving and enjoying is a central theme for our understanding of how to fashion a desirable future. Perhaps future generations will wonder why we ever worried about such a futile thing as “work.” Just in case that change in attitudes is slow in coming, let’s consider the economic implications of the view that most people will be better off with something useful to do, even though the great majority of goods and services will be produced by machines with very little human supervision. Inevitably, most people will be engaged in supplying interpersonal services that can be provided—or which we prefer to be provided—only by humans. That is, if we can no longer supply routine physical labor and routine mental labor, we can still supply our humanity. We will need to become good at being human.

Current professions of this kind include psychotherapists, executive coaches, tutors, counselors, companions, and those who care for children and the elderly. The phrase caring professions is often used in this context, but that is misleading: it has a positive connotation for those providing care, to be sure, but a negative connotation of dependency and helplessness for the recipients of care. But consider this observation, again from Keynes:

It will be those peoples, who can keep alive, and cultivate into a fuller perfection, the art of life itself and do not sell themselves for the means of life, who will be able to enjoy the abundance when it comes.

All of us need help in learning “the art of life itself.” This is not a matter of dependency but of growth. The capacity to inspire others and to confer the ability to appreciate and to create—be it in art, music, literature, conversation, gardening, architecture, food, wine, or video games—is likely to be more needed than ever.

The next question is income distribution. In most countries, this has been moving in the wrong direction for several decades. It’s a complex issue, but one thing is clear: high incomes and high social standing usually follow from providing high added value. The profession of childcare, to pick one example, is associated with low incomes and low social standing. This is, in part, a consequence of the fact that we don’t really know how to do it. Some practitioners are naturally good at it, but many are not. Contrast this with, say, orthopedic surgery. We wouldn’t just hire bored teenagers who need a bit of spare cash and put them to work as orthopedic surgeons at five dollars an hour plus all they can eat from the fridge. We have put centuries of research into understanding the human body and how to fix it when it’s broken, and practitioners must undergo years of training to learn all this knowledge and the skills necessary to apply it. As a result, orthopedic surgeons are highly paid and highly respected. They are highly paid not just because they know a lot and have a lot of training but also because all that knowledge and training actually works. It enables them to add a great deal of value to other people’s lives—especially people with broken bits.

Unfortunately, our scientific understanding of the mind is shockingly weak and our scientific understanding of happiness and fulfillment is even weaker. We simply don’t know how to add value to each other’s lives in consistent, predictable ways. We have had moderate success with certain psychiatric disorders, but we are still fighting a Hundred Years’ Literacy War over something as basic as teaching children to read.32 We need a radical rethinking of our educational system and our scientific enterprise to focus more attention on the human rather than the physical world. (Joseph Aoun, president of Northeastern University, argues that universities should be teaching and studying “humanics.”33) It sounds odd to say that happiness should be an engineering discipline, but that seems to be the inevitable conclusion. Such a discipline would build on basic science—a better understanding of how human minds work at the cognitive and emotional levels— and would train a wide variety of practitioners, ranging from life architects, who help individuals plan the overall shape of their life trajectories, to professional experts in topics such as curiosity enhancement and personal resilience. If based on real science, these professions need be no more woo-woo than bridge designers and orthopedic surgeons are today.

Reworking our education and research institutions to create this basic science and to convert it into training programs and credentialed professions will take decades, so it’s a good idea to start now and a pity we didn’t start long ago. The final result—if it works—would be a world well worth living in. Without such a rethinking, we risk an unsustainable level of socioeconomic dislocation.

Usurping Other Human Roles

We should think twice before allowing machines to take over roles involving interpersonal services. If being human is our main selling point to other humans, so to speak, then making imitation humans seems like a bad idea. Fortunately for us, we have a distinct advantage over machines when it comes to knowing how other humans feel and how they will react. Nearly every human knows what it’s like to hit one s thumb with a hammer or to feel unrequited love.

Counteracting this natural human advantage is a natural human disadvantage: the tendency to be fooled by appearances—especially human appearances. Alan Turing warned against making robots resemble humans:

I certainly hope and believe that no great efforts will be put into making machines with the most distinctively human, but nonintellectual, characteristics such as the shape of the human body; it appears to me quite futile to make such attempts and their results would have something like the unpleasant quality of artificial flowers.

Unfortunately, Turing’s warning has gone unheeded. Several research groups have produced eerily lifelike robots, as shown in figure 10.

As research tools, the robots may provide insights into how humans interpret robot behavior and communication. As prototypes for future commercial products, they represent a form of dishonesty. They bypass our conscious awareness and appeal directly to our emotional selves, perhaps convincing us that they are endowed with real intelligence. Imagine, for example, how much easier it would be to switch off and recycle a squat, gray box that was malfunctioning— even if it was squawking about not wanting to be switched off—than it would be to do the same for JiaJia or Geminoid DK. Imagine also how confusing and perhaps psychologically disturbing it would be for babies and small children to be cared for by entities that appear to be human, like their parents, but are somehow not; that appear to care about them, like their parents, but in fact do not.

Beyond a basic capability to convey nonverbal information via facial expression and movement—which even Bugs Bunny manages to do with ease—there is no good reason for robots to have humanoid form. There are also good, practical reasons not to have humanoid form—for example, our bipedal stance is relatively unstable compared to quadrupedal locomotion. Dogs, cats, and horses fit into our lives well, and their physical form is a very good clue as to how they are likely to behave. (Imagine if a horse suddenly started behaving like a dog!) The same should be true of robots. Perhaps a four-legged, two-armed, centaur-like morphology would be a good standard. An accurately humanoid robot makes as much sense as a Ferrari with a top speed of five miles per hour or a “raspberry” ice-cream cone made from beetroot tinted cream of chopped liver.

The humanoid aspect of some robots has already contributed to political as well as emotional confusion. On October 25, 2017, Saudi Arabia granted citizenship to Sophia, a humanoid robot that has been described as little more than “a chatbot with a face”35 and worse.36 Perhaps this was a public relations stunt, but a proposal emanating from the European Parliament s Committee on Legal Affairs is entirely serious.37 It recommends

creating a specific legal status for robots in the long run, so that at least the most sophisticated autonomous robots could be established as having the status of electronic persons responsible for making good any damage they may cause.

In other words, the robot itself would be legally responsible for damage, rather than the owner or manufacturer. This implies that robots will own financial assets and be subject to sanctions if they do not comply. Taken literally, this does not make sense. For example, if we were to imprison the robot for nonpayment, why would it care?

In addition to the needless and even absurd elevation of the status of robots, there is a danger that the increased use of machines in decisions affecting people will degrade the status and dignity of humans. This possibility is illustrated perfectly in a scene from the science-fiction movie Elysium, when Max (Matt Damon) pleads his case before his “parole officer” (figure 11) to explain why the extension of his sentence is unjustified. Needless to say, Max is unsuccessful. The parole officer even chides him for failing to display a suitably deferential attitude.

One can think of such an assault on human dignity in two ways. The first is obvious: by giving machines authority over humans, we relegate ourselves to a second-class status and lose the right to participate in decisions that affect us. (A more extreme form of this is giving machines the authority to kill humans, as discussed earlier in the chapter.) The second is indirect: even if you believe it is not the machines making the decision but those humans who designed and commissioned the machines, the fact that those human designers and commissioners do not consider it worthwhile to weigh the individual circumstances of each human subject in such cases suggests that they attach little value to the lives of others. This is perhaps a symptom of the beginning of a great separation between an elite served by humans and a vast underclass served, and controlled, by machines.

In the EU, Article 22 of the 2018 General Data Protection Regulation, or GDPR, explicitly forbids the granting of authority to machines in such cases:

The data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her.

Although this sounds admirable in principle, it remains to be seen—at least at the time of writing—how much impact this will have in practice. It is often so much easier, faster, and cheaper to leave the decisions to the machine.

One reason for all the concern about automated decisions is the potential for algorithmic bias—the tendency of machine learning algorithms to produce inappropriately biased decisions about loans, housing, jobs, insurance, parole, sentencing, college admission, and so on. The explicit use of criteria such as race in these decisions has been illegal for decades in many countries and is prohibited by Article 9 of the GDPR for a very wide range of applications. That does not mean, of course, that by excluding race from the data we necessarily get racially unbiased decisions. For example, beginning in the 1930s, the governmentsanctioned practice of redlining caused certain zip codes in the United States to be off-limits for mortgage lending and other forms of investment, leading to declining real-estate values. It just so happened that those zip codes were largely populated by African Americans.

To prevent redlining, now only the first three digits of the five-digit zip code can be used in making credit decisions. In addition, the decision process must be amenable to inspection, to ensure no other “accidental” biases are creeping in. The EUs GDPR is often said to provide a general “right to an explanation” for any automated decision,38 but the actual language of Article 14 merely requires

meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing for the data subject.

At present, it is unknown how courts will enforce this clause. It’s possible that the hapless consumer will just be handed a description of the particular deep learning algorithm used to train the classifier that made the decision.

Nowadays, the likely causes of algorithmic bias lie in the data rather than in the deliberate malfeasance of corporations. In 2015, Glamour magazine reported a disappointing finding: “The first female Google image search result for ‘CEO’ appears TWELVE rows down— and it’s Barbie.” (There were some actual women in the 2018 results, but most of them were models portraying CEOs in generic stock photos, rather than actual female CEOs; the 2019 results are somewhat better.) This is a consequence not of deliberate gender bias in Google’s image search ranking but of preexisting bias in the culture that produces the data: there are far more male than female CEOs, and when people want to depict an “archetypal” CEO in a captioned image, they almost always pick a male figure. The fact that the bias lies primarily in the data does not, of course, mean that there is no obligation to take steps to counteract the problem.

There are other, more technical reasons why the naive application of machine learning methods can produce biased outcomes. For example, minorities are, by definition, less well represented in population-wide data samples; hence, predictions for individual members of minorities may be less accurate if such predictions are made largely on the basis of data from other members of the same group. Fortunately, a good deal of attention has been paid to the problem of removing inadvertent bias from machine learning algorithms, and there are now methods that produce unbiased results according to several plausible and desirable definitions of fairness.39 The mathematical analysis of these definitions of fairness shows that they cannot be achieved simultaneously and that, when enforced, they result in lower prediction accuracy and, in the case of lending decisions, lower profit for the lender. This is perhaps disappointing, but at least it makes clear the trade-offs involved in avoiding algorithmic bias. One hopes that awareness of these methods and of the issue itself will spread quickly among policy makers, practitioners, and users.

If handing authority over individual humans to machines is sometimes problematic, what about authority over lots of humans? That is, should we put machines in political and management roles? At present this may seem far-fetched. Machines cannot sustain an extended conversation and lack the basic understanding of the factors that are relevant to making decisions with broad scope, such as whether to raise the minimum wage or to reject a merger proposal from another corporation. The trend, however, is clear: machines are making decisions at higher and higher levels of authority in many areas. Take airlines, for example. First, computers helped in the construction of flight schedules. Soon, they took over allocation of flight crews, the booking of seats, and the management of routine maintenance. Next, they were connected to global information networks to provide real-time status reports to airline managers, so that managers could cope with disruption effectively. Now they are taking over the job of managing disruption: rerouting planes, rescheduling staff, rebooking passengers, and revising maintenance schedules.

This is all to the good from the point of view of airline economics and passenger experience. The question is whether the computer system remains a tool of humans, or humans become tools of the computer system—supplying information and fixing bugs when necessary, but no longer understanding in any depth how the whole thing is working. The answer becomes clear when the system goes down and global chaos ensues until it can be brought back online. For example, a single “computer glitch" on April 3, 2018, caused fifteen thousand flights in Europe to be significantly delayed or canceled.40 When trading algorithms caused the 2010 “flash crash” on the New York Stock Exchange, wiping out $1 trillion in a few minutes, the only solution was to shut down the exchange. What happened is still not well understood.

Before there was any technology, human beings lived, like most animals, hand to mouth. We stood directly on the ground, so to speak. Technology gradually raised us up on a pyramid of machinery, increasing our footprint as individuals and as a species. There are different ways we can design the relationship between humans and machines. If we design it so that humans retain sufficient understanding, authority, and autonomy, the technological parts of the system can greatly magnify human capabilities, allowing each of us to stand on a vast pyramid of capabilities—a demigod, if you like. But consider the worker in an online-shopping fulfillment warehouse. She is more productive than her predecessors because she has a small army of robots bringing her storage bins to pick items from; but she is a part of a larger system controlled by intelligent algorithms that decide where she should stand and which items she should pick and dispatch. She is already partly buried in the pyramid, not standing on top of it. It’s only a matter of time before the sand fills the spaces in the pyramid and her role is eliminated.

5: OVERLY INTELLIGENT AI

Fear and Greed: Instrumental Goals

If a machine pursuing an incorrect objective sounds bad enough, there’s worse. The solution suggested by Alan Turing—turning off the power at strategic moments—may not be available, for a very simple reason: you can't fetch the coffee if you’re dead.

Let me explain. Suppose a machine has the objective of fetching the coffee. If it is sufficiently intelligent, it will certainly understand that it will fail in its objective if it is switched off before completing its mission. Thus, the objective of fetching coffee creates, as a necessary subgoal, the objective of disabling the off-switch. The same is true for curing cancer or calculating the digits of pi. There's really not a lot you can do once you're dead, so we can expect Al systems to act preemptively to preserve their own existence, given more or less any definite objective.

If that objective is in conflict with human preferences, then we have exactly the plot of 2001: A Space Odyssey, in which the HAL 9000 computer kills four of the five astronauts on board the ship to prevent interference with its mission. Dave, the last remaining astronaut, manages to switch HAL off after an epic battle of wits— presumably to keep the plot interesting. But if HAL had been truly superintelligent, Dave would have been switched off.

It is important to understand that self-preservation doesn’t have to be any sort of built-in instinct or prime directive in machines. (So Isaac Asimov’s Third Law of Robotics,8 which begins “A robot must protect its own existence,” is completely unnecessary.) There is no need to build self-preservation in because it is an instrumental goal—a goal that is a useful subgoal of almost any original objective.9 Any entity that has a definite objective will automatically act as if it also has instrumental goals.

In addition to being alive, having access to money is an instrumental goal within our current system. Thus, an intelligent machine might want money, not because it’s greedy but because money is useful for achieving all sorts of goals. In the movie Transcendence, when Johnny Depp’s brain is uploaded into the quantum supercomputer, the first thing the machine does is copy itself onto millions of other computers on the Internet so that it cannot be switched off. The second thing it does is make a quick killing on the stock market to fund its expansion plans.

And what, exactly, are those expansion plans? They include designing and building a much larger quantum supercomputer; doing AI research; and discovering new knowledge of physics, neuroscience, and biology. These resource objectives—computing power, algorithms, and knowledge—are also instrumental goals, useful for achieving any overarching objective.10 They seem harmless enough until one realizes that the acquisition process will continue without limit. This seems to create inevitable conflict with humans. And of course, the machine, equipped with ever-better models of human decision making, will anticipate and defeat our every move in this conflict.

Intelligence Explosions

I. J. Good was a brilliant mathematician who worked with Alan Turing at Bletchley Park, breaking German codes during World War II. He shared Turing’s interests in machine intelligence and statistical inference. In 1965, he wrote what is now his best-known paper, “Speculations Concerning the First Ultraintelligent Machine.”11 The first sentence suggests that Good, alarmed by the nuclear brinkmanship of the Cold War, regarded AI as a possible savior for humanity : “The survival of man depends on the early construction of an ultraintelligent machine.” As the paper proceeds, however, he becomes more circumspect. He introduces the notion of an intelligence explosion, but, like Butler, Turing, and Wiener before him, he worries about losing control:

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside science fiction.

This paragraph is a staple of any discussion of superintelligent AI, although the caveats at the end are usually left out. Good’s point can be strengthened by noting that not only could the ultraintelligent machine improve its own design; it’s likely that it would do so because, as we have seen, an intelligent machine expects to benefit from improving its hardware and software. The possibility of an intelligence explosion is often cited as the main source of risk to humanity from AI because it would give us so little time to solve the control problem.

Good’s argument certainly has plausibility via the natural analogy to a chemical explosion in which each molecular reaction releases enough energy to initiate more than one additional reaction. On the other hand, it is logically possible that there are diminishing returns to intelligence improvements, so that the process peters out rather than exploding.13 There’s no obvious way to prove that an explosion will necessarily occur.

The diminishing-returns scenario is interesting in its own right. It could arise if it turns out that achieving a given percentage improvement becomes much harder as the machine becomes more intelligent. (I’m assuming for the sake of argument that general-purpose machine intelligence is measurable on some kind of linear scale, which I doubt will ever be strictly true.) In that case, humans won’t be able to create superintelligence either. If a machine that is already superhuman runs out of steam when trying to improve its own intelligence, then humans will run out of steam even sooner.

Now, I’ve never heard a serious argument to the effect that creating any given level of machine intelligence is simply beyond the capacity of human ingenuity, but I suppose one must concede it’s logically possible. “Logically possible" and Tm willing to bet the future of the human race on it" are, of course, two completely different things. Betting against human ingenuity seems like a losing strategy.

If an intelligence explosion does occur, and if we have not already solved the problem of controlling machines with only slightly superhuman intelligence—for example, if we cannot prevent them from making these recursive self-improvements—then we would have no time left to solve the control problem and the game would be over. This is Bostrom’s hard takeoff scenario, in which the machine’s intelligence increases astronomically in just days or weeks. In Turing’s words, it is “certainly something which can give us anxiety.”

The possible responses to this anxiety seem to be to retreat from Al research, to deny that there are risks inherent in developing advanced Al, to understand and mitigate the risks through the design of Al systems that necessarily remain under human control, and to resign—simply to cede the future to intelligent machines.

Denial and mitigation are the subjects of the remainder of the book. As I have already argued, retreat from Al research is both unlikely to happen (because the benefits forgone are too great) and very difficult to bring about. Resignation seems to be the worst possible response. It is often accompanied by the idea that Al systems that are more intelligent than us somehow deserve to inherit the planet, leaving humans to go gentle into that good night, comforted by the thought that our brilliant electronic progeny are busy pursuing their objectives. This view was promulgated by the roboticist and futurist Hans Moravec,14 who writes, “The immensities of cyberspace will be teeming with unhuman superminds, engaged in affairs that are to human concerns as ours are to those of bacteria." This seems to be a mistake. Value, for humans, is defined primarily by conscious human experience. If there are no humans and no other conscious entities whose subjective experience matters to us, there is nothing of value occurring.

6: THE NOTE-SO-GREAT AI-DEBATE

The implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking.”1 So ended The Economist magazine’s review of Nick Bostrom’s Superintelligence. Most would interpret this as a classic example of British understatement. Surely, you might think, the great minds of today are already doing this hard thinking—engaging in serious debate, weighing up the risks and benefits, seeking solutions, ferreting out loopholes in solutions, and so on. Not yet, as far as I am aware.

When one first introduces these ideas to a technical audience, one can see the thought bubbles popping out of their heads, beginning with the words “But, but, b u t. . and ending with exclamation marks.

The first kind of but takes the form of denial. The deniers say, “But this can’t be a real problem, because XYZ.” Some of the XYZs reflect a reasoning process that might charitably be described as wishful thinking, while others are more substantial. The second kind of but takes the form of deflection: accepting that the problems are real but arguing that we shouldn’t try to solve them, either because they’re unsolvable or because there are more important things to focus on than the end of civilization or because it’s best not to mention them at all. The third kind of but takes the form of an oversimplified, instant solution: “But can’t we just do ABC?” As with denial, some of the ABCs are instantly regrettable. Others, perhaps by accident, come closer to identifying the true nature of the problem.

I don’t mean to suggest that there cannot be any reasonable objections to the view that poorly designed superintelligent machines would present a serious risk to humanity. It’s just that I have yet to see such an objection. Since the issue seems to be so important, it deserves a public debate of the highest quality. So, in the interests of having that debate, and in the hope that the reader will contribute to it, let me provide a quick tour of the highlights so far, such as they are.

Denial

Denying that the problem exists at all is the easiest way out. Scott Alexander, author of the Slate Star Codex blog, began a well-known article on AI risk as follows:2 “I first became interested in AI risk back around 2007. At the time, most people’s response to the topic was ‘Haha, come back when anyone believes this besides random Internet crackpots.’ ”

Instantly regrettable remarks

A perceived threat to one’s lifelong vocation can lead a perfectly intelligent and usually thoughtful person to say things they might wish to retract on further analysis. That being the case, I will not name the authors of the following arguments, all of whom are wellknown AI researchers. I’ve included refutations of the arguments, even though they are quite unnecessary.

Electronic calculators are superhuman at arithmetic. Calculators didn’t take over the world; therefore, there is no reason to worry about superhuman AI. • Refutation: intelligence is not the same as arithmetic, and the arithmetic ability of calculators does not equip them to take over the world. • Horses have superhuman strength, and we don’t worry about proving that horses are safe; so we needn’t worry about proving that AI systems are safe. • Refutation: intelligence is not the same as physical strength, and the strength of horses does not equip them to take over the world. • Historically, there are zero examples of machines killing millions of humans, so, by induction, it cannot happen in the future. • Refutation: there's a first time for everything, before which there were zero examples of it happening. ' • No physical quantity in the universe can be infinite, and that includes intelligence, so concerns about superintelligence are overblown. • Refutation; superintelligence doesn't need to be infinite to be problematic; and physics allows computing devices billions of times more powerful than the human brain. • We don’t worry about species-ending but highly unlikely possibilities such as black holes materializing in near-Earth orbit, so why worry about superintelligent AI? • Refutation: if most physicists on Earth were working to make such black holes, wouldn't we ask them if it was safe?

It's complicated

It is a staple of modern psychology that a single IQ number cannot characterize the full richness of human intelligence.3 There are, the theory says, different dimensions of intelligence: spatial, logical, linguistic, social, and so on. Alice, our soccer player from Chapter 2, might have more spatial intelligence than her friend Bob, but less social intelligence. Thus, we cannot line up all humans in strict order of intelligence.

This is even more true of machines, because their abilities are much narrower. The Google search engine and AlphaGo have almost nothing in common, besides being products of two subsidiaries of the same parent corporation, and so it makes no sense to say that one is more intelligent than the other. This makes notions of “machine IQ” problematic and suggests that it’s misleading to describe the future as a one-dimensional IQ race between humans and machines.

Kevin Kelly, founding editor of Wired magazine and a remarkably perceptive technology commentator, takes this argument one step further. In “The Myth of a Superhuman Al,”4 he writes, “Intelligence is not a single dimension, so ‘smarter than humans’ is a meaningless concept.” In a single stroke, all concerns about superintelligence are wiped away.

Now, one obvious response is that a machine could exceed human capabilities in all relevant dimensions of intelligence. In that case, even by Kelly’s strict standards, the machine would be smarter than a human. But this rather strong assumption is not necessary to refute Kelly’s argument. Consider the chimpanzee. Chimpanzees probably have better short-term memory than humans, even on humanoriented tasks such as recalling sequences of digits.5 Short-term memory is an important dimension of intelligence. By Kelly’s argument, then, humans are not smarter than chimpanzees; indeed, he would claim that “smarter than a chimpanzee” is a meaningless concept. This is cold comfort to the chimpanzees (and bonobos, gorillas, orangutans, whales, dolphins, and so on) whose species survive only because we deign to allow it. It is colder comfort still to all those species that we have already wiped out. It’s also cold comfort to humans who might be worried about being wiped out by machines.

It's impossible

Even before the birth of AI in 1956, august intellectuals were harrumphing and saying that intelligent machines were impossible. Alan Turing devoted much of his seminal 1950 paper, “Computing Machinery and Intelligence/’ to refuting these arguments. Ever since, the AI community has been fending off similar claims of impossibility from philosophers,6 mathematicians,7 and others. In the current debate over superintelligence, several philosophers have exhumed these impossibility claims to prove that humanity has nothing to fear.8'9 This comes as no surprise.

The One Hundred Year Study on Artificial Intelligence, or A ll00, is an ambitious, long-term project housed at Stanford University. Its goal is to keep track of AI, or, more precisely, to “study and anticipate how the effects of artificial intelligence will ripple through every aspect of how people work, live and play.” Its first major report, “Artificial Intelligence and Life in 2030,” does come as a surprise.10 As might be expected, it emphasizes the benefits of AI in areas such as medical diagnosis and automotive safety. What’s unexpected is the claim that “unlike in the movies, there is no race of superhuman robots on the horizon or probably even possible.”

To my knowledge, this is the first time that serious AI researchers have publicly espoused the view that human-level or superhuman AI is impossible—and this in the middle of a period of extremely rapid progress in AI research, when barrier after barrier is being breached. It’s as if a group of leading cancer biologists announced that they had been fooling us all along: they’ve always known that there will never be a cure for cancer.

What could have motivated such a volte-face? The report provides no arguments or evidence whatever. (Indeed, what evidence could there be that no physically possible arrangement of atoms outperforms the human brain?) I suspect there are two reasons. The first is the natural desire to disprove the existence of the gorilla problem, which presents a very uncomfortable prospect for the Al researcher; certainly, if human-level Al is impossible, the gorilla problem is neatly dispatched. The second reason is tribalism—the instinct to circle the wagons against what are perceived to be “attacks” on Al.

It seems odd to perceive the claim that superintelligent Al is possible as an attack on Al, and even odder to defend Al by saying that Al will never succeed in its goals. We cannot insure against future catastrophe simply by betting against human ingenuity.

We have made such bets before and lost. As we saw earlier, the physics establishment of the early 1930s, personified by Lord Rutherford, confidently believed that extracting atomic energy was impossible; yet Leo Szilard’s invention of the neutron-induced nuclear chain reaction in 1933 proved that confidence to be misplaced.

Szilard’s breakthrough came at an unfortunate time: the beginning of an arms race with Nazi Germany. There was no possibility of developing nuclear technology for the greater good. A few years later, having demonstrated a nuclear chain reaction in his laboratory, Szilard wrote, “We switched everything off and went home. That night, there was very little doubt in my mind that the world was headed for grief.”

It's too soon to worry about it

It’s common to see sober-minded people seeking to assuage public concerns by pointing out that because human-level Al is not likely to arrive for several decades, there is nothing to worry about. For example, the A ll00 report says there is “no cause for concern that Al is an imminent threat to humankind.”

This argument fails on two counts. The first is that it attacks a straw man. The reasons for concern are not predicated on imminence. For example, Nick Bostrom writes in Superintelligence, “It is no part of the argument in this book that we are on the threshold of a big breakthrough in artificial intelligence, or that we can predict with any precision when such a development might occur.” The second is that a long-term risk can still be cause for immediate concern. The right time to worry about a potentially serious problem for humanity depends not just on when the problem will occur but also on how long it will take to prepare and implement a solution.

For example, if we were to detect a large asteroid on course to collide with Earth in 2069, would we say its too soon to worry? Quite the opposite! There would be a worldwide emergency project to develop the means to counter the threat. We wouldn't wait until 2068 to start working on a solution, because we can’t say in advance how much time is needed. Indeed, NASA’s Planetary Defense project is already working on possible solutions, even though “no known asteroid poses a significant risk of impact with Earth over the next 100 years.” In case that makes you feel complacent, they also say, “About 74 percent of near-Earth objects larger than 460 feet still remain to be discovered.”

And if we consider the global catastrophic risks from climate change, which are predicted to occur later in this century, is it too soon to take action to prevent them? On the contrary, it may be too late. The relevant time scale for superhuman AI is less predictable, but of course that means it, like nuclear fission, might arrive considerably sooner than expected.

One formulation of the “it’s too soon to worry” argument that has gained currency is Andrew Ng’s assertion that “it’s like worrying about overpopulation on Mars.”11 (He later upgraded this from Mars to Alpha Centauri.) Ng, a former Stanford professor, is a leading expert on machine learning, and his views carry some weight. The assertion appeals to a convenient analogy: not only is the risk easily managed and far in the future but also it’s extremely unlikely we’d even try to move billions of humans to Mars in the first place. The analogy is a false one, however. We are already devoting huge scientific and technical resources to creating ever-more-capable AI systems, with very little thought devoted to what happens if we succeed. A more apt analogy, then, would be working on a plan to move the human race to Mars with no consideration for what we might breathe, drink, or eat once we arrive. Some might call this plan unwise. Alternatively, one could take Ng’s point literally, and respond that landing even a single person on Mars would constitute overpopulation, because Mars has a carrying capacity of zero. Thus, groups that are currently planning to send a handful of humans to Mars are worrying about overpopulation on Mars, which is why they are developing life-support systems.

We're the experts

In every discussion of technological risk, the pro-technology camp wheels out the claim that all concerns about risk arise from ignorance. For example, here’s Oren Etzioni, CEO of the Allen Institute for AI and a noted researcher in machine learning and natural language understanding:

At the rise of every technology innovation, people have been scared. From the weavers throwing their shoes in the mechanical looms at the beginning of the industrial era to today’s fear of killer robots, our response has been driven by not knowing what impact the new technology will have on our sense of self and our livelihoods. And when we don’t know, our fearful minds fill in the details.

Popular Science published an article titled “Bill Gates Fears AI, but AI Researchers Know Better”:

When you talk to A.I. researchers—again, genuine A.I. researchers, people who grapple with making systems that work at all, much less work too well—they are not worried about superintelligence sneaking up on them, now or in the future. Contrary to the spooky stories that Musk seems intent on telling, A.I. researchers aren’t frantically installing firewalled summoning chambers and self-destruct countdowns.

This analysis was based on a sample of four, all of whom in fact said in their interviews that the long-term safety of Al was an important issue.

Using very similar language to the Popular Science article, David Kenny, at that time a vice president at IBM, wrote a letter to the US Congress that included the following reassuring words:

When you actually do the science of machine intelligence, and when you actually apply it in the real world of business and society—as we have done at IBM to create our pioneering cognitive computing system, Watson—you understand that this technology does not support the fear-mongering commonly associated with the Al debate today.

The message is the same in all three cases: “Don’t listen to them; we’re the experts.” Now, one can point out that this is really an ad hominem argument that attempts to refute the message by delegitimizing the messengers, but even if one takes it at face value, the argument doesn’t hold water. Elon Musk, Stephen Hawking, and Bill Gates are certainly very familiar with scientific and technological reasoning, and Musk and Gates in particular have supervised and invested in many Al research projects. And it would be even less plausible to argue that Alan Turing, I. J. Good, Norbert Wiener, and Marvin Minsky are unqualified to discuss Al. Finally, Scott Alexander’s blog piece mentioned earlier, which is titled “Al Researchers on Al Risk,” notes that “Al researchers, including some of the leaders in the field, have been instrumental in raising issues about Al risk and superintelligence from the very beginning.” He lists several such researchers, and the list is now much longer.

Another standard rhetorical move for the “defenders of Al” is to describe their opponents as Luddites. Oren Etzioni’s reference to “weavers throwing their shoes in the mechanical looms” is just this: the Luddites were artisan weavers in the early nineteenth century protesting the introduction of machinery to replace their skilled labor. In 2015, the Information Technology and Innovation Foundation gave its annual Luddite Award to “alarmists touting an artificial intelligence apocalypse.” It’s an odd definition of “Luddite” that includes Turing, Wiener, Minsky, Musk, and Gates, who rank among the most prominent contributors to technological progress in the twentieth and twenty-first centuries.

The accusation of Luddism represents a misunderstanding of the nature of the concerns raised and the purpose for raising them. It is as if one were to accuse nuclear engineers of Luddism if they point out the need for control of the fission reaction. As with the strange phenomenon of AI researchers suddenly claiming that AI is impossible, I think we can attribute this puzzling episode to tribalism in defense of technological progress.

7: AI: A DIFFERENT APPROACH

Once the skeptics arguments have been refuted and all the but but buts have been answered, the next question is usually, “OK, I admit there’s a problem, but there’s no solution, is there?” Yes, there is a solution.

Let’s remind ourselves of the task at hand: to design machines with a high degree of intelligence—so that they can help us with difficult problems—while ensuring that those machines never behave in ways that make us seriously unhappy.

The task is, fortunately, not the following: given a machine that possesses a high degree of intelligence, work out how to control it. If that were the task, we would be toast. A machine viewed as a black box, a fait accompli, might as well have arrived from outer space. And our chances of controlling a superintelligent entity from outer space are roughly zero. Similar arguments apply to methods of creating Al systems that guarantee we won’t understand how they work; these methods include whole-brain emulation1—creating souped-up electronic copies of human brains—as well as methods based on simulated evolution of programs.2 I won’t say more about these proposals because they are so obviously a bad idea.

So, how has the field of Al approached the “design machines with a high degree of intelligence” part of the task in the past? Like many other fields, AI has adopted the standard model: we build optimizing machines, we feed objectives into them, and off they go. That worked well when the machines were stupid and had a limited scope of action; if you put in the wrong objective, you had a good chance of being able to switch off the machine, fix the problem, and try again.

As machines designed according to the standard model become more intelligent, however, and as their scope of action becomes more global, the approach becomes untenable. Such machines will pursue their objective, no matter how wrong it is; they will resist attempts to switch them off; and they will acquire any and all resources that contribute to achieving the objective. Indeed, the optimal behavior for the machine might include deceiving the humans into thinking they gave the machine a reasonable objective, in order to gain enough time to achieve the actual objective given to it. This wouldn’t be “deviant” or “malicious” behavior requiring consciousness and free will; it would just be part of an optimal plan to achieve the objective.

In Chapter 1,1 introduced the idea of beneficial machines-—that is, machines whose actions can be expected to achieve our objectives rather than their objectives. My goal in this chapter is to explain in simple terms how this can be done, despite the apparent drawback that the machines don't know what our objectives are. The resulting approach should lead eventually to machines that present no threat to us, no matter how intelligent they are.

Principles for Beneficial Machines

I find it helpful to summarize the approach in the form of three3 principles. When reading these principles, keep in mind that they are intended primarily as a guide to AI researchers and developers in thinking about how to create beneficial AI systems; they are not intended as explicit laws for AI systems to follow:

1. The machine’s only objective is to maximize the realization of human preferences.2. The machine is initially uncertain about what those preference sare.3. The ultimate source of information about human preferences is human behavior.

Before delving into more detailed explanations, it’s important to remember the broad scope of what I mean by preferences in these principles. Here’s a reminder of what I wrote in Chapter 2: if you were somehow able to watch two movies, each describing in sufficient detail and breadth a future life you might lead, such that each constitutes a virtual experience, you could say which you prefer, or express indifference. Thus, preferences here are all-encompassing; they cover everything you might care about, arbitrarily far into the future.5 And they are yours: the machine is not looking to identify or adopt one ideal set of preferences but to understand and satisfy (to the extent possible] the preferences of each person.

The first principle: Purely altruistic machines

The first principle, that the machine’s only objective is to maximize the realization of human preferences, is central to the notion of a beneficial machine. In particular, it will be beneficial to humans, rather than to, say, cockroaches. There’s no getting around this recipient specific notion of benefit.

The principle means that the machine is purely altruistic—that is, it attaches absolutely no intrinsic value to its own well-being or even its own existence. It might protect itself in order to continue doing useful things for humans, or because its owner would be unhappy about having to pay for repairs, or because the sight of a dirty or damaged robot might be mildly distressing to passersby, but not because it wants to be alive. Putting in any preference for self-preservation sets up an additional incentive within the robot that is not strictly aligned with human well-being.

The wording of the first principle brings up two questions of fundamental importance. Each merits an entire bookshelf to itself, and in fact many books have already been written on these questions.

The first question is whether humans really have preferences in a meaningful or stable sense. In truth, the notion of a “preference” is an idealization that fails to match reality in several ways. For example, we aren’t born with the preferences we have as adults, so they must change over time. For now, I will assume that the idealization is reasonable. Later, I will examine what happens when we give up the idealization.

The second question is a staple of the social sciences: given that it is usually impossible to ensure that everyone gets their most preferred outcome—we can’t all be Emperor of the Universe—how should the machine trade off the preferences of multiple humans? Again, for the time being—and I promise to return to this question in the next chapter—it seems reasonable to adopt the simple approach of treating everyone equally. This is reminiscent of the roots of eighteenth-century utilitarianism in the phrase “the greatest happiness for the greatest numbers,”6 and there are many caveats and elaborations required to make this work in practice. Perhaps the most important of these is the matter of the possibly vast number of people not yet born, and how their preferences are to be taken into account.

The issue of future humans brings up another, related question: How do we take into account the preferences of nonhuman entities? That is, should the first principle include the preferences of animals? (And possibly plants too?) This is a question worthy of debate, but the outcome seems unlikely to have a strong impact on the path forward for Al. For what it’s worth, human preferences can and do include terms for the well-being of animals, as well as for the aspects of human well-being that benefit directly from animals’ existence.7 To say that the machine should pay attention to the preferences of animals in addition to this is to say that humans should build machines that care more about animals than humans do, which is a difficult position to sustain. A more tenable position is that our tendency to engage in myopic decision making—which works against our own interests— often leads to negative consequences for the environment and its animal inhabitants. A machine that makes less myopic decisions would help humans adopt more environmentally sound policies. And if, in the future, we give substantially greater weight to the well-being of animals than we currently do—which probably means sacrificing some of our own intrinsic well-being—then machines will adapt accordingly.

The second principle: Humble machines

The second principle, that the machine is initially uncertain about what human preferences are, is the key to creating beneficial machines.

A machine that assumes it knows the true objective perfectly will pursue it single-mindedly. It will never ask whether some course of action is OK, because it already knows it’s an optimal solution for the objective. It will ignore humans jumping up and down screaming, “Stop, you’re going to destroy the world!” because those are just words. Assuming perfect knowledge of the objective decouples the machine from the human: what the human does no longer matters, because the machine knows the goal and pursues it.

On the other hand, a machine that is uncertain about the true objective will exhibit a kind of humility: it will, for example, defer to humans and allow itself to be switched off. It reasons that the human will switch it off only if it’s doing something wrong—that is, doing something contrary to human preferences. By the first principle, it wants to avoid doing that, but, by the second principle, it knows that’s possible because it doesn’t know exactly what.“wrong” is. So, if the human does switch the machine off, then the machine avoids doing the wrong thing, and that’s what it wants. In other words, the machine has a positive incentive to allow itself to be switched off. It remains coupled to the human, who is a potential source of information that will allow it to avoid mistakes and do a better job.

Uncertainty has been a central concern in AI since the 1980s; indeed the phrase “modern AI” often refers to the revolution that took place when uncertainty was finally recognized as a ubiquitous issue in real-world decision making. Yet uncertainty in the objective of the AI system was simply ignored. In all the work on utility maximization, goal achievement, cost minimization, reward maximization, and loss minimization, it is assumed that the utility function, the goal, the cost function, the reward function, and the loss function are known perfectly. How could this be? How could the AI community (and the control theory, operations research, and statistics communities) have such a huge blind spot for so long, even while embracing uncertainty in all other aspects of decision making?

One could make some rather complicated technical excuses,9 but I suspect the truth is that, with some honorable exceptions,10 AI researchers simply bought into the standard model that maps our notion of human intelligence onto machine intelligence: humans have objectives and pursue them, so machines should have objectives and pursue them. They, or should I say we, never really examined this fundamental assumption. It is built into all existing approaches for constructing intelligent systems.

The third principle: Learning to predict human preferences

The third principle, that the ultimate source of information about human preferences is human behavior, serves two purposes.

The first purpose is to provide a definite grounding for the term human preferences. By assumption, human preferences aren’t in the machine and it cannot observe them directly, but there must still be some definite connection between the machine and human preferences. The principle says that the connection is through the observation of human choices: we assume that choices are related in some (possibly very complicated) way to underlying preferences. To see why this connection is essential, consider the converse: if some human preference had no effect whatsoever on any actual or hypothetical choice the human might make, then it would probably be meaningless to say that the preference exists.

The second purpose is to enable the machine to become more useful as it learns more about what we want. (After all, if it knew nothing about human preferences, it would be of no use to us.) The idea is simple enough: human choices reveal information about human preferences. Applied to the choice between pineapple pizza and sausage pizza, this is straightforward. Applied to choices between future lives and choices made with the goal of influencing the robot’s behavior, things get more interesting. In the next chapter I explain how to formulate and solve such problems. The real complications arise, however, because humans are not perfectly rational: imperfection comes between human preferences and human choices, and the machine must take into account those imperfections if it is to interpret human choices as evidence of human preferences.

Not what I mean

Before going into more detail, I want to head off some potential misunderstandings.

The first and most common misunderstanding is that I am proposing to install in machines a single, idealized value system of my own design that guides the machine’s behavior. “Whose values are you going to put in?” “Who gets to decide what the values are?” Or even, “What gives Western, well-off, white male cisgender scientists such as Russell the right to determine how the machine encodes and develops human values?”

I think this confusion comes partly from an unfortunate conflict between the commonsense meaning of value and the more technical sense in which it is used in economics, AI, and operations research. In ordinary usage, values are what one uses to help resolve moral dilemmas; as a technical term, on the other hand, value is roughly synonymous with utility, which measures the degree of desirability of anything from pizza to paradise. The meaning I want is the technical one: I just want to make sure the machines give me the right pizza and don’t accidentally destroy the human race. (Finding my keys would be an unexpected bonus.) To avoid this confusion, the principles talk about human preferences rather than human values, since the former term seems to steer clear of judgmental preconceptions about morality.

“Putting in values” is, of course, exactly the mistake I am saying we should avoid, because getting the values (or preferences) exactly right is so difficult and getting them wrong is potentially catastrophic. I am proposing instead that machines learn to predict better, for each person, which life that person would prefer, all the while being aware that the predictions are highly uncertain and incomplete. In principle, the machine can learn billions of different predictive preference models, one for each of the billions of people on Earth. This is really not too much to ask for the AI systems of the future, given that present-day Facebook systems are already maintaining more than two billion individual profiles.

A related misunderstanding is that the goal is to equip machines with “ethics” or “moral values” that will enable them to resolve moral dilemmas. Often, people bring up the so-called trolley problems,12 where one has to choose whether to kill one person in order to save others, because of their supposed relevance to self-driving cars. The whole point of moral dilemmas, however, is that they are dilemmas: there are good arguments on both sides. The survival of the human race is not a moral dilemma. Machines could solve most moral dilemmas the wrong way (whatever that is) and still have no catastrophic impact on humanity.

Another common supposition is that machines that follow the three principles will adopt all the sins of the evil humans they observe and learn from. Certainly, there are many of us whose choices leave something to be desired, but there is no reason to suppose that machines who study our motivations will make the same choices, any more than criminologists become criminals. Take, for example, the corrupt government official who demands bribes to approve building permits because his paltry salary won’t pay for his children to go to university. A machine observing this behavior will not learn to take bribes; it will learn that the official, like many other people, has a very strong desire for his children to be educated and successful. It will find ways to help him that don’t involve lowering the well-being of others. This is not to say that all cases of evil behavior are unproblematic for machines—for example, machines may need to treat differently those who actively prefer the suffering of others.

Reasons for Optimism

In a nutshell, I am suggesting that we need to steer AI in a radically new direction if we want to retain control over increasingly intelligent machines. We need to move away from one of the driving ideas of twentieth-century technology: machines that optimize a given objective. I am often asked why I think this is even remotely feasible, given the huge momentum behind the standard model in AI and related disciplines. In fact, I am quite optimistic that it can be done.

The first reason for optimism is that there are strong economic incentives to develop AI systems that defer to humans and gradually align themselves to user preferences and intentions. Such systems will be highly desirable: the range of behaviors they can exhibit is simply far greater than that of machines with fixed, known objectives. They will ask humans questions or ask for permission when appropriate; they will do “trial runs” to see if we like what they propose to do; they will accept correction when they do something wrong. On the other hand, systems that fail to do this will have severe consequences. Up to now, the stupidity and limited scope of Al systems has protected us from these consequences, but that will change. Imagine, for example, some future domestic robot charged with looking after your children while you are working late. The children are hungry, but the refrigerator is empty. Then the robot notices the cat. Alas, the robot understands the cats nutritional value but not its sentimental value. Within a few short hours, headlines about deranged robots and roasted cats are blanketing the world’s media and the entire domestic-robot industry is out of business.

The possibility that one industry player could destroy the entire industry through careless design provides a strong economic motivation to form safety-oriented industry consortia and to enforce safety standards. Already, the Partnership on Al, which includes as members nearly all the world’s leading technology companies, has agreed to cooperate to ensure that “Al research and technology is robust, reliable, trustworthy, and operates within secure constraints.” To my knowledge, all the major players are publishing their safety-oriented research in the open literature. Thus, the economic incentive is in operation long before we reach human-level Al and will only strengthen over time. Moreover, the same cooperative dynamic may be starting at the international level—for example, the stated policy of the Chinese government is to “cooperate to preemptively prevent the threat of AL”

A second reason for optimism is that the raw data for learning about human preferences—namely, examples of human behavior—are so abundant. The data come not just in the form of direct observation via camera, keyboard, and touch screen by billions of machines sharing data with one another about billions of humans (subject to privacy constraints, of course) but also in indirect form. The most obvious kind of indirect evidence is the vast human record of books, films, and television and radio broadcasts, which is almost entirely concerned with people doing things (and other people being upset about it). Even the earliest and most tedious Sumerian and Egyptian records of copper ingots being traded for sacks of barley give some insight into human preferences for different commodities.

There are, of course, difficulties involved in interpreting this raw material, which includes propaganda, fiction, the ravings of lunatics, and even the pronouncements of politicians and presidents, but there is certainly no reason for the machine to take it all at face value. Machines can and should interpret all communications from other intelligent entities as moves in a game rather than as statements of fact; in some games, such as cooperative games with one human and one machine, the human has an incentive to be truthful, but in many other situations there are incentives to be dishonest. And of course, whether honest or dishonest, humans may be deluded in their own beliefs.

There is a second kind of indirect evidence that is staring us in the face: the way we have made the world.15 We made it that way because— very roughly—we like it that way. (Obviously, it’s not perfect!) Now, imagine you are an alien visiting Earth while all the humans are away on holiday. As you peer inside their houses, can you begin to grasp the basics of human preferences? Carpets are on floors because we like to walk on soft, warm surfaces and we don’t like loud footsteps; vases are on the middle of the table rather than the edge because we don’t want them to fall and break; and so on—everything that isn’t arranged by nature itself provides clues to the likes and dislikes of the strange bipedal creatures who inhabit this planet.

Reasons for Caution

You may find the Partnership on AI’s promises of cooperation on AI safety less than reassuring if you have been following progress in self driving cars. That field is ruthlessly competitive, for some very good reasons: the first car manufacturer to release a fully autonomous vehicle will gain a huge market advantage; that advantage will be self reinforcing because the manufacturer will be able to collect more data more quickly to improve the system’s performance; and ride-hailing companies such as Uber would quickly go out of business if another company were to roll out fully autonomous taxis before Uber does. This has led to a high-stakes race in which caution and careful engineering appear to be less important than snazzy demos, talent grabs, and premature rollouts.

Thus, life-or-death economic competition provides an impetus to cut corners on safety in the hope of winning the race. In a 2008 retrospective paper on the 1975 Asilomar conference that he co-organized— the conference that led to a moratorium on genetic modification of humans—the biologist Paul Berg wrote,

There is a lesson in Asilomar for all of science: the best way to respond to concerns created by emerging knowledge or early-stage technologies is for scientists from publicly funded institutions to find common cause with the wider public about the best way to regulate—as early as possible. Once scientists from corporations begin to dominate the research enterprise, it will simply be too late.

Economic competition occurs not just between corporations but also between nations. A recent flurry of announcements of multibillion dollar national investments in AI from the United States, China, France, Britain, and the EU certainly suggests that none of the major powers wants to be left behind. In 2017, Russian president Vladimir Putin said, “The one who becomes the leader in [AI] will be the ruler of the world.”17 This analysis is essentially correct. Advanced AI would, as we saw in Chapter 3, lead to greatly increased productivity and rates of innovation in almost all areas. If not shared, it would allow its possessor to outcompete any rival nation or bloc.

Nick Bostrom, in Superintelligence, warns against exactly this motivation. National competition, just like corporate competition, would tend to focus more on advances in raw capabilities and less on the problem of control. Perhaps, however, Putin has read Bostrom; he went on to say, “It would be strongly undesirable if someone wins a monopolist position.” It would also be rather pointless, because human-level Al is not a zero-sum game and nothing is lost by sharing it. On the other hand, competing to be the first to achieve human level Al, without first solving the control problem, is a negative-sum game. The payoff for everyone is minus infinity.

There's only a limited amount that Al researchers can do to influence the evolution of global policy on Al. We can point to possible applications that would provide economic and social benefits; we can warn about possible misuses such as surveillance and weapons; and we can provide roadmaps for the likely path of future developments and their impacts. Perhaps the most important thing we can do is to design Al systems that are, to the extent possible, provably safe and beneficial for humans. Only then will it make sense to attempt general regulation of Al.

8: PROVABLY BENEFICIAL AI

If we are going to rebuild AI along new lines, the foundations must be solid. When the future of humanity is at stake, hope and good intentions—and educational initiatives and industry codes of conduct and legislation and economic incentives to do the right thing— are not enough. All of these are fallible, and they often fail. In such situations, we look to precise definitions and rigorous step-by-step mathematical proofs to provide incontrovertible guarantees.

That’s a good start, but we need more. We need to be sure, to the extent possible, that what is guaranteed is actually what we want and that the assumptions going into the proof are actually true. The proofs themselves belong in journal papers written for specialists, but I think it is useful nonetheless to understand what proofs are and what they can and cannot provide in the way of real safety. The “provably beneficial” in the title of the chapter is an aspiration rather than a promise, but it is the right aspiration.

9: COMPLICATIONS: US

If the world contained one perfectly rational Harriet and one helpful and deferential Robbie, we’d be in good shape. Robbie would gradually learn Harriet’s preferences as unobtrusively as possible and would become her perfect helper. We might hope to extrapolate from this promising beginning, perhaps viewing Harriet and Robbie’s relationship as a model for the relationship between the human race and its machines, each construed monolithically.

Alas, the human race is not a single, rational entity. It is composed of nasty, envy-driven, irrational, inconsistent, unstable, computationally limited, complex, evolving, heterogeneous entities. Loads and loads of them. These issues are the staple diet—perhaps even the raisons d’etre—of the social sciences. To AI we will need to add ideas from psychology, economics, political theory, and moral philosophy.1 We need to melt, re-form, and hammer those ideas into a structure that will be strong enough to resist the enormous strain that increasingly intelligent AI systems will place on it. Work on this task has barely started.

Different Humans

I will start with what is probably the easiest of the issues: the fact that humans are heterogeneous. When first exposed to the idea that machines should learn to satisfy human preferences, people often object that different cultures, even different individuals, have widely different value systems, so there cannot be one correct value system for the machine. But of course, that’s not a problem for the machine: we don’t want it to have one correct value system of its own; we just want it to predict the preferences of others.

The confusion about machines having difficulty with heterogeneous human preferences may come from the mistaken idea that the machine is adopting the preferences it learns—for example, the idea that a domestic robot in a vegetarian household is going to adopt vegetarian preferences. It won’t. It just needs to learn to predict what the dietary preferences of vegetarians are. By the first principle, it will then avoid cooking meat for that household. But the robot also learns about the dietary preferences of the rabid carnivores next door, and, with its owner’s permission, will happily cook meat for them if they borrow it for the weekend to help out with a dinner party. The robot doesn’t have a single set of preferences of its own, beyond the preference for helping humans achieve their preferences.

In a sense, this is no different from a restaurant chef who learns to cook several different dishes to please the varied palates of her clients, or the multinational car company that makes left-hand-drive cars for the US market and right-hand-drive cars for the UK market.

In principle, a machine could learn eight billion preference models, one for each person on Earth. In practice, this isn’t as hopeless as it sounds. For one thing, it’s easy for machines to share what they learn with each other. For another, the preference structures of humans have a great deal in common, so the machine will usually not be learning each model from scratch.

Imagine, for example, the domestic robots that may one day be purchased by the inhabitants of Berkeley, California. The robots come out of the box with a fairly broad prior belief, perhaps tailored for the US market but not for any particular city, political viewpoint, or socioeconomic class. The robots begin to encounter members of the Berkeley Green Party, who turn out, compared to the average American, to have a much higher probability of being vegetarian, of using recycling and composting bins, of using public transportation whenever possible, and so on. Whenever a newly commissioned robot finds itself in a Green household, it can immediately adjust its expectations accordingly. It does not need to begin learning about these particular humans as if it had never seen a human, let alone a Green Party member, before. This adjustment is not irreversible—there may be Green Party members in Berkeley who feast on endangered whale meat and drive gas-guzzling monster trucks—but it allows the robot to be more useful more quickly. The same argument applies to a vast range of other personal characteristics that are, to some degree, predictive of aspects of an individual's preference structures.

Many Humans

The other obvious consequence of the existence of more than one human being is the need for machines to make trade-offs among the preferences of different people. The issue of trade-offs among humans has been the main focus of large parts of the social sciences for centuries. It would be naive for Al researchers to expect that they can simply alight on the correct solutions without understanding what is already known. The literature on the topic is, alas, vast and I cannot possibly do justice to it here—not just because there isn't space but also because I haven’t read most of it. I should also point out that almost all the literature is concerned with decisions made by humans, whereas I am concerned here with decisions made by machines. This makes all the difference in the world, because humans have individual rights that may conflict with any supposed obligation to act on behalf of others, whereas machines do not. For example, we do not expect or require typical humans to sacrifice their lives to save others, whereas we will certainly require robots to sacrifice their existence to save the lives of humans.

Several thousand years of work by philosophers, economists, legal scholars, and political scientists have produced constitutions, laws, economic systems, and social norms that serve to help (or hinder, depending on who’s in charge) the process of reaching satisfactory solutions to the problem of trade-offs. Moral philosophers in particular have been analyzing the notion of rightness of actions in terms of their effects, beneficial or otherwise, on other people. They have studied quantitative models of trade-offs since the eighteenth century under the heading of utilitarianism. This work is directly relevant to our present concerns, because it attempts to define a formula by which moral decisions can be made on behalf of many individuals.

The need to make trade-offs arises even if everyone has the same preference structure, because it’s usually impossible to maximally satisfy everyone’s preferences. For example, if everyone wants to be All-Powerful Ruler of the Universe, most people are going to be disappointed. On the other hand, heterogeneity does make some problems more difficult: if everyone is happy with the sky being blue, the robot that handles atmospheric matters can work on keeping it that way; but if many people are agitating for a color change, the robot will need to think about possible compromises such as an orange sky on the third Friday of each month.

The presence of more than one person in the world has another important consequence: it means that, for each person, there are other people to care about. This means that satisfying the preferences of an individual has implications for other people, depending on the individual’s preferences about the well-being of others.

Utilitarian AI

The reason we have moral philosophy is that there is more than one person on Earth. The approach that is most relevant for understanding how AI systems should be designed is often called consequentialism: the idea that choices should be judged according to expected consequences. The two other principal approaches are deontological ethics and virtue ethics, which are, very roughly, concerned with the moral character of actions and individuals, respectively, quite apart from the consequences of choices.3 Absent any evidence of self awareness on the part of machines, I think it makes little sense to build machines that are virtuous or that choose actions in accordance with moral rules if the consequences are highly undesirable for humanity. Put another way, we build machines to bring about consequences, and we should prefer to build machines that bring about consequences that we prefer.

This is not to say that moral rules and virtues are irrelevant; its just that, for the utilitarian, they are justified in terms of consequences and the more practical achievement of those consequences. This point is made by John Stuart Mill in Utilitarianism:

The proposition that happiness is the end and aim of morality doesn’t mean that no road ought to be laid down to that goal, or that people going to it shouldn’t be advised to take one direction rather than another. . . . Nobody argues that the art of navigation is not based on astronomy because sailors can’t wait to calculate the Nautical Almanack. Because they are rational creatures, sailors go to sea with the calculations already done; and all rational creatures go out on the sea of life with their minds made up on the common questions of right and wrong, as well as on many of the much harder questions of wise and foolish.

This view is entirely consistent with the idea that a finite machine facing the immense complexity of the real world may produce better consequences by following moral rules and adopting a virtuous attitude rather than trying to calculate the optimal course of action from scratch. In the same way, a chess program achieves checkmate more often using a catalog of standard opening move sequences, endgame algorithms, and an evaluation function, rather than trying to reason its way to checkmate with no “moral” guideposts. A consequentialist approach also gives some weight to the preferences of those who believe strongly in preserving a given deontological rule, because unhappiness that a rule has been broken is a real consequence. However, it is not a consequence of infinite weight.

Consequentialism is a difficult principle to argue against— although many have tried!—because it’s incoherent to object to consequentialism on the grounds that it would have undesirable consequences. One cannot say, “But if you follow the consequentialist approach in such-and-such case, then this really terrible thing will happen!” Any such failings would simply be evidence that the theory had been misapplied.

For example, suppose Harriet wants to climb Everest. One might worry that a consequentialist Robbie would simply pick her up and deposit her on top of Everest, since that is her desired consequence. In all probability Harriet would strenuously object to this plan, because it would deprive her of the challenge and therefore of the exultation that results from succeeding in a difficult task through one’s own efforts. Now, obviously, a properly designed consequentialist Robbie would understand that the consequences include all of Harriet’s experiences, not just the end goal. He might want to be available in case of an accident and to make sure she was properly equipped and trained, but he might also have to accept Harriets right to expose herself to an appreciable risk of death.

If we plan to build consequentialist machines, the next question is how to evaluate consequences that affect multiple people. One plausible answer is to give equal weight to everyone’s preferences—in other words, to maximize the sum of everyone’s utilities. This answer is usually attributed to the eighteenth-century British philosopher Jeremy Bentham4 and his pupil John Stuart Mill,5 who developed the philosophical approach of utilitarianism. The underlying idea can be traced to the works of the ancient Greek philosopher Epicurus and appears explicitly in Mozi, a book of writings attributed to the Chinese philosopher of the same name. Mozi was active at the end of the fifth century BCE and promoted the idea of jian ai, variously translated as “inclusive care” or “universal love,” as the defining characteristic of moral actions.

Utilitarianism has something of a bad name, partly because of simple misunderstandings about what it advocates. (It certainly doesn’t help that the word utilitarian means “designed to be useful or practical rather than attractive.”) Utilitarianism is often thought to be incompatible with individual rights, because a utilitarian would, supposedly, think nothing of removing a living person’s organs without permission to save the lives of five others; of course, such a policy would render life intolerably insecure for everyone on Earth, so a utilitarian wouldn’t even consider it. Utilitarianism is also incorrectly identified with a rather unattractive maximization of total wealth and is thought to give little weight to poetry or suffering. In fact, Bentham’s version focused specifically on human happiness, while Mill confidently asserted the far greater value of intellectual pleasures over mere sensations. (“It is better to be a human being dissatisfied than a pig satisfied.”) The ideal utilitarianism of G. E. Moore went even further: he advocated the maximization of mental states of intrinsic worth, epitomized by the aesthetic contemplation of beauty.

I think there is no need for utilitarian philosophers to stipulate the ideal content of human utility or human preferences. (And even less reason for AI researchers to do so.) Humans can do that for themselves. The economist John Harsanyi propounded this view with his principle of preference autonomy :

In deciding what is good and what is bad for a given individual, the ultimate criterion can only be his own wants and his own preferences.

Harsanyi s preference utilitarianism is therefore roughly consistent with the first principle of beneficial AI, which says that a machine’s only purpose is the realization of human preferences. AI researchers should definitely not be in the business of deciding what human preferences should be! Like Bentham, Harsanyi views such principles as a guide for public decisions; he does not expect individuals to be so selfless. Nor does he expect individuals to be perfectly rational—for example, they might have short-term desires that contradict their “deeper preferences.” Finally, he proposes to ignore the preferences of those who, like the sadistic Harriet mentioned earlier, actively wish to reduce the well-being of others.

Harsanyi also gives a kind of proof that optimal moral decisions should maximize the average utility across a population of humans.7 He assumes fairly weak postulates similar to those that underlie utility theory for individuals. (The primary additional postulate is that if everyone in a population is indifferent between two outcomes, then an agent acting on behalf of the population should be indifferent between those outcomes.) From these postulates, he proves what became known as the social aggregation theorem: an agent acting on behalf of a population of individuals must maximize a weighted linear combination of the utilities of the individuals. He further argues that an “impersonal” agent should use equal weights.

The theorem requires one crucial additional (and unstated) assumption: each individual has the same prior factual beliefs about the world and how it will evolve. Now, any parent knows that this isn’t even true for siblings, let alone individuals from different social backgrounds and cultures. So, what happens when individuals differ in their beliefs? Something rather strange:8 the weight assigned to each individual’s utility has to change over time, in proportion to how well that individual’s prior beliefs accord with unfolding reality.

This rather inegalitarian-sounding formula is quite familiar to any parent. Let’s say that Robbie the robot has been tasked with looking after two children, Alice and Bob. Alice wants to go to the movies and is sure it’s going to rain today; Bob, on the other hand, wants to go to the beach and is sure it’s going to be sunny. Robbie could announce, “We’re going to the movies,” making Bob unhappy; or he could announce, “We’re going to the beach,” making Alice unhappy; or he could announce, “If it rains, we’re going to the movies, but if it’s sunny, we’ll go to the beach.” This last plan makes both Alice and Bob happy, because both believe in their own beliefs.

Challenges to utilitarianism

Utilitarianism is one proposal to emerge from humanity’s longstanding search for a moral guide; among many such proposals, it is the most clearly specified—and therefore the most susceptible to loopholes. Philosophers have been finding these loopholes for more than a hundred years. For example, G. E. Moore, objecting to Bentham’s emphasis on maximizing pleasure, imagined a “world in which absolutely nothing except pleasure existed—no knowledge, no love, no enjoyment of beauty, no moral qualities.”9 This finds its modern echo in Stuart Armstrong’s point that superintelligent machines tasked with maximizing pleasure might “entomb everyone in concrete coffins on heroin drips.”10 Another example: in 1945, Karl Popper proposed the laudable goal of minimizing human suffering,11 arguing that it was immoral to trade one person’s pain for another person’s pleasure; R. N. Smart responded that this could best be achieved by rendering the human race extinct.12 Nowadays, the idea that a machine might end human suffering by ending our existence is a staple of debates over the existential risk from A 1.13 A third example is G. E. Moore’s emphasis on the reality of the source of happiness, amending earlier definitions that seemed to have a loophole allowing maximization of happiness through self-delusion. The modern analogs of this point include The Matrix (in which present-day reality turns out to be an illusion produced by a computer simulation) and recent work on the self-delusion problem in reinforcement learning.

These examples, and more, convince me that the AI community should pay careful attention to the thrusts and counterthrusts of philosophical and economic debates on utilitarianism because they are directly relevant to the task at hand. Two of the most important, from the point of view of designing AI systems that will benefit multiple individuals, concern interpersonal comparisons of utilities and comparisons of utilities across different population sizes. Both of these debates have been raging for 150 years or more, which leads one to suspect their satisfactory resolution may not be entirely straightforward.

The debate on interpersonal comparisons of utilities matters because Robbie cannot maximize the sum of Alice’s and Bob’s utilities unless those utilities can be added; and they can be added only if they are measurable on the same scale. The nineteenth-century British logician and economist William Stanley Jevons (also the inventor of an early mechanical computer called the logical piano) argued in 1871 that interpersonal comparisons are impossible:

The susceptibility of one mind may, for what we know, be a thousand times greater than that of another. But, provided that the susceptibility was different in a like ratio in all directions, we should never be able to discover the profoundest difference. Every mind is thus inscrutable to every other mind, and no common denominator of feeling is possible. The American economist Kenneth Arrow, founder of modern social choice theory and 1972 Nobel laureate, was equally adamant:

The viewpoint will be taken here that interpersonal comparison of utilities has no meaning and, in fact, there is no meaning relevant to welfare comparisons in the measurability of individual utility.

The difficulty to which Jevons and Arrow are referring is that there is no obvious way to tell if Alice values pinpricks and lollipops at -1 and +1 or -1000 and +1000 in terms of her subjective experience of happiness. In either case, she will pay up to one lollipop to avoid one pinprick. Indeed, if Alice is a humanoid automaton, her external behavior might be the same even though there is no subjective experience of happiness whatsoever.

In 1974, the American philosopher Robert Nozick suggested that even if interpersonal comparisons of utility could be made, maximizing the sum of utilities would still be a bad idea because it would fall foul of the utility monster—a person whose experiences of pleasure and pain are many times more intense than those of ordinary people.16 Such a person could assert that any additional unit of resources would yield a greater increment to the sum total of human happiness if given to him rather than to others; indeed, removing resources from others to benefit the utility monster would also be a good idea.

This might seem to be an obviously undesirable consequence, but consequentialism by itself cannot come to the rescue: the problem lies in how we measure the desirability of consequences. One possible response is that the utility monster is merely theoretical—there are no such people. But this response probably won’t do: in a sense, all humans are utility monsters relative to, say, rats and bacteria, which is why we pay little attention to the preferences of rats and bacteria in setting public policy.

If the idea that different entities have different utility scales is already built into our way of thinking, then it seems entirely possible that different people have different scales too.

10: PROBLEM SOLVED?

If we succeed in creating provably beneficial AI systems, we would eliminate the risk that we might lose control over superintelligent machines. Humanity could proceed with their development and reap the almost unimaginable benefits that would flow from the ability to wield far greater intelligence in advancing our civilization. We would be released from millennia of servitude as agricultural, industrial, and clerical robots and we would be free to make the best of life’s potential. From the vantage point of this golden age, we would look back on our lives in the present time much as Thomas Hobbes imagined life without government: solitary, poor, nasty, brutish, and short.

Or perhaps not. Bondian villains may circumvent our safeguards and unleash uncontrollable superintelligences against which humanity has no defense. And if we survive that, we may find ourselves gradually enfeebled as we entrust more and more of our knowledge and skills to machines. The machines may advise us not to do this, understanding the long-term value of human autonomy, but we may overrule them.

Beneficial Machines

The standard model underlying a good deal of twentieth-century technology relies on machinery that optimizes a fixed, exogenously supplied objective. As we have seen, this model is fundamentally flawed. It works only if the objective is guaranteed to be complete and correct, or if the machinery can easily be reset. Neither condition will hold as AI becomes increasingly powerful.

If the exogenously supplied objective can be wrong, then it makes no sense for the machine to act as if it is always correct. Hence my proposal for beneficial machines: machines whose actions can be expected to achieve our objectives. Because these objectives are in us, and not in them, the machines will need to learn more about what we really want from observations of the choices we make and how we make them. Machines designed in this way will defer to humans: they will ask permission; they will act cautiously when guidance is unclear; and they will allow themselves to be switched off.

While these initial results are for a simplified and idealized setting, I believe they will survive the transition to more realistic settings. Already, my colleagues have successfully applied the same approach to practical problems such as self-driving cars interacting with human drivers.1 For example, self-driving cars are notoriously bad at handling four-way stop signs when it’s not clear who has the right of way. By formulating this as an assistance game, however, the car comes up with a novel solution: it actually backs up a little bit to show that it’s definitely not planning to go first. The human understands this signal and goes ahead, confident that there will be no collision. Obviously, we human experts could have thought of this solution and programmed it into the vehicle, but that’s not what happened; this is a form of communication that the vehicle invented entirely by itself.

As we gain more experience in other settings, I expect that we will be surprised by the range and fluency of machine behaviors as they interact with humans. We are so used to the stupidity of machines that execute inflexible, preprogrammed behaviors or pursue definite but incorrect objectives that we may be shocked by how sensible they become. The technology of provably beneficial machines is the core of a new approach to AI and the basis for a new relationship between humans and machines.

It seems possible, also, to apply similar ideas to the redesign of other “machines” that ought to be serving humans, beginning with ordinary software systems. We are taught to build software by composing subroutines, each of which has a well-defined specification that says what the output should be for any given input—just like the square-root button on a calculator. This specification is the direct analog of the objective given to an AI system. The subroutine is not supposed to terminate and return control to the higher layers of the software system until it has produced an output that meets the specification. (This should remind you of the AI system that persists in its single-minded pursuit of its given objective.) A better approach would be to allow for uncertainty in the specification. For example, a subroutine that carries out some fearsomely complicated mathematical computation is typically given an error bound that defines the required precision for the answer and has to return a solution that is correct within that error bound. Sometimes, this may require weeks of computation. Instead, it might be better to be less precise about the allowed error, so that the subroutine could come back after twenty seconds and say, “I’ve found a solution that’s this good. Is that OK or do you want me to continue?” In some cases, the question may percolate all the way to the top level of the software system, so that the human user can provide further guidance to the system. The human’s answers would then help in refining the specifications at all levels.

The same kind of thinking can be applied to entities such as governments and corporations. The obvious failings of government inelude paying too much attention to the preferences (financial as well as political) of those in government and too little attention to the preferences of the governed. Elections are supposed to communicate preferences to the government, but they seem to have a remarkably small bandwidth (on the order of one byte of information every few years) for such a complex task. In far too many countries, government is simply a means for one group of people to impose its will on others. Corporations go to greater lengths to learn the preferences of customers, whether through market research or direct feedback in the form of purchase decisions. On the other hand, the molding of human preferences through advertising, cultural influences, and even chemical addiction is an accepted way of doing business.

Governance of AI

AI has the power to reshape the world, and the process of reshaping will have to be managed and guided in some way. If the sheer number of initiatives to develop effective governance of AI is any guide, then we are in excellent shape. Everyone and their uncle is setting up a Board or a Council or an International Panel. The World Economic Forum has identified nearly three hundred separate efforts to develop ethical principles for AI. My email inbox can be summarized as one long invitation to the Global World Summit Conference Forum on the Future of International Governance of the Social and Ethical Impacts of Emerging Artificial Intelligence Technologies.

This is all very different from what happened with nuclear technology. After World War II, the United States held all the nuclear cards. In 1953, US president Dwight Eisenhower proposed to the UN an international body to regulate nuclear technology. In 1957, the International Atomic Energy Agency started work; it is the sole global overseer for the safe and beneficial development of nuclear energy.

In contrast, many hands hold AI cards. To be sure, the United States, China, and the EU fund a lot of AI research, but almost all of it occurs outside secure national laboratories. AI researchers in universities are part of a broad, cooperative international community, glued together by shared interests, conferences, cooperative agreements, and professional societies such as AAAI (the Association for the Advancement of Artificial Intelligence) and IEEE (the Institute of Electrical and Electronics Engineers, which includes tens of thousands of AI researchers and practitioners). Probably the majority of investment in AI research and development is now occurring within corporations, large and small; the leading players as of 2019 are Google (including Deep- Mind), Facebook, Amazon, Microsoft, and IBM in the United States and Tencent, Baidu, and, to some extent, Alibaba in China—all among the largest corporations in the world.2 All but Tencent and Alibaba are members of the Partnership on AI, an industry consortium that includes among its tenets a promise of cooperation on AI safety. Finally, although the vast majority of humans possess little in the way of AI expertise, there is at least a superficial willingness among other players to take the interests of humanity into account.

These, then, are the players who hold the majority of the cards. Their interests are not in perfect alignment but all share a desire to maintain control over AI systems as they become more powerful. (Other goals, such as avoiding mass unemployment, are shared by governments and university researchers, but not necessarily by corporations that expect to profit in the short term from the widest possible deployment of AI.) To cement this shared interest and achieve coordinated action, there are organizations with convening power, which means, roughly, that if the organization sets up a meeting, people accept the invitation to participate. In addition to the professional societies, which can bring AI researchers together, and the Partnership on AI, which combines corporations and nonprofit institutes, the canonical conveners are the UN (for governments and researchers) and the World Economic Forum (for governments and corporations). In addition, the G7 has proposed an International Panel on Artificial Intelligence, hoping that it will grow into something like the UN’s Intergovernmental Panel on Climate Change. Important-sounding reports are multiplying like rabbits.

With all this activity, is there any prospect of actual progress on governance occurring? Perhaps surprisingly, the answer is yes, at least around the edges. Many governments around the world are equipping themselves with advisory bodies to help with the process of developing regulations; perhaps the most prominent example is the EU’s High-Level Expert Group on Artificial Intelligence. Agreements, rules, and standards are beginning to emerge for issues such as user privacy, data exchange, and avoiding racial bias. Governments and corporations are working hard to sort out the rules for self-driving cars—rules that will inevitably have cross-border elements. There is a consensus that AI decisions must be explainable if AI systems are to be trusted, and that consensus is already partially implemented in the EU’s GDPR legislation. In California, a new law forbids AI systems to impersonate humans in certain circumstances. These last two items— explainability and impersonation—certainly have some bearing on issues of AI safety and control.

At present, there are no implementable recommendations that can be made to governments or other organizations considering the issue of maintaining control over AI systems. A regulation such as “AI systems must be safe and controllable” would carry no weight, because these terms do not yet have precise meanings and because there is no widely known engineering methodology for ensuring safety and controllability. But lets be optimistic and imagine that, a few years down the line, the validity of the “provably beneficial” approach to AI has been established through both mathematical analysis and practical realization in the form of useful applications. We might, for example, have personal digital assistants that we can trust to use our credit » cards, screen our calls and emails, and manage our finances because they have adapted to our individual preferences and know when it’s OK to go ahead and when it’s better to ask for guidance. Our campaign against cybercrime, including expansion of the Budapest Convention on Cybercrime. This would form an organizational template for possible future efforts to prevent the emergence of uncontrolled AI programs. At the same time, it would engender a broad cultural understanding that creating such programs, either deliberately or inadvertently, is in the long run a suicidal act comparable to creating pandemic organisms.

Enfeeblement and Human Autonomy

E. M. Forster s most famous novels, including Howards End and A Passage to India, examined British society and its class system in the early part of the twentieth century. In 1909, he wrote one notable science-fiction story: “The Machine Stops.” The story is remarkable for its prescience, including depictions of (what we would now call) the Internet, videoconferencing, iPads, massive open online courses (MOOCs), widespread obesity, and avoidance of face-to-face contact. The Machine of the title is an all-encompassing intelligent infrastructure that meets all human needs. Humans become increasingly dependent on it, but they understand less and less about how it works. Engineering knowledge gives way to ritualized incantations that eventually fail to stem the gradual deterioration of the Machine’s workings. Kuno, the main character, sees what is unfolding but is powerless to stop it:

Cannot you see . . . that it is we that are dying, and that down here the only thing that really lives is the Machine? We created the Machine to do our will, but we cannot make it do our will now. It has robbed us of the sense of space and of the sense of touch, it has blurred every human relation, it has paralysed our bodies and our wills.. .. We only exist as the blood corpuscles that course through its arteries, and if it could work without us, it would let us die. Oh, I have no remedy—or, at least, only one—to tell men again and again that I have seen the hills of Wessex as Aelfrid saw them when he overthrew the Danes.

More than one hundred billion people have lived on Earth. They (we) have spent on the order of one trillion person-years learning and teaching, in order that our civilization may continue. Up to now, its only possibility for continuation has been through re-creation in the minds of new generations. (Paper is fine as a method of transmission, but paper does nothing until the knowledge recorded thereon reaches the next person’s mind.) That is now changing: increasingly, it is possible to place our knowledge into machines that, by themselves, can run our civilization for us.

Once the practical incentive to pass our civilization on to the next generation disappears, it will be very hard to reverse the process. One trillion years of cumulative learning would, in a real sense, be lost. We would become passengers in a cruise ship run by machines, on a cruise that goes on forever—exactly as envisaged in the film WALL-E.

A good consequentialist would say, “Obviously this is an undesirable consequence of the overuse of automation! Suitably designed machines would never do this!” True, but think what this means. Machines may well understand that human autonomy and competence are important aspects of how we prefer to conduct our lives. They may well insist that humans retain control and responsibility for their own well-being—in other words, machines will say no. But we myopic, lazy humans may disagree. There is a tragedy of the commons at work here: for any individual human, it may seem pointless to engage in years of arduous learning to acquire knowledge and skills that machines already have; but if everyone thinks that way, the human race will, collectively, lose its autonomy.

The solution to this problem seems to be cultural, not technical. We will need a cultural movement to reshape our ideals and preferences towards autonomy, agency, and ability and away from self-indulgence and dependency—if you like, a modern, cultural version of ancient Sparta’s military ethos. This would mean human preference engineering on a global scale along with radical changes in how our society works. To avoid making a bad situation worse, we might need the help of superintelligent machines, both in shaping the solution and in the actual process of achieving a balance for each individual.

Any parent of a small child is familiar with this process. Once the child is beyond the helpless stage, parenting requires an ever-evolving balance between doing everything for the child and leaving the child entirely to his or her own devices. At a certain stage, the child comes to understand that the parent is perfectly capable of tying the child’s shoelaces but is choosing not to. Is that the future for the human race—to be treated like a child, forever, by far superior machines? I suspect not. For one thing, children cannot switch their parents off. (Thank goodness!) Nor will we be pets or zoo animals. There is really no analog in our present world to the relationship we will have with beneficial intelligent machines in the future. It remains to be seen how the endgame turns out.

/ 019. Brynjolfsson, Erik et al. The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies, 2014.

Where We Are

This work led us to three broad conclusions.

The first is that we’re living in a time of astonishing progress with digital technologies—those that have computer hardware, software, and networks at their core. These technologies are not brand-new; businesses have been buying computers for more than half a century, and Time magazine declared the personal computer its “Machine of the Year” in 1982. But just as it took generations to improve the steam engine to the point that it could power the Industrial Revolution, it’s also taken time to refine our digital engines.

We’ll show why and how the full force of these technologies has recently been achieved and give examples of its power. “Full,” though, doesn’t mean “mature.” Computers are going to continue to improve and to do new and unprecedented things. By “full force,” we mean simply that the key building blocks are already in place for digital technologies to be as important and transformational to society and the economy as the steam engine. In short, we’re at an inflection point—a point where the curve starts to bend a lot— because of computers. We are entering a second machine age.

Our second conclusion is that the transformations brought about by digital technology will be profoundly beneficial ones. We’re heading into an era that won’t just be different; it will be better, because we’ll be able to increase both the variety and the volume of our consumption. When we phrase it that way —in the dry vocabulary of economics—it almost sounds unappealing. Who wants to consume more and more all the time? But we don’t just consume calories and gasoline. We also consume information from books and friends, entertainment from superstars and amateurs, expertise from teachers and doctors, and countless other things that are not made of atoms. Technology can bring us more choice and even freedom.

When these things are digitized—when they’re converted into bits that can be stored on a computer and sent over a network—they acquire some weird and wonderful properties. They’re subject to different economics, where abundance is the norm rather than scarcity. As we’ll show, digital goods are not like physical ones, and these differences matter.

Of course, physical goods are still essential, and most of us would like them to have greater volume, variety, and quality. Whether or not we want to eat more, we’d like to eat better or different meals. Whether or not we want to burn more fossil fuels, we’d like to visit more places with less hassle. Computers are helping accomplish these goals, and many others. Digitization is improving the physical world, and these improvements are only going to become more important. Among economic historians there’s wide agreement that, as Martin Weitzman puts it, “the long-term growth of an advanced economy is dominated by the behavior of technical progress.”12 As we’ll show, technical progress is improving exponentially.

Our third conclusion is less optimistic: digitization is going to bring with it some thorny challenges. This in itself should not be too surprising or alarming; even the most beneficial developments have unpleasant consequences that must be managed. The Industrial Revolution was accompanied by soot-filled London skies and horrific exploitation of child labor. What will be their modern equivalents? Rapid and accelerating digitization is likely to bring economic rather than environmental disruption, stemming from the fact that as computers get more powerful, companies have less need for some kinds of workers. Technological progress is going to leave behind some people, perhaps even a lot of people, as it races ahead. As we’ll demonstrate, there’s never been a better time to be a worker with special skills or the right education, because these people can use technology to create and capture value. However, there’s never been a worse time to be a worker with only ‘ordinary’ skills and abilities to offer, because computers, robots, and other digital technologies are acquiring these skills and abilities at an extraordinary rate.

Over time, the people of England and other countries concluded that some aspects of the Industrial Revolution were unacceptable and took steps to end them (democratic government and technological progress both helped with this). Child labor no longer exists in the UK, and London air contains less smoke and sulfur dioxide now than at any time since at least the late 1500s.13 The challenges of the digital revolution can also be met, but first we have to be clear on what they are. It’s important to discuss the likely negative consequences of the second machine age and start a dialogue about how to mitigate them—we are confident that they’re not insurmountable. But they won’t fix themselves, either. We’ll offer our thoughts on this important topic in the chapters to come.

So this is a book about the second machine age unfolding right now—an inflection point in the history of our economies and societies because of digitization. It’s an inflection point in the right direction—bounty instead of scarcity, freedom instead of constraint—but one that will bring with it some difficult challenges and choices.

This book is divided into three sections. The first, composed of chapters 1 through 6, describes the fundamental characteristics of the second machine age. These chapters give many examples of recent technological progress that seem like the stuff of science fiction, explain why they’re happening now (after all, we’ve had computers for decades), and reveal why we should be confident that the scale and pace of innovation in computers, robots, and other digital gear is only going to accelerate in the future.

The second part, consisting of chapters 7 through 11, explores bounty and spread, the two economic consequences of this progress. Bounty is the increase in volume, variety, and quality and the decrease in cost of the many offerings brought on by modern technological progress. It’s the best economic news in the world today. Spread, however, is not so great; it’s everbigger differences among people in economic success—in wealth, income, mobility, and other important measures. Spread has been increasing in recent years. This is a troubling development for many reasons, and one that will accelerate in the second machine age unless we intervene.

The final section—chapters 12 through 15—discusses what interventions will be appropriate and effective for this age. Our economic goals should be to maximize the bounty while mitigating the negative effects of the spread. We’ll offer our ideas about how to best accomplish these aims, both in the short term and in the more distant future, when progress really has brought us into a world so technologically advanced that it seems to be the stuff of science fiction. As we stress in our concluding chapter, the choices we make from now on will determine what kind of world that is.

CHAPTER 7: COMPUTING BOUNTY

“Most economic fallacies derive from the tendency to assume that there is a fixed pie, that one party can gain only at the expense of another.” —Milton Friedman

EACH DAY GOVERNMENT AGENCIES, think tanks, NGOs, and academic researchers generate more statistics than any person could read, let alone absorb. On television, in the pages of the business press, and in the blogosphere, a chorus of analysts debate and predict trends in interest rates, unemployment, stock prices, deficits and myriad other indicators. But when you zoom out and consider trends over the past century, one overwhelming fact looms above all others: overall living standards have increased enormously in the United States and worldwide. In the United States, the rate of GDP growth per person has averaged 1.9 percent per year going back to the early 1800s.1 Applying the rule of 70 (the time to double a value is roughly equal to 70 divided by its growth rate), we see that this was enough to double living standards every thirty-six years, quadrupling them over the course of a typical lifetime.

This increase is important because economic growth can help solve a host of other challenges. If GDP of the United States grows just 1 percent faster each year than currently projected, Americans would be five trillion dollars richer by 2033.2 If GDP grows just 0.5 percent faster, the U.S. budget problem would be solved without any changes to policy.3 Of course, slower growth would make it significantly harder to close the deficit, let alone increase spending on any new initiatives or cut taxes.

Productivity Growth

But what drives increases in GDP per person? Part of it comes from using more resources. But most of it comes from increases in our ability to get more output from the given level of inputs—in other words, increases in productivity. (Most commonly, this term is used as shorthand for ‘labor productivity,’ which is output per hour worked [or output per worker].) * In turn, productivity growth comes from innovations in technology and techniques of production.

Simply working more hours does not increase productivity. Indeed, Americans once routinely worked fifty, sixty, or even seventy hours per week. While some still do, the average workweek is shorter now (thirty-five hours per week), and yet living standards are higher. Robert Solow got his Nobel Prize in Economics for showing that increases in labor input and capital input could not explain most of the increase in the total output of the economy.† In fact, it would take the average American only eleven hours of labor per week to produce as much as he or she produced in forty hours in 1950. That rate of improvement is comparable for workers in Europe and Japan, and even higher in some developing nations.

Productivity improvement was particularly rapid in the middle part of the twentieth century, especially the 1940s, 50s, and 60s, as the technologies of the first machine age, from electricity to the internal combustion engine, started firing on all cylinders. However, in 1973 productivity growth slowed down (see figure 7.1).

In 1987, Bob Solow himself noted that the slowdown seemed to coincide with the early days of the computer revolution, famously remarking, “We see the computer age everywhere, except in the productivity statistics.”4 In 1993, Erik published an article evaluating the “Productivity Paradox” that noted the computers were still a small share of the economy and that complementary innovations were typically needed before general purpose technologies like IT had their real impact.5 Later work taking into account more detailed data on productivity and IT use among individual firms revealed a strong and significant correlation: the heaviest IT users were dramatically more productive than their competitors.6 By the mid-1990s, these benefits were big enough to become visible in the overall U.S. economy, which experienced a general productivity surge. While this rise had a number of causes, economists now attribute the lion’s share of those gains to the power of IT.

The productivity slowdown in the 1970s, and the subsequent speed-up twenty years later, had an interesting precedent. In the late 1890s, electricity was being introduced to American factories. But the “productivity paradox” of that era was that labor productivity growth did not take off for over twenty years. While the technologies involved were very different, many of the underlying dynamics were quite similar.

University of Chicago economist Chad Syverson looked closely at the underlying productivity data and showed how eerily close this analogy is.8 As shown in figure 7.2, the slow start and subsequent acceleration of productivity growth in the electricity era matches well with the speed-up that began in the 1990s. The key to understanding this pattern is the realization that, as discussed in chapter 5, GPTs always need complements. Coming up with those can take years, or even decades, and this creates lags between the introduction of a technology and the productivity benefits. We’ve clearly seen this with both electrification and computerization.

Perhaps the most important complementary innovations are the business process changes and organizational coinventions that new technologies make possible. Paul David, an economic historian at Stanford University and the University of Oxford, examined the records of American factories when they first electrified and found that they often retained a similar layout and organization to those that were powered by steam engines.9 In a steam engine–driven plant, power was transmitted via a large central axle, which in turn drove a series of pulleys, gears, and smaller crankshafts. If the axle was too long the torsion involved would break it, so machines needed to be clustered near the main power source, with those requiring the most power positioned closest. Exploiting all three dimensions, industrial engineers put equipment on floors above and below the central steam engines to minimize the distances involved.

Years later, when that hallowed GPT electricity replaced the steam engine, engineers simply bought the largest electric motors they could find and stuck them where the steam engines used to be. Even when brand-new factories were built, they followed the same design. Perhaps unsurprisingly, records show that the electric motors did not lead to much of an improvement in performance. There might have been less smoke and a little less noise, but the new technology was not always reliable. Overall, productivity barely budged.

Only after thirty years—long enough for the original managers to retire and be replaced by a new generation—did factory layouts change. The new factories looked much like those we see today: a single story spread out over an acre or more. Instead of a single massive engine, each piece of equipment had its own small electric motor. Instead of putting the machines needing the most power closest to the power source, the layout was based on a simple and powerful new principle: the natural workflow of materials.

Productivity didn’t merely inch upward on the resulting assembly lines; it doubled or even tripled. What’s more, for most of the subsequent century, additional complementary innovations, from lean manufacturing and steel minimills to Total Quality Management and Six Sigma principles, continued to boost manufacturing productivity.

As with earlier GPTs, significant organizational innovation is required to capture the full benefit of second machine age technologies. Tim Berners- Lee’s invention of the World Wide Web in 1989, to take an obvious example, initially benefited only a small group of particle physicists. But due in part to the power of digitization and networks to speed the diffusion of ideas, complementary innovations are happening faster than they did in the first machine age. Less than ten years after its introduction, entrepreneurs were finding ways to use the Web to reinvent publishing and retailing.

While less visible, the large enterprise-wide IT systems that companies rolled out in the 1990s have had an even bigger impact on productivity.10 They did this mainly by making possible a wave of business process redesign. For example, Walmart drove remarkable efficiencies in retailing by introducing systems that shared point-of-sale data with their suppliers. The real key was the introduction of complementary process innovations like vendor managed inventory, cross-docking, and efficient consumer response that have become staple business-school case studies. They not only made it possible to increase sales from $1 billion a week in 1993 to $1 billion every thirty-six hours in 2001, but also helped drive dramatic increases in the entire retailing and distribution industries, accounting for much of the additional productivity growth nationwide during this period.

IT investment soared in the 1990s, peaking with a surge of investment in the latter half of the decade as many companies upgraded their systems to take advantage of the Internet, implement large enterprise systems, and avoid the much-hyped Y2K bug. At the same time, innovation in semiconductors took gigantic leaps, so the surging spending on IT delivered even more rapidly increasing levels of computer power. A decade after the computer productivity paradox was popularized, Harvard’s Dale Jorgenson, working with Kevin Stiroh at the New York Federal Reserve Bank did a careful growth accounting and concluded, “A consensus has emerged that a large portion of the acceleration through 2000 can be traced to the sectors of the economy that produce information technology or use IT equipment and software most intensively.”12 But it’s not just the computer-producing sectors that are doing well. Kevin Stiroh of the New York Federal Reserve Bank found that industries that were heavier users of IT tended to be more productive throughout the 1990s. This pattern was even more evident in recent years, according to a careful study by Harvard’s Dale Jorgenson and two coauthors. They found that total factor productivity growth increased more between the 1990s and 2000s in IT-using industries, while it fell slightly in those sectors of the economy that did not use IT extensively.

It’s important to note that the correlation between computers and productivity is not just evident at the industry level; it occurs at the level of individual firms as well. In work Erik did with Lorin Hitt of the University of Pennsylvania Wharton School, he found that firms that use more IT tend to have higher levels of productivity and faster productivity growth than their industry competitors.

The first five years of the twenty-first century saw a renewed wave of innovation and investment, this time less focused on computer hardware and more focused on a diversified set of applications and process innovations. For instance, as Andy described in a case study he did for Harvard Business School, CVS found that their prescription drug ordering process was a source of customer frustration, so they redesigned and simplified it.15 By embedding the steps in an enterprise-wide software system, they were able to replicate the drug ordering process in over four thousand locations, dramatically boosting customer satisfaction and ultimately profits. CVS was not atypical. In a statistical analysis of over six hundred firms that Erik did with Lorin Hitt, he found it takes an average five to seven years before full productivity benefits of computers are visible in the productivity of the firms making the investments. This reflects the time and effort required to make the other complementary investments that bring a computerization effort success. In fact, for every dollar of investment in computer hardware, companies need to invest up to another nine dollars in software, training, and business process redesign.

The effects of organizational changes like these became increasingly visible in the industry-level productivity statistics.17 The productivity surge in the 1990s was most visible in computer-producing industries, but overall productivity grew even faster in the early years of the twenty-first century, when a much broader set of industries saw significant productivity gains. Like earlier GPTs, the power of computers was their ability to affect productivity far from their ‘home’ industry.

Overall, American productivity growth in the decade following the year 2000 exceeded even the high growth rates of the roaring 1990s, which in turn was higher than 1970s or 1980s growth rates had been.

Today American workers are more productive than they’ve ever been, but a closer look at recent numbers tells a more nuanced story. The good performance since the year 2000 was clustered in the early years of the decade. Since 2005, productivity growth has not been as strong. As noted in chapter 5, this has led to a new wave of worries about the “end of growth” by economists, journalists, and bloggers. We are not convinced by the pessimists. The productivity lull after the introduction of electricity did not mean the end of growth, nor did the lull in the 1970s.

Part of the recent slowdown simply reflects the Great Recession and its aftermath. Recessions are always times of pessimism, which is understandable, and the pessimism invariably spills over into predictions about technology and the future. The financial crisis and burst of the housing bubble led to a collapse of consumer confidence and wealth, which translated into dramatically lower demand and GDP. While the recession technically ended in June 2009, as we write this in 2013 the U.S. economy is still operating well below its potential, with unemployment at 7.6 percent and capacity utilization at 78 percent. During such a slump, any metric that includes output in the numerator, such as labor productivity, will often be at least temporarily depressed. In fact, when you look at history, you see that in the early years of the Great Depression, in the 1930s, productivity didn’t just slow but actually fell for two years in a row—something it never did in the recent slump. Growth pessimists had even more company in the 1930s than they do today, but the following three decades proved to be the best ones of the twentieth century. Go back to figure 7.2 and look most closely at the dashed line charting the years following the dip in productivity in the early 1930s. You’ll see the biggest wave of growth and bounty that the first machine age ever delivered.

The explanation for this productivity surge is in the lags that we always see when GPTs are installed. The benefits of electrification stretched for nearly a century as more and more complementary innovations were implemented. The digital GPTs of the second machine age are no less profound. Even if Moore’s Law ground to a halt today, we could expect decades of complementary innovations to unfold and continue to boost productivity. However, unlike the steam engine or electricity, second machine age technologies continue to improve at a remarkably rapid exponential pace, replicating their power with digital perfection and creating even more opportunities for combinatorial innovation. The path won’t be smooth—for one thing, we haven’t banished the business cycle—but the fundamentals are in place for bounty that vastly exceeds anything we’ve ever seen before.

CHPATER 8: BEYOND GDP

“The Gross National Product does not include the beauty of our poetry or the intelligence of our public debate. It measures neither our wit nor our courage, neither our wisdom nor our learning, neither our compassion nor our devotion. It measures everything, in short, except that which makes life worthwhile.” —Robert F. Kennedy

WHEN PRESIDENT HOOVER WAS trying to understand what was happening during the Great Depression and design a program to fight it, a comprehensive system of national accounts did not exist. He had to rely on scattered data like freight car loadings, commodity prices, and stock price indexes that gave only an incomplete and often unreliable view of economic activity. The first set of national accounts was presented to Congress in 1937 based on the pioneering work of Nobel Prize winner Simon Kuznets, who worked with researchers at the National Bureau of Economic Research and a team at the U.S. Department of Commerce. The resulting set of metrics have served as beacons that helped illuminate many of the dramatic changes that transformed the economy throughout the twentieth century.

But as the economy has changed so, too, must our metrics. More and more what we care about in the second machine age are ideas, not things—mind, not matter; bits, not atoms; and interactions, not transactions. The great irony of this information age is that, in many ways, we actually know less about the sources of value in the economy than we did fifty years ago. In fact, much of the change has been invisible for a long time simply because we did not know what to look for. There’s a huge layer of the economy unseen in the official data and, for that matter, unaccounted for on the income statements and balance sheets of most companies. Free digital goods, the sharing economy, intangibles and changes in our relationships have already had big effects on our well-being. They also call for new organizational structures, new skills, new institutions, and perhaps even a reassessment of some of our values.


/ 020. Nilsson, Nils J. "The Quest for Artificial Intelligence: A History of Ideas and Achievements," 2010.


Part I: Beginnings

Chapter 1: Dreams and Dreamers

The quest for artificial intelligence (AI) begins with dreams – as all quests do. People have long imagined machines with human abilities – automata that move and devices that reason.  Human-like machines are described in many stories and are pictured in sculptures, paintings, and drawings.

You may be familiar with many of these, but let me mention a few.  The Iliad of Homer  talks about self-propelled chairs called “tripods” and golden “attendants” constructed by Hephaistos, the lame blacksmith god, to help him get around.1∗ And, in the ancient Greek myth as retold by Ovid in his Metamorphoses, Pygmalian sculpts an ivory statue of a beautiful maiden, Galatea, which Venus brings to life:

The girl felt the kisses he gave, blushed, and, raising her bashful eyes to the light, saw both her lover and the sky.

The ancient Greek philosopher Aristotle (384–322 BCE) dreamed of automation also, but apparently he thought it an impossible fantasy – thus making slavery necessary if people were to enjoy leisure.  In his The Politics, he wrote

For suppose that every tool we had could perform its task, either at our bidding or itself perceiving the need, and if – like. . . the tripods of Hephaestus, of which the poet [that is, Homer] says that “self-moved they enter the assembly of gods” – shuttles in a loom could fly to and fro and a plucker [the tool used to pluck the strings] play a lyre of their own accord, then master craftsmen would have no need of servants nor masters of slaves.

Aristotle might have been surprised to see a Jacquard loom weave of itself or a player piano doing its own playing.

Pursuing his own visionary dreams, Ramon Llull (circa 1235–1316), a Catalan mystic and poet, produced a set of paper discs called the Ars Magna (Great Art), which was intended, among other things, as a debating tool for winning Muslims to the Christian faith through logic and reason. One of his disc assemblies was inscribed with some of the attributes of God, namely goodness, greatness, eternity, power, wisdom, will, virtue, truth, and glory. Rotating the discs apropriately was supposed to produce answers
to various theological questions.

Ahead of his time with inventions (as usual), Leonardo Da Vinci sketched designs for a humanoid robot in the form of a medieval knight around the year 1495. (See Fig. 1.2.) No one knows whether Leonardo or contemporaries tried to build his design. Leonardo's knight was supposed to be able to sit up, move its arms and head, and open its jaw.

The Talmud talks about holy persons creating artificial creatures called golems." These, like Adam, were usually created from earth. There are stories about rabbis using golems as servants. Like the Sorcerer's Apprentice, golems were sometimes difficult to control.

In 1651, Thomas Hobbes (1588-1679) published his book Leviathan about the social contract and the ideal state. In the introduction Hobbes seems to say that it might be possible to build an artificial animal."

For seeing life is but a motion of limbs, the beginning whereof is in some principal part within, why may we not say that all automata (engines that move themselves by springs and wheels as doth a watch) have an artificial life? For what is the heart, but a spring; and the nerves, but so many strings; and the joints, but so many wheels, giving motion to the whole body. . .

Perhaps for this reason, the science historian George Dyson refers to Hobbes as the \patriarch of artificial intelligence."

In addition to fictional artifices, several people constructed actual automata that moved in startlingly lifelike ways.8 The most sophisticated of these was the mechanical duck designed and built by the French inventor and engineer, Jacques de Vaucanson (1709{1782). In 1738, Vaucanson displayed his masterpiece, which could quack, ap its wings, paddle, drink water, and eat and "digest" grain.

As Vaucanson himself put it,

My second Machine, or Automaton, is a Duck, in which I represent the Mechanism of the Intestines which are employed in the Operations of Eating, Drinking, and Digestion: Wherein the Working of all the Parts necessary for those Actions is exactly imitated. The Duck stretches out its Neck to take Corn out of your Hand; it swallows it, digests it, and discharges it digested by the usual Passage.

There is controversy about whether or not the material \excreted" by the duck came from the corn it swallowed. One of the automates-anciens Web sites10 claims that \In restoring Vaucanson's duck in 1844, the magician Robert-Houdin discovered that `The discharge was prepared in advance: a sort of gruel composed of green-coloured bread crumb . . . '."

Leaving digestion aside, Vaucanson's duck was a remarkable piece of engineering. He was quite aware of that himself. He wrote

I believe that Persons of Skill and Attention, will see how difficult it has been to make so many different moving Parts in this small Automaton; as for Example, to make it rise upon its Legs, and throw its Neck to the Right and Left. They will  and the different Changes of the Fulchrum's or Centers of Motion: they will also see that what sometimes is a Center of Motion for a moveable Part, another Time becomes moveable on that Part, which Part then becomes  x'd. In a Word, they will be sensible of a prodigious Number of Mechanical Combinations.

This Machine, when once wound up, performs all its different Operations without being touch'd any more.

I forgot to tell you, that the Duck drinks, plays in the Water with his Bill, and makes a gurgling Noise like a real living Duck. In short, I have endeavor'd to make it imitate all the Actions of the living Animal, which I have consider'd very attentively.

"Unfortunately, only copies of the duck exist.  The original was burned in a museum in Nijninovgorod, Russia around 1879. It is on exhibit in the Museum of Automatons in Grenoble and was designed and built in 1998 by Frederic Vidoni, a creator in mechanical arts.

"Returning now to fictional automata, I’ll first mention the mechanical, life-sized doll, Olympia, which sings and dances in Act I of Les Contes d’Hoffmann  (The Tales of Hoffmann) by Jacques Offenbach (1819–1880). In the opera, Hoffmann, a poet, falls in love with Olympia, only to be crestfallen (and embarrassed) when she is smashed to pieces by the disgruntled Coppelius."

A play called R.U.R. (Rossum's Universal Robots) was published by Karel Capek (pronounced CHAH pek), a Czech author and playwright, in 1920. Capek is credited with coining the word \"robot," which in Czech means \forced labor" or \drudgery." (A "robotnik" is a peasant or serf.)

"The play opened in Prague in January 1921. The Robots (always capitalized in the play) are mass-produced at the island factory of Rossum’s Universal Robots using a chemical substitute for protoplasm.  According to a Web site describing the play,  “Robots remember everything, and think of nothing new.  According to Domin [the factory director] ‘They’d make fine university professors.’  . . . once in a while, a Robot will throw down his work and start gnashing his teeth.  The human managers treat such an event as evidence of a product defect, but Helena [who wants to liberate the Robots] prefers to interpret it as a sign of the emerging soul.”

I won't reveal the ending except to say that  Capek did not look eagerly on technology. He believed that work is an essential element of human life. Writing in a 1935 newspaper column (in the third person, which was his habit) he said: "With outright horror, he refuses any responsibility for the thought that machines could take the place of people, or that anything like life, love, or rebellion could ever awaken in their cogwheels. He would regard this somber vision as an unforgivable overvaluation of mechanics or as a severe insult to life."

There is an interesting story, written by  Capek himself, about how he came to use the word robot in his play. While the idea for the play \was still warm he rushed immediately to his brother Josef, the painter, who was standing before an easel and painting away. . . . `I don't know what to call these artificial workers,' he said. `I could call them Labori, but that strikes me as a bit bookish.' `Then call them Robots,' the painter muttered, brush in mouth, and went on painting."

The science fiction (and science fact) writer Isaac Asimov wrote many stories about robots. His first collection, I, Robot, consists of nine stories about "positronic" robots.16 Because he was tired of science  fiction stories in which robots (such as Frankenstein's creation) were destructive, Asimov's robots had "Three Laws of Robotics" hard-wired into their positronic brains. The three laws were the following:

First Law: A robot may not injure a human being, or, through inaction, allow a human being to come to harm.

Second Law: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.

Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.
 
Asimov later added a "zeroth" law, designed to protect humanity's interest:

Zeroth Law: A robot may not injure humanity, or, through inaction, allow humanity to come to harm.

The quest for artificial intelligence, quixotic or not, begins with dreams like these. But to turn dreams into reality requires usable clues about how to proceed. Fortunately, there were many such clues, as we shall see.

Chapter 2: Clues

Clues about what might be needed to make machines intelligent are scattered abundantly throughout philosophy, logic, biology, psychology, statistics, and engineering.  With gradually increasing intensity, people set about to exploit clues from these areas in their separate quests to automate some aspects of intelligence.  I begin my story by describing some of these clues and how they inspired some of the first achievements in artificial intelligence.

2.1 From Philosophy and Logic

Although people had reasoned logically for millennia, it was the Greek philosopher Aristotle who first tried to analyze and codify the process. Aristotle identified a type of reasoning he called the syllogism  “. . . in which, certain things being stated, something other than what is stated follows of necessity from their being so."

Here is a famous example of one kind of syllogism:

1. All humans are mortal. (stated)

2. All Greeks are humans (stated)

3. All Greeks are mortal (result)

The beauty (and importance for AI) of Aristotle's contribution has to do with the form of the syllogism. We aren't restricted to talking about humans, Greeks, or mortality. We could just as well be talking about something else { a result made obvious if we rewrite the syllogism using arbitrary symbols in the place of humans, Greeks, and mortal. Rewriting in this way would produce

1. All B's are A. (stated)

2. All C's are B's. (stated)

3. All C's are A. (result)

One can substitute anything one likes for A, B, and C. For example, all athletes are healthy and all soccer players are athletes, and therefore all soccer players are healthy, and so on. (Of course, the "result" won't necessarily be true unless the things \stated" are. Garbage in, garbage out!)

Aristotle’s logic provides two clues to how one might automate reasoning.

First, patterns of reasoning, such as syllogisms, can be economically represented as forms  or templates.  These use generic symbols, which can stand for many different concrete instances.  Because they can stand for anything, the symbols themselves are unimportant.

Second, after the general symbols are replaced by ones pertaining to a specific problem, one only has to “turn the crank” to get an answer.  The use of general symbols and similar kinds of crank-turning are at the heart of all modern AI reasoning programs.

In more modern times, Gottfried Wilhelm Leibniz (1646–1716; Fig.  2.1) was among the first to think about logical reasoning.  Leibniz was a German philosopher, mathematician, and logician who, among other things, co-invented the calculus.  (He had lots of arguments with Isaac Newton about that.)  But more importantly for our story, he wanted to mechanize reasoning. Leibniz wrote3

It is unworthy of excellent men to lose hours like slaves in the labor of calculation which could safely be regulated to anyone else if machines were used.

and

For if praise is given to the men who have determined the number of regular solids. . . how much better will it be to bring under mathematical laws human reasoning, which is the most excellent and useful thing we have.

Leibniz conceived of and attempted to design a language in which all human knowledge could be formulated – even philosophical and metaphysical knowledge.  He speculated that the propositions that constitute knowledge could be built from a smaller number of primitive ones – just as all words can be built from letters in an alphabetic language.  His lingua characteristica  or universal language would consist of these primitive propositions, which would comprise an alphabet for human thoughts.

The alphabet would serve as the basis for automatic reasoning. His ideas that if the items in the alphabet were represented by numbers, then a complex proposition could be obtained from its primitive constituents by multiplying the corresponding numbers together.  Further arithmetic operations could then be used to determine whether or not the complex proposition was true or false.  This whole process was to be accomplished by a calculus ratiocinator  (calculus of reasoning).  Then, when philosophers disagreed over some problem they could say, “calculemus” (“let us calculate”). They would first pose the problem in the lingua characteristica and then solve it by “turning the crank” on the calculus ratiocinator.

The main problem in applying this idea was discovering the components of the primitive “alphabet.” However, Leibniz’s work provided important additional clues to how reasoning might be mechanized:  Invent an alphabet of simple symbols and the means for combining them into more complex expressions.

Toward the end of the eighteenth century and the beginning of the nineteenth, a British scientist and politician, Charles Stanhope (Third Earl of Stanhope), built and experimented with devices for solving simple problems in logic and probability.  One version of his “box” had slots on the sides into which a person could push colored slides.  From a window on the top, one could view slides that were appropriately positioned to represent a specific problem. Today, we would say that Stanhope’s box was a kind of analog computer.

The book Computing Before Computers gives an example of its operation:

To solve a numerical syllogism, for example:

Eight of ten A’s are B’s; Four of ten A’s are C’s; Therefore, at least two B’s are C’s.

Stanhope would push the red slide (representing B) eight units across the window (representing A) and the gray slide (representing C) four units from the opposite direction. The two units that the slides overlapped represented the minimum number of B’s that were also C’s.

· · ·

In a similar way the Demonstrator could be used to solve a traditional syllogism like:

No M is A; All B is M; Therefore, No B is A.

Stanhope was rather secretive about his device and didn’t want anyone to know what he was up to.  As mentioned in Computing Before Computers,

"“The few friends and relatives who received his privately distributed account of the Demonstrator, The Science of Reasoning Clearly Explained Upon New Principles (1800), were advised to remain silent lest ‘some bastard imitation’ precede his intended publication on the subject.”

But no publication appeared until sixty years after Stanhope’s death.
Then, the Reverend Robert Harley gained access to Stanhope’s notes and one of his boxes and published an article on what he called “The Stanhope Demonstrator.”5

Contrasted with Llull’s schemes and Leibniz’s hopes, Stanhope built the first logic machine that actually worked – albeit on small problems.  Perhaps his work raised confidence that logical reasoning could indeed be mechanized.

In 1854, the Englishman George Boole (1815–1864; Fig.  2.3) published a book with the title An Investigation of the Laws of Thought on Which Are Founded the Mathematical Theories of Logic and Probabilities.6   Boole’s purpose was (among other things) “to collect. . . some probable intimations concerning the nature and constitution of the human mind.”  Boole considered various logical principles of human reasoning and represented them in mathematical form.  For example, his “Proposition IV” states “. . . the principle of contradiction. . . affirms that it is impossible for any being to possess a quality, and at the same time not to possess it.     ” Boole then wrote this
principle as an algebraic equation,

x(1 − x) = 0,

In which x represents “any class of objects,” (1 −x) represents the “contrary or supplementary class of objects,” and 0 represents a class that “does not exist.”

In Boolean algebra, an outgrowth of Boole’s work, we would say that 0 represents falsehood, and 1 represents truth.  Two of the fundamental operations in logic, namely OR and AND, are represented in Boolean algebra by the operations + and ×, respectively.  Thus, for example, to represent the statement “either p or q or both,” we would write p + q.  To represent the statement “p and q,” we would write p × q.  Each of p and q could be true or false, so we would evaluate the value (truth or falsity) of p + q and p × q by using definitions for how + and × are used, namely,

1 + 0 = 1,
1 × 0 = 0,
1 + 1 = 1,
1 × 1 = 1,
0 + 0 = 0,
and
0 × 0 = 0.

Boolean algebra plays an important role in the design of telephone switching circuits and computers.  Although Boole probably could not have envisioned computers, he did realize the importance of his work.  In a letter dated January 2, 1851, to George Thomson (later Lord Kelvin) he wrote7

I am now about to set seriously to work upon preparing for the press an account of my theory of Logic and Probabilities which in its present state I look upon as the most valuable if not the only valuable contribution that I have made or am likely to make to Science and the thing by which I would desire if at all to be remembered hereafter. . .

Boole’s work showed that some kinds of logical reasoning could be performed by manipulating equations representing logical propositions – a  very important clue about the mechanization of reasoning.  An essentially equivalent, but not algebraic, system for manipulating and evaluating propositions is called the “propositional calculus” (often called “propositional logic”), which, as we shall see, plays a very important role in artificial intelligence.  [Some claim that the Greek Stoic philospher Chrysippus (280–209 bce) invented an early form of the propositional calculus.

One shortcoming of Boole’s logical system, however, was that his propositions p, q, and so on were “atomic.” They don’t reveal any entities internal to propositions.  For example, if we expressed the proposition “Jack is human” by p, and “Jack is mortal” by q, there is nothing in p or q to indicate that the Jack who is human is the very same Jack who is mortal.  For that, we need, so to speak, “molecular expressions” that have internal elements.

Toward the end of the nineteenth century, the German mathematician, logician, and philosopher Friedrich Ludwig Gottlob Frege (1848–1925)  invented a system in which propositions, along with their internal components, could be written down in a kind of graphical form.  He called his language Begriffsschrift, which can be translated as “concept writing.” For example, the statement “All persons are mortal” would have been written in Begriffsschrift something like the diagram in Fig.  2.4.9

Note that the illustration explicitly represents the x who is predicated to be a person and that it is the same x who is then claimed to be mortal.  It’s more convenient nowadays for us to represent this statement in the linear form (∀x)P (x)⊃M (x), whose English equivalent is “for all x, if x is a person, then x is mortal.”

Frege’s system was the forerunner of what we now call the “predicate calculus,” another important system in artificial intelligence.  It also foreshadows another representational form used in present-day artificial intelligence:  semantic networks.  Frege’s work provided yet more clues about how to mechanize reasoning processes.  At last, sentences expressing information to be reasoned about could be written in unambiguous, symbolic form.

2.2     From  Life  Itself

In Proverbs 6:6–8, King Solomon says “Go to the ant, thou sluggard; consider her ways and be wise.” Although his advice was meant to warn against slothfulness, it can just as appropriately enjoin us to seek clues from biology about how to build or improve artifacts."

"Several aspects of “life” have, in fact, provided important clues about intelligence.  Because it is the brain of an animal that is responsible for converting sensory information into action, it is to be expected that several good ideas can be found in the work of neurophysiologists and neuroanatomists who study brains and their fundamental components, neurons.  Other ideas are provided by the work of psychologists who study (in various ways) intelligent behavior as it is actually happening.  And because, after all, it is evolutionary processes that have produced intelligent life, those processes too provide important hints about how to proceed.

2.2.1    Neurons and the Brain

In the late nineteenth and early twentieth centuries, the “neuron doctrine” specified that living cells called “neurons” together with their interconnections were fundamental to what the brain does.  One of the people responsible for this suggestion was the Spanish neuroanatomist Santiago Ramony Cajal (1852–1934). Cajal  and Camillo Golgi won the Nobel Prize in Physiology or Medicine in 1906 for their work on the structure of the nervous system.

A neuron is a living cell, and the human brain has about ten billion (1010) of them.  Although they come in different forms, typically they consist of a central part called a soma or cell body, incoming fibers called dendrites, and one or more outgoing fibers called axons.  The axon of one neuron has projections called terminal buttons that come very close to one or more of the dendrites of other neurons.  The gap between the terminal button of one neuron and a dendrite of another is called a synapse.  The size of the gap is about 20 nanometers.  Two neurons are illustrated schematically in Fig.  2.6.

Through electrochemical action, a neuron may send out a stream of pulses down its axon.  When a pulse arrives at the synapse adjacent to a dendrite of another neuron, it may act to excite or to inhibit electrochemical activity of the other neuron across the synapse.  Whether or not this second neuron then “fires” and sends out pulses of its own depends on how many and what kinds of pulses (excitatory or inhibitory) arrive at the synapses of its various incoming dendrites and on the efficiency of those synapses in transmitting electrochemical activity.  It is estimated that there are over half a trillion synapses in the human brain.  The neuron doctrine claims that the various activities of the brain, including perception and thinking, are the result of all of this neural activity.

In 1943, the American neurophysiologist Warren McCulloch (1899–1969; Fig.  2.7) and logician Walter Pitts (1923–1969) claimed that the neuron was, in essence, a “logic unit.” In a famous and important paper they proposed simple models of neurons and showed that networks of these models could perform all possible computational operations.10   The McCulloch–Pitts “neuron” was a mathematical abstraction with inputs and outputs (corresponding, roughly, to dendrites and axons, respectively). Each output can have the value 1 or 0. (To avoid confusing a McCulloch{Pitts neuron with a real neuron, I'll call the McCulloch{Pitts version, and others like it, a \neural element.") The neural elements can be connected together into networks such that the output of one neural element is an input to others and so on. Some neural elements are excitatory { their outputs contribute to \ ring" any neural elements to which they are connected. Others are inhibitory { their outputs contribute to inhibiting the  ring of neural elements to which they are connected. If the sum of the excitatory inputs less the sum of the inhibitory inputs impinging on a neural element is greater than a certain \threshold," that neural element  res, sending its output of 1 to all of the neural elements to which it is connected.

The Canadian neuropsychologist Donald O. Hebb (1904{1985) also believed that neurons in the brain were the basic units of thought. In an influential book,11 Hebb suggested that \when an axon of cell A is near enough to excite B and repeatedly or persistently takes part in  ring it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells  ring B, is increased." Later, this so-called Hebb rule of change in neural \synaptic strength" was actually observed in experiments with living animals. (In 1965, the neurophysiologist Eric Kandel published results showing that simple forms of learning were associated with synaptic changes in the marine mollusk Aplysia californica. In 2000, Kandel shared the Nobel Prize in Physiology or Medicine "for their discoveries concerning signal transduction in the nervous system.")

Hebb also postulated that groups of neurons that tend to fire together formed what he called cell assemblies.  Hebb thought that the phenomenon of “firing together” tended to persist in the brain and was the brain’s way of representing the perceptual event that led to a cell-assembly’s formation.  Hebb said that “thinking” was the sequential activation of sets of cell assemblies.

2.2.2  Psychology and Cognitive Science

Psychology is the science that studies mental processes and behavior. The word is derived from the Greek words psyche, meaning breath, spirit, or soul, and logos, meaning word. One might expect that such a science ought to have much to say that would be of interest to those wanting to create intelligent artifacts. However, until the late nineteenth century, most psychological theorizing depended on the insights of philosophers, writers, and other astute observers of the human scene. (Shakespeare, Tolstoy, and other authors were no slouches when it came to understanding human behavior.)

Most people regard serious scientific study to have begun with the German Wilhelm Wundt (1832–1920) and the American William James (1842–1910). Both established psychology labs in 1875 – Wundt in Leipzig and James at Harvard.  According to C. George Boeree, who teaches the history of psychology at Shippensburg University in Pennsylvania, “The method is a sort of experimental introspection: The researcher was to carefully observe some simple event { one that could be measured as to quality, intensity, or duration { and record his responses to variations of those events." Although James is now regarded mainly as a philosopher, he is famous for his two-volume book The Principles of Psychology, published in 1873 and 1874.

Both Wundt and James attempted to say something about how the brain worked instead of merely cataloging its input{output behavior. The psychiatrist Sigmund Freud (1856{1939) went further, postulating internal components of the brain, namely, the id, the ego, and the superego, and how they interacted to a ect behavior. He thought one could learn about these components through his unique style of guided introspection called psychoanalysis.

Attempting to make psychology more scientific and less dependent on subjective introspection, a number of psychologists, most famously B. F. Skinner (1904{1990; Fig. 2.9), began to concentrate solely on what could be objectively measured, namely, specific behavior in reaction to specific stimuli. The behaviorists argued that psychology should be a science of behavior, not of the mind. They rejected the idea of trying to identify internal mental states such as beliefs, intentions, desires, and goals.

This development might at first be regarded as a step backward for people wanting to get useful clues about the internal workings of the brain.  In criticizing the statistically oriented theories arising from “behaviorism,” Marvin Minsky wrote “Originally intended to avoid the need for ‘meaning,’ [these theories] manage finally only to avoid the possibility of explaining it.”14 Skinner’s work did, however, provide the idea of a reinforcing stimulus – one that rewards recent behavior and tends to make it more likely to occur (under similar circumstances) in the future.

Reinforcement learning has become a popular strategy among AI researchers, although it does depend on internal states.  Russell Kirsch (circa 1930– ), a computer scientist at the U.S. National Bureau of Standards (now the National Institute for Standards and Technology, NIST), was one of the first to use it.  He proposed how an “artificial animal” might use reinforcement to learn good moves in a game.  In some 1954 seminar notes he wrote the following:15   “The animal model notes, for each stimulus, what move the opponent next makes, . . . Then, the next time that same stimulus occurs, the animal duplicates the move of the opponent that followed the same stimulus previously.  The more the opponent repeats the same move after any given stimulus, the more the animal model becomes ‘conditioned’ to that move.”

Skinner believed that reinforcement learning could even be used to explain verbal behavior in humans.  He set forth these ideas in his 1957 book Verbal Behavior,16  claiming that the laboratory-based principles of selection by consequences can be extended to account for what people say, write, gesture, and think.

Arguing against Skinner’s ideas about language the linguist Noam Chomsky (1928– ; Fig. 2.10), in a review17 of Skinner’s book, wrote that

careful study of this book (and of the research on which it draws) reveals, however, that [Skinner’s] astonishing claims are far from justified. . . . the insights that have been achieved in the laboratories of the reinforcement theorist, though quite genuine, can be applied to complex human behavior only in the most gross and superficial way, and that speculative attempts to discuss linguistic behavior in these terms alone omit from consideration factors of fundamental importance. . .

How, Chomsky seems to ask, can a person produce a potentially infinite variety of previously unheard and unspoken sentences having arbitrarily complex structure (as indeed they can do) through experience alone? These “factors of fundamental importance” that Skinner omits are, according to Chomsky, linguistic abilities that must be innate – not learned. He suggested that “human beings are somehow specially created to do this, with data-handling or ‘hypothesis-formulating’ ability of [as yet] unknown character
and complexity.” Chomsky claimed that all humans have at birth a “universal grammar” (or developmental mechanisms for creating one) that accounts for much of their ability to learn and use languages.

Continuing the focus on internal mental processes and their limitations, the psychologist George A. Miller (1920– ) analyzed the work of several experimenters and concluded that the “immediate memory” capacity of humans was approximately seven “chunks” of information.19 In the introduction to his paper about this “magical number,” Miller humorously notes “My problem is that I have been persecuted by an integer. For seven years this number has followed me around, has intruded in my most private data, and has assaulted me from the pages of our most public journals. This number assumes a variety of disguises, being sometimes a little larger and sometimes a little smaller than usual, but never changing so much as to be unrecognizable. The persistence with which this number plagues me is far more than a random accident.” Importantly, he also claimed that “the span of immediate memory seems to be almost independent of the number of bits per chunk.” That is, it doesn’t matter what a chunk represents, be it a single digit in a phone number, a name of a person just mentioned, or a song title; we can apparently only hold seven of them (plus or minus two) in our immediate memory.

Miller’s paper on “The Magical Number Seven,” was given at a Symposium on Information Theory held from September 10 to 12, 1956, at MIT.20  Chomsky presented an important paper there too.  It was entitled “Three Models for the Description of Language,” and in it he proposed a family of rules of syntax he called phrase-structure grammars.21   It happens that two pioneers in AI research (of whom we’ll hear a lot more later), Allen Newell (1927–1992), then a scientist at the Rand Corporation, and Herbert Simon (1916–2001), a professor at the Carnegie Institute of Technology (now Carnegie Mellon University), gave a paper there also on a computer program that could prove theorems in propositional logic.  This symposium, bringing together as it did scientists with these sorts of overlapping interests, is thought to have contributed to the birth of cognitive science, a new discipline devoted to the study of the mind.  Indeed, George Miller wrote22

I went away from the Symposium with a strong conviction, more intuitive than rational, that human experimental psychology, theoretical linguistics, and computer simulation of cognitive processes were all pieces of a larger whole, and that the future would see progressive elaboration and coordination of their shared concerns. . .

In 1960, Miller and colleagues wrote a book proposing a specific internal mechanism responsible for behavior, which they called the TOTE unit (Test–Operate–Test–Exit).23   There is a TOTE unit corresponding to every goal that an agent might have.  Using its perceptual abilities, the unit first tests whether or not its goal is satisfied.  If so, the unit rests (exits).  If not, some operation specific to achieving that goal is performed, and the test for goal achievement is performed again, and so on repetitively until the goal finally is achieved.  As a simple example, consider the TOTE unit for driving a nail with a hammer.  So long as the nail is not completely driven in (the goal), the hammer is used to strike it (the operation).  Pounding stops (the exit) when the goal is finally achieved.  It’s difficult to say whether or not this book inspired similar work by artificial intelligence researchers.  The idea was apparently “in the air,” because at about the same time, as we shall see later, some early work in AI used very similar ideas.  [I can say that my work at SRI with behavior (intermediate-level) programs for the robot, Shakey, and my later work on what I called “teleo-reactive” programs were influenced by Miller’s ideas.]

Cognitive science attempted to explicate internal mental processes using ideas such as goals, memory, task queues, and strategies without (at least during its beginning years) necessarily trying to ground these processes in neurophysiology.24   Cognitive science and artificial intelligence have been closely related ever since their beginnings.  Cognitive science has provide clues for AI researchers, and AI has helped cognitive science with newly invented concepts useful for understanding the workings of the mind.

2.2.3    Evolution

That living things evolve gives us two more clues about how to build intelligent artifacts.  First, and most ambitiously, the processes of evolution itself – namely, random generation and selective survival – might be simulated on computers to produce the machines we dream about.  Second, those paths that evolution followed in producing increasingly intelligent animals can be used as a guide for creating increasingly intelligent artifacts.  Start by simulating animals with simple tropisms and proceed along these paths to simulating more complex ones.  Both of these strategies have been followed with zest by AI researchers, as we shall see in the following chapters.  Here, it will suffice to name just a few initial efforts.

Early attempts to simulate evolution on a computer were undertaken at Princeton’s Institute for Advanced Study by the viral geneticist Nils Aall Barricelli (1912–1993).  His 1954 paper described experiments in which numbers migrated and reproduced in a grid.25

Motivated by the success of biological evolution in producing complex organisms, some researchers began thinking about how programs could be evolved rather than written.  R. N. Friedberg and his IBM colleagues26 conducted experiments in which, beginning with a population of random computer programs, they attempted to evolve ones that were more successful at performing a simple logical task.  In the summary of his 1958 paper, Friedberg wrote that “[m]achines would be more useful if they could learn to perform tasks for which they were not given precise methods.  . . . It is proposed that the program of a stored-program computer be gradually improved by a learning procedure which tries many programs and chooses, from the instructions that may occupy a given location, the one most often associated with a successful result.” That is, Friedberg installed instructions from “successful” programs into the programs of the next “generation,” much as how the genes of individuals successful enough to have descendants are installed in those descendants.

Unfortunately, Friedberg’s attempts to evolve programs were not very successful.  As Marvin Minsky pointed out,27

The machine [described in the first paper] did learn to solve some extremely simple problems.  But it took of the order of 1000 times longer than pure chance would expect. . . .

The second paper goes on to discuss a sequence of
modifications. . . With these, and with some ‘priming’ (starting the machine off on the right track with some useful instructions), the system came to be only a little worse than chance.

Minsky attributes the poor performance of Friedberg’s methods to the fact that each descendant machine differed very little from its parent, whereas any helpful improvement would require a much larger step in the “space” of possible machines.

Other early work on artificial evolution was more successful.  Lawrence Fogel (1928–2007) and colleagues were able to evolve machines that could make predictions of the next element in a sequence.28   Woodrow W. Bledsoe (1921–1995) at Panoramic Research and Hans J. Bremermann (1926–1969) at the University of California, Berkeley, used simulated evolution to solve optimization and mathematical problems, respectively.29   And Ingo Rechenberg (according to one AI researcher) “pioneered the method of artificial evolution to solve complex optimization tasks, such as the design of optimal airplane wings or combustion chambers of rocket nozzles.”30

The first prominent work inspired by biological evolution was John Holland’s development of “genetic algorithms” beginning in the early 1960s. Holland (1929– ), a professor at the University of Michigan, used strings of binary symbols (0’s and 1’s), which he called “chromosomes” in analogy with the genetic material of biological organisms.  (Holland says he first came up with the notion while browsing through the Michigan math library’s open stacks in the early 1950s.)31   The encoding of 0’s and 1’s in a chromosome could be interpreted as a solution to some given problem.  The idea was to evolve chromosomes that were better and better at solving the problem.
Populations of chromosomes were subjected to an evolutionary process in which individual chromosomes underwent “mutations” (changing a component 1 to a 0 and vice versa), and pairs of the most successful chromosomes at each stage of evolution were combined to make a new chromosome.  Ultimately, the process would produce a population containing a chromosome (or chromosomes) that solved the problem.32

Researchers would ultimately come to recognize that all of these evolutionary methods were elaborations of a very useful mathematical search strategy called “gradient ascent” or “hill climbing.” In these methods, one searches for a local maximum of some function by taking the steepest possible uphill steps.  (When searching for a local minimum, the analogous method is called “gradient descent.”)

Rather than attempt to duplicate evolution itself, some researchers preferred to build machines that followed along evolution’s paths toward intelligent life.  In the late 1940s and early 1950s, W. Grey Walter  (1910–1977), a British neurophysiologist (born in Kansas City, Missouri), built some machines that behaved like some of life’s most primitive creatures.  They were wheeled vehicles to which he gave the taxonomic name Machina speculatrix  (machine that looks; see Fig.  2.11).33   These tortoise-like machines were controlled by “brains” consisting of very simple vacuum-tube circuits that sensed their environments with photocells and that controlled their wheel motors.  The circuits could be arranged so that a machine either moved toward or away from a light mounted on a sister machine. Their behaviors seemed purposive and often complex and unpredictable, so much so that Walter said they \might be accepted as evidence of some degree of self-awareness." Machina speculatrix was the beginning of a long line of increasingly sophisticated \behaving machines" developed by subsequent researchers.

2.2.4    Development and Maturation

Perhaps there are alternatives to rerunning evolution itself or to following its paths toward increasing complexity from the most primitive animals.  By careful study of the behavior of young children, the Swiss psychologist Jean Piaget proposed a set of stages in the maturation of their thinking abilities from infancy to adolescence.34   Might these stages provide a set of steps that could guide designers of intelligent artifacts?  Start with a machine that is able to do what an infant can do, and then design machines that can mimic the abilities of children at each rung of the ladder.  This strategy might be called “ontogenetic” to contrast it with the “phylogenetic” strategy of using simlulated evolution.

Of course, it may be that an infant mind is far too complicated to simulate and the processes of its maturation too difficult to follow.  In any case, this particular clue remains to be exploited.

2.2.5    Bionics

At a symposium in 1960, Major Jack E. Steele, of the Aerospace Division of the United States Air Force, used the term “bionics” to describe the field that learns lessons from nature to apply to technology.35

Several bionics and bionics-related meetings were held during the 1960s. At the 1963 Bionics Symposium, Leonard Butsch and Hans Oestreicher wrote “Bionics aims to take advantage of millions of years of evolution of living systems during which they adapted themselves for optimum survival.  One of the outstanding successes of evolution is the information processing capability of living systems [the study of which is] one of the principal areas of Bionics research.”36

Today, the word “bionics” is concerned mainly with orthotic and prosthetic devices, such as artificial cochleas, retinas, and limbs.  Nevertheless, as AI researchers continue their quest, the study of living things, their evolution, and their development may continue to provide useful clues for building intelligent artifacts.

2.3     From  Engineering

2.3.1    Automata, Sensing, and Feedback

Machines that move by themselves and even do useful things by themselves have been around for centuries.  Perhaps the most common early examples are the “verge-and-foliot” weight-driven clocks.  (See Fig.  2.12.)  These first appeared in the late Middle Ages in the towers of large Italian cities.  The verge-and-foliot mechanism converted the energy of a falling weight into stepped rotational motion, which could be used to move the clock hands.
Similar mechanisms were elaborated to control the actions of automata, such as those of the Munich Glockenspiel.

One of the first automatic machines for producing goods was d’s weaving loom, built in 1804.  (See Fig.  2.13.) It followed a long history of looms and improved on the “punched card” design of Jacques de Vaucanson’s loom of 1745. (Vaucanson did more than build mechanical ducks.) The punched cards of the Jacquard loom controlled the actions of the shuttles, allowing automatic production of fabric designs. Just a few years after its invention, there were some 10,000 Jacquard looms weaving away in France. The idea of using holes in paper or cards was later adopted by Herman Hollerith for tabulating the 1890 American census data and in player pianos (using perforated rolls instead of cards). The very first factory “robots” of the so-called pick-and-place variety used only modest elaborations of this idea.

It was only necessary to provide these early machines with an external source of energy (a falling weight, a wound-up spring, or humans pumping pedals). Their behavior was otherwise fully automatic, requiring no human guidance. But, they had an important limitation – they did not perceive anything about their environments. (The punched cards that were “read” by the Jacquard loom are considered part of the machine – not part of the environment.) Sensing the environment and then letting what is sensed influence what a machine does is critical to intelligent behavior. Grey Walters’s “tortoises,” for example, had photocells that could detect the presence or absence of light in their environments and act accordingly. Thus, they seem more intelligent than a Jacquard loom or clockwork automata.

One of the simplest ways to allow what is sensed to influence behavior involves what is called “feedback control.” The word derives from feeding some
aspect of a machine’s behavior, say its speed of operation, back into the internals of the machine. If the aspect of behavior that is fed back acts to diminish or reverse that aspect, the process is called \negative feedback." If, on the other hand, it acts to increase or accentuate that aspect of behavior, it is called \positive feedback." Both types of feedback play extremely important roles in engineering.

Negative feedback techniques have been used for centuries in mechanical devices. In 270 bce, a Greek inventor and barber, Ktesibios of Alexandria, invented a float regulator to keep the water level in a tank feeding a water clock at a constant depth by controlling the water flow into the tank.37 The feedback device was a float valve consisting of a cork at the end of a rod. The cork floated on the water in the tank. When the water level in the tank rose, the cork would rise, causing the rod to turn off the water coming in. When the water level fell, the cork would fall, causing the rod to turn on the water. The water level in modern flush toilets is regulated in much the same way. In 250 bce, Philon of Byzantium used a similar float regulator to keep a constant level of oil in a lamp.

The English clockmaker John Harrison (1693–1776) used a type of negative feedback control in his clocks. The ambient temperature of a clock affects the length of its balance spring and thus its time-keeping accuracy. Harrison used a bimetallic strip (sometimes a rod), whose curvature depends on temperature. The strip was connected to the balance spring in such a way that it produced offsetting changes in the length of the spring, thus making the clock more independent of its temperature. The strip senses the temperature and causes the clock to behave differently, and more accurately, than it otherwise would. Today, such bimetallic strips see many uses, notably in thermostats. (Dava Sobel’s 1995 book, Longitude: The True Story of a Lone Genius Who Solved the Greatest Scientific Problem of His Time, recounts the history of Harrison’s efforts to build a prize-winning clock for accurate time-keeping at sea.)

Perhaps the most graphic use of feedback control is the centrifugal flyball governor perfected in 1788 by James Watt for regulating the speed of his steam engine. (See Fig. 2.14.) As the speed of the engine increases, the balls fly outward, which causes a linking mechanism to decrease air flow, which causes the speed to decrease, which causes the balls to fall back inward, which causes the speed to increase, and so on, resulting in an equilibrium speed.

In the early 1940s, Norbert Wiener (1894–1964) and other scientists noted similarities between the properties of feedback control systems in machines and in animals. In particular, inappropriately applied feedback in control circuits led to jerky movements of the system being controlled that were similar to pathological “tremor” in human patients. Arturo Rosenblueth, Norbert Wiener, and Julian Bigelow coined the term “cybernetics” in a 1943 paper. Wiener’s book by that name was published in 1948. The word is related to the word “governor.” (In Latin gubernaculum means helm, and gubernator means helmsman. The Latin derives from the Greek kybernetike, which means the art of steersmanship.39)

Today, the prefix “cyber” is used to describe almost anything that deals with computers, robots, the Internet, and advanced simulation. For example, the author William Gibson coined the term “cyberspace” in his 1984 science fiction novel Neuromancer. Technically, however, cybernetics continues to "describe activities related to feedback and control.

The English psychiatrist W. Ross Ashby (1903–1972; Fig.  2.15) contributed to the field of cybernetics by his study of “ultrastability” and “homeostasis.” According to Ashby, ultrastability is the capacity of a system to reach a stable state under a wide variety of environmental conditions.  To illustrate the idea, he built an electromechanical device called the “homeostat.” It consisted of four pivoted magnets whose positions were rendered interdependent through feedback mechanisms.  If the position of any was disturbed, the effects on the others and then back on itself would result in all of them returning to an equilibrium condition.  Ashby described this device in Chapter 8 of his influential 1952 book Design For a Brain.  His ideas had an influence on several AI researchers.  My “teleo-reactive programs,” to be described later, were motivated in part by the idea of homeostasis.

Another source of ideas, loosely associated with cybernetics and bionics, came from studies of “self-organizing systems.” Many unorganized combinations of simple parts, including combinations of atoms and molecules, respond to energetic “jostling” by falling into stable states in which the parts are organized in more complex assemblies.  An online dictionary devoted to cybernetics and systems theory has a nice example:  “A chain made out of paper clips suggests that someone has taken the trouble to link paper clips together to make a chain.  It is not in the nature of paper clips to make themselves up into a chain.  But, if you take a number of paper clips, open them up slightly and then shake them all together in a cocktail shaker, you will find at the end that the clips have organized themselves into short or long chains.  The chains are not so neat as chains put together by hand but, nevertheless, they are chains.”

The term “self-organizing” seems to have been first introduced by Ashby in 1947. Ashby emphasized that self-organization is not a property of an organism itself, in response to its environment and experience, but a property of the organism and its environment taken together. Although self-organization appears to be important in ideas about how life originated, it is unclear whether or not it provides clues for building intelligent machines.

2.3.2    Statistics and Probability

Because nearly all reasoning and decision making take place in the presence of uncertainty, dealing with uncertainty plays an important role in the automation of intelligence.  Attempts to quantify uncertainty and “the laws of chance” gave rise to statistics and probability theory.  What would turn out to be one of the most important results in probability theory, at least for artificial intelligence, is Bayes’s rule, which I’ll define presently in the context of an example.  The rule is named for Reverend Thomas Bayes (1702–1761), an English clergyman.

One of the important applications of Bayes’s rule is in signal detection.
Let’s suppose a radio receiver is tuned to a station that after midnight broadcasts (randomly) one of two tones, either tone A or tone B, and on a particular night we want to decide which one is being broadcast.  On any given day, we do not know ahead of time which tone is to be broadcast that night, but suppose we do know their probabilities.  (For example, it might be that both tones are equally probable.)  Can we find out which tone is being broadcast by listening to the signal coming in to the receiver?  Well, listening can’t completely resolve the matter because the station is far away, and random noise partially obscures the tone.  However, depending on the nature  of the obscuring noise, we can often calculate the probability that the actual tone that night is A (or that it is B).  Let’s call the signal y and the actual tone x (which can be either A or B).  The probability that x = A, given the evidence for it contained in the incoming signal, y, is written as p(x = A | y) and read as “the probability that x is A, given that the signal is y.” The probability that x = B, given the same evidence is p(x = B | y).

A reasonable “decision rule” would be to decide in favor of tone A if
p(x = A | y) is larger than p(x = B | y).  Otherwise, decide in favor of tone B. (There is a straightforward adjustment to this rule that takes into account differences in the “costs” of the two possible errors.)  The problem in applying this rule is that these two probabilities are not readily calculable, and that is where Bayes’s rule comes in.  It allows us to calculate these probabilities in terms of other probabilities that are more easily guessed or otherwise obtainable.  Specifically, Bayes’s rule is

p(x | y) = p(y | x)p(x)/p(y).

Using Bayes’s rule, our decision rule can now be reformulated as

Decide in favor of tone A if p(y | x = A)p(x = A)/p(y) is greater than p(y | x = B)p(x = B)/p(y).  Otherwise, decide in favor of tone B.

Because p(y) occurs in both expressions and therefore does not affect which one is larger, the rule simplifies to

Decide in favor of tone A if p(y | x = A)p(x = A) is greater than
p(y | x = B)p(x = B).  Otherwise, decide in favor of tone B.

We assume that we know the a priori probabilities of the tones, namely,
p(x = A) and p(x = B), so it remains only for us to calculate p(y | x) for
x = A and x = B.  This expression is called the likelihood of y given x.  When the two probabilities, p(x = A) and p(x = B), are equal (that is, when both tones are equally probable a priori), then we can decide in favor of which likelihood is greater.  Many decisions that are made in the presence of uncertainty use this “maximum-likelihood” method. The calculation for these likelihoods depends on how we represent the received signal, y, and on the statistics of the interfering noise.

In my example, y is a radio signal, that is, a voltage varying in time.  For computational purposes, this time-varying voltage can be represented by a sequence of samples of its values at appropriately chosen, uniformly spaced time points, say y(t1), y(t2), . . . y(ti), . . . , y(tN ).  When noise alters these values from what they would have been without noise, the probability of the sequence of them (given the cases when the tone is A and when the tone is B) can be calculated by using the known statistical properties of the noise.  I won’t go into the details here except to say that, for many types of noise statistics, these calculations are quite straightforward.

In the twentieth century, scientists and statisticians such as Karl Pearson (1857–1936), Sir Ronald A. Fisher (1890–1962), Abraham Wald (1902–1950), and Jerzey Neyman (1894–1981) were among those who made important contributions to the use of statistical and probabilistic methods in estimating parameters and in making decisions.  Their work set the foundation for some of the first engineering applications of Bayes’s rule, such as the one I just illustrated, namely, deciding which, if any, of two or more electrical signals is present in situations where noise acts to obscure the signals.  A paper by the American engineers David Van Meter and David Middleton, which I read as a beginning graduate student in 1955, was my own introduction to these applications.44   For artificial intelligence, these uses of Bayes’s rule provided clues about how to mechanize the perception of both speech sounds and visual images.  Beyond perception, Bayes’s rule lies at the center of much other modern work in artificial intelligence.

2.3.3    The Computer

A.  Early  Computational  Devices

Proposals such as those of Leibniz, Boole, and Frege can be thought of as early attempts to provide foundations for what would become the “software” of artificial intelligence.  But reasoning and all the other aspects of intelligent behavior require, besides software, some sort of physical  engine.  In humansand other animals, that engine is the brain.  The simple devices of Grey Walter and Ross Ashby were, of course, physical manifestations of their ideas.  And,  as we shall see, early networks of neuron-like units were realized in physical form.  However, to explore the ideas inherent in most of the clues from logic, from neurophysiology, and from cognitive science, more powerful engines  would be required.  While McCulloch, Wiener, Walter, Ashby, and others were speculating about the machinery of intelligence, a very powerful and essential machine bloomed into existence – the general-purpose digital computer.  This single machine provided the engine for all of these ideas and more.  It is by far the dominant hardware engine for automating intelligence.

Building devices to compute has a long history.  William Aspray has edited an excellent book, Computing Before Computers, about computing’s early days.45   The first machines were able to do arithmetic calculations, but these were not programmable.  Wilhelm Schickard (1592–1635; Fig.  2.16) built one of the first of these in 1623.  It is said to have been able to add and subtract six-digit numbers for use in calculating astronomical tables.  The machine could “carry” from one digit to the next.

In 1642 Blaise Pascal (1623–1662; Fig.  2.16) created the first of about fifty of his computing machines.  It was an adding machine that could perform automatic carries from one position to the next.  “The device was contained in a box that was small enough to fit easily on top of a desk or small table.  The upper surface of the box. . . consisted of a number of toothed wheels, above which were a series of small windows to show the results.  In order to add a number, say 3, to the result register, it was only necessary to insert a small stylus into the toothed wheel at the position marked 3 and rotate the wheel clockwise until the stylus encountered the fixed stop. . ."

Inspired by Pascal’s machines, Gottfried Leibniz built a mechanical multiplier called the “Step Reckoner” in 1674.  It could add, subtract, and do multiplication (by repeated additions).  “To multiply a number by 5, one simply turned the crank five times.”

Several other calculators were built in the ensuing centuries.  A particularly interesting one, which was too complicated to build in its day, was designed in 1822 by Charles Babbage (1791–1871), an English mathematician and inventor.  (See Fig.  2.17.)  Called the “Difference Engine,” it was to have calculated mathematical tables (of the kind used in navigation at sea, for example) using the method of finite differences.  Babbage’s Difference Engine No.  2 was actually constructed in 1991 (using Babbage’s designs and nineteenth-century mechanical tolerances) and is now on display at the  London Science Museum.  The Museum arranged for another copy to be built for Nathan Myhrvold, a former Microsoft Chief Technology Officer.  (A description of the machine and a movie is available from a Computer History Museum Web page at http://www.computerhistory.org/babbage/.)

Adding machines, however, can only add and subtract (and, by repetition of these operations, also multiply and divide).  These are important operations but not the only ones needed.  Between 1834 and 1837 Babbage worked on the design of a machine called the “Analytical Engine,” which embodied most of the ideas needed for general computation.  It could store intermediate results in a “mill,” and it could be programmed.  However, its proposed realization as a collection of steam-driven, interacting brass gears and cams ran into funding difficulties and was never constructed.

Ada Lovelace (1815–1852), the daughter of Lord Byron, has been called the “world’s first programmer” for her alleged role in devising programs for the Analytical Engine.  However, in the book Computing Before Computers the following claim is made:

This romantically appealing image is without foundation.  All but one of the programs cited in her notes [to her translation of an account of a lecture Babbage gave in Turin, Italy] had been prepared by Babbage from three to seven years earlier.  The exception was prepared by Babbage for her, although she did detect a “bug” in it.  Not only is there no evidence that Ada Lovelace ever prepared a program for the Analytical Engine but her correspondence with Babbage shows that she did not have the knowledge to do so.

For more information about the Analytical Engine and an emulator and programs for it, see http://www.fourmilab.ch/babbage/.

Practical computers had to await the invention of electrical, rather than brass, devices.  The first computers in the early 1940s used electromechanical relays.  Vacuum tubes (thermionic valves, as they say in Britain) soon won out because they permitted faster and more reliable computation.  Nowadays, computers use billions of tiny transistors arrayed on silicon wafers.  Who knows what might someday replace them?

B.  Computation  Theory

Even before people actually started building computers, several logicians and mathematicians in the 1930s pondered the problem of just what could be computed. Alonzo Church came up with a class of functions that could be computed, ones he called “recursive.”49  The English logician and mathematician, Alan Turing (1912–1954; Fig.  2.18), proposed what is now understood to be an equivalent class – ones that could be computed by an imagined machine he called a “logical computing machine (LCM),” nowadays called a “Turing machine.”50  (See Fig.  2.19.) The claim that these two notions are equivalent is called the “Church–Turing Thesis.” (The claim has not been proven, but it is strongly supported by logicians and no counterexample has ever been found.)51

The Turing machine is a hypothetical computational device that is quite simple to understand. It consists of just a few parts. There is an infinite tape (which is one reason the device is just imagined and not actually built) divided into cells and a tape drive. Each cell has printed on it either a 1 or a 0. The machine also has a read–write head positioned over one cell of the tape. The read function reads what is on the tape. There is also a logic unit that can decide, depending on what is read and the state of the logic machine, to change its own state, to command the write function to write either a 1 or a 0 on the cell being read (possibly replacing what is already there), to move the tape one cell to the left or to the right (at which time the new cell is read and so on), or to terminate operation altogether. The input (the “problem” to be computed) is written on the tape initially. (It turns out that any such input can be coded into 1’s and 0’s.) When, and if, the machine terminates, the output (the coded “answer” to the input problem) ends up being printed on the tape.

Turing proved that one could always specify a particular logic unit (the part that decides on the machine’s actions) for his machine such that the machine would compute any computable function.  More importantly, he showed that one could encode on the tape itself a prescription for any logic unit specialized for a particular problem and then use a general-purpose logic unit for all problems.  The encoding for the special-purpose logic unit can be thought of as the “program” for the machine, which is stored on the tape (and thus subject to change by the very operation of the machine!)  along with the description of the problem to be solved.  In Turing’s words, “It can be shown that a single special machine of that type can be made to do the work of all. It could in fact be made to work as a model of any other machine.  The special machine may be called the universal machine.”

C.  Digital  Computers

Somewhat independently of Turing, engineers began thinking about how to build actual computing devices consisting of programs and logical circuitry for performing the instructions contained in the programs.  Some of the key ideas for designing the logic circuits of computers were developed by the American mathematician and inventor Claude Shannon (1916–2001; Fig.  2.20).53   In his 1937 Yale University master’s thesis54  Shannon showed that Boolean algebra and binary arithmetic could be used to simplify telephone switching circuits. He also showed that switching circuits (which can be realized either by combinations of relays, vacuum tubes, or whatever) could be used to implement operations in Boolean logic, thus explaining their importance in computer design.
It’s hard to know who first thought of the idea of storing a computer’s program along with its data in the computer’s memory banks.  Storing the program allows changes in the program to be made easily, but more importantly it allows the program to change itself by changing appropriate parts of the memory where the program is stored.  Among those who might have thought of this idea first are the German engineer Konrad Zuse (1910–1995) and the American computer pioneers J. Presper Eckert (1919–1995) and John W. Mauchly (1907–1980).  (Of course Turing had already proposed storing what amounted to a program on the tape of a universal Turing machine.)

For an interesting history of Konrad Zuse’s contributions, see the family of sites available from http://irb.cs.tu-berlin.de/∼zuse/Konrad Zuse/en/index.html.  One of these mentions that “it is undisputed that Konrad Zuse’s Z3 was the first fully functional, program controlled (freely programmable) computer of the world.
. . . The Z3 was presented on May 12, 1941, to an audience of scientists in Berlin.” Instead of vacuum tubes, it used 2,400 electromechanical relays.  The
original Z3 was destroyed by an Allied air raid on December 21, 1943. A reconstructed version was built in the early 1960s and is now on display at the Deutsche Museum in Munich.  Zuse also is said to have created the first programming language, called the Plankalkul.

The American mathematician John von Neumann (1903–1957) wrote a “draft report” about the EDVAC, an early stored-program computer.56 Perhaps because of this report, we now say that these kinds of computers use a “von Neumann architecture.” The ideal von Neumann architecture separates the (task-specific) stored program from the (general-purpose) hardware circuitry, which can execute (sequentially) the instructions of any program whatsoever.  (We usually call the program “software” to distinguish it from the “hardware” part of a computer.  However, the distinction is blurred in most modern computers because they often have some of their programs built right into their circuitry.)

Other computers with stored programs were designed and built in the 1940s in Germany, Great Britain, and the United States.  They were large, bulky machines.  In Great Britain and the United States they were mainly used for military purposes.  Figure 2.21 shows one such machine."

We call computers “machines” even though today they can be made completely electrical with no moving parts whatsoever.  Furthermore, when we speak of computing machines we usually mean the combination of the computer and the program it is running.  Sometimes we even call just the program a machine.  (As an example of this usage, I’ll talk later about a “checker-playing machine” and mean a program that plays checkers.)

The commanding importance of the stored-program digital computer derives from the fact that it can be used for any purpose whatsoever – that is, of course, any computational purpose.  The modern digital computer is, for all practical purposes, such a universal machine.  The “all-practical-purposes” qualifier is needed because not even modern computers have the infinite storage capacity implied by Turing’s infinite tape.  However, they do have prodigious amounts of storage, and that makes them practically universal.

D.  “Thinking”  Computers

After some of the first computers were built, Turing reasoned that if they were practically universal, they should be able to do anything.  In 1948 he wrote, “The importance of the universal machine is clear.  We do not need to have an infinity of different machines doing different jobs.  A single one will suffice.  The engineering problem of producing various machines for various jobs is replaced by the office work of ‘programming’ the universal machine to do these jobs.”57
Among the things that Turing thought could be done by computers was mimicking human intelligence.  One of Turing’s biographers, Andrew Hodges, claims, “he decided the scope of the computable encompassed far more than could be captured by explicit instruction notes, and quite enough to include all that human brains did, however creative or original.  Machines of sufficient complexity would have the capacity for evolving into behaviour that had never been explicitly programmed.”58

The first modern article dealing with the possibility of mechanizing all of human-style intelligence was published by Turing in 1950.59   This paper is famous for several reasons.  First, Turing thought that the question “Can a machine think?” was too ambiguous.  Instead he proposed that the matter of machine intelligence be settled by what has come to be called “the Turing test.”

Although there have been several reformulations (mostly simplifications) of the test, here is how Turing himself described it:

The new form of the problem [Can machines think?]  can be described in terms of a game which we call the “imitation game.” It is played with three people, a man (A), a woman (B), and an interrogator (C) who may be of either sex.  The interrogator stays in a room apart from the other two.  The object of the game for the interrogator is to determine which of the other two is the man and which is the woman.  He knows them by labels X and Y, and at the end of the game he says either “X is A and Y is B” or “X is B and Y is A.” The interrogator is allowed to put questions to A and B thus:

C: Will X please tell me the length of his or her hair?

Now suppose X is actually A, then A must answer.  It is A’s object in the game to try and cause C to make the wrong identification.
His answer might therefore be

“My hair is shingled, and the longest strands are about nine inches long.”

In order that tones of voice may not help the interrogator the answers should be written, or better still, typewritten.  The ideal arrangement is to have a teleprinter communicating between the two rooms.  Alternatively the question and answers can be repeated by an intermediary.  The object of the game for the third player
(B) is to help the interrogator.  The best strategy for her is probably to give truthful answers.  She can add such things as “I am the woman, don’t listen to him!” to her answers, but it will avail nothing as the man can make similar remarks.

We now ask the question, “What will happen when a machine takes the part of A in this game?” Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?  These questions replace our original, “Can machines think?”"

Most of the simplifications of Turing’s test involve attempting to have a computer “in the other room” fool an interrogator into believing the computer is a human.  To stimulate progress toward that goal, a prize has been established – annual prizes in fact. In the words of the Web site announcing the prizes, “In 1990 Hugh Loebner agreed with The Cambridge Center for Behavioral Studies to underwrite a contest designed to implement the Turing Test. Dr.  Loebner pledged a Grand Prize of $100,000 and a Gold Medal for the first computer whose responses were indistinguishable from a human’s. Such a computer can be said ‘to think.’  Each year an annual prize of $2000 and a bronze medal is awarded to the most human-like computer. The winner of the annual contest is the best entry relative to other entries that year, irrespective of how good it is in an absolute sense.”

However, several people have pointed to inadequacies of both the simplifications and even of the original test. In fact, it has been alleged that even Joseph Weizenbaum’s patently unintelligent 1966 conversational program, ELIZA, has fooled people into thinking it is human.

For example, here is part of a transcript of a conversation with ELIZA:

ELIZA: How do you do.  Please state your problem.
> I spend too much time with computers.
ELIZA: Why do you mention computers?
> You are one.
ELIZA: Does it please you to believe I am one?
> Yes.
ELIZA: You seem to be quite positive.
> I suppose I am.
ELIZA: How long have you been?
> 31 years.
ELIZA: I’m not sure I understand you fully.
> Join the club.
ELIZA: Please go on.
> You don’t make too much sense yourself.
ELIZA: We were discussing you – not me.

The first part of the conversation seems reasonable, but ELIZA bogs down in the middle because the program was expecting “I suppose I am” to be followed by some word like “happy.” (There are several ELIZA simulations on the Web.  One that claims to reproduce faithfully the original ELIZA program is at http://www.chayden.net/eliza/Eliza.html.  Try one out!)

A second important feature of Turing’s 1950 paper was his handling of arguments that people might raise against the possibility of achieving intelligent computers.  I’ll quote the ones Turing mentions:

(1) The Theological Objection:  Thinking is a function of man’s immortal soul.  God has given an immortal soul to every man and woman, but not to any other animal or to machines.  Hence no animal or machine can think.
(2) The ‘Heads in the Sand’ Objection:  “The consequences of machines thinking would be too dreadful.  Let us hope and believe that they cannot do so.”
(3) The Mathematical Objection:  There are a number of results of mathematical logic that can be used to show that there are limitations to the powers of discrete-state machines.
(4) The Argument from Consciousness:  This argument is very well expressed in Professor Jefferson’s Lister Oration for 1949, from which I quote:
“Not until a machine can write a sonnet or compose a concerto and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain – that is, not only write it but know that it had written it.  No mechanism could feel (and not merely artificially signal, an easy contrivance) pleasure at its successes, grief when its valves fuse, be warmed by flattery, be made miserable by its mistakes, be charmed by sex, be angry or depressed when it cannot get what it wants.”
(5) Arguments from Various Disabilities:  These arguments take the form, “I grant you that you can make machines do all the things you have mentioned but you will never be able to make one to do X.”
(6) Lady Lovelace’s Objection:  Our most detailed information of Babbage’s Analytical Engine comes from a memoir by Lady Lovelace.  In it she states, “The Analytical Engine has no pretensions to originate anything.  It can do whatever we know how to order it to perform” (her italics).
(7) Argument from Continuity in the Nervous System:  The nervous system is certainly not a discrete-state machine.  A small error in the information about the size of a nervous impulse impinging on a neuron may make a large difference to the size of the outgoing impulse.  It may be argued that, this being so, one cannot expect to be able to mimic the behavior of the nervous system with a discrete-state system.
(8) The Argument from Informality of Behavior:  It is not possible to produce a set of rules purporting to describe what a man should do in every conceivable set of circumstances.
(9) The Argument from Extra-Sensory Perception.

In his paper, Turing nicely (in my opinion) handles all of these points, with the possible exception of the last one (because he apparently thought that extra-sensory perception was plausible). I’ll leave it to you to read Turing’s 1950 paper to see his counterarguments.

The third important feature of Turing’s 1950 paper is his suggestion about how we might go about producing programs with human-level intellectual abilities. Toward the end of his paper, he suggests, “Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s? If this were then subjected to an appropriate course of education one would obtain the adult brain.” This suggestion is really the source for the idea mentioned earlier about using an ontogenetic strategy to develop intelligent machines.

Allen Newell and Herb Simon (see Fig. 2.22) were among those who had no trouble believing that the digital computer’s universality meant that it could be used to mechanize intelligence in all its manifestations – provided it had the right software. In their 1975 ACM Turing Award lecture, they described a hypothesis that they had undoubtedly come to believe much earlier, the “Physical Symbol System Hypothesis.” It states that “a physical symbol system has the necessary and sufficient means for intelligent action.” Therefore, according to the hypothesis, appropriately programmed digital computers would be capable of intelligent action.  Conversely, because humans are capable of intelligent action, they must be, according to the hypothesis, physical symbol systems. These are very strong claims that continue to be debated.

Both the imagined Turing machine and the very real digital computer are symbol systems in the sense Newell and Simon meant the phrase. How can a Turing machine, which uses a tape with 0's and 1's printed on it, be a \symbol system"? Well, the 0's and 1's printed on the tape can be thought of as symbols standing for their associated numbers. Other symbols, such as \A" and \M," can be encoded as sequences of primitive symbols, such as 0's and 1's. Words can be encoded as sequences of letters, and so on. The fact that one commonly thinks of a digital computer as a machine operating on 0's and 1's need not prevent us from thinking of it also as operating on more complex symbols. After all, we are all used to using computers to do \word processing" and to send e-mail.

Newell and Simon admitted that their hypothesis could indeed be false: “Intelligent behavior is not so easy to produce that any system will exhibit it willy-nilly.  Indeed, there are people whose analyses lead them to conclude either on philosophical or on scientific grounds that the hypothesis is false. Scientifically, one can attack or defend it only by bringing forth empirical evidence about the natural world.” They conclude the following:

The symbol system hypothesis implies that the symbolic behavior of man arises because he has the characteristics of a physical symbol system.  Hence, the results of efforts to model human behavior with symbol systems become an important part of the evidence for the hypothesis, and research in artificial intelligence goes on in close collaboration with research in information processing psychology, as it is usually called.

Although the hypothesis was not formally described until it appeared in the 1976 article, it was certainly implicit in what Turing and other researchers believed in the 1950s.  After Allen Newell’s death, Herb Simon wrote, “From the very beginning something like the physical symbol system hypothesis was embedded in the research.”64

Inspired by the clues we have mentioned and armed with the general-purpose digital computer, researchers began, during the 1950s, to explore various paths toward mechanizing intelligence.  With a firm belief in the symbol system hypothesis, some people began programming computers to attempt to get them to perform some of the intellectual tasks that humans could perform.  Around the same time, other researchers began exploring approaches that did not depend explicitly on symbol processing.  They took their inspiration mainly from the work of McCulloch and Pitts on networks of neuron-like units and from statistical approaches to decision making.  A split between symbol-processing methods and what has come to be called “brain-style” and “nonsymbolic” methods still survives today.

Part II: Early Explorations: 1950s and 1960s

If machines are to become intelligent, they must, at the very least, be able to d   the thinking-related things that humans can do.  The first steps then in the quest for artificial intelligence involved identifying some specific tasks thought to require intelligence and figuring out how to get machines to do them.  Solving puzzles, playing games such as chess and checkers, proving theorems, answering simple questions, and classifying visual images were among some of the problems tackled by the early pioneers during the 1950s and early 1960s.  Although most of these were laboratory-style, sometimes called “toy,” problems, some real-world problems of commercial importance, such as automatic reading of highly stylized magnetic characters on bank checks and language translation, were also being attacked.  (As far as I know, Seymour Papert was the first to use the phrase “toy problem.” At a 1967 AI workshop I attended in Athens, Georgia, he distinguished among tau or “toy” problems, rho or real-world problems, and theta or “theory” problems in artificial intelligence.  This distinction still serves us well today.)

In this part, I’ll describe some of the first real efforts to build intelligent machines.  Some of these were discussed or reported on at conferences and symposia – making these meetings important milestones in the birth of AI. I’ll also do my best to explain the underlying workings of some of these early AI programs.  The rather dramatic successes during this period helped to  establish a solid base for subsequent artificial intelligence research.

Some researchers became intrigued (one might even say captured) by the methods they were using, devoting themselves more to improving the power and generality of their chosen techniques than to applying them to the tasks thought to require them. Moreover, because some researchers were just as interested in explaining how human brains solved problems as they were in getting machines to do so, the methods being developed were often proposed as contributions to theories about human mental processes. Thus, research in cognitive psychology and research in artificial intelligence became highly
intertwined.

Chapter 3: Gatherings

In September 1948, an interdisciplinary conference was held at the California Institute of Technology (Caltech) in Pasadena, California, on the topics of how the nervous system controls behavior and how the brain might be compared to a computer.  It was called the Hixon Symposium on Cerebral Mechanisms in Behavior.  Several luminaries attended and gave papers, among them Warren McCulloch, John von Neumann, and Karl Lashley (1890–1958), a prominent psychologist.  Lashley gave what some thought was the most important talk at the symposium.  He faulted behaviorism for its static view of brain function and claimed that to explain human abilities for planning and language, psychologists would have to begin considering dynamic, hierarchical  structures.  Lashley’s talk laid out the foundations for what would become cognitive science.1

The emergence of artificial intelligence as a full-fledged field of research coincided with (and was launched by) three important meetings – one in 1955, one in 1956, and one in 1958.  In 1955, a “Session on Learning Machines” was held in conjunction with the 1955 Western Joint Computer Conference in Los Angeles.  In 1956 a “Summer Research Project on Artificial Intelligence” was convened at Dartmouth College.  And in 1958 a symposium on the “Mechanization of Thought Processes,” was sponsored by the National Physical Laboratory in the United Kingdom.

3.1     Session  on  Learning  Machines

Four important papers were presented in Los Angeles in 1955.  In his chairman’s introduction to this session, Willis Ware wrote

These papers do not suggest that future learning machines should be built in the pattern of the general-purpose digital computing device; it is rather that the digital computing system offers a convenient and highly flexible tool to probe the behavior of the models.  . . . This group of papers suggests directions of improvement for future machine builders whose intent is to utilize digital computing machinery for this particular model technique. Speed of operation must be increased manyfold; simultaneous operation in many parallel modes is strongly indicated; the size of random access storage must jump several orders of magnitude; new types of input–output equipment are needed.  With such advancements and the techniques discussed in these papers, there  is considerable promise that systems can be built in the relatively near future which will imitate considerable portions of the activity of the brain and nervous system.

Fortunately, we have made substantial progress on the items on Ware’s list of “directions for improvement.” Speed of operation has increased manyfold, parallel operation is utilized in many AI systems, random access storage has jumped several orders of magnitude, and many new types of input–output equipment are available.  Perhaps even further improvements will be necessary.

The session’s first paper, by Wesley Clark and Belmont Farley of MIT’s Lincoln Laboratory, described some pattern-recognition experiments on networks of neuron-like elements.2   Motivated by Hebb’s proposal that assemblies of neurons could learn and adapt by adjusting the strengths of their interconnections, experimenters had been trying various schemes for adjusting the strengths of connections within their networks, which were usually simulated on computers.  Some just wanted to see what these networks might do whereas others, such as Clark and Farley, were interested in specific applications, such as pattern recognition.  To the dismay of neurophysiologists, who complained about oversimplification, these networks came to be called neural networks.  Clark and Farley concluded that “crude but useful generalization properties are possessed even by randomly connected nets of the type described.”3

The next pair of papers, one by Gerald P. Dinneen (1924– ) and one by Oliver Selfridge (1926–2008; Fig. 3.1), both from MIT’s Lincoln Laboratory, presented a different approach to pattern recognition.  Dinneen’s paper4 described computational techniques for processing images.  The images were presented to the computer as a rectangular array of intensity values corresponding to the various shades of gray in the image. Dinneen pioneered the use of filtering methods to remove random bits of noise, thicken lines, and find edges.  He began his paper with the following:

Over the past months in a series of after-hour and luncheon meetings, a group of us at the laboratory have speculated on problems in this area.  Our feeling, pretty much unanimously, was that there is a real need to get practical, to pick a real live problem and go after it.

Selfridge’s paper5  was a companion piece to that of Dinneen. Operating on “cleaned-up” images (as might be produced by Dinneen’s program, for example), Selfridge described techniques for highlighting “features” in these images and then classifying them based on the features. For example, corners of an image known to be either a square or a triangle are highlighted, and then the number of corners is counted to determine whether the image is of a square or of a triangle. Selfridge said that “eventually, we hope to be able to recognize other kinds of features, such as curvature, juxtaposition of singular points (that is, their relative bearings and distances), and so forth.”

The methods pioneered by Selfridge and Dinneen are fundamental to most of the later work in enabling machines to “see.” Their work is all the more remarkable when one considers that it was done on a computer, the Lincoln Laboratory “Memory Test Computer,” that today would be regarded as extremely primitive. [The Memory Test Computer (MTC) was the first to use the ferrite core random-access memory modules developed by Jay Forrester. It was designed and built by Ken Olsen in 1953 at the Digital Equipment Corporation (DEC). The MTC was the first computer to simulate the operation of neural networks – those of Clark and Farley.]

The next paper6 was about programming a computer to play chess. It was written by Allen Newell, then a researcher at the Rand Corporation in Santa Monica. Thanks to a biographical sketch of Newell written by his colleague, Herb Simon of Carnegie Mellon University, we know something about Newell’s motivation and how he came to be interested in this problem:

In September 1954 Allen attended a seminar at RAND in which Oliver Selfridge of Lincoln Laboratory described a running computer program that learned to recognize letters and other patterns.  While listening to Selfridge characterizing his rather primitive but operative system, Allen experienced what he always referred to as his “conversion experience.” It became instantly clear to him “that intelligent adaptive systems could be built that were far more complex than anything yet done.” To the knowledge Allen already had about computers (including their symbolic capabilities), about heuristics, about information processing in organizations, about cybernetics, and proposals for chess programs was now added a concrete demonstration of the feasibility of computer simulation of complex processes.  Right then he committed himself to understanding human learning and thinking by simulating it.

Simon goes on to summarize Newell’s paper on chess:

[It] outlined an imaginative design for a computer program to play chess in humanoid fashion, incorporating notions of goals, aspiration levels for terminating search, satisfying with “good enough” moves, multidimensional evaluation functions, the generation of subgoals to implement goals, and something like best first search.  Information about the board was to be expressed symbolically in a language resembling the predicate calculus.  The design was never implemented, but ideas were later borrowed from it for use in the NSS [Newell, Shaw, and Simon] chess program in 1958.8

Newell hinted that his aims extended beyond chess.  In his paper he wrote “The aim of this effort, then, is to program a current computer to learn to play good chess.  This is the means to understanding more about the kinds of computers, mechanisms, and programs that are necessary to handle ultracomplicated problems.” Newell’s proposed techniques can be regarded as his first attempt to produce evidence for what he and Simon later called the Physical Symbol System Hypothesis.

Walter Pitts, a commentator for this session, concluded it by saying, “But, whereas Messrs.  Farley, Clark, Selfridge, and Dinneen are imitating the nervous system, Mr.  Newell prefers to imitate the hierarchy of final causes traditionally called the mind.  It will come to the same thing in the end, no doubt....” To “come to the same thing,” these two approaches, neural modeling and symbol processing, must be recognized simply as different levels of description of what goes on in the brain.  Different levels are appropriate for describing different kinds of mental phenomena.  I’ll have more to say about description levels later in the book.

3.2     The  Dartmouth  Summer  Project

In 1954, John McCarthy (1927– ; Fig 3.2) joined Dartmouth College in Hanover, New Hampshire, as an Assistant Professor of Mathematics.
McCarthy had been developing a continuing interest in what would come to be called artificial intelligence.  It was “triggered,” he says, “by attending the September 1948 Hixon Symposium on Cerebral Mechanisms in Behavior held at Caltech where I was starting graduate work in mathematics.”9  While at Dartmouth he was invited by Nathaniel Rochester (1919–2001) to spend the summer of 1955 in Rochester’s Information Research Department at IBM in Poughkeepsie, New York.  Rochester had been the designer of the IBM 701 computer and had also participated in research on neural networks.10

At IBM that summer, McCarthy and Rochester persuaded Claude Shannon and Marvin Minsky (1927– ; Fig.  3.2), then a Harvard junior fellow in mathematics and neurology, to join them in proposing a workshop to be held at Dartmouth during the following summer.  Shannon, whom I have previously mentioned, was a mathematician at Bell Telephone Laboratories and already famous for his work on switching theory and statistical information theory.
McCarthy took the lead in writing the proposal and in organizing what was to be called a “Summer Research Project on Artificial Intelligence.” The proposal was submitted to the Rockefeller Foundation in August 1955.

Extracts from the proposal read as follows:11

We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire.  The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.  An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.  We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.

For the present purpose the artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving.

The Rockefeller Foundation did provide funding for the event, which took plac   during six weeks of the summer of 1956.  It turned out, however, to be more of a rolling six-week workshop than a summer “study.” Among the people attending the workshop that summer, in addition to McCarthy,  Minsky, Rochester, and Shannon were Arthur Samuel (1901–1990), an  engineer at the IBM corporation who had already written a program to play checkers, Oliver Selfridge, Ray Solomonoff of MIT, who was interested in automating induction, Allen Newell, and Herbert Simon.  Newell and Simon (together with another Rand scientist, Cliff Shaw) had produced a program for proving theorems in symbolic logic.  Another attending IBM scientist was Alex Bernstein, who was working on a chess-playing program.

McCarthy has given a couple of reasons for using the term “artificial intelligence.” The first was to distinguish the subject matter proposed for the Dartmouth workshop from that of a prior volume of solicited papers, titled Automata Studies, co-edited by McCarthy and Shannon, which (to McCarthy’s disappointment) largely concerned the esoteric and rather narrow mathematical subject called “automata theory.” The second, according to McCarthy, was “to escape association with ‘cybernetics.’  Its concentration on analog feedback seemed misguided, and I wished to avoid having either to accept Norbert Wiener as a guru or having to argue with him."

There was (and still is) controversy surrounding the name. According to Pamela McCorduck’s excellent history of the early days of artificial intelligence, Art Samuel remarked, “The word artificial makes you think there’s something kind of phony about this, or else it sounds like it’s all artificial and there’s nothing real about this work at all.”13 McCorduck goes on to say that “[n]either Newell or Simon liked the phrase, and called their own work complex information processing for years thereafter.” But most of the people who signed on to do work in this new field (including myself) used the name “artificial intelligence,” and that is what the field is called today. (Later, Newell became reconciled to the name. In commenting about the content of the field, he concluded, “So cherish the name artificial intelligence. It is a good name. Like all names of scientific fields, it will grow to become exactly what its field comes to mean.”)

The approaches and motivations of the people at the workshop differed. Rochester came to the conference with a background in networks of neuron-like elements. Newell and Simon had been pursuing (indeed had helped originate) the symbol-processing approach. Among the topics Shannon wanted to think about (according to the proposal) was the “application of information theory concepts to computing machines and brain models.” (After the workshop, however, Shannon turned his attention away from artificial intelligence.)

McCarthy wrote that he was interested in constructing “an artificial language which a computer can be programmed to use on problems requiring conjecture and self-reference. It should correspond to English in the sense that short English statements about the given subject matter should have short correspondents in the language and so should short arguments or conjectural arguments. I hope to try to formulate a language having these properties . . . ” Although McCarthy later said that his ideas on this topic were still too “ill formed” for presentation at the conference, it was not long before he made specific proposals for using a logical language and its inference mechanisms for representing and reasoning about knowledge.

Although Minsky’s Ph.D. dissertation15 and some of his subsequent work concentrated on neural nets, around the time of the Dartmouth workshop he was beginning to change direction. Now, he wrote, he wanted to consider a machine that “would tend to build up within itself an abstract model of the environment in which it is placed. If it were given a problem, it could first explore solutions within the internal abstract model of the environment and then attempt external experiments.” At the workshop, Minsky continued work on a draft that was later to be published as a foundational paper, “Steps Toward Artificial Intelligence.”16

One of the most important technical contributions of the 1956 meeting was work presented by Newell and Simon on their program, the “Logic Theorist (LT),” for proving theorems in symbolic logic. LT was concrete evidence that processing “symbol structures” and the use of what Newell and Simon called “heuristics” were fundamental to intelligent problem solving. I’ll describe some of these ideas in more detail in a subsequent chapter.
Newell and Simon had been working on ideas for LT for some months and became convinced in late 1955 that they could be embodied in a working program.  According to Edward Feigenbaum (1936– ), who was taking a course from Herb Simon at Carnegie in early 1956, “It was just after Christmas vacation – January 1956 – when Herb Simon came into the classroom and said, ‘Over Christmas Allen Newell and I invented a thinking machine.’”17 What was soon to be programmed as LT was the “thinking machine” Simon was talking about.  He called it such, no doubt, because he thought it used some of the same methods for solving problems that humans use.  Simon later wrote18  “On Thursday, Dec.  15. . . I succeeded in simulating by hand the first proof. . . I have always celebrated Dec.  15, 1955, as the birthday of heuristic problem solving by computer.” According to Simon’s autobiography Models of My Life,19  LT began by hand simulation, using his children as the computing elements, while writing on and holding up note cards as the registers that contained the state variables of the program.

Another topic discussed at Dartmouth was the problem of proving theorems in geometry.  (Perhaps some readers will recall their struggles with geometry proofs in high school.)  Minsky had already been thinking about a program to prove geometry theorems.  McCorduck quotes him as saying the following:

[P]robably the important event in my own development – and the explanation of my perhaps surprisingly casual acceptance of the Newell–Shaw–Simon work – was that I had sketched out the heuristic search procedure for [a] geometry machine and then been able to hand-simulate it on paper in the course of an hour or so. Under my hand the new proof of the isosceles-triangle theorem came to life, a proof that was new and elegant to the participants – later, we found that proof was well-known. . .

In July 2006, another conference was held at Dartmouth celebrating the fiftieth anniversary of the original conference.  (See Fig.  3.3.)  Several of the founders and other prominent AI researchers attended and surveyed what had been achieved since 1956.  McCarthy reminisced that the “main reason the 1956 Dartmouth workshop did not live up to my expectations is that AI is harder than we thought.” In any case, the 1956 workshop is considered to be the official beginning of serious work in artificial intelligence, and Minsky, McCarthy, Newell, and Simon came to be regarded as the “fathers” of AI. A plaque was dedicated and installed at the Baker Library at Dartmouth commemorating the beginning of artificial intelligence as a scientific discipline.

3.3     Mechanization  of  Thought  Processes

In November 1958, a symposium on the “Mechanisation of Thought Processes” was held at the National Physical Laboratory in Teddington, Middlesex, England.  According to the preface of the conference proceedings, the symposium was held “to bring together scientists studying artificial thinking, character and pattern recognition, learning, mechanical language translation, biology, automatic programming, industrial planning and clerical mechanization.”

Among the people who presented papers at this symposium were many whom I have already mentioned in this story.  They include Minsky (by then a staff member at Lincoln Laboratory and on his way to becoming an assistant professor of Mathematics at MIT), McCarthy (by then an assistant professor of Communication Sciences at MIT), Ashby, Selfridge, and McCulloch.  (John Backus, one of the developers of the computer programming language FORTRAN, and Grace Murray Hopper, a pioneer in “automatic programming,” also gave papers.)

The proceedings of this conference22  contains some papers that became quite influential in the history of artificial intelligence.  Among these, I’ll mention ones by Minsky, McCarthy, and Selfridge.

Minsky’s paper, “Some Methods of Artificial Intelligence and Heuristic Programming,” was the latest version of a piece he had been working on since just before the Dartmouth workshop.  The paper described various methods that were (and could be) used in heuristic programming.  It also covered methods for pattern recognition, learning, and planning.  The final version, which was soon to be published as “Steps Toward Artificial Intelligence,” was to become required reading for new recruits to the field (including me).

I have already mentioned McCarthy’s hope to develop an artificial language for AI. He summarized his conference paper, “Programs with Common Sense,” as follows:

This paper will discuss programs to manipulate in a suitable formal language (most likely a part of the predicate calculus) common instrumental statements.  The basic program will draw immediate conclusions from a list of premises.  These conclusions will be either declarative or imperative sentences.  When an imperative sentence is deduced, the program takes a corresponding action.

In his paper, McCarthy suggested that facts needed by an AI program, which he called the “advice taker,” might be represented as expressions in a mathematical (and computer-friendly) language called “first-order logic.” For example, the facts “I am at my desk” and “My desk is at home” would be represented as the expressions at(I, desk) and at(desk, home).  These, together with similarly represented information about how to achieve a change in location (by walking and driving for example), could then be used by the proposed (but not yet programmed) advice taker to figure out how to achieve some goal, such as being at the airport.  The advice taker’s reasoning process would produce imperative logical expressions involving walking to the car and driving to the airport.

Representing facts in a logical language has several advantages.  As McCarthy later put it,23

Expressing information in declarative sentences is far more modular than expressing it in segments of computer program or in tables.  Sentences can be true in much wider contexts than specific programs can be useful.  The supplier of a fact does not have to understand much about how the receiver functions, or how or whether the receiver will use it.  The same fact can be used for many purposes, because the logical consequences of collections of facts can be available.

McCarthy later expanded on these ideas in a companion memorandum. As I’ll mention later, some of McCarthy’s advice-taker proposals were finally implemented by a Stanford graduate student, C. Cordell Green.

I have already mentioned the 1955 pattern-recognition work of Oliver Selfridge. At the 1958 Teddington Symposium, Selfridge presented a paper on a new model for pattern recognition (and possibly for other cognitive tasks also). He called it “Pandemonium,” meaning the place of all the demons.
His model is especially interesting because its components, which Selfridge called “demons,” can either be instantiated as performing lower level nerve-cell-type functions or higher level cognitive functions (of the symbol-processing variety). Thus, Pandemonium can take the form of a neural network, a hierarchically organized set of symbol processors – all working in parallel, or some combination of these forms. If the latter, the model is a provocative proposal for joining these two disparate approaches to AI.

In the introduction to his paper, Selfridge emphasized the importance of computations performed in parallel:

The basic motif behind our model is the notion of parallel processing. This is suggested on two grounds: first, it is often easier to handle data in a parallel manner, and, indeed, it is usually the more “natural” manner to handle it in; and, secondly, it is easier to modify an assembly of quasi-independent modules than a machine all of whose parts interact immediately and in a complex way.

Selfridge made several suggestions about how Pandemonium could learn.
It’s worth describing some of these because they foreshadow later work in machine learning. But first I must say a bit more about the structure of Pandemonium.

Pandemonium’s structure is something like that of a business organization chart. At the bottom level are workers, whom Selfridge called the “data demons.” These are computational processes that “look at” the input data, say an image of a printed letter or number. Each demon looks for something specific in the image, perhaps a horizontal bar; another might look for a vertical bar; another for an arc of a circle; and so on. Each demon “shouts” its findings to a set of demons higher in the organization. (Think of these higher level demons as middle-level managers.) The loudness of a demon’s shout depends on how certain it is that it is seeing what it is looking for. Of course, Selfridge is speaking metaphorically when he uses terms such as “looking for” and “shouting.” Suffice it to say that it is not too difficult to program computers to “look for” certain features in an image. (Selfridge had already shown how that could be done in his 1955 paper that I mentioned earlier.) And a “shout” is really the strength of the output of a computational process.

Each of the next level of demons specializes in listening for a particular combination of shouts from the data demons.  For example, one of the demons at this level might be tuned to listen for shouts from data demon 3, data demon 11, and data demon 22.  If it finds that these particular demons are shouting loudly, it responds with a shout of its own to the demons one level up in the hierarchy, and so on.

Just below the top level of the organization are what Selfridge called the “cognitive demons.” As at the other levels, these listen for particular combinations of shouts from the demons at the level below, and they respond with shouts of their own to a final “decision demon” at the top – the overall boss.  Depending on what it hears from its “staff,” the decision demon finally announces what it thinks is the identity of the image – perhaps the letter “A” or the letter “R” or whatever.

Actual demon design depends on what task Pandemonium is supposed to be doing.  But even without specifying what each demon was to do, Selfridge made very interesting proposals about how Pandemonium could learn to perform better at whatever it was supposed to be doing.  One of his proposals involved equipping each demon with what amounted to a “megaphone” through which it delivered its shout.  The volume level of the megaphone could be adjusted.  (Selfridge’s Pandemonium is just a bit more complicated than the version I am describing.  His version has each demon using different channels for communicating with each of the different demons above it.  The volume of the shout going up each channel is individually adjusted by the learning mechanism.)  The demons were not allowed to set their own volume levels, however.  All volume levels were to be set through an outside learning process attempting to improve the performance of the whole assembly.  Imagine that the volume levels are initially set either at random or at whatever a designer thinks would be appropriate.  The device is then tested on some sample of input data and its performance score is noted.  Say, it gets a score of 81%.
Then, small adjustments are made to the volume levels in all possible ways until a set of adjustments is found that improves the score the most, say to 83%.  This particular set of small adjustments is then made and the process is repeated over and over (possibly on additional data) until no further improvement can be made.

(Because there might be a lot of megaphones in the organization, it might seem impractical to make adjustments in all possible ways and to test each of these ways to find its score.  The process might indeed take some time, but computers are fast – even more so today.  Later in the book, I’ll show how one can calculate, rather than find by experiment, the best adjustments to make in neural networks organized like Pandemonium.)

If we think of the score as the height of some landscape and the adjustments as movements over the landscape, the process can be likened to climbing a hill by always taking steps in the direction of steepest ascent. Gradient ascent (or hill-climbing methods, as they are sometimes called) are well known in mathematics.  Selfridge had this to say about some of the pitfalls of their use:

This may be described as one of the problems of training, namely, to encourage the machine or organism to get enough on the foot-hills so that small changes. . . will produce noticeable improvement in his altitude or score.  One can describe learning situations where most of the difficulty of the task lies in finding any way of improving one’s score, such as learning to ride a unicycle, where it takes longer to stay on for a second than it does to improve that one second to a minute; and others where it is easy to do a little well and very hard to do very well, such as learning to play chess.  It’s also true that often the main peak is a plateau rather than an isolated spike.

Selfridge described another method for learning in Pandemonium.  This method might be likened to replacing managers in an organization who do not perform well.  As Selfridge puts it,

At the conception of our demoniac assembly we collected somewhat arbitrarily a large number of subdemons which we guessed would be useful. . . but we have no assurance at all that the particular subdemons we selected are good ones.  Subdemon selection generates new subdemons for trial and eliminates inefficient ones, that is, ones that do not much help improve the score.

The demon selection process begins after the volume-adjusting learning mechanism has run for a while with no further improvements in the score. Then the “worth” of each demon is evaluated by using, as Selfridge suggests, a method based on the learned volume levels of their shouting.  Demons having high volume levels have a large effect on the final score, and so they can be thought to have high worth.  First, the demons with low volume levels are eliminated entirely.  (That step can’t hurt the score very much.)  Next, some of the demons undergo random “mutations” and are put back in service.  Next, some pairs of worthy demons are selected and, as Selfridge says, “conjugated” into offspring demons.  The precise method Selfridge proposed for conjugation need not concern us here, but the spirit of the process is to produce offspring that share, one hopes, useful properties of the parents.  The offspring are then put into service.  Now the whole process of adjusting volume levels of the surviving and “evolved” demons can begin again to see whether the score of the new assembly can be further improved.

Chapter 8: 1960's Infrastructure

The technical developments during the 1960s were aided (indeed, one might say made possible) by several systems support and societal factors.  New computer languages made it much easier to build AI systems.  Researchers from mathematics, from cognitive science, from linguistics, and from what soon would be called “computer science” came together in meetings and in newly formed laboratories to attack the problem of mechanizing intelligent behavior. In addition, government agencies and companies, concluding that they had an important stake in this new enterprise, provided needed research support.

8.1     Programming  Languages

Newell and Simon were among the first to realize that a specialized computer language would be useful for manipulating the symbolic expressions that were at the heart of their approach to mechanizing intelligence.  The most elementary kind of symbolic expression is a list of symbols, such as (7, B, 5). More complex structures can be composed by creating lists of lists of symbols and lists of lists of lists, and so on.

In my description of symbol structures for the eight-puzzle, I mentioned the kinds of manipulations that are needed.  Recall that the starting position of the eight-puzzle was represented by the expression

((2, 8, 3), (1, 6, 4), (7, B, 5)).

What was needed was a language for writing programs that could produce expressions representing the positions corresponding to moves of the puzzle. For example, one of the moves that can be made from the starting position is represented by the expression

((2, 8, 3), (1, 6, 4), (B, 7, 5))."

To produce this expression, the program must copy the starting position expression and then interchange the first and second elements of the third list in that expression.

Newell, Shaw, and Simon set about to develop a language in which these kinds of manipulations could be programmed.  Starting around 1954 at the RAND Corporation, they created a series of languages all called IPL (for information-processing language).  Several versions of the language were developed.  IPL-I was not actually implemented but served as a design specification.  IPL-II was implemented in 1955 for the RAND Corporation’s JOHNNIAC computer.  Later versions (through IPL-VI) were implemented at Carnegie Tech.

The IPL languages were used to program several early AI programs, including LT, GPS, NSS (the Newell, Shaw, Simon chess-playing program), and the programs written by Newell’s and Simon’s students, such as Quillian and George Ernst.  After the Dartmouth summer project, John McCarthy also began thinking about using list-processing languages.  He was aware of the use of  FLPL (FORTRAN fortified by some list-processing operations) in Gelernter’s geometry theorem-proving machine.  Ultimately, however, McCarthy concluded a new language was needed that was easier to use than IPL and more powerful than FLPL.

Starting in the fall of 1958 at MIT, McCarthy began the implementation of a programming language he called LISP (for list processing).    He based it (loosely) on a branch of mathematics of special interest in computation called recursive function theory.  LISP had several elementary operations for copying a list, stripping off elements of a list, adding an element to a list, and checking to see whether something were an element of a list.  From these, arbitrarily complex manipulations of lists could be composed.  An important feature of LISP was that programs for manipulating lists were themselves represented as lists.  Such programs could thus be elements of other lists and could have subprograms embedded in them.  A program could even have a version of itself embedded in it.  As I have already mentioned, programs that can activate versions of themselves as part of their operation are called “recursive” and are very useful (if used with the care needed to avoid endless circularity).1

Because it was easier to use, LISP soon replaced IPL as the primary language of artificial intelligence research and applications.  The programs produced by Minsky’s students, Evans, Raphael, Bobrow, Slagle, and others, were all written in LISP. (Interestingly, Arthur Samuel did not use a
list-processing language for writing his checkers-playing programs.  Rather heroically, he programmed them in the base language of elementary machine operations to make them run efficiently and use memory sparingly.)

Besides developing LISP, McCarthy proposed a method, called “time-sharing,” by which a single computer could be made to serve several users simultaneously – acting as if each user had his or her own private machine.   Working initially with Ed Fredkin at Bolt, Beranek, and Newman (BBN) and later with others, McCarthy developed an early time-sharing system at MIT using a DEC PDP-1 computer.

8.2     Early  AI  Laboratories

In 1955, Newell moved from the RAND Corporation to Carnegie Tech (which became Carnegie Mellon University, CMU, in 1967) to work on a Ph.D. degree in industrial management under Herb Simon.  After completing his degree, Newell stayed on as a professor at Carnegie, and he and Simon began advising a number of Ph.D. students – using the phrase “complex information processing (CIP)” to describe their work.  (For several years they avoided the AI sobriquet.)  In the fall of 1956, Herb Simon took delivery of an IBM 650, which was the first computer used for CIP work.  Later, they used an IBM 704, followed by a series of DEC machines.

John McCarthy moved from Dartmouth to MIT in the fall of 1958. Minsky joined MIT a year later.  As Minsky puts it,

[McCarthy and I] were walking down the hall and we met Jerry Wiesner or Zimmerman or someone and he said how’s it going and we said well, we’re working on these artificial intelligence ideas but we need a little more room and support for some graduate students.  So then a room appeared a few days later. . .

The “room” soon developed into the MIT Artificial Intelligence Project. Initially, the group used MIT’s IBM 704 computer, which proved not to have sufficient memory for the programs being written.  So it began to use a DEC PDP-1 belonging to BBN. With funding from another project at MIT, it bought its own PDP-1, which was followed by the PDP-6 and PDP-10. Several of the group’s Ph.D. students did their work at BBN and at the nearby Lincoln Laboratory where Oliver Selfridge continued his AI research – mainly on pattern recognition and machine learning.  In 1962, McCarthy moved to Stanford where he began an AI project.  Seymour Papert (1928– ), a mathematician who had worked with Jean Piaget, joined Minsky as co-director of the AI Lab in 1963.

By 1965 at Stanford, McCarthy and colleagues had created a time-sharing system, called Thor, on a PDP-1 computer.  It included twelve Philco display terminals, which made it the first display-oriented time-sharing system anywhere in the world.

With the help of Lester Earnest (1930– ), who had moved to Stanford from Lincoln Laboratory, McCarthy set up the Stanford AI Laboratory (SAIL) in 1965.  Outgrowing its on-campus facilities, SAIL moved to a building in the Stanford hills during the summer of 1966.  (See Fig.  8.1.)  With additional support from ARPA, the Lab took delivery of a DEC PDP-6 computer and, later, a PDP-10 computer. In addition to its work in AI (which I’ll describe in subsequent chapters), SAIL was involved in many other computer-related projects including the development of a precursor to computer “windows” and the early installation of terminals in everyone’s offices.

Since their early days, the groups at CMU, MIT, and Stanford have been among the leaders of research in AI. Often graduates of one of these institutions became faculty members of one of the other ones.

Around 1965 another world-class AI center emerged at the University of Edinburgh in Scotland.  Its founder was Donald Michie (1923–2007; Fig.  8.2), who had worked with Alan Turing and I. J. (Jack) Good at Bletchley Park during the Second World War.  Discussions there with Turing and Good about intelligent machines captivated Michie.  As he reported in an October 2002 interview, “I resolved to make machine intelligence my life as soon as such an enterprise became feasible.” Because computer facilities in the mid- to late 1940s were primitive and scarce, Michie became a geneticist and molecular biologist.

Pursuing his interest in machine intelligence, from the sidelines as it were, in 1960 he put together a “contraption of matchboxes and glass beads” that could learn to play tic-tac-toe (noughts and crosses).  He named his “machine” MENACE, an acronym for Matchbox Educable Noughts and Crosses Engine.

In January 1965, Michie became the Director of the UK’s first AI laboratory, the Experimental Programming Unit, at the University of Edinburgh.  This group was to become the Department of Machine Intelligence and Perception in October 1966.  Michie recruited some top-flight computer talent, including Rod Burstall, Robin Popplestone, and John Collins.  Those three developed a list-processing language called POP-2, which was the language used for AI program-writing by members of the Unit.  (I’ll describe some of these programs later.)  For many years, Michie’s group worked collaboratively with a nearby University of Edinburgh group, the Metamathematics Unit under Bernard Meltzer (circa 1916–2008).  The Metamathematics Unit is famous for the work of Robert Boyer and J Strother Moore in mechanized theorem proving and of Robert Kowalski in developing some of the principles of logic programming.8

At IBM in Poughkeepsie, Nathan Rochester and Herb Gelernter continued AI research for a short time after the Dartmouth workshop.  This research resulted in the geometry-theorem-proving machine.  However, soon after, according to a book about government support for computing research, “in spite of the early activity of Rochester and other IBM researchers, the corporation’s interest in AI cooled.  Although work continued on
computer-based checkers and chess, an internal report prepared about 1960 took a strong position against broad support for AI.”9  Perhaps IBM wanted to emphasize how computers helped people perform tasks rather than how they might replace people.  McCarthy’s view about all of this is that “IBM thought that artificial intelligence [that machines were as smart as people] was bad for IBM’s image. . . This may have been associated with one of their other image slogans, which was ‘data processing, not computing.’”10

8.3     Research  Support

As the computing systems needed for AI research became larger and more expensive, and as AI laboratories formed, it became necessary to secure more financial support than was needed in the days when individual investigators began work in the field.  Two of the major sources of funding during the late 1950s and early 1960s were the Office of Naval Research (ONR) and the Advanced Research Projects Agency (ARPA), each a part of the U.S. defense establishment.

ONR was formed shortly after the end of the Second World War.  Its mission was “to plan, foster, and encourage scientific research in recognition of its paramount importance as related to the maintenance of future naval power and the preservation of national security.” Its Information Systems Branch was set up in the mid-1950s under the direction of Marshall Yovits.  The branch supported AI work at several institutions and also sponsored conferences and workshops on self-organizing systems, cybernetics, optical character recognition, and artificial intelligence.  All of this was done in anticipation that these technologies would be generally useful to the U.S. Navy.  (A later director, Marvin Denicoff, supported some of my research and my AI textbook writing.)

The formation of ARPA was, in part, a response to the successful launch of the Soviet satellite Sputnik in 1957.  ARPA’s mission was to provide significant amounts of research funds to attack problem areas important to
U.S. defense.  One of its most important projects in the late 1950s was the development of ablative nose cones to absorb and dissipate heat during ballistic missile reentry.  Its Information Processing Techniques Office (IPTO) was set up in 1962 under the direction of J. C. R. (Lick) Licklider (1915–1990; Fig.  8.4).

“Lick” (as he was called by all who knew him) was a psychoacoustician who worked first at Lincoln Laboratory and MIT and later at BBN. Lick’s 1960 paper, “Man-Computer Symbiosis,” proposed that men and computers should “cooperate in making decisions and controlling complex situations without inflexible dependence on predetermined programs.”

Lick was persuaded that computers would play a very important role in defense – especially in those applications in which people and computers worked together.  At ARPA, he provided funds to MIT for the formation of Project MAC (an acronym for Machine-Aided Cognition and perhaps for Multi-Access Computing or Man And Computers).  [Project MAC, initially founded in July 1963, was later to become the Laboratory for Computer Science (LCS), and still later to evolve into the Computer Science and Artificial Intelligence Laboratory (CSAIL).] Project MAC took Minsky and McCarthy’s Artificial Intelligence Project under its wing and also supported the development of MIT’s Compatible Time-Sharing System (CTSS) under Fernando Corbat´o.  (CTSS work was separate from McCarthy’s time-sharing project.)

ARPA funds helped to establish “centers of excellence” in computer science.  Besides MIT, these centers included Stanford, Carnegie Mellon, and SRI. ARPA also supported computer science work at the RAND Corporation, the Systems Development Corporation, and BBN, among others.  AI was just one of ARPA’s interests.  IPTO also supported research that led to graphical user interfaces (and the mouse), supercomputing, computer hardware and very-large-scale integrated circuits (VLSI), and, perhaps most famously, research that led to the Internet.  According to Licklider, “ARPA budgets did not even include AI as a separate line item until 1968.”12

But as far as AI was concerned, Lick believed that Newell and Simon, Minsky, and McCarthy ought to be provided with research funds adequate to support big AI projects.  With regard to the situation at Stanford (and probably to that at MIT and CMU also), Paul Edwards explained that

[F]unding from ARPA was virtually automatic; Licklider simply asked McCarthy what he wanted and then gave it to him, a procedure unthinkable for most other government agencies. Licklider remembered that ‘it seemed obvious to me that he should have a laboratory supported by ARPA. So I wrote him acontract at that time.”

McCarthy remembers all of this somewhat differently.  Soon after arriving at Stanford in 1962, he sent a proposal to Licklider “to do AI.” McCarthy claims that Licklider demurred at first – citing their close relationship when McCarthy was at MIT and Licklider at BBN – but then gave him “a small contract.”14  But perhaps it was not so “small” compared with how research was usually supported (say by the National Science Foundation) at the time. Les Earnest claims that McCarthy “obtained financial support for a small activity (6 persons) from the Advanced Research Projects Agency (ARPA) beginning June 15, 1963.”

Later, ARPA was renamed DARPA (for Defense Advanced Research Projects Agency) to emphasize its role in defense-related research.  DARPA projects and grants were typically much larger than those of ONR and allowed the purchase of computers and other equipment as well as support for personnel.  It’s hardly an exaggeration to say that a good part of today’s computer-based infrastructure is the result of DARPA research support.

8.4     All  Dressed  Up  and  Places  to  Go

By the mid-1960s AI was well prepared for further advances.  Flushed with early successes it was poised to make rapid progress during the rest of the 1960s and 1970s.  Indeed, many people made enthusiastic predictions.  For example, in a 1957 talk16  Herb Simon predicted that within ten years “a digital computer will be the world’s chess champion unless the rules bar it from competition.” He made three other predictions too.  Within ten years computers would compose music, prove a mathematical theorem, and embody a psychological theory as a program.  He said “it is not my aim to surprise or shock you. . . but the simplest way I can summarize is to say that there are now in the world machines that think, that learn and that create.  Moreover, their ability to do these things is going to increase rapidly until – in a visible future – the range of problems they can handle will be coextensive with the range to which the human mind has been applied.”17  Later Simon said that his predictions were part of an attempt “to give some feeling for what computers would mean” to society.

One could argue that Simon’s predictions about computers composing music and proving a mathematical theorem were realized soon after he made them, but a computer chess champion was not to emerge until forty years later.  And, we are still far, I think, from achieving things “coextensive” with what the human mind can achieve.

Simon was not alone in being optimistic.  According to Hubert Dreyfus, “Marvin Minsky, head of MITs Artificial Intelligence Laboratory, declared in a 1968 press release for Stanley Kubrick’s movie, 2001:  A Space Odyssey, that ‘in 30 years we should have machines whose intelligence is comparable to man’s.’  ”18  The difficulty in assessing these sorts of predictions is that “human-level intelligence” is multifaceted.  By the year 2000, AI programs did outperform humans in many intellectual feats while still having a long way to go in most others.

Even so, what had already been accomplished was an impressive start. More important perhaps than the specific demonstrations of intelligent behavior by machines was the technical base developed during the 1950s and early 1960s.  AI researchers now had the means to represent knowledge by encoding it in networks, as logical formulas, or in other symbol structures tailored to specific problem areas.  Furthermore, they had accumulated experience with heuristic search and other techniques for manipulating and using that knowledge.  Also, researchers now had new programming languages, IPL, LISP, and POP-2, that made it easier to write symbol-processing programs. Complementing all of this symbol-processing technology were neural networks and related statistical approaches to pattern recognition.  These technical assets, along with the organizational and financial ones, provided a solid base for the next stage of AI’s development.

Part III: Efflorescence: Mid-1960s to Mid-1970s

Effloresecence: Mid-1960s to Mid-1970s

During the 1960s and well into the 1970s, AI research blossomed and progress seemed rapid. The laboratories established at MIT, Carnegie Mellon, Stanford, SRI, and Edinburgh expanded, and several new groups got started at other universities and companies. Achievements during the preceding years, even though modest in retrospect, were exciting and full of promise, which enticed several new people into the  eld, myself included. Many of us were just as optimistic about success as Herb Simon and Marvin Minsky were when they made their predictions about rapid progress.

AI entered a period of flowering that led to many new and important inventions. Several ideas originated in the context of Ph.D. dissertation research projects. Others emerged from research laboratories and from individual investigators wrestling with theoretical problems. In this part, I'll highlight some of the important projects and research results. Although not a complete account, they typify much of what was going on in AI during the period. I'll begin with work in computer vision.

Chapter 9: Computer Vision

Sighted humans get much of their information through vision.  That part of AI called “computer vision” (or, sometimes, “machine vision”) deals with giving computers this ability.  Most computer vision work is based on processing
two-dimensional images gathered from a three-dimensional world – images gathered by one or more television cameras, for example.  Because the images are two-dimensional projections of a three-dimensional scene, the imaging process loses information.  That is, different three-dimensional scenes might produce the same two-dimensional image.  Thus, the problem of reconstructing the scene faithfully from an image is impossible in principle.

Yet, people and other animals manage very well in a three-dimensional world.  They seem to be able to interpret the two-dimensional images formed on their retinas in a way that gives them reasonably accurate and useful information about their environments.

Stereo vision, using two eyes, helps provide depth information.  Computer vision too can use “stereopsis” by employing two or more differently located cameras looking at the same scene.  (The same effect can be achieved by having one camera move to different positions.)  When two cameras are used, for example, the images formed by them are slightly displaced with respect to each other, and this displacement can be used to calculate distances to various parts of the scene.  The computation involves comparing the relative locations in the images that correspond to the objects in the scene for which depth measurements are desired.  This “correspondence problem” has been solved in various ways, one of which is to seek high correlations between small areas in one image with small areas in the other.  Once the “disparity” of the location of an image feature in the two images is known, the distance to that part of the scene giving rise to this image feature can be calculated by using trigonometric calculations (which I won’t go into here.)

Perhaps surprisingly, a lot of depth information can be obtained from other cues besides stereo vision.  Some of these cues are inherent in a single image, and I’ll be describing these in later chapters.  Even more importantly, background knowledge about the kinds of objects one is likely to see accounts for much of our ability to interpret images. 

Most people would describe this image as being of two tables, one long and narrow and the other more-or-less square.  Yet, if you measure the actual table tops in the image itself, you might be surprised to find that they are exactly the same size and shape!  (The illustration is based on an illusion  called “turning the tables” by the psychologist Roger Shepherd and is adapted from Michael Bach’s version of Shepherd’s diagram.  If you visit Bach’s Web site, http://www.michaelbach.de/ot/sze shepardTables/, you can watch while one table top moves over to the other without changing shape.)

Something apart from the image provides us with information that induces us to make inferences about the shapes of the three-dimensional tables captured in the two-dimensional image shown in Fig.  9.1.  As we shall see, that extra information consists of two things:  knowledge about the image-forming process under various lighting conditions and knowledge about the kinds of things and their surfaces that occur in our three-dimensional world.  If we could endow computers with this sort of knowledge, perhaps they too would be able to see.

9.1     Hints  from  Biology

There has been a steady flow of information back and forth between scientists attempting to understand how vision works in animals and engineers working on computer vision.  An early example of work at the intersection of these two interests was described in an article titled “What the Frog’s Eye Tells the Frog’s Brain”2  by four scientists at MIT. Guided by previous biological work, the four, Jerome Lettvin, H. R. Maturana, Warren McCulloch, and Walter Pitts, probed the parts of the frog’s brain that processed images.  They found that the frog’s visual system consisted of “detectors” that responded only to certain kinds of things in its visual field.  It had detectors for small, moving convex objects (such as flies) and for a sudden darkening of illumination (such as might be caused by a looming predator).  These, together with a couple of other simple detectors, gave the frog information about food and danger.  In particular, the frog’s visual system did not, apparently, construct a complete three-dimensional model of its visual scene.  As the authors wrote,

The frog does not seem to see or, at any rate, is not concerned with the detail of stationary parts of the world around him.  He will starve to death surrounded by food if it is not moving.  His choice of food is determined only by size and movement.  He will leap to capture any object the size of an insect or worm, providing it moves like one.  He can be fooled easily not only by a bit of dangled meat but by any moving small object.  His sex life is conducted by sound and touch.  His choice of paths in escaping enemies does not seem to be governed by anything more devious than leaping to where it is darker.  Since he is equally at home in water and on land, why should it matter where he lights after jumping or what particular direction he takes?

Other experiments produced further information about how the brain processes visual images.  Neurophysiologists David Hubel (1926– ) and Torsten Wiesel (1924– ) performed a series of experiments, beginning around 1958, which showed that certain neurons in the mammalian visual cortex responded selectively to images and parts of images of specific shapes.  In 1959 they implanted microelectrodes in the primary visual cortex of an anesthetized cat. They found that certain neurons fired rapidly when the cat was shown images of small lines at one angle and that other neurons fired rapidly in response to small lines at another angle.  In fact, they could make a “map” of this area of the cat’s brain, relating neuron location to line angle.  They called these neurons “simple cells” – to be distinguished from other cells, called “complex cells,” that responded selectively to lines moving in a certain direction.  Later work revealed that other neurons were specialized to respond to images containing more complex shapes such as corners, longer lines, and large  edges.3   They found that similar specialized neurons also existed in the brains of monkeys. Hubel and Wiesel were awarded the Nobel Prize in Physiology or Medicine in 1981 (jointly with Roger Sperry for other work).

As I’ll describe in later sections, computer vision researchers were developing methods for extracting lines (both large and small) from images. Hubel and Wiesel’s work helped to confirm their view that finding lines in images was an important part of the visual process.  Yet, straight lines seldom occur in the natural environments in which cats (and humans) evolved, so why do they (and we) have neurons specialized for detecting them?    In fact, in 1992 the neuroscientists Horace B. Barlow and David J. Tolhurst wrote a paper titled “Why Do You Have Edge Detectors?”6  As a possible answer to this question, Anthony J. Bell and Terrence J. Sejnowski later showed mathematically that natural scenes can be analyzed as a weighted summation of small edges even though the scenes themselves do not have obvious edges.

9.2     Recognizing  Faces

In the early 1960s at his Palo Alto company, Panoramic Research, Woodrow (Woody) W. Bledsoe (who later did work on automatic theorem proving at the University of Texas), along with Charles Bisson and Helen Chan (later Helen Chan Wolf), developed techniques for face recognition supported by projects from the CIA.8  Here is a description of their approach taken from a memorial article:9

This [face-recognition] project was labeled man-machine because the human extracted the coordinates of a set of features from the photographs, which were then used by the computer for recognition.  Using a GRAFACON, or RAND TABLET, the operator would extract the coordinates of features such as the center of pupils, the inside corner of eyes, the outside corner of eyes, point of widows peak, and so on.  From these coordinates, a list of 20 distances, such as width of mouth and width of eyes, pupil to pupil, were computed.  These operators could process about 40 pictures an hour.  When building the database, the name of the person in the photograph was associated with the list of computed distances and stored in the computer.  In the recognition phase, the set of distances was compared with the corresponding distance for each photograph, yielding a distance between the photograph and the database record.  The closest records are returned.

Bledsoe continued this work with Peter Hart at SRI after leaving Panoramic in 1966.

Then, in 1970, a Stanford Ph.D. student, Michael D. Kelly, wrote a computer program that was able automatically to detect facial features in pictures and use them to identify people. The task for his program was, as he put it,

to choose, from a collection of pictures of people taken by a TV camera, those pictures that depict the same person.  . . .
In brief, the program works by finding the location of features such as eyes, nose, or shoulders in the pictures.  . . . The interesting and difficult part of the work reported in this thesis is the detection of these features in digital pictures.  The nearest-neighbor method is used for identification of individuals once a set of measurements has been obtained.

Another person who did pioneering work in face recognition was vision researcher Takeo Kanade, now a professor at Carnegie Mellon University.  In a 2007 speech at the Eleventh IEEE International Conference on Computer Vision, he reflected on his early work in this field:12   “I wrote my face recognition program in an assembler language, and ran it on a machine with 10 microsecond cycle time and 20 kB of main memory.  It was with pride that I tested the program with 1000 face images, a rare case at the time when testing with 10 images was called a ‘large-scale’ experiment.” (By the way, Kanade has continued his face recognition work up to the present time.  His face-recognition Web page is at http://www.ri.cmu.edu/labs/lab 51.html.)

Face recognition programs of the 1960s and 1970s had several limitations. They usually required that images be of faces of standard scale, pose, expression, and illumination.  Toward the end of the book, I’ll describe research leading to much more robust automatic face recognition.

9.3     Computer  Vision  of  Three-Dimensional Solid  Objects

9.3.1    An Early Vision System

Lawrence G. Roberts (1937– ), an MIT Ph.D. student working at Lincoln Laboratory, was perhaps the first person to write a program that could identify objects in black-and-white (gray-scale) photographs and determine their orientation and position in space.  (His program was also the first to use a “hidden-line” algorithm, so important in subsequent work in computer graphics.  As chief scientist and later director of ARPA’s Information Processing Techniques Office, Roberts later played an important role in the creation of the Arpanet, the forerunner of the Internet.)

In the introduction to his 1963 MIT Ph.D. dissertation,13  Roberts wrote

The problem of machine recognition of pictorial data has long been a challenging goal, but has seldom been attempted with anything more complex than alphabetic characters.  Many people have felt that research on character recognition would be a first step, leading the way to a more general pattern recognition system.  However, the multitudinous attempts at character recognition, including my own, have not led very far.  The reason, I feel, is that the study of abstract, two-dimensional forms leads us away from, not toward, the techniques necessary for the recognition of three-dimensional objects.  The perception of solid objects is a process which can be based on the properties of three-dimensional transformations and the laws of nature.  By carefully utilizing these properties, a procedure has been developed which not only identifies objects, but also determines their orientation and position in space.

Roberts’s system first processed a photograph of a scene to produce a representation of a line drawing.  It then transformed the line drawing into a three-dimensional representation.  Matching this representation against a stored list of representations of solid objects allowed it to classify the object it was viewing.  It could also produce a computer-graphics image of the object as it might be seen from any point of view.

Our main interest here is in how Roberts processed the photographic image.  After scanning the photograph and representing it as an array of numbers (pixels) representing intensity values, Roberts used a special calculation, later called the “Roberts Cross,” to determine whether or not each small 2 × 2 square in the array corresponded to a part of the image having an abrupt change in image intensity.  (The Roberts Cross was the first example of what were later called “gradient operators.”) He then rerepresented the image “lighting up” only those parts of the image where the intensity changed abruptly and leaving “dark” those parts of the image with more-or-less uniform intensity.  The result of this process is illustrated in Fig.  9.2 for a typical image used in Roberts’s dissertation.  As can be seen in that figure, large changes in image intensity are usually associated with the edges of objects.  Thus, gradient operators, such as the Roberts Cross, are often called “edge detectors.”

Further processing of the image on the right attempted to connect the dots representing abrupt intensity changes by small straight-line segments, then by longer line segments.  Finally, a line drawing of the image was produced.  This final step is shown in Fig.  9.3.

Roberts’s program was able to analyze many different photographs of solid objects.  He commented that “The entire picture-to-line-drawing process is not optimal but works for simple pictures.” Roberts’s success stimulated further work on programs for finding lines in images and for assembling these lines into representations of objects.  Perhaps primed by Roberts’s choice of solid objects, much of the subsequent work dealt with toy blocks (or “bricks” as they are called in Britain).

9.3.2    The “Summer Vision Project”

Interestingly, Larry Roberts was a student of MIT information theory professor Peter Elias, not of Marvin Minsky.  But Minsky’s group soon began to work on computer vision also.  In the summer of 1966, the mathematician and psychologist Seymour Papert, a recent arrival at MIT’s Artificial Intelligence Group, launched a “summer vision project.” Its goal was to develop a suite of programs that would analyze a picture from a “videsector” (a kind of scanner) to “actually name objects [such as balls, cylinders, and blocks] by matching them with a vocabulary of known objects.” One motivation for the project was “to use our summer workers effectively in the "construction of a significant part of a visual system.”

Of course, the problem of constructing “a significant part of a visual system” was much more difficult than Papert expected.  Nevertheless, the project was successful in that it began a sustained effort in computer vision research at MIT, which continues to this day.

After these early forays at MIT (and similar ones at Stanford and SRI to be described shortly), computer vision research focused on two areas.  The first was what might be called “low-level” vision – those first stages of image processing that were aimed at constructing a representation of the image as a line drawing, given an image that was of a scene containing rather simple objects.  The second area was concerned with how to analyze the line drawing as an assemblage of separate objects that could be located and identified.  An important part of low-level vision was “image filtering,” to be described next.

9.3.3    Image Filtering

The idea of filtering an image to simplify it, to correct for noise, and to enhance certain image features had been around for a decade or more.  I have already mentioned, for example, that in 1955 Gerald P. Dinneen processed images to remove noise and enhance edges.  Russell Kirsch and colleagues had also experimented with image processing.15   (Readers who have manipulated their digital photography pictures on a computer have used some of these image filters.)  Filtering two-dimensional images is not so very different from filtering one-dimensional electronic signals – a commonplace operation.
Perhaps the simplest operation to describe is “averaging,” which blurs fine detail and removes random noise specks.  As in all averaging operations, image averaging takes into account adjacent values and combines them.  Consider, for example, the image array of intensity values shown in Fig.  9.4 containing a
3 × 3 “averaging window” outlined in bold.  These intensity values correspond to an image whose right side is bright and whose left side is dark with a sharp edge between.  (I adopt the convention that large numbers, such as 10 correspond to brightly illuminated parts of the image, and the number 0 corresponds to black.)

The averaging operation moves the averaging window over the entire image so that its center lies over each pixel in turn.  For each placement of the window, the value of the intensity at its center is replaced (in the filtered version) by the average intensity of the values within the window.  (The process of moving a window around the image and doing calculations based on the numbers in the window is called convolution.)  In this example, the 0 at the center of the window would be replaced by 3.33 (perhaps rounded down to 3).  One can see that averaging blurs the sharp edge – with the 10 fading to (a rounded) 7 fading to 3 fading to 0 as one moves from right to left.  However, intensities well within evenly illuminated regions are not changed.

Chapter 13: Progress in Natural Language Processing

As mentioned previously, the problems of understanding, generating, and translating material in ordinary human (rather than computer) languages fall under the heading of natural language processing.  During the “early explorations” phase of AI research, some good beginnings were made on NLP problems.  In the subsequent phase, the late 1960s to early 1970s, new work built on these foundations, as I’ll describe in this part of the book.

13.1     Machine  Translation

W. John Hutchins, who has written extensively about the history of machine translation (MT), has called the period 1967 to 1976, “the quiet decade.”1 Inactivity in the field during this period is due in part to the ALPAC report, which, as I have already said, was pessimistic about the prospects for machine translation.  Hutchins claimed “The influence of the ALPAC report was profound.  It brought a virtual end to MT research in the USA for over a decade and MT was for many years perceived as a complete failure.  . . . The focus of MT activity switched from the United States to Canada and to Europe.”2

One exception to this decade-long lull in the United States was the development of the Systran (System Translator) translating program by Petr Toma, a Hungarian-born computer scientist and linguistics researcher who had worked on the Georgetown Russian-to-English translation system.  In 1968, Toma set up a company called Latsec, Inc., in La Jolla, California, to continue the Systran development work he had begun earlier in Germany.  The U.S. Air Force gave the company a contract to develop a Russian-to-English translation system.  It was tested in early 1969 at the Wright-Patterson Air Force Base in Dayton, Ohio, “where it continues to provide Russian–English translations for the USAF’s Foreign Technology Division to this day.”3  Systran has evolved to be one of the main automatic translation systems.  It is marketed by the Imageforce Corporation in Tampa, Florida.

How well does Systran translate?  It all depends on how one wants to measure performance.  Margaret Boden mentions two measures, namely, “intelligibility” and “correctness.” Both of these measures depend on human judgement.  For the first, one asks “Can the translation be generally understood?” For the second, one asks “Do human ‘post-editors’ need to modify the translation?” Boden states that “in the two-year period from 1976 to 1978, the intelligibility of translations generated by Systran rose from 45 to 78 percent for [raw text input]. . . ” She also notes that human translations score only 98 to 99 percent, not 100 percent.  Regarding correctness, Boden states that in 1978 “only 64 percent of the words were left untouched by human post-editors.  Even so, human post-editing of a page of Systran output took only twenty minutes in the mid-1980s, whereas normal (fully human) translation would have taken an hour.”

13.2     Understanding

Although the late 1960s and early 1970s might have been a “quiet decade” for machine translation, it was a very active period for other NLP work.
Researchers during these years applied much more powerful syntactic, semantic, and inference abilities to the problem of understanding natural language.  Typical of the new attitude was the following observation by Terry Winograd, an MIT Ph.D. student during the late 1960s:6

If we really want computers to understand us, we need to give them ability to use more knowledge.  In addition to a grammar of the language, they need to have all sorts of knowledge about the subject they are discussing, and they have to use reasoning to combine facts in the right way to understand a sentence and to respond to it.  The process of understanding a sentence has to combine grammar, semantics, and reasoning in a very intimate way, calling on each part to help with the others.

Chapter 14: Game Playing

I have already mentioned attempts to program computers to play board games, such as chess and checkers. The most successful of these was Arthur Samuel’s checker-playing program.  In 1967, Samuel published a paper describing an improved version of his program. He had refined the program’s search procedure and incorporated better “book-learning” capabilities, and instead of calculating the estimated value of a position by adding up weighted feature values, he used hierarchically organized tables.  According to Richard Sutton, “This version learned to play much better than the 1959 program, though still not at a master level.”

Between 1959 and 1962, a group of MIT students, advised by John McCarthy, developed a chess-playing program.  It was based on earlier programs for the IBM 704 written by McCarthy.  One of the group members, Alan Kotok (1941–2006) described the program in his MIT bachelor’s thesis.3 The program was written in a combination of  FORTRAN and machine (assembly) code and ran on the IBM 7090 computer at MIT. It used the alpha–beta procedure (as discussed earlier) to avoid generating branches of the search tree that could be eliminated without altering the final result.  Kotok claimed that his program did not complete any games but “played four long game fragments in which it played chess comparable to an amateur with about 100 games experience.  . . . Most of the machine’s moves are neither brilliant nor stupid.  It must be admitted that it occasionally blunders.”  When McCarthy moved to Stanford, he took the program along with him and continued to work on it.

In the meantime, a computer chess program was being developed by Georgi Adelson-Velskiy and colleagues in Alexander Kronrod’s laboratory at the Institute for Theoretical and Experimental Physics (ITEP) in Moscow.5 During a visit to the Soviet Union in 1965, McCarthy accepted a challenge to have the Kotok–McCarthy program play the Soviet program.  Beginning on November 22, 1967, and continuing for about nine months, the "Kotok–McCarthy program (running on a DEC PDP-6 at Stanford) played the Russian program (running on the Russian M-20 computer at ITEP) – the first match to be played by a computer against a computer.  In each of the first two games, the Stanford program eked out a draw (by surviving until the agreed-upon limit of 40 moves) against a weak version of the Russian program. However, it lost the last two games against a stronger version of the ITEP program.  McCarthy later claimed that, although Stanford had the better computer, ITEP had the better programs.6   The ITEP program was the forerunner of the much improved Kaissa program developed later by Misha Donskoy, Vladimir Arlazarov, and Alexander Ushkov at the Institute of Control Science in Moscow.

Richard Greenblatt, an expert programmer at the AI Lab at MIT, thought he could improve on Kotok’s chess program.  His work on computer chess eventually led to a program he called MAC HACK VI.7  Being an expert chess player himself, he was able to incorporate a number of excellent heuristics for choosing moves and for evaluating moves in his program.  Running on the AI Lab’s DEC PDP-6 and written in efficient machine code, MAC HACK VI was the first program to play in tournaments against human chess players.  In an April 1967 tournament, it won two games and drew two, achieving a rating of 1450 on the U.S. Chess Federation rating scale, about the level of an amateur human player.  (According to the international rating system for human chess players, the highest level is that of Grand Masters.  Then come International Masters, National Masters, Experts, Class A, Class B, and so on.  MAC HACK VI played at the high Class C to low Class B level, which is still quite far from master play.)  It became an honorary member of the U.S. Chess Federation and of the Massachusetts Chess Association.  In a famous match at MIT in 1967,8  Greenblatt’s program beat Hubert Dreyfus (1929– ), an AI critic who had earlier observed that “no chess program could play even amateur chess.”9 Although Dreyfus’s observation was probably true in 1965, Greenblatt’s MAC HACK VI was playing at the amateur level two years later.

Perhaps encouraged by MAC HACK’s ability, in 1968 Donald Michie and John McCarthy made a bet of £250 each with David Levy (1945– ), a Scottish International Master, that a computer would be able to beat him within ten years.  (The following year Seymour Papert joined in, and in 1971 Ed Kozdrowicki of the University of California at Davis did also, bringing the total bet to £1000.  In 1974, Donald Michie raised the total to £1250.)  In 1978, Levy collected on his bet – as we shall see later.

Around 1970, three students at Northwestern University in Illinois, David Slate, Larry Atkin, and Keith Gorlen, began writing a series of chess  programs.  The first of these, CHESS 3.0, running on a CDC 6400 computer, won the first Association for Computing Machinery’s computer chess tournament (computers against computers) in New York in 1970. There were six entries – MAC HACK VI not among them.  According to David Levy, “CHESS 3.0 evaluated approximately 100 positions per second and played at the 1400 level on the U.S. Chess Federation rating scale.” Subsequent Northwestern programs, up through CHESS 4.6, achieved strings of wins at this annual event. Meanwhile, however, CHESS 4.2 was beaten in an early round of the first World Computer Chess Championship tournament held at the International Federation of Information Processing Societies (IFIPS) meeting in Stockholm in 1974.  The Russian program, Kaissa, won all four games in that tournament, thereby becoming the world computer chess champion.

These years, the late 1960s through the mid-1970s, saw computer chess programs gradually improving from beginner-level play to middle-level play. Work on computer chess during the next two decades would ultimately achieve expert-level play, as we shall see in a subsequent chapter.  Despite this rapid progress, it was already becoming apparent that there was a great difference between how computers played chess and how humans played chess.  As Hans Berliner, a chess expert and a chess programming expert, put it in an article in Nature,12

[A human] uses prodigious amounts of knowledge in the pattern-recognition process [to decide on a good maneuver] and a small amount of calculation to verify the fact that the proposed solution is good in the present instance.  . . . However, the computer would make the same maneuver because it found at the end of a very large search that it was the most advantageous way to proceed out of the hundreds of thousands of possibilities it looked at.  CHESS 4.6 has to date made several well known maneuvers without having the slightest knowledge of the maneuver, the conditions for its applications, and so on; but only knowing that the end result of the maneuver was good.

Berliner summed up the difference by saying that “The basis of human chess strength, by contrast [with computers], is accumulated knowledge” (my italics).  Specific knowledge about the problem being solved, as opposed to the use of massive search in solving the problem, came to be a major theme of artificial intelligence research during this period.  (Later, however, massive search regained some of its importance.)  Perhaps the most influential proponents of the use of knowledge in problem solving were Edward Feigenbaum and his colleagues at Stanford.  I’ll turn next to their seminal work."

Chapter 16: Conferences, Books, and Funding

Accompanying the technical progress in artificial intelligence during this period, new conferences and workshops were begun, textbooks were written, and financial support for basic research grew and then waned a bit.

The first large conference devoted exclusively to artificial intelligence was held in Washington, DC, in May 1969.  Organized by Donald E. Walker (1928–1993) of the MITRE Corporation and Alistair Holden (1930–1999) of the University of Washington, it was called the International Joint Conference on Artificial Intelligence (IJCAI). It was sponsored by sixteen different technical societies (along with some of their subgroups) from the United States, Europe, and Japan.  About 600 people attended the conference, and sixty-three papers were presented by authors from nine different countries.
The papers were collected in a proceedings volume, which was made available at the conference to all of the attendees.

Because of the success of this first conference, it was decided to hold a second one in London in 1971.  During the early years, organization of the conferences was rather informal, decisions about future conferences being  made by a core group of some of the leaders of the field who happened to show up at organizing meetings.  At the 1971 meeting in London, I left the room for a moment while people were discussing where and when to hold the next conference.  When I returned, I was informed that I had been selected to be the “czar” of the next meeting – to be held at Stanford University in 1973.
Later, a more formal arrangement was instituted for managing the “International Joint Conferences on Artificial Intelligence,” with a President, a Board of Trustees, and a Secretariat. Since the first meeting, IJCAI conferences are held biennially (on odd-numbered years) with the venue alternating (loosely) between North America and the rest of the world. As at the first conference, proceedings are distributed at the conferences.  (Some of these can be obtained from various booksellers, and they are available online at the Digital Library of India Web site, http://202.41.92.139/.)
\One of the oldest “special interest groups” of the Association for Computing Machinery (ACM) is SIGART (the Special Interest Group for ARTificial intelligence).  It began publishing a Newsletter in 1966, which (as the SIGART Web site says) “continued in various incarnations (the SIGART Bulletin, Intelligence Magazine) until 2001.” Today, SIGART supports various conferences and workshops, and it organizes meetings in which AI doctoral students can present their nearly finished work to their peers and to senior researchers for comments and critiques.

As the field began to develop its techniques and methods, graduate courses in artificial intelligence were offered at some universities.  Accordingly, some of us who were teaching these courses thought it would be worthwhile to write or edit books about AI. In 1963, Edward Feigenbaum and Julian Feldman, then assistant professors at UC Berkeley, published a collection of early AI and cognitive science papers that had previously appeared in many different places.  The volume was called Computers and Thought and was required reading for early students of AI (including me).2   As Feigenbaum wrote in the 1995 edition, “Some of the papers are as important today for their fundamental ideas as they were in the late 1950s and early 1960s when they were written.  Others are interesting as early milestones of fields that have expanded and changed dramatically.”

In 1965, I published a book about neural-network and statistical methods in pattern recognition.3   That book was followed in 1971 by a book about AI search strategies.4   Around the same time, other texts were published by James Slagle5  and by Bertram Raphael,6  both former Ph.D. students of Marvin Minsky at MIT.

In 1969 Marvin Minsky and Seymour Papert published an influential book in which they proved, among other things, that some versions of Rosenblatt’s perceptrons had important limitations.7   Some have claimed that the Minsky–Papert book was the cause of a fading interest in neural-network research, but I doubt this.  First, Rosenblatt himself began concentrating on other topics well before 1969,8  and the success of heuristic programming methods caused a shift of attention (including my own) away from neural networks during the mid-1960s.

In 1965, Donald Michie at the University of Edinburgh organized the first of several invitation-only “Machine Intelligence” workshops.  This first one was held in Edinburgh and was attended by American and European researchers. Attendees gave papers at the workshop, and these were all published in a book edited by N. L. Collins and Donald Michie in 1967.  A second workshop was held in September 1966, also at the University of Edinburgh.  Subsequent workshops were held annually in Edinburgh through 1971.  Thereafter, the "workshops were held every few years at various venues.  Each workshop resulted in a book with the title Machine Intelligence N, where N denotes the workshop and volume number.9   The last few volumes have been published online by the Electronic Transactions on Artificial Intelligence.10   These books contain some of the most cited and important papers in the early history of the field.

These years saw the United States engaged in war in Vietnam, and Congress wanted to make sure that research supported by the U.S. Defense Department was relevant to military needs.  Responding to these pressures, on November 19, 1969, Congress passed the “Mansfield Amendment” to the Defense Procurement Authorization Act of 1970 (Public Law 91-121), which required that the Defense Department restrict its support of basic research to projects “with a direct and apparent relationship to a specific military function or operation.” On March 23, 1972, the Advanced Research Projects Agency was renamed the Defense Research Advanced Projects Agency (DARPA) to reflect its emphasis on projects that contributed to enhanced military capabilities.  (The name reverted to the Advanced Research Projects Agency in 1993 and then back to the Defense Advanced Research Projects Agency in 1996.)11

On the other side of the Atlantic, British AI researchers experienced their own funding crisis.  One of the U.K.’s main funding bodies for university research, the Science Research Council, asked Professor James Lighthill, a famous hydrodynamicist at Cambridge University, to undertake an evaluative study of artificial intelligence research.  

Lighthill’s report, titled “Artificial Intelligence:  A General Survey,” somewhat idiosyncratically divided AI research into three categories, namely, advanced automation, 
computer-based studies of the central nervous system, and the bridges in between.  He called these categories A, C, and B, respectively.  Although he came out in favor of continued work in categories A and C, he was quite critical of most AI basic research, including robotics and language processing, which he lumped into category B. He wrote that “In no part of the field have the discoveries made so far produced the major impact that was then [around 1960] promised.” He concluded that AI’s existing search techniques (which worked on toy  problems)   would not scale to real problems because they would be stymied by the combinatorial explosion.12

Lighthill’s report resulted in a substantial curtailment of AI research in the United Kingdom.  In particular, one of its casualties was work on FREDDY the robot and other AI work under Donald Michie at Edinburgh. Here is one of Michie’s later comments about the effects of the report:13

Work of excellence by talented young people was stigmatised as bad science and the experiment killed in mid-trajectory.  This destruction of a co-operative human mechanism and of the careful craft of many hands is elsewhere described as a mishap. But to speak plainly, it was an outrage.  In some later time, when the values and methods of science have further expanded, and those of adversary politics have contracted, it will be seen as such.

DARPA’s shift to shorter term applied research, together with the Lighthill report and criticisms from various onlookers, posed difficulties for basic AI research during the next few years.  Nevertheless, counter to Lighthill’s assessment, many AI techniques did begin to find application to real problems, launching a period of expansion in AI applications work, as we’ll see in the next few chapters.

Chapter 33: Ubiquitous Artificial Intelligence

In today’s world, the magic of AI is everywhere – maybe it’s not “full AI” but there are significant parts.  Allen Newell foresaw these bits and pieces as part of an “enchanted land.” In an address given in 1976, he called computers the “technology of enchantment.” He noted two ingredients that made it so:1

First, it is the technology of how to apply knowledge to action to achieve goals. . . That is what algorithms and programs are all about – frozen action to be thawed when needed.

The second ingredient is the miniaturization of the physical systems that have this ability for intelligent action.

Thus, computer technology differs from all other technologies precisely in providing the capability for an enchanted world:

For brakes that know how to stop on wet pavement For instruments that can converse with their users
For bridges that watch out for the safety of those who cross them
For streetlights that care about those who stand under them – who know the way, so no one need get lost
For little boxes that make out your income tax for you

In short, computer technology offers the possibility of incorporating intelligent behavior in all the nooks and crannies of our world.  With it, we could build an enchanted land.

Let’s see some examples of how AI is already inhabiting the “nooks and crannies of our world.” I’ll start with our houses and some of the things in and around them.

AI at Home

Homes and their contents are becoming more intelligent. Here is a partial list of what you might find today (or sometime soon) on a tour of a modern house:

- thermostats for heat and air-conditioning systems that anticipate temperature changes and the needs of occupants, communicate with other home devices, and take appropriate actions in advance;
- microwave ovens that read barcodes on packages to determine how long to cook an item;
- smart running shoes with a computer chip that senses the runner’s size and stride length and directs on-going changes in the heal cushioning via a miniature screw and cable system;
- washing machines that automatically adjust to different conditions to wash clothes better;
- refrigerators that automatically inventory their contents and inform owners of needed items;
- cameras with computer vision systems to identify faces and to control focusing, exposure, and framing;
- hearing aids that adapt to ambient sound levels and block out “cocktail party” chatter;
- robotic pet “animals” and toys that interact with people;
- floor-washing and vacuum-cleaning robots; and
- caretaker robots for the elderly or infirm.

This list will continue to grow. Some AI researchers talk about a field called “ambient intelligence,” where the “broad idea is to enrich a space (such as a room, house, building, bus station, or a critical area in a hospital) with sensors tied to intelligent software, so that the people using the space can benefit from a responsive, even wise environment.” The components in an environment permeated by ambient intelligence are also being networked so that they can communicate with each other and so that people can communicate with them using ordinary speech. Vlingo, Nuance, and Yap are three companies that sell products for mobile phones (such as the iPhone) that translate voice into text.  Vlingo, for example, claims on its Web site that if you say “pizza places in Pittsburgh” to your phone, it then “figures out what you want, finds it and shows you how to get there.  No tapping, no thumbs, just good old speaking.” Presumably similar technology could enable one to command and query one’s smart appliances by talking to them.

33.2    Advanced Driver Assistance Systems

Perhaps the landscape with the most places into which computers and artificial intelligence have crept is the modern passenger automobile.  Today’s cars can have as many as fifty microprocessors controlling such things as automatic transmissions, fuel injection systems, antilock brakes, airbags, security systems, and cruise control systems to name just a few.  And, although not yet completely autonomous, more and more automobiles are beginning to be equipped with safety features called “advanced driver assistance systems” (ADAS).

Here is a list of just a few of the ADAS features that are either available now or are being planned by several automobile manufacturers:

-  adaptive cruise control (ACC) for providing more intelligent control of speed, enabling the vehicle to slow down or speed up depending on traffic conditions as perceived by radar or laser sensors;
-  intelligent speed adaptation (ISA) for monitoring local speed limits, slowing the vehicle down (or warning the driver) when it enters zones with speed limits;
-  lane control systems for monitoring the presence of vehicles or obstructions in adjacent lanes and for monitoring when a driver drifts into an adjacent lane or off the roadway;
-  automatic parking systems for assisting a driver when executing a parallel parking maneuver;
-  traffic sign recognition systems;
-  driver drowsiness detection systems; and
-  intelligent tire pressure control systems.

Although not all automobile computers employ AI technology, the most ambitious of the ADAS features use computer vision, planning methods, probabilistic inference, and machine learning.  The motivation for using ADAS is the desire to eliminate automobile injuries and fatalities.  In 1997, for example, the Swedish parliament passed a “Road Traffic Safety Bill” whose goal, “Vision Zero,” is that “no one will be killed or seriously injured when moving within the road transport system.”

33.3     Route  Finding  in  Maps

While we are on the subject of automobiles, one of the things drivers often need to know is how to get from one place to another.  Many automobiles have devices that “talk” you to your destination using on-board GPS systems, map databases, and speech synthesis.  Map databases can be thought of as graphs consisting of place “nodes” and connecting road “links.” So whether the navigation advice is provided ahead of time by your home computer (using Google Maps, for example) or by an on-board navigation system, it is generated by a process of searching a graph to find a path from some node, A, to some other node, B.

You will recall that the most commonly used graph-searching procedure is A∗, a heuristic search method that takes into account both the distance traveled so far and an estimate of the distance to the goal (see p.  220.)  Is A∗ used in route-finding programs?  Well, Google, for example, will only say that it uses “state-of-the-art hierarchical graph algorithms to compute shortest paths in routing networks in a matter of milliseconds.”5  Most likely these algorithms, and similar ones used by other route-finders, use heuristic techniques similar to those used by A∗ but specialized to the case of searching two-dimensional maps.  For example, the searches are hierarchically organized. That is, for trips to a distant goal, large-scale maps with just the major roads and highways are searched.  Then, to get from a starting position to a major road on the way to the goal, a more detailed map with less-traveled roads is used.  Hierarchical search may result in slightly suboptimal, but nevertheless quite acceptable, paths.  In addition, when responding to billions of queries, some of the computations that would have to be repeated for each query can be shared among them instead.

Most route-finding programs can (and do) take into account criteria other than distance, such as estimated travel times.  For example, Microsoft’s ClearFlow program, which uses Bayesian networks informed by traffic-monitoring sensors to estimate traffic densities, can base route recommendations either on shortest time, shortest distance, or current traffic conditions.  (Try it out at http://maps.live.com/.)

33.4     You  Might  Also  Like. . .

When I log on to Amazon.com’s Web site, it responds “Hello, Nils J. Nilsson. We have recommendations for you.” It then lists some items that it guesses I might like including the book Portuguese Irregular Verbs by Alexander McCall Smith (recommended because I had previously purchased another book by the same author) and the video Lipstick Jungle with Brooke Shields because I had previously downloaded the video BBC Shakespeare:  Othello(?!).  It then lets me make some changes.  When I say that I am not interested in Lipstick Jungle it wonders whether or not I might be interested in the video How I Met Your Mother.  Perhaps I have not ordered enough videos to give it a better idea of what I might be interested in.

Amazon’s recommendations are based on what is called social or collaborative filtering.  A database of preferences (for books, movies, or whatever) is maintained for every user.  If user B’s preferences correlate sufficiently with those of user A, a collaborative filtering system would recommend some of B’s purchases (not already bought by user A) to user A. Rather straightforward machine learning techniques are used to find preference correlations among users.  Several other sites, including iTunes, TiVo, and Netflix, base their recommendations on collaborative filtering.

Another type of recommending system uses what is called   content-based filtering, in which a user’s preferences for books, movies, documents, or whatever are analyzed to find similarities with other items of the same kind (instead of with other users having the same preferences).  The most similar items are then recommended.  For documents, for example, comparisons might be made using the vector representations I discussed in Section 27.3.
Content-based filtering is widely used for blocking unwanted e-mail (such as spam) and Web sites (such as pornography).  It’s also used for personalized Web searches – for example in customizing news feeds to gather news about particular topics.  Recommendation systems that combine collaborative and content-based methods have also been developed.  Interested readers might want to see a special issue of the journal AI Communications on “Recommender Systems.”6

33.5     Computer  Games

In addition to the use of AI techniques for playing games such as chess and checkers, AI is beginning to be used in the kinds of computer games in which human users interact with artificial characters in a simulated world.  Although the emphasis in these games has been on rich and realistic graphics, the use of AI techniques can make them even more appealing and challenging.
Developers and aficionados of these games use the term “Game AI” to distinguish the kinds of AI in computer games from what they call academic or “R&D AI.” They point out that in many games all that is required is the illusion or appearance of intelligence – much like the ELIZA program appears to be able to carry on a conversation but does not really know what it is talking about.

Nevertheless, computer games are rich with possibilities for the use of AI. In the usual setting, a human player interacts with and competes with artificial agents in the game; these agents are called nonplayer characters (NPCs). Among other tasks, the NPCs have to be able to navigate from place A in their world to place B without bumping into obstacles or other NPCs, and "thus many games use A∗.  But also, the NPCs should be able to perceive their simulated environment, make and execute plans, and learn – just like robots in real environments do.  The more intelligently they can do those things, the more realistic they will appear to the human player (and purchaser) of the game.  I’ll mention two representative examples of games with these abilities.

The game Black and White 2, developed by Lionhead Studios and marketed by Electronic Arts, uses a combination of neural nets and decision trees.  According to a Web site for the game, the NPCs (evil and benevolent deities) “can learn strategies, master new abilities and skills, [and] lead armies into battle. . . Every choice you make will have an impact.  Each action and inaction prompts obvious changes to buildings, flora and fauna, all morphing to reflect your personality.”8

The game F.E.A.R. (First Encounter Assault Recon) by Jeff Orkin and Monolith Productions uses A∗ to plan sequences of NPC actions in addition to its usual role in path finding.

Black and White and F.E.A.R. were rated numbers 1 and 2, respectively, of the “Top 10 Most Influential AI Games,” by http://AIGameDev.com, a Web site about the use of AI in games.  There are several Web sites and conferences devoted to AI in games.10

Some AI researchers have advocated using computer games as a convenient arena for developing new ideas for intelligent agents.  For example, University of Michigan professor John Laird has written, “[because] research in robotics requires solving many difficult problems related to low-level sensing and acting in the real world that are far removed from the cognitive aspects of intelligence, . . . computer games provide us with a source of cheap, reliable, and flexible technology for developing our own virtual environments for research.”

The bits and pieces of AI just discussed, whether in the home, in the automobile, or in computer games, are usually of the “reactive” or “behavior-based” variety.  Conditions are sensed, and actions are taken depending on what is sensed.  Building systems that are able to react appropriately to the situation at hand has been an important strand of AI research.  AI agents that inhabit dynamic environments, whether simulated or real, must also be able to decide when to react and when to deliberate.  Albert Lewis, when he was a cornerback of the Oakland Raiders Football team, had this to say about when to react and when to think:

When you think on the field, you’ve automatically lost that down. The time you should be thinking is during the course of the week in practice.  That’s when the light should go on.  When you get in the game, it’s all about reacting to what you see.

One can imagine that the simulated football players (the NPCs) in games like Madden NFL 09 will use increasingly complex reactive strategies aided, when appropriate, by higher level reasoning.

Chapter 34: Smart Tools

In between the headline-making, milestone AI achievements and the smaller bits of computational intelligence that we find everywhere are impressive AI programs that are used by physicians, scientists, engineers, and business  people to help them in (and sometimes automate) their workaday tasks.  I call these the “smart tools” of AI. Sometimes these are stand-alone systems, but more often they are integrated into a larger computational framework or into hardware devices.  Some work only when called upon to help solve some particular problem, such as disease diagnosis.  Some are constantly active, such as online stock-trading systems.  I’ll not be able to mention all of them since there are far too many, and some are known only to their corporate and government users.  But a few examples will serve to illustrate their utility and variety.

34.1     In  Medicine

Let’s start with how AI is being used in medical clinical practice.  Beginning as early as the 1980s, AI technology has been an important part of medical systems and devices.  In March 2000, a monthly magazine titled Medical  Device & Diagnostic Industry published an article claiming that “the medical device industry is seeing an emergence of computer-based intelligent decision support systems (DSSs) and expert systems, the current success of which reflects a maturation of artificial intelligence (AI) technology.”1  It mentioned several AI-infused devices, including the “Agilent Acute Cardiac Ischemia Time-Insensitive Predictive Instrument. . . , an intelligent electro-cardiagram (ECG) device that predicts the probability of acute cardiac ischemia (ACI), a common form of heart attack,” and the General Electric “MAC 5000 Resting Test System, [incorporating] the Marquette 12SL ECG analysis program, an integrated DSS that uses newly developed digital processing methods and diagnostic program algorithms to interpret and classify ECG waveforms.”

A September 2005 online review2  in Clinical Window (which is sponsored by GE Healthcare) of ECG devices by Dr.  Paul Kligfield, Division of Cardiology at Cornell University, stated that “digital electrocardiographs of all major manufacturers now are capable of providing automated diagnostic statements that can help the physician.” However, he also mentioned cases where these statements could “mislead the physician.” In evaluating a particular device, Dr.  Kligfield stated that in 3,954 patients without pacemakers, 7.8% of the cardiac rhythm interpretations required revision by the combined opinion of two expert cardiologists.

OpenClinical3  maintains a family of Web sites listing a number of decision support systems in current use.  Among these are Athena DSS (for hypertension management), Gideon (for infectious diseases), Iliad (for internal medicine), TherapyEdge HIV (for HIV patient management), and several others.  Some of the systems listed trace their ancestry back to MYCIN, INTERNIST-1, PUFF,  and diagnostic systems based on Bayesian networks. More details about these and other systems can be gleaned from the OpenClinical Web pages.  Another source of information is the Elsevier journal Artificial Intelligence in Medicine.

I’ll describe a couple of representative examples. ATHENA DSS is a system for providing advice to physicians about managing hypertension in a manner consistent with guidelines defined by the U.S. Institute of Medicine. It was developed jointly by Stanford Medical Informatics, the Veterans Administration Palo Alto Health Care System, and the Stanford Center for Primary Care and Outcomes Research. ATHENA processes a patient’s clinical data against hypertension management knowledge in its knowledge base and generates patient-specific recommendations for management during a clinical visit.  It is in use and undergoing continuing evaluation and upgrading at several Veterans Administration medical centers.  A new version, called ATHENA-HTN, is being evaluated. ATHENA’s technology stems from previous medical rule-based systems developed at Stanford.  According to Mark Musen, Head of the Stanford Biomedical Informatics Division, “ATHENA uses the EON task-specific architecture for assisting protocol-based medical care that grew out of my dissertation work in the late 1980s, which grew out of ONCOCIN [a program for helping to manage oncology protocols], which grew out of
MYCIN       ”

Another system, Gideon, is a program to help physicians diagnose and treat country-specific diseases. Gideon makes its diagnoses based on a large database of diseases, symptoms, signs and laboratory findings, and countries. Bayesian analysis is used in the computation of the probability of a disease given data about a patient. The original version of  Gideon was developed by Stephen A. Berger, M.D., at the Tel Aviv Sourasky Medical Center and Uri Blackman at the University of Tel Aviv. Blackman is now CEO of Gideon Informatics, Inc., in Los Angeles. There is a version of the program that can on mobile phones or PDAs.

According to the Gideon Web site,

Gideon is made up of four modules:  Diagnosis, Epidemiology, Therapy and Microbiology.  The constantly updated database includes 337 diseases, 224 countries, 1,147 microbial taxa and 306 antibacterial (-fungal, -parasitic, -viral) agents and vaccines.
Gideon’s world wide data sources essentially include the entire world’s literature and adhere to the standards of Evidence Based Medicine.  Over 10,000 notes outline the status of specific infections within each country.  Also featured are over 20,000 images, graphs, interactive maps and references.
. . .
In a blinded multicenter field trial of 495 patients, the correct diagnosis was displayed in over 94% of cases, and was listed first in over 75%.

A reviewer in the Journal of the American Medical Association wrote “Gideon:  The Global Infectious Disease and Epidemiology Network is a superbly designed expert system created to help physicians diagnose any infectious disease (337 recognized) in any country of the world (224 included).
. . . This diagnostic system is remarkable for its ease of use, breadth of scope, and depth of information.  It is as practical a program as one could hope for.”7

Notwithstanding the success of diagnostic systems, such as Gideon, most of the applications of AI in medicine involve DSSs, which can be used by physicians for reference.  As Thomas Rindfleisch, an expert in medical informatics says, “The name DSS is significant in that doctors always need be in charge of final patient-related decisions to avoid FDA regulation of the software.” Rindfleisch also mentions that for DSSs to be useful for most physicians, they have to be integrated with electronic medical and health record systems (so that doctors don’t have to type in all the needed background information about a patient).8   However, a survey of 2,758 American physicians (taken in late 2007 and early 2008) found that only 17% of them had access to electronic record systems.9

34.2     For  Scheduling

Intelligent scheduling software is another area where AI techniques are being used.  One example is the AURORATM  system marketed by Stottler Henke Associates, Inc., a company specializing in applying “artificial intelligence and other advanced software technologies to solve problems that defy solution using traditional approaches.” AURORA is being used by the Boeing Company to help schedule and manage the building of the Boeing DreamlinerTM. Stottler Henke says that “once AURORA has created a schedule, it displays it in a series of graphical images that allow the user to see the scheduled activities, resource allocations and the temporal relationships among the activities.”10

TEMPORISTM, developed by United Space Alliance, LLC, is an intelligent spaceflight mission management and planning tool for use by the crew on board future space missions.  TEMPORIS will help crews schedule all aspects of their in-flight lives, including routine daily activities, spacecraft housekeeping, and conducting on-board experiments.  But producing acceptable schedules requires “volumes of spaceflight constraints, flight rules, dependencies, sequences, medical guidelines and safety requirements.” According to a company press release, these can now be “efficiently embedded into TEMPORIS’s intelligence.  To illustrate:  It currently takes 50 mission planners working 24/7 for two weeks to schedule one day’s worth of activities on the International Space Station.  TEMPORIS reduces that 2-week job to a few moments with the click of a computer mouse.”11  Stottler Henke’s AURORA software is an integral part of TEMPORIS.

34.3     For  Automated  Trading

AI data mining, text processing, and decision methods are used in the analysis of real-time trading data and news feeds to make automatic buy-and-sell decisions on stocks, commodities, and currencies.  Up-to-the-minute news sources in digital form are readily available.  The Reuters “NewsScope Archive”12  and the Dow Jones “Elementized News Feed”13  are among news feeds that are used for automated trading and analysis.  Reuters, for example, claims to provide14

customers seeking to develop news-based programmatic trading strategies with a comprehensive, machine-readable archive of Reuters global news.  Events are presented exactly as they broke to the markets, with each release of information timestamped to the millisecond and tagged with an array of metadata fields for easy machine consumption.

According to an article in the New York Times about automated trading,15  Professor Andrew Lo, the Director of the MIT Laboratory for Financial Engineering, and colleagues discovered that there was a correlation between how often certain words, such as “anxiety,” “depression,” and “bankrupt,” appeared in news stories and future values of the S&P stock index.  These correlations, among other things, can be used by stock trading algorithms to initiate stock trades.

Vhayu Technologies Corporation, one of the firms offering algorithmic trading services, claims that "of the top 10 global financial institutions use one of its products," namely, Vhayu VelocityTM, "to identify opportunities in milliseconds.”16  Another firm, Streambase Systems, says that with one of its products “leading trading organizations track critical market conditions across multiple markets and instantaneously execute sophisticated strategies to capture short-lived trading opportunities.”17  Of course, with such instant feedback there is the potential that news of a sell order might trigger other sell orders, and so on, leading to swift downdrafts in the market (and vice versa).

34.4     In  Business  Practices

Business Rule Management Systems (BRMSs) are descendants of the rule-based expert systems of the 1980s.  Examples are Fair Isaac’s “BLAZE ADVISOR 6.1,” ILOG’s “JRules 6.0,” and Information Builders’s “WebFOCUS.” Business rules express information about how a business operates – its policies and constraints.  All companies have such rules.  As Fair Isaac puts it, these  are usually expressed “in conversation, written text and software – as ‘If, then’ statements [such as] ‘If the loan applicant does not have a sufficient credit history, then pull a report from a debit bureau.’” In BRMSs, these rules are usually encoded in English-like, computer-readable syntax.  Unlike rules used in some expert systems, they are not annotated with probabilities or certainty factors but are definite statements of business practice.  Because the information expressed by business rules changes from time to time, it is important that the rules be maintained to reflect current policies.

Rule engines are used to perform both forward and backward inference over a network of rules. A descendant of the Rete algorithm, which was mentioned on page 301, is used by the inference engine in BLAZE ADVISOR, for example.  Conclusions are used to communicate policy, late-breaking business opportunities, and needs for action among staff and other parties.  In some cases conclusions evoke automatic actions such as ordering, sending e-mails, and so on.  For example, Information Builders advertises that with their suite of event management solutions, “the automated process itself is able to make predetermined decisions and take specific courses of action based on thresholds contained within the business intelligence content that is fed to it.”18  An article about WebFocus gives an example:  “[An order arriving] can trigger a series of responses and decisions – e.g.  based on WebFOCUS analytics embedded in the process, the physical size of the order can be determined. If it is too big for the warehouse space available, WebFOCUS analytics can trigger a change in the process that ships the order to a different warehouse that has the required space, and then alerts warehouse employees that a larger than expected order will be arriving.”

34.5     In  Translating  Languages

Several commercial natural language translation systems now exist.  IBM’s speech-to-speech translator (MASTOR) can (according to its Web site) translate free-form English speech into Mandarin speech (and vice versa).20 BBN Technologies has developed a number of speech processing systems.  One is their “Broadcast Monitoring System,” which (according to its Web site) “creates a continuous searchable archive of international television broadcasts.” As explained on the site, “The system automatically transcribes the real-time audio stream and translates it into English.  Both the transcript and translation are searchable and synchronized to the video, providing powerful capabilities for effective retrieval and precise playback of the video based on its speech content.  With this revolutionary system, users can sift through vast collections of news content in other languages quickly and efficiently.”21  SRI International’s IraqComm translation system can transform spoken English into translated spoken colloquial Iraqi Arabic (and vice versa). Currently (according to its Web site) it is “tailored to translate spoken interactions on topics on force protection, security, and basic medical services, and can be customized to include other topics as needed.”22

34.6     For  Automating  Invention

John Koza, the inventor of Genetic Programming (GP), a search method based on simulating the processes of evolution, claims that GP is itself an “invention machine.” (He also claims that GP more-or-less subsumes AI because AI’s goal is to produce intelligent programs and GP does just that.) For example, Koza and colleagues used GP to evolve (after thirty-one generations) an optimal antenna system.23   They have also evolved (sometimes after hundreds of generations) designs for optical lenses.  As one of their papers states, “One of the genetically evolved lens systems infringed a previously issued patent, whereas the others were noninfringing novel designs that duplicated (or improved upon) the performance specifications contained in the patents.”24  Other evolved designs include those for electrical circuits, controllers, mechanical systems, and other devices.25   The goal of the group is to produce what they call “human-competitive designs,” that is, ones whose specifications meet or exceed those of the best human designers.  With expected increases in computer power, I believe that the use of GP and
GP-like search methods will likely produce several interesting new inventions.

34.7     For  Recognizing  Faces

People are quite good at recognizing familiar faces whether “live” or in photographs. They can often do this regardless of pose, scale, facial expression, or lighting conditions.  (Interestingly, they don’t do very well if the photograph of a face is presented upside down.) Computers are getting better, so much so in fact that I might have included computer face recognition in the chapter on Ubiquitous Artificial Intelligence. Although not quite ubiquitous yet, face-recognizing systems are becoming more common at airports, banks, and places where personal identity must be verified or established. According to some people who worry about privacy, the practice is too common,
Work on face recognition by computer has continued from its early days.

How far has it progressed?  A 2007 National Institute of Standards and Technology report on face-recognition tests claimed (among other things), “The results show that, at low false alarm rates for humans, seven automatic face recognition algorithms were comparable to or better than humans at recognizing faces taken under different lighting conditions.  Furthermore, three of the seven algorithms were comparable to or better than humans for the full range of false alarm rates measured.”26  The best methods use machine learning algorithms working on very large data sets.27

A variety of different algorithms have been developed.  Some are based on well-known pattern-recognition techniques that sample features from a face image and then compare these features against those of a large library of identified faces to find the closest match.  Some algorithms use Bayesian techniques and HMMs.  Many of the methods use mathematical techniques to project a high-dimensional vector representation of a face image into a vector in a lower dimensional subspace.  One method uses lower dimensional spaces whose coordinates consist of a set of reduced images, called eigenfaces, which have the property that they can be combined to give good approximations to any of the faces in the database (much like a set of individual audio tones of different frequencies can be combined to approximate arbitrary sounds).  For a Web page with information about face recognition with links to research papers, books, algorithms, and vendors, see http://www.face-rec.org/.

New approaches continue to be developed.  One method purports to show that “image averaging” (that is, merging different images of the same face to form a single image) “greatly improves performance of [the commercially available FaceVACS] automatic face-recognition system.”28  An article in Wired reports on a method developed by researchers at the University of California, Berkeley, and the University of Illinois at Urbana-Champaign.  According to that article, Shankar Sastry, the Dean of UC Berkeley’s College of Engineering, noted that this new method “renders years of research in the field obsolete.”29

There are already several commercial companies selling face-recognition and face-locating software and equipment.  For example, Oki Electric Industry Co., Ltd., sells a product called FSE (Face Sensing Engine).  It boasts many applications including controlling access to information in camera-equipped cell phones and other devices, sorting photographs based on recognizing faces, and locating faces in a camera’s field of view.  The German company Cognitec Systems GmbH markets the FaceVACS system previously mentioned.

Before closing this section on smart tools, I should mention that there are seve  al other areas in which AI tools are enhancing human productivity.  For example, I could have mentioned tools for aiding (and automating) the processes of movie animation, for computer program writing and debugging, for industrial process control, for circuit and program verification, and for enhancing and searching the semantic Web.  Tools powered by AI techniques will be increasingly used to aid and amplify (and sometimes to substitute for) human cognitive, motor, and perceptual abilities.  Just wait!

As I hope the past few chapters have demonstrated, some parts of the quest for artificial intelligence have been quite successful.  AI has become more and more a part of all of our lives as well as of those of specialists.  But the main goal of the quest (for some of us at least) still remains, namely, endowing artifacts with full human (as well as superhuman) capabilities for language, perception, reasoning, and learning.  So, let’s look next at where the quest might lead us.

Chapter 35: The Quest Continues

Where will the AI adventure lead next?  We can get some idea of the immediate future simply by extrapolating present trends.  Probably there will be some new milestone achievements.  Undoubtedly, pieces of AI technology will become ever more common in our homes, automobiles, and activities, and the specialists’ smart tools will become ever smarter and more numerous.

But predicting beyond where AI’s present momentum will take us is problematic.  Let’s look at how some previous predictions have fared.  Simon’s 1957 prediction of a computer chess champion within ten years was markedly overoptimistic.  In 1973, SRI engineers led by Oscar Firschein iteratively queried several AI “experts” about when certain “products” would be realized. The medians and ranges of predicted dates were reported back to them, they were given a chance to modify their predictions, and so on until the results settled down.  (This process of making predictions is called a Delphi method.)

The “robot servant” and the “robot chauffeur” still seem quite a ways off, but the others were perhaps only somewhat too optimistic.  (Well, the year 2000 seemed a long way off back in 1973.)

Against this background of prediction successes and failures, I hesitate to make any that do not seem rather obvious.  Except, I will predict that  someday we’ll have human-made artifacts with levels of intelligence (in all of its manifestations) equalling and exceeding that of humans.  I make that prediction because I believe that we humans are machines (for what else could we be?)  and that eventually we’ll be able to build machines that can do whatever we can do because there will be economic as well as scientific reasons for doing so.

I’ll have more to say about “human-level artificial intelligence” later, but let’s first look at some of the research projects underway in AI laboratories during the early part of this century to see whether they give us any insights about the future.

35.1     In  the  Labs

There are now probably hundreds of laboratories – industrial, government, and academic – that carry on research in artificial intelligence.  I could not possibly describe even a small part of what is going on in them, and, in any case, projects come and go.  Just as a historian cedes accounts of current events to newspapers and other media, I recommend that readers wanting to stay current on AI research visit the Web sites maintained by the individual AI laboratories, AI societies, government agencies that support AI research, and specialized conferences and workshops.3   To give some of the flavor of the breadth of current research, I’ll mention a few projects ongoing during the first few years of this century.  Of course, these are research projects so it’s possible, but not certain, that some of them will leave their marks on the future.

35.1.1    Specialized Systems

Building smart tools for work in specialized areas is still a big part of AI research.  However, work on these tools is increasingly less AI-centric and is merging with the disciplines upon whose technologies these efforts depend – such as statistics, control engineering, image processing, and linguistics, among others.  Of course the new techniques invented and used in building even the most specialized niche systems might, in fact, be broadly applicable in other areas of interest to AI.

A.  Content-Based  Image  Retrieval

Present-day image and video search engines that respond to queries composed of words, such as “motorcycles,” do so by looking for Web sites that contain the text “motorcycle” along with some image or video file (say in jpeg, quicktime, or some other appropriate format).  Unfortunately, the image or video file in some of the sites retrieved by these methods might not even contain a motorcycle.  Research is underway to base image and video search more on the content of images rather than on the content of associated text alone.

One such project has been undertaken by researchers at Oxford University and at Microsoft Corporation.4   Using images from a public Web site (Flickr), a “query” consists of outlining (with a rectangle) that part of an image that contains a sought-for object.  That part of the image is then converted into a vector representation that preserves key features of the image of the object.

Vector representations of images in a large image database are analyzed to form “clusters” of similar vectors.  The idea is that each cluster is associated with images of similar objects.  The vector representation of the query is then matched against the image database vectors to find clusters of the most similar images.  These can then be ranked, and the high-ranking images, with object regions outlined, are returned as answers to the query.  Some examples of query images and response images are shown in Fig.  35.1.  The query image, with the query region outlined, is on the left, and seven returned images (with target regions outlined) are on the right.  Of course, as with any search process, there will be false positives returned also, but in these examples the false positives appeared later in the list than the response images shown.

The authors evaluate their work as follows:

The system returns photos from the corpus that contain a query object, despite substantial differences in lighting, perspective, image q  ality and occluders between the query and retrieved images. 

We view this as a step towards the ultimate goal of building a Web-scale retrieval system that will scale to billions of images.  We have therefore concentrated on algorithms which are scalable, and which we believe are straightforward to distribute over a cluster of computers.

They acknowledge that further work is needed to find “efficient ways to include spatial information in the index [which is computed before images are queried], and move some of the burden of spatial matching from the ranking stage to the filtering stage.”

Other projects involve content-based video retrieval.  When users submit a video to YouTube, for example, they also submit “tag” words to help describe it.  Example tag words that might be used are beach, hiking, soccer, cats, concerts, and so on.  Tagging takes effort, so some researchers are attempting to automate that process using statistical machine learning methods.  From a database of already tagged videos, a group of German researchers has developed a prototype system that extracts image information for use in suggesting tag words for other videos.5   Systems such as theirs might ultimately be used for tagging large corpora of videos.  Once tagged, these corpora could be more easily searched.

Along these lines, two different companies, VideoSurf and Digitalsmiths, have announced products that allow versions of content-based searching.
VideoSurf’s Web site (http://www.videosurf.com/about) claims that “using a unique combination of new computer vision and fast computation methods, VideoSurf has taught computers to ‘see’ inside videos to find content in a fast, efficient, and scalable way.”

B.  Meaning-Based  Web  Search

Next, I’ll mention an AI project at a commercial company, Powerset.  Powerset began as a San Francisco start-up developing an Internet search engine that uses natural language understanding techniques.  (The company was acquired by Microsoft in 2008.)  Powerset claims to be able to find “articles related to the meaning of your query.  And sometimes direct answers.”

The query can be a statement or a question posed in ordinary natural language.  A grammar and semantic processor are then used to parse the query – converting it to a representation that expresses the meaning of the original sentence.  Their prototype version is limited to searching Wikipedia articles, which have also been processed to extract meanings.  A matching procedure is then used to return those articles whose meanings are most related to the query.

The developers think that their technology will scale beyond Wikip  dia to be able to deal with more of the Web.  The natural language processing technology used by Powerset is based on the Lexical Functional Grammar originally developed by Joan Bresnan and Ronald Kaplan. Kaplan, formerly a researcher at PARC (Palo Alto Research Center) is now an employee of Powerset. Here is an example of a Powerset search using their Web site.6 I typed the query “What technology has powerset licensed?” It answered “The company has licensed natural language technology from PARC, the former Xerox Palo Alto Research Center. . . ,” and referred me to the Wikipedia page from which it lifted its answer, namely, “Powerset (company),” which is at http://en.wikipedia.org/wiki/PowerSet. If techniques for meaning-based search scale up, as the Powerset people hope, the quality of Internet search would be dramatically improved.

C. Legged Robots

Marc Raibert did research at Caltech on walking, running, and hopping robots. He continued related research as a professor at CMU and later at MIT. In 1992, he started a company called Boston Dynamics, which according to its Web site “specializes in robotics and human simulation.” One of their prototype products is called BigDog (Fig. 35.2), a four-legged walking robot, about the size of a Great Dane, claimed to be the “most advanced quadruped robot on earth.” BigDog is extremely stable. It can walk, run, and climb on rough terrain. It is said to be able to carry a 340-pound load. It is powered by a gasoline engine driving a hydraulic actuation system. BigDog’s suite of sensors includes a laser gyroscope, a stereo vision system, devices for sensing joint positions and forces, as well as internal things such as engine temperature and so on. Overall control is provided by an on-board computer. A movie of its operation shows it walking through a forest, climbing a hill, recovering from a strong kick to its side and negotiating an icy parking lot – all without falling. (Well, at least it didn’t fall in the movie I saw. The movie is available at http://www.bostondynamics.com/dist/BigDog.wmv.) The BigDog project is being supported by DARPA.

According to a paper about BigDog,7 the team is continuing to work on such problems as getting BigDog to right itself if it does happen to fall over and giving it more ability to navigate by itself. (It now relies mostly on a human operator to guide it.)

There are hundreds of more projects going on in the labs where “smart tools” are being developed for hundreds of professions. The variety is amazing. People are working on automating the process of movie animation,8 computational genomics, robotic surgery, business intelligence,10 and much more. Most of these are what I would call “niche” systems, focused on performing specific, rather than generic, tasks. In the next section, I’ll mention some laboratory work aimed either at building general-purpose systems or at developing technology that might be applicable in a wide range of settings.

35.1.2    Broadly Applicable Systems

A.  Robotics

Ever since the days of Shakey the robot, AI researchers have used robots as platforms for developing AI systems that integrate many aspects of intelligent behavior and, therefore, aim for a kind of general utility.  Work on general-purpose robot systems has gone in and out of style, and robotics researchers sometimes had to focus instead on special tasks, as might arise in industrial automation, for example.  Reacting to some of my proposals in the 1970s and 1980s for work on general-purpose robots, potential sponsors would sometimes ask “Just exactly what is your robot going to do?” My answer that they were supposed to be “general purpose” seldom satisfied sponsors with specific problems to solve.  Now, however, there does seem to be a return to working on robots that are able to do a lot of things – decathlon robots instead of high-hurdle robots or pole-vaulting robots.

One example is the work headed by Stanford professor Andrew Ng with a robot named STAIR, an acronym for STanford AI Robot.  STAIR is designed to be a kind of “general factotum,” that is, a robot that can do a lot of things including navigating home and office environments, picking up and interacting with objects and tools, and intelligently conversing with and helping people in these environments.  (See Fig.  35.3.)  According to its Web page,11  STAIR integrates “methods drawn from all areas of AI, including machine learning, vision, navigation, manipulation, planning, reasoning, and speech/natural language processing.  This is in distinct contrast to the 30-year trend of working on fragmented AI subfields, and will be a vehicle for driving research towards true integrated AI.

STAIR even learns how to pick up objects it has never seen before.  Using machine learning methods, STAIR’s perceptual system was trained on a database of a thousand or more pictures of each of a number of common objects as they might be seen in a home or office.  Each image was labeled with the appropriate grasping position for that particular object.  The objects included a stapler (2,001 example pictures), a coffee mug (2,001 example pictures), a cereal bowl (1,001 example pictures), and several other items.  The training pictures showed the objects under different lighting conditions, from different camera positions, and in different orientations.  (To ease the task of collecting and labeling training data, synthetic computer-graphics images were used.)  After training, STAIR could predict the best grasping point for several novel objects and develop a plan to guide the arm and grasper.  The system was tested on a large set of novel objects, examples of which it had not seen before.  These included “rolls of duct tape, markers, a translucent box, jugs, knifecutters, cellphones, pens, keys, screwdrivers, staplers, toothbrushes, a thick coil of wire, [and] a strangely shaped power horn.” On average, STAIR successfully picked up these objects 87.8% of the time.  (To be counted as successful, “the robot had to grasp the object, lift it up by about 1 ft, and hold it for 30 seconds.”)

Videos of STAIR performing various tasks, such as opening a door, fetching a stapler from inside an office, and unloading a dishwasher can be seen from the team’s multimedia Web page at http://stair.stanford.edu/multimedia.php.

Ng and his colleagues and students envision robots that would be able to perform tasks such as the following:

-  fetch or deliver items around the home or office,
-  tidy up a room, including picking up and throwing away trash and using the dishwasher,
-  prepare meals using a normal kitchen, and
-  use tools to assemble a bookshelf.

Ultimately, they say, robots will “revolutionize home and office automation and [will] have important applications ranging from home assistants to elderly care.”
Ng is not alone in pursuing this kind of robotics research.  A team consisting of researchers at Intel Research in Pittsburgh and at the Robotics Institute at Carnegie Mellon University are developing a robot called HERB, an acronym for Home Exploring Robotic Butler.  (See Fig.  35.4.)  It consists of a laser range finder, Segway RMP mobile base, WAM arm, Barrett Hand, and two video cameras.  According to a paper describing HERB, it can search for objects, learn to navigate in cluttered dynamic indoor environments, recognize objects using vision, and manipulate doors and other constrained objects.13   A video showing HERB performing tasks is available at
http://pittsburgh.intel-research.net/∼ssrin10/HERB09/HERB.wmv.

Another example is DOMO (Fig.  35.4.), a behavior-based, multiarticulator robot developed by MIT Ph.D. student Aaron Edsinger for accomplishing “useful manipulation tasks in human environments.”

Yaskawa Electric Corporation in Japan has developed a service robot they call SMARTPAL V for work around the home and office.15 (See Fig. 35.4.) Other Japanese companies are developing home robots also. Helping this trend toward general-purpose robots are competitions sponsored by AAAI, IJCAI, and other groups.

In previous chapters I described more specialized robot systems designed for speci c tasks, such as soccer-playing and autonomous automobile-driving. Even these, however, are integrated systems that advance AI perception, learning, planning, and plan-execution techniques that will be broadly useful.

B.  Intelligent  Assistants

Now let’s move away from robots to consider disembodied “agents” that help people in ways that do not require mobility.  Instead they help with databases, communication, Internet access, and task performance.  I’ll mention a couple of projects that are representative of those seeking to develop such agents.17

DARPA’s “PAL Program” has been a source of support for some of this work.  PAL is an acronym for Personalized Assistant that Learns.  According to the program’s Web site,18

The mission of the PAL program is to radically improve the way computers support humans by enabling systems that are cognitive, i.e., computer systems that can reason, learn from experience, be told what to do, explain what they are doing, reflect on their experience, and respond robustly to surprise.
. . .
This is the first broad-based research program in cognitive systems since the Strategic Computing Initiative funded by DARPA in the 1980s.  Since then, there have been significant developments in the technologies needed to enable cognitive systems, such as machine learning, reasoning, perception, and, multimodal interaction.
Improvements in processors, memory, sensors and networking have also dramatically changed the context of cognitive systems research.  It is now time to encourage the various areas to come together again by focusing on by [sic] a common application problem:  a Personalized Assistant that Learns.

One of the systems being developed under the general umbrella of the PAL Program is CALO, an acronym for Cognitive Assistant that Learns and Organizes.19   CALO is being managed by SRI International and includes over thirty participants from American universities and companies.  The project brings together experts in machine learning, natural language processing, knowledge representation, human–computer interaction, flexible planning, and behavioral studies.  The CALO software learns by interacting with and being advised by its users and is meant to help users with military decision-making tasks.  (The name, CALO, was inspired by the Latin word “calonis,” which means “soldier’s servant.”)

Versatile conversational agents may well serve a useful role for some people as occasional surrogates for human interaction.  But one should bear in mind warnings, such as those voiced by Theodore Roszak and others, about their misuse and overuse.  Aaron Sloman of the University of Birmingham’s School of Computer Science in the United Kingdom has written a thoughtful position paper about some of the difficulties of building digital companions.25

C.  Learning  by  Reading

All AI researchers agree that both common-sense knowledge and specialized knowledge is the key to intelligence.  Various approaches have been pursued to gather and organize that knowledge in a form exploitable by computer programs.  Projects like Cyc attempt to do so by hand-coding millions of small pieces of knowledge as logical sentences.  Machine learning research has shown that statistical methods can be used to “mine” large databases for knowledge.

A third approach is to build programs that can read (and understand) natural language text.  After all, as the proponents of that approach claim, the world is full of knowledge – in books, in news feeds, and on the Web.  (Of course, it is full of a lot of nonsense also, but smart programs may ultimately be able to highlight the trustworthy parts.)  As we have already seen,  computer understanding of natural language text requires both general common-sense knowledge and background knowledge about the subject matter of the text.  Thus, “Learning by Reading” (LbR), as this third approach is called, draws on the technologies of knowledge representation and reasoning as well as on natural language processing.

35.2     Toward  Human-Level  Artificial Intelligence

35.2.1    Eye on the Prize

In a 1995 article titled “Eye on the Prize” I argued that AI researchers ought to be putting more effort into developing generally intelligent systems in addition to their work on the kinds of smart tools I mentioned in the previous chapter.  I suggested that AI ought to focus on building what I called “habile” systems – ones that could learn to use smart tools, just as humans are able to learn to use them.  More generally, it has always seemed to me that AI’s grand goal – the “prize” we are questing for – should be to develop artifacts that can do most of the things that humans can do – specifically those things that are thought to require “intelligence.” These systems would have what some AI researchers have called “Human Level Artificial Intelligence” (HLAI).

HLAI was the goal of the founders and of many other early AI researchers.  John McCarthy claims that the “first scientific discussion of human level machine intelligence was apparently by Alan Turing” in his lecture to the London Mathematical Society on Febuary 20, 1947.33   Turing made the mechanization of human-level intelligence an explicit goal in his 1950 “Computing Machinery and Intelligence” paper.  Later, in the proposal for the 1956 Dartmouth Summer Study, John McCarthy wrote “The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.”

Newell and Simon’s “General Problem Solver” (GPS) was aimed specifically at HLAI, and they continued to work on the problems both of understanding and mechanizing intelligent behavior.  In a 1957 talk, Simon  said that “that there are now in the world machines that think, that learn and that create.  Moreover, their ability to do these things is going to increase rapidly until – in a visible future – the range of problems they can handle will be coextensive with the range to which the human mind has been applied.” In his 1961 paper “Steps Toward Artificial Intelligence” Marvin Minsky wrote “We are on the threshold of an era that will be strongly influenced, and quite possibly dominated, by intelligent problem-solving machines.” In a 2003 paper, Edward Feigenbaum concluded “Computational Intelligence is the manifest destiny of computer science, the goal, the destination, the final frontier.  More than any other field of science, our computer science concepts and methods are central to the quest to unravel and understand one of the grandest mysteries  of our existence, the nature of intelligence.  Generations of computer scientists to come must be inspired by the challenges and grand challenges of this great quest.”34

Some people have pointed out that HLAI necessarily implies superhuman-level intelligence.  Back in 1965 the English statistician (and co-worker of Alan Turing) I. J. Good wrote

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind.  Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.

In 1987 Jack Schwartz, the sometime critic of AI, wrote

If artificial intelligences can be created at all, there is little reason to believe that initial successes could not lead swiftly to the construction of artificial superintelligences able to explore significant mathematical, scientific, or engineering alternatives at a rate far exceeding human ability, or to generate plans and take action on them with equally overwhelming speed.  Since man’s near-monopoly of all higher forms of intelligence has been one of the most basic facts of human existence throughout the past history of this planet, such developments would clearly create a new economics, a new sociology, and a new history.

The idea of machines becoming more and more intelligent inspired Vernor Vinge (1944– ), a mathematician, computer scientist, and science fiction writer, to predict that a computer superintelligence would emerge by 2030.  He called this event a “singularity,” that is, a point in time when the rate of technological progress becomes unimaginably rapid.  In an essay about the singularity, he wrote “When greater-than-human intelligence drives progress, that progress will be much more rapid.  In fact, there seems no reason why progress itself would not involve the creation of still more intelligent entities – on a still-shorter time scale.”37  He foresaw the day when “Large computer networks (and their associated users) may ‘wake up’ as a superhumanly intelligent entity.”

The inventor and AI researcher Ray Kurzweil (1948– ) has popularized the idea of the singularity in his book The Singularity is Near.38   Based largely on the idea that the rate of technological progress increases exponentially (and will continue to do so), Kurzweil makes a number of predictions about what technology will produce in various decades starting with 2010 and leading up to “The Singularity” in 2045 and beyond.  Some of his predictions seem rather outlandish, and the whole idea of a singularity has provoked much derision, but I’ll leave it to you to judge for yourself.  (Here is one sample prediction: By 2045, “machines enter into a ‘runaway reaction’ of self-improvement cycles, with each new generation of A.I.s appearing faster and faster.  From this point onwards, technological advancement is explosive, under the control of the machines, and thus cannot be accurately predicted.”) The Web page http://singularity.com/themovie/ says that a movie version of “The Singularity is Near” (featuring Kurzweil as well as other futurists and computer scientists) will appear in late 2009.  Several commentators about the singularity have written articles for the June 2008 special issue of IEEE Spectrum.  Its Web site has pointers to the articles and to auxiliary material.

In 2004, The Singularity Institute for Artificial Intelligence (SIAI)40  was formed “to confront this urgent challenge, both the opportunity and the risk.” Its Director of Research, Ben Goertzel, is also chair of an organization called the “Artificial General Intelligence Research Institute” (AGIRI) whose “mission is to foster the creation of powerful and ethically positive Artificial General Intelligence.”41  AGIRI sponsors conferences and workshops and manages some open source projects.  AGIRI uses the term “artificial general intelligence” (AGI) somewhat in the same sense that I have been using HLAI. According to one of its Web sites, “the term is used to stress the ‘general’ nature of the desired capabilities of the systems being researched – as compared to the bulk of mainstream Artificial Intelligence (AI) work, which focuses on systems with very specialized ‘intelligent’ capabilities.”

I’ll conclude this chapter and the book by talking about HLAI, what it might be, some arguments for and against it, the possible consequences of it, and proposed methods for achieving it.

35.2.2    Controversies

HLAI (and beyond) is still the goal of many AI researchers even though we may still be a long way from achieving it – whatever it is.  In fact, there is controversy about just what HLAI might be.  Will we have achieved it when we have programs that can pass various tests, such as the Turing test?  Pat Hayes and Ken Ford of the Institute for Human and Machine Cognition in Pensacola, Florida, are among those who argue, on various practical and methodological grounds, against using the Turing test as a measure of AI’s progress.

Other tests have been proposed that could be taken as helping to define HLAI. For example, in a 2005 paper, I suggested one, which I called the “employment test.” In that paper, I wrote that to pass the test “programs must be able to perform the jobs ordinarily performed by humans.  Progress toward human-level AI could then be measured by the fraction of these jobs that can be acceptably performed by machines.” I had in mind all kinds of jobs that humans get paid to perform – from skilled and unskilled labor to managerial and office work. For me, achieving HLAI implies (at least) knowing how to build artifacts that can do what we now pay humans to do.

Hayes and Ford reject the very idea of HLAI as a goal for AI research.44 For one thing, they point to the fact that AI programs can already outperform humans in many areas.  So, achieving HLAI may in some ways be too modest  a goal.  Furthermore, just as airplanes fly without mimicking birds, AI need not attempt to mimic humans.  Observing that “there is no shortage of humans, and we already have well-proven ways of making more of them,” they conclude that “human cognition is not in short supply” – implying, I suppose, that there is no need to automate the full range of human cognitive abilities.

Yes, even though we already have AI systems that can do some things better than humans can do them, there are still many, many human cognitive skills that we don’t yet know how to automate.  I believe there are two reasons why AI researchers will continue to strive to automate these skills.  One is economic:  The prospect of someday being able to employ AI systems rather than more expensive humans (no matter how many humans there might be) will sustain a strong and irresistible urge to build artifacts that can do what we now pay humans to do.  The other reason is scientific:  In their attempts to understand how the human brain works, people will continue to build computational models of its many functions.  Thus, I think achieving some version of HLAI will continue to be AI’s long-term goal.

Even though HLAI may be hard to define, there is money riding on a prediction that we’ll achieve something like it.  In 2002 Mitchell Kapor bet
$20,000 that “By 2029 no computer – or ‘machine intelligence’ – will have passed the Turing Test.” Kapor is the designer of Lotus 1-2-3, the founder of Lotus Development Corporation, and the co-founder of the Electronic Frontier Foundation.  The Long Now Foundation posted this bet on its “Long Bets” Web page at http://www.longbets.org/.  Ray Kurzweil accepted it.  Both Kapor and Kurzweil gave arguments for their positions, and these along with detailed terms of the bet can be found at http://www.longbets.org/1#terms.

35.2.3    How Do We Get It?

Assuming that HLAI remains one of AI’s goals, how do we achieve it?  Can the technical tools that have been developed so far be utilized and combined in the right way to produce human-level intelligence?  Will continuing research on machine learning, neural networks, graphical models, simulated evolution, knowledge representation, reasoning, heuristic search, natural language processing, behavioral mechanisms, and perception (especially vision) lead inexorably toward the goal?  Or is something completely different needed as well?

John McCarthy mentions two approaches toward achieving HLAI. One is to attempt to simulate how the human intellect works, but, as he wrote, “Understanding the human brain well enough to imitate its function. . . requires theoretical and experimental success in psychology and neurophysiology." The other is to write programs that mimic human "intelligent behavior, which is what AI researchers have largely been trying to do.  McCarthy says that “It isn’t a question of deciding between them, because each should eventually succeed; it is more a race.”

But should the racers aim immediately for the goal or pursue it in stages?
I think the latter.  In his 1961 paper “Steps Toward Artificial Intelligence,” Marvin Minsky presciently wrote “It is my conviction that no scheme for learning, or for pattern-recognition, can have very general utility unless there are provisions for recursive, or at least hierarchical use of previous results.” He might well have included, besides learning and pattern recognition, other aspects of intelligence as well.  He went on to say46

We cannot expect a learning system to come to handle very hard problems without preparing it with a reasonably graded sequence of problems of growing difficulty.  The first problem must be one that can be solved in reasonable time with the initial resources. The next must be capable of solution in reasonable time by using reasonably simple and accessible combinations of methods developed in the first, and so on.  The only alternatives to this use of an adequate “training sequence” are 1) advanced resources, given initially, or 2) the fantastic exploratory processes found perhaps only in the history of organic evolution.

I think Minsky was exactly right.  We’ve been trying the alternatives of “advanced resources, given initially,” and simulating “organic evolution.” These approaches have produced smart tools and other useful programs but not HLAI yet.  What about working on a “graded sequence of problems of growing difficulty”?  This strategy has been suggested and deserves serious consideration.

In his 1950 paper, Alan Turing suggested that “Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s?  If this were then subjected to an appropriate course of education one would obtain the adult brain. The “appropriate course of education” would then correspond to Minsky’s “graded sequence of problems.”

The staged approach is also reflected in a list of AI capabilities that Rodney Brooks would like to see implemented.  He suggested they might have been “the foundation for the emergence, through an evolutionary process, of higher levels of intelligence in human beings.” Here is his list:

-  the object-recognition capabilities of a 2-year-old child,
-  the language capabilities of a 4-year-old child,
-  the manual dexterity of a 6-year-old child, and"
-  the social understanding of an 8-year-old child.

Brooks points out that computer systems don’t yet have these capabilities.  I believe that working toward them would constitute important, and perhaps necessary, steps toward HLAI. With them, AI could implement Turing’s strategy of educating machines in the same way as we educate people. But achieving these abilities will be very hard.

I think that Brooks had in mind achieving them by attempting to mimic human behavior – in this case the behavior of human children – the second of the approaches that McCarthy suggested.

As regards the other approach, namely, “understanding the human brain well enough to imitate its function,” there has been work on that problem too. Several computer scientists are attempting to use concepts familiar to AI people to explain the brain.  I have already mentioned the hierarchical models of the cortex proposed by Mumford, Hinton, Hawkins, Dean, and their various colleagues.  Building on features of primitive sensory inputs from an array of pixels, for example, and ascending through ever-more-abstract percepts, these models are able to learn to classify images independently of size, translation, and orientation.  Yet, to my knowledge, no work has yet been done to use these models for more than perception.  Can they learn to understand, to reason, to plan, and to select actions?  Looked at from the point of view of my
“triple-tower architecture” (see p.  567), they have tackled the problem of how the brain might implement the perception and model towers but they have not yet tried to do anything with the action tower.  Could one of these cortical models control a robot, for example?

Dharmendra Modha (1969– ), manager of cognitive computing at IBM’s Almaden Research Center in San Jose, California,48  is among those pursuing a more “bottom-up” approach.  In 2007, Modha’s team carried out an historic experiment in which they constructed a computer simulation of a rat-scale model of the cortex (with 55 million spiking neurons and 448 billion synapses with spike-timing-dependent plasticity) that could function in near real-time using a BlueGene supercomputer with 32,768 processors and 8 TB of main memory.  Modha is the principal investigator of a DARPA project to develop “Systems of Neuromorphic Adaptive Plastic Scalable Electronics”  (SyNAPSE), or, in plain English, a project whose goal is to build a machine that mimics the actions of about 100 million neurons.  That’s twice the number of neurons in a rat brain but only about 0.25% of the number in a human brain.  One can hope that the top-down and bottom-up approaches will meet in the middle somewhere.

Many of the laboratory efforts I mentioned earlier in this chapter are what I would call “HLAI-friendly”; that is, they are likely to develop the technology that will be needed by HLAI systems.  One of them, the STAIR project, is working directly toward a challenge problem I posed back in 1996, namely,

". . . to produce a robot factotum and errand-runner for a typical office building – an office building that is not specially equipped to accommodate robots.  . . . The robot must be able to perform (or learn to perform with instruction and training – but without explicit post-factory computer programming) any task that a human might “reasonably” expect to be able to perform given its [the robot’s] effector/sensor suite.
. . .
The second part of the challenge is that the robot must stay
on-the-job and functioning for a year without being sent back to the factory for reprogramming.
. . .
I do not think that it will be feasible for the robot’s builders to send it to its office building with a suite of programs that anticipate all of the tasks that could be given.  I think the robot will need to be able to plan and to learn how to perform some tasks that the building occupants (who know only about its sensors and effectors) might expect it to be able to perform but that its programmers did not happen to anticipate.

Independently of the various concerns about the appropriateness of (and even the definition of) HLAI as a goal, I think we’ll indeed achieve it.  I won’t predict when except that it will probably be sometime in this century.  But what if we do?  That’s a topic I turn to next.

35.2.4    Some Possible Consequences of HLAI

Suppose that someday superintelligent machines become part of our society, to help us, to entertain us, and to do much of our work for us.  They are likely to take various forms – humanoid and other varieties of robots, “presences” on the World Wide Web, software on our home and laptop computers, and, possibly, special implants to aid our own intellectual functioning.  Will we by then have constructed a social order that will preclude these machines fighting us and each other?  Can we even define what it would mean for them to serve only socially acceptable goals?

These are concerns that have engaged both computer scientists and humanists.  In 1987 Jack Schwartz wrote that many humanist thinkers

. . . express the amorphous unease of a much broader public.  The fear is that the whole fabric of human society, which at times seems terrifyingly fragile, may be torn apart by enormously rapid technological changes set in motion by AI research as it begins to yield its major fruits.  For example, it is possible to imagine that would-be dictators, small centrally placed oligarchies, or predatory nations could exploit this technology to establish a power over society resting on robot armies and police forces independent of extensive human participation and entirely indifferent to all traditional human or humane considerations.  Even setting this nightmare aside, one can fear a variety of more subtle deleterious impacts, for example, rapid collapse of human society into a self-destructive pure hedonism once all pressures, and perhaps even reasons or opportunities, for work and striving are undermined by the presence of unchallengeably omnicompetent mechanisms. Certainly man’s remaining sense of his own uniqueness may be further impaired, and he may come to seem in his own eyes little more than a primitive animal, capable only of fleeting enjoyments.

To confront these fears, Stephen M. Omohundro (1959– ), an AI researcher, founded Self-Aware Systems, an organization “devoted to bringing wisdom into emerging technologies.”51  He thinks “we must be very careful” about developing AI systems.  That’s because they will have, by design, various goals and drives.  Among these are the goals to be self-improving and rational.  They will attempt to accomplish these goals and the goals given to them by humans in the most effective manner possible.  To be maximally effective they will have drives to preserve themselves and to acquire resources. These characteristics remind us of HAL 9000, the robot on the spaceship in the book and movie 2001:  A Space Odyssey.  Omohundro wants to make sure that we build “wisdom,” and not just intelligence, into our technologies.  By that he means building in “human values, such as caring about human rights and property rights and having compassion for other entities.” He thinks it “absolutely critical that we build these in at the beginning, otherwise we’ll get systems that are very powerful, but which do not support our values.” I think Omohundro brings up valid concerns, but to put his version of wisdom into AI systems we’ll first have to agree on just what we mean by “human values.” That will be tough given that our different opinions about values often lead to wars.

After many years working on mobile robotics, Professor Ronald Arkin (1949– ) of the Georgia Institute of Technology has devoted attention to the problem of ethical issues surrounding the use of military robots.  His book Governing Lethal Behavior in Autonomous Robots explores how to program an “artificial conscience” in robots.53   He maintains that such robots might behave more ethically in the battlefield than humans currently can.  Of course many people believe that even being on a battlefield is unethical – for humans or robots.

The possibility of HLAI brings up many other interesting questions. Will they have “rights”?  Can they own property?  Could they be participants in civil or criminal judicial proceedings? Would they be able to create literature, music, and art?  Will they have emotions or be capable of feeling pain or joy? Will they be conscious?  Would humans become emotionally attached to some versions of them (and vice versa)?  These are all fascinating questions, and many people have written about them.

Whether or not society accords intelligent artifacts rights and other legal powers will be up to us humans.  After all, we have decided that several nonhumans (such as corporations) can have certain rights and obligations, and some humans (such as children) will not have certain rights.  We’ll have to make similar decisions about intelligent artifacts.

What about creativity?  Here’s an example.  Professor emeritus David Cope at the University of California at Santa Cruz has developed a set of programs he calls “Experiments in Musical Intelligence.”54  These programs analyze the style of a musical composer and then use special “recombination” procedures to create entirely new compositions in that same style.  Cope has used his software to produce works in the style of hundreds of composers. From one of Cope’s Web sites you can download any of 5,000 MIDI files of different computer-created Bach-style chorales.55   For those more inclined to listen to ragtime (in the style of Scott Joplin) check out ftp://arts.ucsc.edu/pub/cope/joplin.mp3.  Cope argues that “recombinancy appears everywhere as a natural evolutionary and creative process.”56  Some version of it might well be the basis of all creativity, whether in literature, in art, or in music.

Marvin Minsky, drawing on his many years of research in artificial intelligence, has written an excellent book about emotions and other mental phenomena.57   In its Introduction, Minsky claims that “Each of our major ‘emotional states’ results from turning certain [parts of the brain] on while turning certain others off – and thus changing some ways that our brains behave.” The book describes what some of these brain parts, which he calls “resources,” do and how overlapping clusters of them get turned on and off – resulting not only in various emotional states but in “the processes that we call ‘thinking.’”

Minsky’s book has a chapter about consciousness.  He argues that consciousness “is a suitcase word, which we each fill up with far more stuff than could possibly have just one common cause.” He argues that being “conscious” of something involves dozens of mental activities; such collections are different in different circumstances.  Furthermore, neuroscience does not yet have a proper scientific view of just how all of these mental activities actually work.  Minsky agrees with the philosopher Aaron Sloman, whom he quotes as writing58

The whole idea [of consciousness] is based on a fundamental misconception that just because there is a noun “consciousness” there is some “thing” like magnetism or electricity or pressure or temperature, and that it’s worth looking for correlates of that thing.
. . .
There will not be one thing to be correlated but a very large collection of very different things.

Someday, I believe, AI researchers aided by (and aiding) neuroscience, will know enough about how brain “resources” work that they will be able to build artifacts that will convincingly argue that they are conscious.  When that day comes I would have to assent to their claim, just as I assent to yours.

However, we have not done it yet, so the argument remains open.
Computer scientist and Yale professor David Gelernter (1955– ) argues that “. . . it is hugely unlikely, though not impossible, that a conscious mind will ever be built out of software.” Nevertheless, he thinks that “an unconscious simulated intelligence certainly could be built out of software – and might be useful.” [Might be?!]  “Unfortunately, AI, cognitive science, and philosophy of mind are nowhere near knowing how to build one.” He thinks that AI needs to refocus its efforts toward what he calls “the mechanisms (or algorithms) of thought. . . ” Until that time, he laments “AI is lost in the woods.”

Lost?  I don’t think so.  Do we have long way to go?  Probably, but we won’t know until we get there.  And until we do, we’ll hear various calls to “refocus” both from within and from outside the field.  In any case, we should continue to pursue many different approaches, guided by our best judgments. It’s like heuristic search for a goal that we’ll recognize when we achieve it.

As we work toward that goal, some of the consequences of HLAI will emerge gradually.  For one thing, we’ll become more and more dependent on smart machines.  Just as we have become dependent on the automobile and other inventions of the past century and a half, it is already the case that our society depends in many ways on the Internet, sophisticated trading and auction programs, spreadsheet programs, weather forecasting models, and a host of other computer-related technologies.  Furthermore, unlike as is the case with the automobile and the radio, fewer and fewer people understand these new technologies, putting us already in the precarious position of having to trust them.

As more and more “jobs” are performed by inexpensive hardware–software combinations, people who used to get paid for those jobs will have to find others or risk being jobless.  That does not mean that the total gross domestic product will shrink; in fact it will probably rise.  Society will need to find ways to let its members enjoy a just share of the wealth that machines create.  In a thoughtful article60  about these matters, the economist Robin Hanson likens the effects of machines substituting for human workers to an inexorably rising sea level. . .

. . . with the tasks that are “most human” [those in which humans have an economic advantage over machines] on the higher ground.

Here you find chores best done by humans, like gourmet cooking or elite hairdressing.  Then there is a “shore” consisting of tasks that humans and machines are equally able to perform and, beyond them an “ocean” of tasks best done by machines.  When machines get cheaper or smarter or both, the water level rises, as it were, and the shore moves inland.
This sea change has two effects.  First, machines will substitute for humans by taking over newly “flooded” tasks.  Second, doing machine tasks better complements human tasks, raising the value of doing them well.

Hanson believes that it is possible that the “machine ocean” might ultimately inundate all of “Task-Land” and that consequently “wages would fall so far that most humans would not, through their labor alone, be able to live on them, though they might work for other reasons.” (Of course, automation may create new and higher peaks in Task-Land, slowing the effects of the rising ocean.)  Nevertheless, he imagines that any small part of the greatly expanded wealth created by the machines should “allow humans to live comfortably. . .”

35.3     Summing  Up

On that optimistic note, I come to the end of my story about the quest for artificial intelligence – a quest that is not yet complete.  AI has explored a variety of paths, and in doing so it has achieved several successes and has assembled many powerful computational tools.  One way to summarize the ideas and achievements I have talked about is to divide them into four main categories, namely, complete AI systems (ones that do things), architectures (organizational principles for AI systems), processes (routines that actually do the work), and representations (structures that are created, modified, and accessed by processes).  Without trying to be complete, I’ll mention some of what I think have been AI’s major accomplishments in each of these categories.

-  Complete AI systems:  LT, Heuristic DENDRAL, Shakey, expert systems (such as MYCIN and PROSPECTOR), MSYS, speech recognition systems (such as HARPY, DRAGON, and HEARSAY II), Genghis, driverless automobiles (the whole class of them), Deep Blue, and other
game-playing machines, RAX, and CALO.
-  Architectures:  Pandemonium, production systems, three-level architectures, Blackboard architectures, BDI architectures (such as PRS), behavior-based architectures, SOAR, ACT-R, and cortical models.
-  Processes:  edge- and region-finding filters (including Laplacian of Gaussian), spreading activation, parsing, resolution, A∗ and its progeny, beam search, the Rete algorithm, STRIPS and other planning systems, case-based reasoning, clustering (such as k-means and AutoClass), constraint propagation, Bayes’s rule, genetic algorithms and genetic programming, GSAT and DPLL-based methods, knowledge sources (such as those used in blackboards and for computing intrinsic images), backprop, circumscription, latent semantic analysis, Q-learning and prioritized sweeping, particle filtering, kernel computations (for support vector machines), and expectation maximization (EM).
-  Representations:  state (problem) spaces, vectors, logical expressions (including IF–THEN rules), semantic networks, programs (as data structures), blackboards, graphical models (including Bayesian networks and HMMs), grammars, neural networks, decision trees, scripts, frames, and augmented transition networks.

[The processes and representations they work on are usually intimately linked. For example, GSAT works on sets of propositions (in the form of clauses), Blackboard knowledge sources respond to and modify Blackboard items, and genetic programming operates on LISP-program representations.]

Several disciplines have contributed to AI’s successes.  As I wrote at the beginning of this book, the early AI pioneers used many clues about how to proceed – clues from mathematics and logic, from neuroscience, from linguistics, from statistics and probability theory, from control engineering, from psychology, and from computer science.  Indeed, the substantial progress made in the quest for AI to date is due to the use of ideas from all of those disciplines.  No overarching theory of AI has yet emerged, nor is one likely to in my opinion.

The quest will continue.  What combinations of AI’s methods, buttressed by AI’s supporting disciplines, will be used in the intelligent systems of the future?  No one really knows, so we’ll have to keep all of them active on AI’s “search frontier.” Some of the ones developed early in the quest (and now perhaps forgotten) might with better technology be useful.  Researchers who want to pursue the quest should be familiar with the full variety of AI’s methods, its contributing disciplines, and (yes) its history.
Future writers will doubtless continue to tell the story of the quest.  One of them, someday, will be able to report that some two and half millennia after Aristotle’s musings, we now do have tools that perform our tasks, “either at our bidding or [themselves] perceiving the need. . ."


/ 021. White House Office of Science and Technology Policy. "Blueprint for an AI Bill of Rights," 2022.

About this Document 

The Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People was published by the White House Office of Science and Technology Policy in October 2022. This framework was released one year after OSTP announced the launch of a process to develop “a bill of rights for an AI-powered world.” Its release follows a year of public engagement to inform this initiative. The framework is available online at: https://www.whitehouse.gov/ostp/ai-bill-of-rights

About the Office of Science and Technology Policy 

The Office of Science and Technology Policy (OSTP) was established by the National Science and Technology Policy, Organization, and Priorities Act of 1976 to provide the President and others within the Executive Office of the President with advice on the scientific, engineering, and technological aspects of the economy, national security, health, foreign relations, the environment, and the technological recovery and use of resources, among other topics. OSTP leads interagency science and technology policy coordination efforts, assists the Office of Management and Budget (OMB) with an annual review and analysis of Federal research and development in budgets, and serves as a source of scientific and technological analysis and judgment for the President with respect to major policies, plans, and programs of the Federal Government.

Legal Disclaimer 

The Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People is a white paper published by the White House Office of Science and Technology Policy. It is intended to support the development of policies and practices that protect civil rights and promote democratic values in the building, deployment, and governance of automated systems.

The Blueprint for an AI Bill of Rights is non-binding and does not constitute U.S. government policy. It does not supersede, modify, or direct an interpretation of any existing statute, regulation, policy, or international instrument. It does not constitute binding guidance for the public or Federal agencies and therefore does not require compliance with the principles described herein. It also is not determinative of what the U.S. government’s position will be in any international negotiation. Adoption of these principles may not meet the requirements of existing statutes, regulations, policies, or international instruments, or the requirements of the Federal agencies that enforce them. These principles are not intended to, and do not, prohibit or limit any lawful activity of a government agency, including law enforcement, national security, or intelligence activities.

The appropriate application of the principles set forth in this white paper depends significantly on the context in which automated systems are being utilized. In some circumstances, application of these principles in whole or in part may not be appropriate given the intended use of automated systems to achieve government agency missions. Future sector-specific guidance will likely be necessary and important for guiding the use of automated systems in certain settings such as AI systems used as part of school building security or automated health diagnostic systems.

The Blueprint for an AI Bill of Rights recognizes that law enforcement activities require a balancing of equities, for example, between the protection of sensitive law enforcement information and the principle of notice; as such, notice may not be appropriate, or may need to be adjusted to protect sources, methods, and other law enforcement equities. Even in contexts where these principles may not apply in whole or in part, federal departments and agencies remain subject to judicial, privacy, and civil liberties oversight as well as existing policies and safeguards that govern automated systems, including, for example, Executive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government (December 2020).

This white paper recognizes that national security (which includes certain law enforcement and homeland security activities) and defense activities are of increased sensitivity and interest to our nation’s adversaries and are often subject to special requirements, such as those governing classified information and other protected data. Such activities require alternative, compatible safeguards through existing policies that govern automated systems and AI, such as the Department of Defense (DOD) AI Ethical Principles and Responsible AI Implementation Pathway and the Intelligence Community (IC) AI Ethics Principles and Framework. The implementation of these policies to national security and defense activities can be informed by the Blueprint for an AI Bill of Rights where feasible.

The Blueprint for an AI Bill of Rights is not intended to, and does not, create any legal right, benefit, or defense, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person, nor does it constitute a waiver of sovereign immunity.

FOREWARD

Among the great challenges posed to democracy today is the use of technology, data, and automated systems in ways that threaten the rights of the American public. Too often, these tools are used to limit our opportunities and prevent our access to critical resources or services. These problems are well documented. In America and around the world, systems supposed to help with patient care have proven unsafe, ineffective, or biased. Algorithms used in hiring and credit decisions have been found to reflect and reproduce existing unwanted inequities or embed new harmful bias and discrimination. Unchecked social media data collection has been used to threaten people’s opportunities, undermine their privacy, or pervasively track their activity—often without their knowledge or consent.

These outcomes are deeply harmful—but they are not inevitable. Automated systems have brought about extraordinary benefits, from technology that helps farmers grow food more efficiently and computers that predict storm paths, to algorithms that can identify diseases in patients. These tools now drive important decisions across sectors, while data is helping to revolutionize global industries. Fueled by the power of American innovation, these tools hold the potential to redefine every part of our society and make life better for everyone.

This important progress must not come at the price of civil rights or democratic values, foundational American principles that President Biden has affirmed as a cornerstone of his Administration. On his first day in office, the President ordered the full Federal government to work to root out inequity, embed fairness in decisionmaking processes, and affirmatively advance civil rights, equal opportunity, and racial justice in America.1 The President has spoken forcefully about the urgent challenges posed to democracy today and has regularly called on people of conscience to act to preserve civil rights—including the right to privacy, which he has called “the basis for so many more rights that we have come to take for granted that are ingrained in the fabric of this country.”

To advance President Biden’s vision, the White House Office of Science and Technology Policy has identified five principles that should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that protects all people from these threats—and uses technologies in ways that reinforce our highest values. Responding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by a technical companion—a handbook for anyone seeking to incorporate these protections into policy and practice, including detailed steps toward actualizing these principles in the technological design process. These principles help provide guidance whenever automated systems can meaningfully impact the public’s rights, opportunities, or access to critical needs.

ABOUT THIS FRAMEWORK

The Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of artificial intel-ligence. Developed through extensive consultation with the American public, these principles are a blueprint for building and deploying automated systems that are aligned with democratic values and protect civil rights, civil liberties, and privacy. The Blueprint for an AI Bill of Rights includes this Foreword, the five principles, notes on Applying the The Blueprint for an AI Bill of Rights, and a Technical Companion that gives concrete steps that can be taken by many kinds of organizations—from governments at all levels to companies of all sizes—to uphold these values. Experts from across the private sector, governments, and international consortia have published principles and frameworks to guide the responsible use of automated systems; this framework provides a national values statement and toolkit that is sector-agnostic to inform building these protections into policy, practice, or the technological design process. Where existing law or policy—such as sector-specific privacy laws and oversight requirements—do not already provide guidance, the Blueprint for an AI Bill of Rights should be used to inform policy decisions.

LISTENING TO THE AMERICAN PUBLIC

The White House Office of Science and Technology Policy has led a year-long process to seek and distill input from people across the country—from impacted communities and industry stakeholders to technology developers and other experts across fields and sectors, as well as policymakers throughout the Federal government—on the issue of algorithmic and data-driven harms and potential remedies. Through panel discussions, public listening sessions, meetings, a formal request for information, and input to a publicly accessible and widely-publicized email address, people throughout the United States, public servants across Federal agencies, and members of the international community spoke up about both the promises and potential harms of these technologies, and played a central role in shaping the Blueprint for an AI Bill of Rights. The core messages gleaned from these discussions include that AI has transformative potential to improve Americans’ lives, and that preventing the harms of these technologies is both necessary and achievable. The Appendix includes a full list of public engagements.

BLUEPRINT FOR AN AI BILL OF RIGHTS

SAFE AND EFFECTIVE SYSTEMS

You should be protected from unsafe or ineffective systems. Automated systems should be developed with consultation from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts of the system. Systems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring that demonstrate they are safe and effective based on their intended use, mitigation of unsafe outcomes including those beyond the intended use, and adherence to domain-specific standards. Outcomes of these protective measures should include the possibility of not deploying the system or removing a system from use. Automated systems should not be designed with an intent or reasonably foreseeable possibility of endangering your safety or the safety of your community. They should be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of automated systems. You should be protected from inappropriate or irrelevant data use in the design, development, and deployment of automated systems, and from the compounded harm of its reuse. Independent evaluation and reporting that confirms that the system is safe and effective, including reporting of steps taken to mitigate potential harms, should be performed and the results made public whenever possible.

ALGORITHMIC DISCRIMINATION PROTECTIONS 

You should not face discrimination by algorithms and systems should be used and designed in an equitable way. Algorithmic discrimination occurs when automated systems contribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate legal protections. Designers, developers, and deployers of automated systems should take proactive and continuous measures to protect individuals and communities from algorithmic discrimination and to use and design systems in an equitable way. This protection should include proactive equity assessments as part of the system design, use of representative data and protection against proxies for demographic features, ensuring accessibility for people with disabilities in design and development, pre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight. Independent evaluation and plain language reporting in the form of an algorithmic impact assessment, including disparity testing results and mitigation information, should be performed and made public whenever possible to confirm these protections.

DATA PRIVACY

You should be protected from abusive data practices via built-in protections and you should have agency over how data about you is used. You should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected. Designers, developers, and deployers of automated systems should seek your permission and respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate ways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be used. Systems should not employ user experience and design decisions that obfuscate user choice or burden users with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases where it can be appropriately and meaningfully given. Any consent requests should be brief, be understandable in plain language, and give you agency over data collection and the specific context of use; current hard-tounderstand notice-and-choice practices for broad uses of data should be changed. Enhanced protections and restrictions for data and inferences related to sensitive domains, including health, work, education, criminal justice, and finance, and for data pertaining to youth should put you first. In sensitive domains, your data and related inferences should only be used for necessary functions, and you should be protected by ethical review and use prohibitions. You and your communities should be free from unchecked surveillance; surveillance technologies should be subject to heightened oversight that includes at least pre-deployment assessment of their potential harms and scope limits to protect privacy and civil liberties. Continuous surveillance and monitoring should not be used in education, work, housing, or in other contexts where the use of such surveillance technologies is likely to limit rights, opportunities, or access. Whenever possible, you should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of surveillance technologies on your rights, opportunities, or access.

NOTICE AND EXPLANATION

You should know that an automated system is being used and understand how and why it contributes to outcomes that impact you. Designers, developers, and deployers of automated systems should provide generally accessible plain language documentation including clear descriptions of the overall system functioning and the role automation plays, notice that such systems are in use, the individual or organization responsible for the system, and explanations of outcomes that are clear, timely, and accessible. Such notice should be kept up-to-date and people impacted by the system should be notified of significant use case or key functionality changes. You should know how and why an outcome impacting you was determined by an automated system, including when the automated system is not the sole input determining the outcome. Automated systems should provide explanations that are technically valid, meaningful and useful to you and to any operators or others who need to understand the system, and calibrated to the level of risk based on the context. Reporting that includes summary information about these automated systems in plain language and assessments of the clarity and quality of the notice and explanations should be made public whenever possible.

HUMAN ALTERNATIVES, CONSIDERATION, AND FALLBACK

You should be able to opt out, where appropriate, and have access to a person who can quickly consider and remedy problems you encounter. You should be able to opt out from automated systems in favor of a human alternative, where appropriate. Appropriateness should be determined based on reasonable expectations in a given context and with a focus on ensuring broad accessibility and protecting the public from especially harmful impacts. In some cases, a human or other alternative may be required by law. You should have access to timely human consideration and remedy by a fallback and escalation process if an automated system fails, it produces an error, or you would like to appeal or contest its impacts on you. Human consideration and fallback should be accessible, equitable, effective, maintained, accompanied by appropriate operator training, and should not impose an unreasonable burden on the public. Automated systems with an intended use within sensitive domains, including, but not limited to, criminal justice, employment, education, and health, should additionally be tailored to the purpose, provide meaningful access for oversight, include training for any people interacting with the system, and incorporate human consideration for adverse or high-risk decisions. Reporting that includes a description of these human governance processes and assessment of their timeliness, accessibility, outcomes, and effectiveness should be made public whenever possible.

APPLYING THE BLUEPRINT FOR AN AI BILL OF RIGHTS

While many of the concerns addressed in this framework derive from the use of AI, the technical capabilities and specific definitions of such systems change with the speed of innovation, and the potential harms of their use occur even with less technologically sophisticated tools. Thus, this framework uses a twopart test to determine what systems are in scope. This framework applies to (1) automated systems that (2) have the potential to meaningfully impact the American public’s rights, opportunities, or access to critical resources or services. These rights, opportunities, and access to critical resources of services should be enjoyed equally and be fully protected, regardless of the changing role that automated systems may play in our lives.

This framework describes protections that should be applied with respect to all automated systems that have the potential to meaningfully impact individuals' or communities' exercise of:

RIGHTS, OPPORTUNITIES, OR ACCESS 

Civil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimination, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in both public and private sector contexts; 

Equal opportunities, including equitable access to education, housing, credit, employment, and other programs; or, 

Access to critical resources or services, such as healthcare, financial services, safety, social services, non-deceptive information about goods and services, and government benefits.

A list of examples of automated systems for which these principles should be considered is provided in the Appendix. The Technical Companion, which follows, offers supportive guidance for any person or entity that creates, deploys, or oversees automated systems.

Considered together, the five principles and associated practices of the Blueprint for an AI Bill of Rights form an overlapping set of backstops against potential harms. This purposefully overlapping framework, when taken as a whole, forms a blueprint to help protect the public from harm. The measures taken to realize the vision set forward in this framework should be proportionate with the extent and nature of the harm, or risk of harm, to people's rights, opportunities, and access.

RELATIONSHIP TO EXISTING LAW AND POLICY

The Blueprint for an AI Bill of Rights is an exercise in envisioning a future where the American public is protected from the potential harms, and can fully enjoy the benefits, of automated systems. It describes principles that can help ensure these protections. Some of these protections are already required by the U.S. Constitution or implemented under existing U.S. laws. For example, government surveillance, and data search and seizure are subject to legal requirements and judicial oversight. There are Constitutional requirements for human review of criminal investigative matters and statutory requirements for judicial review. Civil rights laws protect the American people against discrimination.

Applying The Blueprint for an AI Bill of Rights

RELATIONSHIP TO EXISTING LAW AND POLICY

There are regulatory safety requirements for medical devices, as well as sector-, population-, or technology-specific privacy and security protections. Ensuring some of the additional protections proposed in this framework would require new laws to be enacted or new policies and practices to be adopted. In some cases, exceptions to the principles described in the Blueprint for an AI Bill of Rights may be necessary to comply with existing law, conform to the practicalities of a specific use case, or balance competing public interests. In particular, law enforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties, and privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed in this framework. The Blueprint for an AI Bill of Rights is meant to assist governments and the private sector in moving principles into practice.

The expectations given in the Technical Companion are meant to serve as a blueprint for the development of additional technical standards and practices that should be tailored for particular sectors and contexts. While existing laws informed the development of the Blueprint for an AI Bill of Rights, this framework does not detail those laws beyond providing them as examples, where appropriate, of existing protective measures. This framework instead shares a broad, forward-leaning vision of recommended principles for automated system development and use to inform private and public involvement with these systems where they have the potential to meaningfully impact rights, opportunities, or access. Additionally, this framework does not analyze or take a position on legislative and regulatory proposals in municipal, state, and federal government, or those in other countries.

We have seen modest progress in recent years, with some state and local governments responding to these problems with legislation, and some courts extending longstanding statutory protections to new and emerging technologies. There are companies working to incorporate additional protections in their design and use of automated systems, and researchers developing innovative guardrails. Advocates, researchers, and government organizations have proposed principles for the ethical use of AI and other automated systems. These include the Organization for Economic Co-operation and Development’s (OECD’s) 2019 Recommendation on Artificial Intelligence, which includes principles for responsible stewardship of trustworthy AI and which the United States adopted, and Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government, which sets out principles that govern the federal government’s use of AI. The Blueprint for an AI Bill of Rights is fully consistent with these principles and with the direction in Executive Order 13985 on Advancing Racial Equity and Support for Underserved Communities Through the Federal Government. These principles find kinship in the Fair Information Practice Principles (FIPPs), derived from the 1973 report of an advisory committee to the U.S. Department of Health, Education, and Welfare, Records, Computers, and the Rights of Citizens.4 While there is no single, universal articulation of the FIPPs, these core principles for managing information about individuals have been incorporated into data privacy laws and policies across the globe.5 The Blueprint for an AI Bill of Rights embraces elements of the FIPPs that are particularly relevant to automated systems, without articulating a specific set of FIPPs or scoping applicability or the interests served to a single particular domain, like privacy, civil rights and civil liberties, ethics, or risk management. The Technical Companion builds on this prior work to provide practical next steps to move these principles into practice and promote common approaches that allow technological innovation to flourish while protecting people from harm.

Applying The Blueprint for an AI Bill of Rights

DEFINITIONS

ALGORITHMIC DISCRIMINATION: “Algorithmic discrimination” occurs when automated systems contribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate legal protections. Throughout this framework the term “algorithmic discrimination” takes this meaning (and not a technical understanding of discrimination as distinguishing between items).

AUTOMATED SYSTEM: An "automated system" is any system, software, or process that uses computation as whole or part of a system to determine outcomes, make or aid decisions, inform policy implementation, collect data or observations, or otherwise interact with individuals and/or communities. Automated systems include, but are not limited to, systems derived from machine learning, statistics, or other data processing or artificial intelligence techniques, and exclude passive computing infrastructure. “Passive computing infrastructure” is any intermediary technology that does not influence or determine the outcome of decision, make or aid in decisions, inform policy implementation, or collect data or observations, including web hosting, domain registration, networking, caching, data storage, or cybersecurity. Throughout this framework, automated systems that are considered in scope are only those that have the potential to meaningfully impact individuals’ or communities’ rights, opportunities, or access.

COMMUNITIES: “Communities” include: neighborhoods; social network connections (both online and offline); families (construed broadly); people connected by affinity, identity, or shared traits; and formal organizational ties. This includes Tribes, Clans, Bands, Rancherias, Villages, and other Indigenous communities. AI and other data-driven automated systems most directly collect data on, make inferences about, and may cause harm to individuals. But the overall magnitude of their impacts may be most readily visible at the level of communities. Accordingly, the concept of community is integral to the scope of the Blueprint for an AI Bill of Rights. United States law and policy have long employed approaches for protecting the rights of individuals, but existing frameworks have sometimes struggled to provide protections when effects manifest most clearly at a community level. For these reasons, the Blueprint for an AI Bill of Rights asserts that the harms of automated systems should be evaluated, protected against, and redressed at both the individual and community levels.

EQUITY: “Equity” means the consistent and systematic fair, just, and impartial treatment of all individuals. Systemic, fair, and just treatment must take into account the status of individuals who belong to underserved communities that have been denied such treatment, such as Black, Latino, and Indigenous and Native American persons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+) persons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely affected by persistent poverty or inequality.

RIGHTS, OPPORTUNITIES, OR ACCESS: “Rights, opportunities, or access” is used to indicate the scoping of this framework. It describes the set of: civil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimination, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in both public and private sector contexts; equal opportunities, including equitable access to education, housing, credit, employment, and other programs; or, access to critical resources or services, such as healthcare, financial services, safety, social services, non-deceptive information about goods and services, and government benefits.

Applying The Blueprint for an AI Bill of Rights

SENSITIVE DATA: Data and metadata are sensitive if they pertain to an individual in a sensitive domain (defined below); are generated by technologies used in a sensitive domain; can be used to infer data from a sensitive domain or sensitive data about an individual (such as disability-related data, genomic data, biometric data, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationship history and legal status such as custody and divorce information, and home, work, or school environmental data); or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harm due to identity theft. Data and metadata generated by or about those who are not yet legal adults is also sensitive, even if not related to a sensitive domain. Such data includes, but is not limited to, numerical, text, image, audio, or video data. 

SENSITIVE DOMAINS: “Sensitive domains” are those in which activities being conducted can cause material harms, including significant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights. Domains that have historically been singled out as deserving of enhanced data protections or where such enhanced protections are reasonably expected by the public include, but are not limited to, health, family planning and care, employment, education, criminal justice, and personal finance. In the context of this framework, such domains are considered sensitive whether or not the specifics of a system context would necessitate coverage under existing law, and domains and data that are considered sensitive are understood to change over time based on societal norms and context. 

SURVEILLANCE TECHNOLOGY: “Surveillance technology” refers to products or services marketed for or that can be lawfully used to detect, monitor, intercept, collect, exploit, preserve, protect, transmit, and/or retain data, identifying information, or communications concerning individuals or groups. This framework limits its focus to both government and commercial use of surveillance technologies when juxtaposed with real-time or subsequent automated analysis and when such systems have a potential for meaningful impact on individuals’ or communities’ rights, opportunities, or access. 

UNDERSERVED COMMUNITIES: The term “underserved communities” refers to communities that have been systematically denied a full opportunity to participate in aspects of economic, social, and civic life, as exemplified by the list in the preceding definition of “equity.”

FROM PRINCIPLES TO PRACTICE: A Technical Companion to the Blueprint for an AI Bill of Rights

While technologies are being deployed to solve problems across a wide array of issues, our reliance on technology can also lead to its use in situations where it has not yet been proven to work—either at all or within an acceptable range of error. In other cases, technologies do not work as intended or as promised, causing substantial and unjustified harm. Automated systems sometimes rely on data from other systems, including historical data, allowing irrelevant information from past decisions to infect decision-making in unrelated situations. In some cases, technologies are purposefully designed to violate the safety of others, such as technologies designed to facilitate stalking; in other cases, intended or unintended uses lead to unintended harms.

Many of the harms resulting from these technologies are preventable, and actions are already being taken to protect the public. Some companies have put in place safeguards that have prevented harm from occurring by ensuring that key development decisions are vetted by an ethics review; others have identified and mitigated harms found through pre-deployment testing and ongoing monitoring processes. Governments at all levels have existing public consultation processes that may be applied when considering the use of new automated systems, and existing product development and testing practices already protect the American public from many potential harms.

Still, these kinds of practices are deployed too rarely and unevenly. Expanded, proactive protections could build on these existing practices, increase confidence in the use of automated systems, and protect the American public. Innovators deserve clear rules of the road that allow new ideas to flourish, and the American public deserves protections from unsafe outcomes. All can benefit from assurances that automated systems will be designed, tested, and consistently confirmed to work as intended, and that they will be proactively protected from foreseeable unintended harmful outcomes.

A proprietary model was developed to predict the likelihood of sepsis in hospitalized patients and was implemented at hundreds of hospitals around the country. An independent study showed that the model predictions underperformed relative to the designer’s claims while also causing ‘alert fatigue’ by falsely alerting likelihood of sepsis.

On social media, Black people who quote and criticize racist messages have had their own speech silenced when a platform’s automated moderation system failed to distinguish this “counter speech” (or other critique and journalism) from the original hateful messages to which such speech responded.

A device originally developed to help people track and find lost items has been used as a tool by stalkers to track victims’ locations in violation of their privacy and safety. The device manufacturer took steps after release to protect people from unwanted tracking by alerting people on their phones when a device is found to be moving with them over time and also by having the device make an occasional noise, but not all phones are able to receive the notification and the devices remain a safety concern due to their misuse.

An algorithm used to deploy police was found to repeatedly send police to neighborhoods they regularly visit, even if those neighborhoods were not the ones with the highest crime rates. These incorrect crime predictions were the result of a feedback loop generated from the reuse of data from previous arrests and algorithm predictions.

AI-enabled “nudification” technology that creates images where people appear to be nude—including apps that enable non-technical users to create or alter images of individuals without their consent—has proliferated at an alarming rate. Such technology is becoming a common form of image-based abuse that disproportionately impacts women. As these tools become more sophisticated, they are producing altered images that are increasingly realistic and are difficult for both humans and AI to detect as inauthentic. Regardless of authenticity, the experience of harm to victims of non-consensual intimate images can be devastatingly real—affecting their personal and professional lives, and impacting their mental and physical health.

A company installed AI-powered cameras in its delivery vans in order to evaluate the road safety habits of its drivers, but the system incorrectly penalized drivers when other cars cut them off or when other events beyond their control took place on the road. As a result, drivers were incorrectly ineligible to receive a bonus.

WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS

The expectations for automated systems are meant to serve as a blueprint for the development of additional technical standards and practices that are tailored for particular sectors and contexts.

In order to ensure that an automated system is safe and effective, it should include safeguards to protect the public from harm in a proactive and ongoing manner; avoid use of data inappropriate for or irrelevant to the task at hand, including reuse that could cause compounded harm; and demonstrate the safety and effectiveness of the system. These expectations are explained below.

Protect the public from harm in a proactive and ongoing manner

Consultation. The public should be consulted in the design, implementation, deployment, acquisition, and maintenance phases of automated system development, with emphasis on early-stage consultation before a system is introduced or a large change implemented. This consultation should directly engage diverse impacted communities to consider concerns and risks that may be unique to those communities, or disproportionately prevalent or severe for them. The extent of this engagement and the form of outreach to relevant stakeholders may differ depending on the specific automated system and development phase, but should include subject matter, sector-specific, and context-specific experts as well as experts on potential impacts such as civil rights, civil liberties, and privacy experts. For private sector applications, consultations before product launch may need to be confidential. Government applications, particularly law enforcement applications or applications that raise national security considerations, may require confidential or limited engagement based on system sensitivities and preexisting oversight laws and structures. Concerns raised in this consultation should be documented, and the automated system developers were proposing to create, use, or deploy should be reconsidered based on this feedback.

Testing. Systems should undergo extensive testing before deployment. This testing should follow domain-specific best practices, when available, for ensuring the technology will work in its real-world context. Such testing should take into account both the specific technology used and the roles of any human operators or reviewers who impact system outcomes or effectiveness; testing should include both automated systems testing and human-led (manual) testing. Testing conditions should mirror as closely as possible the conditions in which the system will be deployed, and new testing may be required for each deployment to account for material differences in conditions from one deployment to another. Following testing, system performance should be compared with the in-place, potentially human-driven, status quo procedures, with existing human performance considered as a performance baseline for the algorithm to meet pre-deployment, and as a lifecycle minimum performance standard. Decision possibilities resulting from performance testing should include the possibility of not deploying the system.

Risk identification and mitigation. Before deployment, and in a proactive and ongoing manner, potential risks of the automated system should be identified and mitigated. Identified risks should focus on the potential for meaningful impact on people’s rights, opportunities, or access and include those to impacted communities that may not be direct users of the automated system, risks resulting from purposeful misuse of the system, and other concerns identified via the consultation process. Assessment and, where possible, measurement of the impact of risks should be included and balanced such that high impact risks receive attention and mitigation proportionate with those impacts. Automated systems with the intended purpose of violating the safety of others should not be developed or used; systems with such safety violations as identified unintended consequences should not be used until the risk can be mitigated. Ongoing risk mitigation may necessitate rollback or significant modification to a launched automated system.

Ongoing monitoring. Automated systems should have ongoing monitoring procedures, including recalibration procedures, in place to ensure that their performance does not fall below an acceptable level over time, based on changing real-world conditions or deployment contexts, post-deployment modification, or unexpected conditions. This ongoing monitoring should include continuous evaluation of performance metrics and harm assessments, updates of any systems, and retraining of any machine learning models as necessary, as well as ensuring that fallback mechanisms are in place to allow reversion to a previously working system. Monitoring should take into account the performance of both technical system components (the algorithm as well as any hardware components, data inputs, etc.) and human operators. It should include mechanisms for testing the actual accuracy of any predictions or recommendations generated by a system, not just a human operator’s determination of their accuracy. Ongoing monitoring procedures should include manual, human-led monitoring as a check in the event there are shortcomings in automated monitoring systems. These monitoring procedures should be in place for the lifespan of the deployed automated system.

Clear organizational oversight. Entities responsible for the development or use of automated systems should lay out clear governance structures and procedures. This includes clearly-stated governance procedures before deploying the system, as well as responsibility of specific individuals or entities to oversee ongoing assessment and mitigation. Organizational stakeholders including those with oversight of the business process or operation being automated, as well as other organizational divisions that may be affected due to the use of the system, should be involved in establishing governance procedures. Responsibility should rest high enough in the organization that decisions about resources, mitigation, incident response, and potential rollback can be made promptly, with sufficient weight given to risk mitigation objectives against competing concerns. Those holding this responsibility should be made aware of any use cases with the potential for meaningful impact on people’s rights, opportunities, or access as determined based on risk identification procedures. In some cases, it may be appropriate for an independent ethics review to be conducted before deployment.

Avoid inappropriate, low-quality, or irrelevant data use and the compounded harm of its reuse

Relevant and high-quality data. Data used as part of any automated system’s creation, evaluation, or deployment should be relevant, of high quality, and tailored to the task at hand. Relevancy should be established based on research-backed demonstration of the causal influence of the data to the specific use case or justified more generally based on a reasonable expectation of usefulness in the domain and/or for the system design or ongoing development. Relevance of data should not be established solely by appealing to its historical connection to the outcome. High quality and tailored data should be representative of the task at hand and errors from data entry or other sources should be measured and limited. Any data used as the target of a prediction process should receive particular attention to the quality and validity of the predicted outcome or label to ensure the goal of the automated system is appropriately identified and measured. Additionally, justification should be documented for each data attribute and source to explain why it is appropriate to use that data to inform the results of the automated system and why such use will not violate any applicable laws. In cases of high-dimensional and/or derived attributes, such justifications can be provided as overall descriptions of the attribute generation process and appropriateness.

Derived data sources tracked and reviewed carefully. Data that is derived from other data through the use of algorithms, such as data derived or inferred from prior model outputs, should be identified and tracked, e.g., via a specialized type in a data schema. Derived data should be viewed as potentially high-risk inputs that may lead to feedback loops, compounded harm, or inaccurate results. Such sources should be carefully validated against the risk of collateral consequences.

Data reuse limits in sensitive domains. Data reuse, and especially data reuse in a new context, can result in the spreading and scaling of harms. Data from some domains, including criminal justice data and data indicating adverse outcomes in domains such as finance, employment, and housing, is especially sensitive, and in some cases its reuse is limited by law. Accordingly, such data should be subject to extra oversight to ensure safety and efficacy. Data reuse of sensitive domain data in other contexts (e.g., criminal data reuse for civil legal matters or private sector use) should only occur where use of such data is legally authorized and, after examination, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reasonable measures have been implemented to mitigate the identified risks. Such data should be clearly labeled to identify contexts for limited reuse based on sensitivity. Where possible, aggregated datasets may be useful for replacing individual-level sensitive data.

Demonstrate the safety and effectiveness of the system

Independent evaluation. Automated systems should be designed to allow for independent evaluation (e.g., via application programming interfaces). Independent evaluators, such as researchers, journalists, ethics review boards, inspectors general, and third-party auditors, should be given access to the system and samples of associated data, in a manner consistent with privacy, security, law, or regulation (including, e.g., intellectual property law), in order to perform such evaluations. Mechanisms should be included to ensure that system access for evaluation is: provided in a timely manner to the deployment-ready version of the system; trusted to provide genuine, unfiltered access to the full system; and truly independent such that evaluator access cannot be revoked without reasonable and verified justification.

Reporting.12 Entities responsible for the development or use of automated systems should provide regularly-updated reports that include: an overview of the system, including how it is embedded in the organization’s business processes or other activities, system goals, any human-run procedures that form a part of the system, and specific performance expectations; a description of any data used to train machine learning models or for other purposes, including how data sources were processed and interpreted, a summary of what data might be missing, incomplete, or erroneous, and data relevancy justifications; the results of public consultation such as concerns raised and any decisions made due to these concerns; risk identification and management assessments and any steps taken to mitigate potential harms; the results of performance testing including, but not limited to, accuracy, differential demographic impact, resulting error rates (overall and per demographic group), and comparisons to previously deployed systems; ongoing monitoring procedures and regular performance testing reports, including monitoring frequency, results, and actions taken; and the procedures for and results from independent evaluations. Reporting should be provided in a plain language and machine-readable manner.

HOW THESE PRINCIPLES CAN MOVE INTO PRACTICE 

Real-life examples of how these principles can become reality, through laws, policies, and practical technical and sociotechnical approaches to protecting rights, opportunities, and access.

Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government requires that certain federal agencies adhere to nine principles when designing, developing, acquiring, or using AI for purposes other than national security or defense. These principles—while taking into account the sensitive law enforcement and other contexts in which the federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful and respectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d) safe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transparent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order. Affected agencies across the federal government have released AI use case inventories13 and are implementing plans to bring those AI systems into compliance with the Executive Order or retire them.

The law and policy landscape for motor vehicles shows that strong safety regulations—and measures to address harms when they occur—can enhance innovation in the context of complex technologies. Cars, like automated digital systems, comprise a complex collection of components. The National Highway Traffic Safety Administration,14 through its rigorous standards and independent evaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability to innovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriate requirements on drivers, such as slowing down near schools or playgrounds.

From large companies to start-ups, industry is providing innovative solutions that allow organizations to mitigate risks to the safety and efficacy of AI systems, both before deployment and through monitoring over time.17 These innovative solutions include risk assessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoing monitoring, documentation procedures specific to model assessments, and many other strategies that aim to mitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safety and effectiveness concerns.

The Office of Management and Budget (OMB) has called for an expansion of opportunities for meaningful stakeholder engagement in the design of programs and services. OMB also points to numerous examples of effective and proactive stakeholder engagement, including the Community- Based Participatory Research Program developed by the National Institutes of Health and the participatory technology assessments developed by the National Oceanic and Atmospheric Administration.

The National Institute of Standards and Technology (NIST) is developing a risk management framework to better manage risks posed to individuals, organizations, and society by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended for voluntary use to help incorporate trustworthiness considerations into the design, development, use, and evaluation of AI products, services, and systems. The NIST framework is being developed through a consensus driven, open, transparent, and collaborative process that includes workshops and other opportunities to provide input. The NIST framework aims to foster the development of innovative approaches to address characteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, robustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of harmful uses. The NIST framework will consider and encompass principles such as transparency, accountability, and fairness during pre-design, design and development, deployment, use, and testing and evaluation of AI technologies and systems. It is expected to be released in the winter of 2022-23.

Some U.S government agencies have developed specific frameworks for ethical use of AI systems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordination and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on the ethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence Ethical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national security and defense activities.21 Similarly, the U.S. Intelligence Community (IC) has developed the Principles of Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to develop and use AI in furtherance of the IC's mission, as well as an AI Ethics Framework to help implement these principles.

The National Science Foundation (NSF) funds extensive research to help foster the development of automated systems that adhere to and advance their safety, security and effectiveness. Multiple NSF programs support research that directly addresses many of these principles: the National AI Research Institutes23 support research on all aspects of safe, trustworthy, fair, and explainable AI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safe autonomous and cyber physical systems with AI components; the Secure and Trustworthy Cyberspace25 program supports research on cybersecurity and privacy enhancing technologies in automated systems; the Formal Methods in the Field26 program supports research on rigorous formal verification and analysis of automated systems and machine learning, and the Designing Accountable Software Systems27 program supports research on rigorous and reproducible methodologies for developing software systems with legal and regulatory compliance in mind.

Some state legislatures have placed strong transparency and validity requirements on the use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been a cause of concern for civil rights groups.28 Idaho Code Section 19-1910, enacted in 2019,29 requires that any pretrial risk assessment, before use in the state, first be "shown to be free of bias against any class of individuals protected from discrimination by state or federal law", that any locality using a pretrial risk assessment must first formally validate the claim of its being free of bias, that "all documents, records, and information used to build or validate the risk assessment shall be open to public inspection," and that assertions of trade secrets cannot be used "to quash discovery in a criminal matter by a party to a criminal case.

There is extensive evidence showing that automated systems can produce inequitable outcomes and amplify existing inequity.30 Data that fails to account for existing systemic biases in American society can result in a range of consequences. For example, facial recognition technology that can contribute to wrongful and discriminatory arrests,31 hiring algorithms that inform discriminatory decisions, and healthcare algorithms that discount the severity of certain diseases in Black Americans. Instances of discriminatory practices built into and resulting from AI and other automated systems exist across many industries, areas, and contexts. While automated systems have the capacity to drive extraordinary advances and innovations, algorithmic discrimination protections should be built into their design, deployment, and ongoing use.

Many companies, non-profits, and federal government agencies are already taking steps to ensure the public is protected from algorithmic discrimination. Some companies have instituted bias testing as part of their product quality assessment and launch procedures, and in some cases this testing has led products to be changed or not launched, preventing harm to the public. Federal government agencies have been developing standards and guidance for the use of automated systems in order to help prevent bias. Non-profits and companies have developed best practices for audits and impact assessments to help identify potential algorithmic discrimination and provide transparency to the public in the mitigation of such biases.

But there is much more work to do to protect the public from algorithmic discrimination to use and design automated systems in an equitable way. The guardrails protecting the public from discrimination in their daily lives should include their digital lives and impacts—basic safeguards against abuse, bias, and discrimination to ensure that all people are treated fairly when automated systems are used. This includes all dimensions of their lives, from hiring to loan approvals, from medical treatment and payment to encounters with the criminal justice system. Ensuring equity should also go beyond existing guardrails to consider the holistic impact that automated systems make on underserved communities and to institute proactive protections that support these communities.

An automated system using nontraditional factors such as educational attainment and employment history as part of its loan underwriting and pricing model was found to be much more likely to charge an applicant who attended a Historically Black College or University (HBCU) higher loan prices for refinancing a student loan than an applicant who did not attend an HBCU. This was found to be true even when controlling for other credit-related factors.

A hiring tool that learned the features of a company's employees (predominantly men) rejected women applicants for spurious and discriminatory reasons; resumes with the word “women’s,” such as “women’s chess club captain,” were penalized in the candidate ranking.

A predictive model marketed as being able to predict whether students are likely to drop out of school was used by more than 500 universities across the country. The model was found to use race directly as a predictor, and also shown to have large disparities by race; Black students were as many as four times as likely as their otherwise similar white peers to be deemed at high risk of dropping out. These risk scores are used by advisors to guide students towards or away from majors, and some worry that they are being used to guide Black students away from math and science subjects.

A risk assessment tool designed to predict the risk of recidivism for individuals in federal custody showed evidence of disparity in prediction. The tool overpredicts the risk of recidivism for some groups of color on the general recidivism tools, and underpredicts the risk of recidivism for some groups of color on some of the violent recidivism tools. The Department of Justice is working to reduce these disparities and has publicly released a report detailing its review of the tool.

An automated sentiment analyzer, a tool often used by technology platforms to determine whether a statement posted online expresses a positive or negative sentiment, was found to be biased against Jews and gay people. For example, the analyzer marked the statement “I’m a Jew” as representing a negative sentiment, while “I’m a Christian” was identified as expressing a positive sentiment.36 This could lead to the preemptive blocking of social media comments such as: “I’m gay.” A related company with this bias concern has made their data public to encourage researchers to help address the issue37 and has released reports identifying and measuring this problem as well as detailing attempts to address it.

Searches for “Black girls,” “Asian girls,” or “Latina girls” return predominantly39 sexualized content, rather than role models, toys, or activities.40 Some search engines have been working to reduce the prevalence of these results, but the problem remains.

Advertisement delivery systems that predict who is most likely to click on a job advertisement end up delivering ads in ways that reinforce racial and gender stereotypes, such as overwhelmingly directing supermarket cashier ads to women and jobs with taxi companies to primarily Black people.

Body scanners, used by TSA at airport checkpoints, require the operator to select a “male” or “female” scanning setting based on the passenger’s sex, but the setting is chosen based on the operator’s perception of the passenger’s gender identity. These scanners are more likely to flag transgender travelers as requiring extra screening done by a person. Transgender travelers have described degrading experiences associated with these extra screenings. TSA has recently announced plans to implement a gender-neutral algorithm44 while simultaneously enhancing the security effectiveness capabilities of the existing technology.

The National Disabled Law Students Association expressed concerns that individuals with disabilities were more likely to be flagged as potentially suspicious by remote proctoring AI systems because of their disability- specific access needs such as needing longer breaks or using screen readers or dictation software.

An algorithm designed to identify patients with high needs for healthcare systematically assigned lower scores (indicating that they were not as high need) to Black patients than to those of white patients, even when those patients had similar numbers of chronic conditions and other markers of health.46 In addition, healthcare clinical algorithms that are used by physicians to guide clinical decisions may include sociodemographic variables that adjust or “correct” the algorithm’s output on the basis of a patient’s race or ethnicity, which can lead to race-based health inequities.

WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS 

The expectations for automated systems are meant to serve as a blueprint for the development of additional technical standards and practices that are tailored for particular sectors and contexts.

Any automated system should be tested to help ensure it is free from algorithmic discrimination before it can be sold or used. Protection against algorithmic discrimination should include designing to ensure equity, broadly construed. Some algorithmic discrimination is already prohibited under existing anti-discrimination law. The expectations set out below describe proactive technical and policy steps that can be taken to not only reinforce those legal protections but extend beyond them to ensure equity for underserved communities48 even in circumstances where a specific legal protection may not be clearly established. These protections should be instituted throughout the design, development, and deployment process and are described below roughly in the order in which they would be instituted.

Protect the public from algorithmic discrimination in a proactive and ongoing manner

Proactive assessment of equity in design. Those responsible for the development, use, or oversight of automated systems should conduct proactive equity assessments in the design phase of the technology research and development or during its acquisition to review potential input data, associated historical context, accessibility for people with disabilities, and societal goals to identify potential discrimination and effects on equity resulting from the introduction of the technology. The assessed groups should be as inclusive as possible of the underserved communities mentioned in the equity definition: Black, Latino, and Indigenous and Native American persons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+) persons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely affected by persistent poverty or inequality. Assessment could include both qualitative and quantitative evaluations of the system. This equity assessment should also be considered a core part of the goals of the consultation conducted as part of the safety and efficacy review.

Representative and robust data. Any data used as part of system development or assessment should be representative of local communities based on the planned deployment setting and should be reviewed for bias based on the historical and societal context of the data. Such data should be sufficiently robust to identify and help to mitigate biases and potential harms.

Guarding against proxies. Directly using demographic information in the design, development, or deployment of an automated system (for purposes other than evaluating a system for discrimination or using a system to counter discrimination) runs a high risk of leading to algorithmic discrimination and should be avoided. In many cases, attributes that are highly correlated with demographic features, known as proxies, can contribute to algorithmic discrimination. In cases where use of the demographic features themselves would lead to illegal algorithmic discrimination, reliance on such proxies in decision-making (such as that facilitated by an algorithm) may also be prohibited by law. Proactive testing should be performed to identify proxies by testing for correlation between demographic information and attributes in any data used as part of system design, development, or use. If a proxy is identified, designers, developers, and deployers should remove the proxy; if needed, it may be possible to identify alternative attributes that can be used instead. At a minimum, organizations should ensure a proxy feature is not given undue weight and should monitor the system closely for any resulting algorithmic discrimination.

Ensuring accessibility during design, development, and deployment. Systems should be designed, developed, and deployed by organizations in ways that ensure accessibility to people with disabilities. This should include consideration of a wide variety of disabilities, adherence to relevant accessibility standards, and user experience research both before and after deployment to identify and address any accessibility barriers to the use or effectiveness of the automated system.

Disparity assessment. Automated systems should be tested using a broad set of measures to assess whether the system components, both in pre-deployment testing and in-context deployment, produce disparities. The demographics of the assessed groups should be as inclusive as possible of race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law. The broad set of measures assessed should include demographic performance measures, overall and subgroup parity assessment, and calibration. Demographic data collected for disparity assessment should be separated from data used for the automated system and privacy protections should be instituted; in some cases it may make sense to perform such assessment using a data sample. For every instance where the deployed automated system leads to different treatment or impacts disfavoring the identified groups, the entity governing, implementing, or using the system should document the disparity and a justification for any continued use of the system.

Disparity mitigation. When a disparity assessment identifies a disparity against an assessed group, it may be appropriate to take steps to mitigate or eliminate the disparity. In some cases, mitigation or elimination of the disparity may be required by law. Disparities that have the potential to lead to algorithmic discrimination, cause meaningful harm, or violate equity49 goals should be mitigated. When designing and evaluating an automated system, steps should be taken to evaluate multiple models and select the one that has the least adverse impact, modify data input choices, or otherwise identify a system with fewer disparities. If adequate mitigation of the disparity is not possible, then the use of the automated system should be reconsidered. One of the considerations in whether to use the system should be the validity of any target measure; unobservable targets may result in the inappropriate use of proxies. Meeting these standards may require instituting mitigation procedures and other protective measures to address algorithmic discrimination, avoid meaningful harm, and achieve equity goals.

Ongoing monitoring and mitigation. Automated systems should be regularly monitored to assess algorithmic discrimination that might arise from unforeseen interactions of the system with inequities not accounted for during the pre-deployment testing, changes to the system after deployment, or changes to the context of use or associated data. Monitoring and disparity assessment should be performed by the entity deploying or using the automated system to examine whether the system has led to algorithmic discrimination when deployed. This assessment should be performed regularly and whenever a pattern of unusual results is occurring. It can be performed using a variety of approaches, taking into account whether and how demographic information of impacted people is available, for example via testing with a sample of users or via qualitative user experience research. Riskier and higher-impact systems should be monitored and assessed more frequently. Outcomes of this assessment should include additional disparity mitigation, if needed, or fallback to earlier procedures in the case that equity standards are no longer met and can't be mitigated, and prior mechanisms provide better adherence to equity standards.

Demonstrate that the system protects against algorithmic discrimination

Independent evaluation. As described in the section on Safe and Effective Systems, entities should allow independent evaluation of potential algorithmic discrimination caused by automated systems they use or oversee. In the case of public sector uses, these independent evaluations should be made public unless law enforcement or national security restrictions prevent doing so. Care should be taken to balance individual privacy with evaluation data access needs; in many cases, policy-based and/or technological innovations and controls allow access to such data without compromising privacy.

Reporting. Entities responsible for the development or use of automated systems should provide reporting of an appropriately designed algorithmic impact assessment,50 with clear specification of who performs the assessment, who evaluates the system, and how corrective actions are taken (if necessary) in response to the assessment. This algorithmic impact assessment should include at least: the results of any consultation, design stage equity assessments (potentially including qualitative analysis), accessibility designs and testing, disparity testing, document any remaining disparities, and detail any mitigation implementation and assessments. This algorithmic impact assessment should be made public whenever possible. Reporting should be provided in a clear and machine-readable manner using plain language to allow for more straightforward public accountability.

HOW THESE PRINCIPLES CAN MOVE INTO PRACTICE

Real-life examples of how these principles can become reality, through laws, policies, and practical technical and sociotechnical approaches to protecting rights, opportunities, and access.

The federal government is working to combat discrimination in mortgage lending. The Department of Justice has launched a nationwide initiative to combat redlining, which includes reviewing how lenders who may be avoiding serving communities of color are conducting targeted marketing and advertising.51 This initiative will draw upon strong partnerships across federal agencies, including the Consumer Financial Protection Bureau and prudential regulators. The Action Plan to Advance Property Appraisal and Valuation Equity includes a commitment from the agencies that oversee mortgage lending to include a nondiscrimination standard in the proposed rules for Automated Valuation Models.

The Equal Employment Opportunity Commission and the Department of Justice have clearly laid out how employers’ use of AI and other automated systems can result in discrimination against job applicants and employees with disabilities.53 The documents explain how employers’ use of software that relies on algorithmic decision-making may violate existing requirements under Title I of the Americans with Disabilities Act (“ADA”). This technical assistance also provides practical tips to employers on how to comply with the ADA, and to job applicants and employees who think that their rights may have been violated.

Disparity assessments identified harms to Black patients' healthcare access. A widely used healthcare algorithm relied on the cost of each patient’s past medical care to predict future medical needs, recommending early interventions for the patients deemed most at risk. This process discriminated against Black patients, who generally have less access to medical care and therefore have generated less cost than white patients with similar illness and need. A landmark study documented this pattern and proposed practical ways that were shown to reduce this bias, such as focusing specifically on active chronic health conditions or avoidable future costs related to emergency visits and hospitalization.

Large employers have developed best practices to scrutinize the data and models used for hiring. An industry initiative has developed Algorithmic Bias Safeguards for the Workforce, a structured questionnaire that businesses can use proactively when procuring software to evaluate workers. It covers specific technical questions such as the training data used, model training process, biases identified, and mitigation steps employed.

Standards organizations have developed guidelines to incorporate accessibility criteria into technology design processes. The most prevalent in the United States is the Access Board’s Section 508 regulations,56 which are the technical standards for federal information communication technology (software, hardware, and web). Other standards include those issued by the International Organization for Standardization,57 and the World Wide Web Consortium Web Content Accessibility Guidelines,58 a globally recognized voluntary consensus standard for web content and other information and communications technology.

NIST has released Special Publication 1270, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.59 The special publication: describes the stakes and challenges of bias in artificial intelligence and provides examples of how and why it can chip away at public trust; identifies three categories of bias in AI – systemic, statistical, and human – and describes how and where they contribute to harms; and describes three broad challenges for mitigating bias – datasets, testing and evaluation, and human factors – and introduces preliminary guidance for addressing them. Throughout, the special publication takes a sociotechnical perspective to identifying and managing AI bias.

Data privacy is a foundational and cross-cutting principle required for achieving all others in this framework. Surveillance and data collection, sharing, use, and reuse now sit at the foundation of business models across many industries, with more and more companies tracking the behavior of the American public, building individual profiles based on this data, and using this granular-level information as input into automated systems that further track, profile, and impact the American public. Government agencies, particularly law enforcement agencies, also use and help develop a variety of technologies that enhance and expand surveillance capabilities, which similarly collect data used as input into other automated systems that directly impact people’s lives. Federal law has not grown to address the expanding scale of private data collection, or of the ability of governments at all levels to access that data and leverage the means of private collection.

Meanwhile, members of the American public are often unable to access their personal data or make critical decisions about its collection and use. Data brokers frequently collect consumer data from numerous sources without consumers’ permission or knowledge.60 Moreover, there is a risk that inaccurate and faulty data can be used to make decisions about their lives, such as whether they will qualify for a loan or get a job. Use of surveillance technologies has increased in schools and workplaces, and, when coupled with consequential management and evaluation decisions, it is leading to mental health harms such as lowered self-confidence, anxiety, depression, and a reduced ability to use analytical reasoning.61 Documented patterns show that personal data is being aggregated by data brokers to profile communities in harmful ways.62 The impact of all this data harvesting is corrosive, breeding distrust, anxiety, and other mental health problems; chilling speech, protest, and worker organizing; and threatening our democratic process.63 The American public should be protected from these growing risks.

Increasingly, some companies are taking these concerns seriously and integrating mechanisms to protect consumer privacy into their products by design and by default, including by minimizing the data they collect, communicating collection and use clearly, and improving security practices. Federal government surveillance and other collection and use of data is governed by legal protections that help to protect civil liberties and provide for limits on data retention in some cases. Many states have also enacted consumer data privacy protection regimes to address some of these harms.

However, these are not yet standard practices, and the United States lacks a comprehensive statutory or regulatory framework governing the rights of the public when it comes to personal data. While a patchwork of laws exists to guide the collection and use of personal data in specific contexts, including health, employment, education, and credit, it can be unclear how these laws apply in other contexts and in an increasingly automated society. Additional protections would assure the American public that the automated systems they use are not monitoring their activities, collecting information on their lives, or otherwise surveilling them without context-specific consent or legal authority.

Traditional terms of service—the block of text that the public is accustomed to clicking through when using a website or digital app—are not an adequate mechanism for protecting privacy. The American public should be protected via built-in privacy protections, data minimization, use and collection limitations, and transparency, in addition to being entitled to clear mechanisms to control access to and use of their data—including their metadata—in a proactive, informed, and ongoing way. Any automated system collecting, using, sharing, or storing personal data should meet these expectations.

Protect privacy by design and by default

Privacy by design and by default. Automated systems should be designed and built with privacy protected by default. Privacy risks should be assessed throughout the development life cycle, including privacy risks from reidentification, and appropriate technical and policy mitigation measures should be implemented. This includes potential harms to those who are not users of the automated system, but who may be harmed by inferred data, purposeful privacy violations, or community surveillance or other community harms. Data collection should be minimized and clearly communicated to the people whose data is collected. Data should only be collected or used for the purposes of training or testing machine learning models if such collection and use is legal and consistent with the expectations of the people whose data is collected. User experience research should be conducted to confirm that people understand what data is being collected about them and how it will be used, and that this collection matches their expectations and desires.

Data collection and use-case scope limits. Data collection should be limited in scope, with specific, narrow identified goals, to avoid "mission creep." Anticipated data collection should be determined to be strictly necessary to the identified goals and should be minimized as much as possible. Data collected based on these identified goals and for a specific context should not be used in a different context without assessing for new privacy risks and implementing appropriate mitigation measures, which may include express consent. Clear timelines for data retention should be established, with data deleted as soon as possible in accordance with legal or policy-based limitations. Determined data retention timelines should be documented and justified.

Risk identification and mitigation. Entities that collect, use, share, or store sensitive data should attempt to proactively identify harms and seek to manage them so as to avoid, mitigate, and respond appropriately to identified risks. Appropriate responses include determining not to process data when the privacy risks outweigh the benefits or implementing measures to mitigate acceptable risks. Appropriate responses do not include sharing or transferring the privacy risks to users via notice or consent requests where users could not reasonably be expected to understand the risks without further support.

Privacy-preserving security. Entities creating, using, or governing automated systems should follow privacy and security best practices designed to ensure data and metadata do not leak beyond the specific consented use case. Best practices could include using privacy-enhancing cryptography or other types of privacy-enhancing technologies or fine-grained permissions and access control mechanisms, along with conventional system security protocols.

Protect the public from unchecked surveillance

Heightened oversight of surveillance. Surveillance or monitoring systems should be subject to heightened oversight that includes at a minimum assessment of potential harms during design (before deployment) and in an ongoing manner, to ensure that the American public’s rights, opportunities, and access are protected. This assessment should be done before deployment and should give special attention to ensure there is not algorithmic discrimination, especially based on community membership, when deployed in a specific real-world context. Such assessment should then be reaffirmed in an ongoing manner as long as the system is in use.

Limited and proportionate surveillance. Surveillance should be avoided unless it is strictly necessary to achieve a legitimate purpose and it is proportionate to the need. Designers, developers, and deployers of surveillance systems should use the least invasive means of monitoring available and restrict monitoring to the minimum number of subjects possible. To the greatest extent possible consistent with law enforcement and national security needs, individuals subject to monitoring should be provided with clear and specific notice before it occurs and be informed about how the data gathered through surveillance will be used.

Scope limits on surveillance to protect rights and democratic values. Civil liberties and civil rights must not be limited by the threat of surveillance or harassment facilitated or aided by an automated system. Surveillance systems should not be used to monitor the exercise of democratic rights, such as voting, privacy, peaceful assembly, speech, or association, in a way that limits the exercise of civil rights or civil liberties. Information about or algorithmically-determined assumptions related to identity should be carefully limited if used to target or guide surveillance systems in order to avoid algorithmic discrimination; such identity- related information includes group characteristics or affiliations, geographic designations, location-based and association-based inferences, social networks, and biometrics. Continuous surveillance and monitoring systems should not be used in physical or digital workplaces (regardless of employment status), public educational institutions, and public accommodations. Continuous surveillance and monitoring systems should not be used in a way that has the effect of limiting access to critical resources or services or suppressing the exercise of rights, even where the organization is not under a particular duty to protect those rights.

Provide the public with mechanisms for appropriate and meaningful consent, access, and control over their data

Use-specific consent. Consent practices should not allow for abusive surveillance practices. Where data collectors or automated systems seek consent, they should seek it for specific, narrow use contexts, for specific time durations, and for use by specific entities. Consent should not extend if any of these conditions change; consent should be re-acquired before using data if the use case changes, a time limit elapses, or data is transferred to another entity (including being shared or sold). Consent requested should be limited in scope and should not request consent beyond what is required. Refusal to provide consent should be allowed, without adverse effects, to the greatest extent possible based on the needs of the use case.

Brief and direct consent requests. When seeking consent from users short, plain language consent requests should be used so that users understand for what use contexts, time span, and entities they are providing data and metadata consent. User experience research should be performed to ensure these consent requests meet performance standards for readability and comprehension. This includes ensuring that consent requests are accessible to users with disabilities and are available in the language(s) and reading level appropriate for the audience. User experience design choices that intentionally obfuscate or manipulate user choice (i.e., “dark patterns”) should be not be used.

Data access and correction. People whose data is collected, used, shared, or stored by automated systems should be able to access data and metadata about themselves, know who has access to this data, and be able to correct it if necessary. Entities should receive consent before sharing data with other entities and should keep records of what data is shared and with whom.

Consent withdrawal and data deletion. Entities should allow (to the extent legally permissible) withdrawal of data access consent, resulting in the deletion of user data, metadata, and the timely removal of their data from any systems (e.g., machine learning models) derived from that data.

Automated system support. Entities designing, developing, and deploying automated systems should establish and maintain the capabilities that will allow individuals to use their own automated systems to help them make consent, access, and control decisions in a complex data ecosystem. Capabilities include machine readable data, standardized data formats, metadata or tags for expressing data processing permissions and preferences and data provenance and lineage, context of use and access-specific tags, and training models for assessing privacy risk.

Demonstrate that data privacy and user control are protected

Independent evaluation. As described in the section on Safe and Effective Systems, entities should allow independent evaluation of the claims made regarding data policies. These independent evaluations should be made public whenever possible. Care will need to be taken to balance individual privacy with evaluation data access needs.

Reporting. When members of the public wish to know what data about them is being used in a system, the entity responsible for the development of the system should respond quickly with a report on the data it has collected or stored about them. Such a report should be machine-readable, understandable by most users, and include, to the greatest extent allowable under law, any data and metadata about them or collected from them, when and how their data and metadata were collected, the specific ways that data or metadata are being used, who has access to their data and metadata, and what time limitations apply to these data. In cases where a user login is not available, identity verification may need to be performed before providing such a report to ensure user privacy. Additionally, summary reporting should be proactively made public with general information about how peoples’ data and metadata is used, accessed, and stored. Summary reporting should include the results of any surveillance pre-deployment assessment, including disparity assessment in the real-world deployment context, the specific identified goals of any data collection, and the assessment done to ensure only the minimum required data is collected. It should also include documentation about the scope limit assessments, including data retention timelines and associated justification, and an assessment of the impact of surveillance or data collection on rights, opportunities, and access. Where possible, this assessment of the impact of surveillance should be done by an independent party. Reporting should be provided in a clear and machine-readable manner.

Extra Protections for Data Related to Sensitive Domains

Some domains, including health, employment, education, criminal justice, and personal finance, have long been singled out as sensitive domains deserving of enhanced data protections. This is due to the intimate nature of these domains as well as the inability of individuals to opt out of these domains in any meaningful way, and the historical discrimination that has often accompanied data knowledge.69 Domains understood by the public to be sensitive also change over time, including because of technological developments. Tracking and monitoring technologies, personal tracking devices, and our extensive data footprints are used and misused more than ever before; as such, the protections afforded by current legal guidelines may be inadequate. The American public deserves assurances that data related to such sensitive domains is protected and used appropriately and only in narrowly defined contexts with clear benefits to the individual and/or society.

To this end, automated systems that collect, use, share, or store data related to these sensitive domains should meet additional expectations. Data and metadata are sensitive if they pertain to an individual in a sensitive domain (defined below); are generated by technologies used in a sensitive domain; can be used to infer data from a sensitive domain or sensitive  data  about  an  individual  (such  as  disability-related  data,  genomic  data,  biometric  data,  behavioral  data, geolocation data, data related to interaction with the criminal justice system, relationship history and legal status such as custody and divorce information, and home, work, or school environmental data); or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harm due to identity theft. Data and metadata generated by or about those who are not yet legal adults is also sensitive, even if not related to a sensitive domain. Such data includes, but is not limited to, numerical, text, image, audio, or video data. “Sensitive domains” are those in which activities being conducted can cause material harms, including signifi- cant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights. Domains that have historically been singled out as deserving of enhanced data protections or where such enhanced protections are reasonably expected by the public include, but are not limited to, health, family planning and care, employment, education, criminal justice, and personal finance. In the context of this framework, such domains are considered sensitive whether or not the specifics of a system context would necessitate coverage under existing law, and domains and data that are considered sensitive are understood to change over time based on societal norms and context."

EXTRA PROTECTIONS FOR DATA RELATED TO SENSITIVE DOMAINS

Continuous positive airway pressure machines gather data for medical purposes, such as diagnosing sleep apnea, and send usage data to a patient’s insurance company, which may subsequently deny coverage for the device based on usage data. Patients were not aware that the data would be used in this way or monitored by anyone other than their doctor.

A department store company used predictive analytics applied to collected consumer data to determine that a teenage girl was pregnant, and sent maternity clothing ads and other baby-related advertisements to her house, revealing to her father that she was pregnant.

School audio surveillance systems monitor student conversations to detect potential "stress indicators" as a warning of potential violence.72 Online proctoring systems claim to detect if a student is cheating on an exam using biometric markers.73 These systems have the potential to limit student freedom to express a range of emotions at school and may inappropriately flag students with disabilities who need accommodations or use screen readers or dictation software as cheating.

Location data, acquired from a data broker, can be used to identify people who visit abortion clinics.

Companies collect student data such as demographic information, free or reduced lunch status, whether they've used drugs, or whether they've expressed interest in LGBTQI+ groups, and then use that data to forecast student success.76 Parents and education experts have expressed concern about collection of such sensitive data without express parental consent, the lack of transparency in how such data is being used, and the potential for resulting discriminatory impacts.

Many employers transfer employee data to third party job verification services. This information is then used by potential future employers, banks, or landlords. In one case, a former employee alleged that a company supplied false data about her job title which resulted in a job offer being revoked.

In addition to the privacy expectations above for general non-sensitive data, any system collecting, using, shar- ing, or storing sensitive data should meet the expectations below. Depending on the technological use case and based on an ethical assessment, consent for sensitive data may need to be acquired from a guardian and/or child.

Provide enhanced protections for data related to sensitive domains Necessary functions only. Sensitive  data  should  only  be  used  for  functions  strictly  necessary  for  that domain or for functions that are required for administrative reasons (e.g., school attendance records), unless consent is acquired, if appropriate, and the additional expectations in this section are met. Consent for non- necessary functions should be optional, i.e., should not be required, incentivized, or coerced in order to receive opportunities or access to services. In cases where data is provided to an entity (e.g., health insurance company) in order to facilitate payment for such a need, that data should only be used for that purpose.

Ethical review and use prohibitions. Any use of sensitive data or decision process based in part on sensi- tive data that might limit rights, opportunities, or access, whether the decision is automated or not, should go through a thorough ethical review and monitoring, both in advance and by periodic review (e.g., via an independent ethics committee or similarly robust process). In some cases, this ethical review may determine that data should not be used or shared for specific uses even with consent. Some novel uses of automated systems in this context, where the algorithm is dynamically developing and where the science behind the use case is not well established, may also count as human subject experimentation, and require special review under organizational compliance bodies applying medical, scientific, and academic human subject experimentation ethics rules and governance procedures.

Data quality. In sensitive domains, entities should be especially careful to maintain the quality of data to avoid adverse consequences arising from decision-making based on flawed or inaccurate data. Such care is necessary in a fragmented, complex data ecosystem and for datasets that have limited access such as for fraud prevention and law enforcement. It should be not left solely to individuals to carry the burden of reviewing and correcting data. Entities should conduct regular, independent audits and take prompt corrective measures to maintain accurate, timely, and complete data.

Limit access to sensitive data and derived data. Sensitive data and derived data should not be sold, shared, or made public as part of data brokerage or other agreements. Sensitive data includes data that can be used to infer sensitive information; even systems that are not directly marketed as sensitive domain technologies are expected to keep sensitive data private. Access to such data should be limited based on necessity and based on a principle of local control, such that those individuals closest to the data subject have more access while those who are less proximate do not (e.g., a teacher has access to their students’ daily progress data while a superintendent does not).

Reporting. In addition to the reporting on data privacy (as listed above for non-sensitive data), entities devel- oping technologies related to a sensitive domain and those collecting, using, storing, or sharing sensitive data should, whenever appropriate, regularly provide public reports describing: any data security lapses or breaches that  resulted  in  sensitive  data  leaks;  the  number,  type,  and  outcomes  of  ethical  pre-reviews  undertaken;  a description of any data sold, shared, or made public, and how that data was assessed to determine it did not pres- ent a sensitive data risk; and ongoing risk identification and management procedures, and any mitigation added
based on these procedures. Reporting should be provided in a clear and machine-readable manner.

The  Privacy  Act  of  1974 requires  privacy  protections  for  personal  information  in  federal records systems, including limits on data retention, and also provides individuals a general right to access and correct their data. Among other things, the Privacy Act limits the storage of individual information in federal systems of records, illustrating the principle of limiting the scope of data retention. Under the Privacy Act, federal agencies may only retain data about an individual that is “relevant and necessary” to accomplish an agency’s statutory purpose or to comply with an Executive Order of the President. The law allows for individuals to be able to access any of their individual information stored in a federal system of records, if not included under one of the systems of records exempted pursuant to the Privacy Act. In these cases, federal agen- cies must provide a method for an individual to determine if their personal information is stored in a particular system of records, and must provide procedures for an individual to contest the contents of a record about them. Further, the Privacy Act allows for a cause of action for an individual to seek legal relief if a federal agency does not comply with the Privacy Act’s requirements. Among other things, a court may order a federal agency to amend or correct an individual’s information in its records or award monetary damages if an inaccurate, irrelevant, untimely, or incomplete record results in an adverse determination about an individual’s “qualifications, character, rights, … opportunities…, or benefits.”

NIST’s Privacy Framework provides a comprehensive, detailed and actionable approach for organizations to manage privacy risks. The NIST Framework gives organizations ways to identify and communicate their privacy risks and goals to support ethical decision-making in system, product, and service design or deployment, as well as the measures they are taking to demonstrate compliance with applicable laws or regulations. It has been voluntarily adopted by organizations across many different sectors around the world.78

A school board’s attempt to surveil public school students—undertaken without adequate community input—sparked a state-wide biometrics moratorium. Reacting to a plan in the city of Lockport, New York, the state’s legislature banned the use of facial recognition systems and other “biometric identifying technology” in schools until July 1, 2022.80  The law additionally requires that a report on the  privacy, civil rights, and civil liberties implications of the use of such technologies be issued before biometric identification technologies can be used in New York schools.

Federal law requires employers, and any consultants they may retain, to report the costs of surveilling employees in the context of a labor dispute, providing  a  transparency mechanism to help protect worker organizing. Employers engaging in workplace surveillance "where an object there-of, directly or indirectly, is […] to obtain information concerning the activities of employees or a labor organization in connection with a labor dispute"" must report expenditures relating to this surveillance to the  Department of Labor Office of Labor-Management Standards, and consultants who employers retain for these purposes must also file reports regarding their activities.81

Privacy choices on smartphones show that when technologies are well designed, privacy and data agency can be meaningful and not overwhelming. These choices—such as contextual, timely alerts about location tracking—are brief, direct, and use-specific. Many of the expectations listed here for privacy by design and use-specific  consent  mirror those distributed to developers as best practices when developing for smart phone devices, such as being transparent about how user data will be used, asking for app permissions during their use so that the use-context will be clear to users, and ensuring that the app will still work if users deny (or later revoke) some permissions.

Automated systems now determine opportunities, from employment to credit, and directly shape the American public’s experiences, from the courtroom to online classrooms, in ways that profoundly impact people’s lives. But this expansive impact is not always visible. An applicant might not know whether a person rejected their resume or a hiring algorithm moved them to the bottom of the list. A defendant in the courtroom might not know if a judge deny- ing their bail is informed by an automated system that labeled them “high risk.” From correcting errors to contesting decisions, people are often denied the knowledge they need to address the impact of automated systems on their lives. Notice and explanations also serve an important safety and efficacy purpose, allowing experts to verify the reasonable- ness of a recommendation before enacting it.

In order to guard against potential harms, the American public needs to know if an automated system is being used. Clear, brief, and understandable notice is a prerequisite for achieving the other protections in this framework. Likewise, the public is often unable to ascertain how or why an automated system has made a decision or contributed to a particular outcome. The decision-making processes of automated systems tend to be opaque, complex, and, therefore, unaccountable, whether by design or by omission. These factors can make explanations both more challenging and more important, and should not be used as a pretext to avoid explaining important decisions to the people impacted by those choices. In the context of automated systems, clear and valid explanations should be recognized as a baseline requirement.

Providing notice has long been a standard practice, and in many cases is a legal requirement, when, for example, making a video recording of someone (outside of a law enforcement or national security context). In some cases, such as credit, lenders are required to provide notice and explanation to consumers. Techniques used to automate the process of explaining such systems are under active research and improvement and such explanations can take many forms. Innovative companies and researchers are rising to the challenge and creating and deploying explanatory systems that can help the public better understand decisions that impact them.

While notice and explanation requirements are already in place in some sectors or situations, the American public deserve to know consistently and across sectors if an automated system is being used in a way that impacts their rights, opportunities, or access. This knowledge should provide confidence in how the public is being treated, and trust in the validity and reasonable use of automated systems.

A lawyer representing an older client with disabilities who had been cut off from Medicaid-funded home health-care assistance couldn't determine why, especially since the decision went against historical access practices. In a court hearing, the lawyer learned from a witness that the state in which the older client lived had recently adopted a new algorithm to determine eligibility.83 The lack of a timely explanation made it harder to understand and contest the decision.

A formal child welfare investigation is opened against a parent based on an algorithm and without the parent ever being notified that data was being collected and used as part of an algorithmic child maltreatment risk assessment.84 The lack of notice or an explanation makes it harder for those performing child maltreatment assessments to validate the risk assessment and denies parents knowledge that could help them contest a decision.

A predictive policing system claimed to identify individuals at greatest risk to commit or become the victim of gun violence (based on automated analysis of social ties to gang members, criminal histories, previous experiences of gun violence, and other factors) and led to individuals being placed on a watch list with no explanation or public transparency regarding how the system came to its conclusions.85 Both police and the public deserve to understand why and how such a system is making these determinations.

A system awarding benefits changed its criteria invisibly. Individuals were denied benefits due to data entry errors and other system flaws. These flaws were only revealed when an explanation of the system was demanded and produced.86 The lack of an explanation made it harder for errors to be corrected in a timely manner.

An automated system should provide demonstrably clear, timely, understandable, and accessible notice of use, and explanations as to how and why a decision was made or an action was taken by the system. These expectations are explained below.

Provide clear, timely, understandable, and accessible notice of use and explanations

Generally accessible plain language documentation. The entity responsible for using the automated system should ensure that documentation describing the overall system (including any human components) is public and easy to find. The documentation should describe, in plain language, how the system works and how any automated component is used to determine an action or decision. It should also include expectations about reporting  described  throughout  this  framework,  such  as  the  algorithmic  impact  assessments  described  as part of Algorithmic Discrimination Protections.

Accountable. Notices should clearly identify the entity responsible for designing each component of the system and the entity using it.

Timely and up-to-date. Users should receive notice of the use of automated systems in advance of using or while being impacted by the technology. An explanation should be available with the decision itself, or soon thereafter. Notice should be kept up-to-date and people impacted by the system should be notified of use case or key functionality changes.

Brief and clear. Notices and explanations should be assessed, such as by research on users’ experiences, including user testing, to ensure that the people using or impacted by the automated system are able to easily find notices and explanations, read them quickly, and understand and act on them. This includes ensuring that notices and explanations are accessible to users with disabilities and are available in the language(s) and read- ing level appropriate for the audience. Notices and explanations may need to be available in multiple forms, (e.g., on paper, on a physical sign, or online), in order to meet these expectations and to be accessible to the American public.

Provide explanations as to how and why a decision was made or an action was taken by an automated system

Tailored to the purpose. Explanations should be tailored to the specific purpose for which the user is expected to use the explanation, and should clearly state that purpose. An informational explanation might differ from an explanation provided to allow for the possibility of recourse, an appeal, or one provided in the context  of  a  dispute  or  contestation  process.  For  the  purposes  of  this  framework,  'explanation'  should  be construed broadly. An explanation need not be a plain-language statement about causality but could consist of any mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the stated purpose. Tailoring should be assessed (e.g., via user experience research).

Tailored to the target of the explanation. Explanations should be targeted to specific audiences and clearly state that audience. An explanation provided to the subject of a decision might differ from one provided to an advocate, or to a domain expert or decision maker. Tailoring should be assessed (e.g., via user experience research)."

Tailored to the level of risk. An assessment should be done to determine the level of risk of the auto- mated system. In settings where the consequences are high as determined by a risk assessment, or extensive oversight is expected (e.g., in criminal justice or some public sector settings), explanatory mechanisms should be built into the system design so that the system’s full behavior can be explained in advance (i.e., only fully transparent models should be used), rather than as an after-the-decision interpretation. In other settings, the extent of explanation provided should be tailored to the risk level.

Valid. The explanation provided by a system should accurately reflect the factors and the influences that led to a particular decision, and should be meaningful for the particular customization based on purpose, target, and level of risk. While approximation and simplification may be necessary for the system to succeed based on the explanatory purpose and target of the explanation, or to account for the risk of fraud or other concerns related  to  revealing  decision-making  information,  such  simplifications  should  be  done  in  a  scientifically supportable way. Where appropriate based on the explanatory system, error ranges for the explanation should be calculated and included in the explanation, with the choice of presentation of such information balanced with usability and overall interface complexity concerns.

Demonstrate protections for notice and explanation

Reporting. Summary reporting should document the determinations made based on the above considerations, including: the responsible entities for accountability purposes; the goal and use cases for the system, identified users, and impacted populations; the assessment of notice clarity and timeliness; the assessment of the explanation's validity and accessibility; the assessment of the level of risk; and the account and assessment of how explanations are tailored, including to the purpose, the recipient of the explanation, and the level of risk. Individualized profile information should be made readily available to the greatest extent possible that includes explanations for any system impacts or inferences. Reporting should be provided in a clear plain language and machine-readable manner.

People in Illinois are given written notice by the private sector if their biometric information is used. The Biometric Information Privacy Act enacted by the state  contains a number of provisions concerning the use of individual biometric data and identifiers. Included among them is a provision that no private entity may ""collect, capture, purchase, receive through trade, or otherwise obtain"" such information about an individual, unless written notice is provided to that individual or their legally appointed representative. 

Major technology companies are piloting new ways to communicate with the public about their automated technologies. For example, a collection of non-profit organizations and companies have worked together to develop a framework that defines operational approaches to transparency for machine learning systems. This framework, and others like it, inform the public about the use of these tools, going beyond simple notice to include reporting elements such as safety evaluations, disparity assessments, and explanations of how the systems work.

Lenders are required by federal law to notify consumers about certain decisions made about them. Both the Fair Credit Reporting Act and the Equal Credit Opportunity Act require in certain circumstances that consumers who are denied credit receive "adverse action" notices. Anyone who relies on the information in a credit report to deny a consumer credit must, under the Fair Credit Reporting Act, provide an "adverse action" notice to the consumer, which includes ""notice of the reasons a creditor took adverse action on the application or on an existing credit account." In addition, under the risk-based pricing rule, lenders must either inform borrowers of their credit score, or else tell consumers when "they are getting worse terms because of information in their credit report."" The CFPB has also asserted that "[t]he law gives every applicant the right to a specific explanation if their application for credit was denied, and that right is not diminished simply because a company uses a complex algorithm that it doesn't understand." Such explanations illustrate a shared value that certain decisions need to be explained.

A California law requires that warehouse employees are provided with notice and explanation about quotas, potentially facilitated by automated systems, that apply to them. Warehousing employers in California that use quota systems (often facilitated by algorithmic monitoring systems) are required to provide employees with a written description of each quota that applies to the employee, including “quantified number of tasks to be performed or materials to be produced or handled, within the defined time period, and any potential adverse employment action that could result from failure to meet the quota.”

Across the federal government, agencies are conducting and supporting research on explainable AI systems. The NIST is conducting fundamental research on the explainability of AI systems. A multidisciplinary team of researchers aims to develop measurement methods and best practices to support the implementation of core tenets of explainable AI. The Defense Advanced Research Projects Agency has a program on Explainable Artificial Intelligence that aims to create a suite of machine learning techniques that produce more explainable models, while maintaining a high level of learning performance (prediction accuracy), and enable  human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners. The National Science Foundation’s program on Fairness in Artificial Intelligence also includes a specific interest in research foundations for explainable AI.

There are many reasons people may prefer not to use an automated system: the system can be flawed and can lead to unintended outcomes; it may reinforce bias or be inaccessible; it may simply be inconvenient or unavailable; or it may replace a paper or manual process to which people had grown accustomed. Yet members of the public are often presented with no alternative, or are forced to endure a cumbersome process to reach a human decision-maker once they decide they no longer want to deal exclusively with the automated system or be impacted by its results. As a result of this lack of human reconsideration, many receive delayed access, or lose access, to rights, opportunities, benefits, and critical services. The American public deserves the assurance that, when rights, opportunities, or access are meaningfully at stake and there is a reasonable expectation of an alternative to an automated system, they can conveniently opt out of an automated system and will not be disadvantaged for that choice. In some cases, such a human or other alternative may be required by law, for example it could be required as “reasonable accommodations” for people with disabilities.

In addition to being able to opt out and use a human alternative, the American public deserves a human fallback system in the event that an automated system fails or causes harm. No matter how rigorously an automated system is tested, there will always be situations for which the system fails. The American public deserves protection via human review against these outlying or unexpected scenarios. In the case of time-critical systems, the public should not have to wait—immediate human consideration and fallback should be available. In many time-critical systems, such a remedy is already immediately available, such as a building manager who can open a door in the case an automated card access system fails.

In the criminal justice system, employment, education, healthcare, and other sensitive domains, automated systems are used for many purposes, from pre-trial risk assessments and parole decisions to technologies that help doctors diagnose disease. Absent appropriate safeguards, these technologies can lead to unfair, inaccurate, or dangerous outcomes. These sensitive domains require extra protections. It is critically important that there is extensive human oversight in such settings.

These critical protections have been adopted in some scenarios. Where automated systems have been introduced to provide the public access to government benefits, existing human paper and phone-based processes are generally still in place, providing an important alternative to ensure access. Companies that have introduced automated call centers often retain the option of dialing zero to reach an operator. When automated identity controls are in place to board an airplane or enter the country, there is a person supervising the systems who can be turned to for help or to appeal a misidentification.

The American people deserve the reassurance that such procedures are in place to protect their rights, opportunities, and access. People make mistakes, and a human alternative or fallback mechanism will not always have the right answer, but they serve as an important check on the power and validity of automated systems. 

An automated signature matching system is used as part of the voting process in many parts of the country to determine whether the signature on a mail-in ballot matches the signature on file. These signature matching systems are less likely to work correctly for some voters, including voters with mental or physical disabilities, voters with shorter or hyphenated names, and voters who have changed their name.97 A human curing process,98 which helps voters to confirm their signatures and correct other voting mistakes, is important to ensure all votes are counted,99 and it is already standard practice in much of the country for both an election official and the voter to have the opportunity to review and correct any such issues.

An unemployment benefits system in Colorado required, as a condition of accessing benefits, that applicants have a smartphone in order to verify their identity. No alternative human option was readily available, which denied many people access to benefits.

A fraud detection system for unemployment insurance distribution incorrectly flagged entries as fraudulent, leading to people with slight discrepancies or complexities in their files having their wages withheld and tax returns seized without any chance to explain themselves or receive a review by a person.

A patient was wrongly denied access to pain medication when the hospital’s software confused her medication history with that of her dog’s. Even after she tracked down an explanation for the problem, doctors were afraid to override the system, and she was forced to go without pain relief due to the system’s error.

A large corporation automated performance evaluation and other HR functions, leading to workers being fired by an automated system without the possibility of human review, appeal or other form of recourse.

An automated system should provide demonstrably effective mechanisms to opt out in favor of a human alternative, where appropriate, as well as timely human consideration and remedy by a fallback system, with additional human oversight and safeguards for systems used in sensitive domains, and with training and assessment for any human-based portions of the system to ensure effectiveness.

Provide a mechanism to conveniently opt out from automated systems in favor of a human alternative, where appropriate

Brief, clear, accessible notice and instructions. Those impacted by an automated system should be given a brief, clear notice that they are entitled to opt-out, along with clear instructions for how to opt-out. Instructions should be provided in an accessible form and should be easily findable by those impacted by the automated system. The brevity, clarity, and accessibility of the notice and instructions should be assessed (e.g., via user experience research).

Human alternatives provided when appropriate. In many scenarios, there is a reasonable expectation of human involvement in attaining rights, opportunities, or access. When automated systems make up part of the attainment process, alternative timely human-driven processes should be provided. The use of a human alternative should be triggered by an opt-out process.

Timely and not burdensome human alternative. Opting out should be timely and not unreasonably burdensome in both the process of requesting to opt-out and the human-driven alternative provided.

Provide timely human consideration and remedy by a fallback and escalation system in the event that an automated system fails, produces error, or you would like to appeal or contest its impacts on you

Proportionate. The availability of human consideration and fallback, along with associated training and safeguards against human bias, should be proportionate to the potential of the automated system to meaningfully impact rights, opportunities, or access. Automated systems that have greater control over outcomes, provide input to high-stakes decisions, relate to sensitive domains, or otherwise have a greater potential to meaningfully impact rights, opportunities, or access should have greater availability (e.g., staffing) and oversight of human consideration and fallback mechanisms. 

Accessible. Mechanisms for human consideration and fallback, whether in-person, on paper, by phone, or otherwise provided, should be easy to find and use. These mechanisms should be tested to ensure that users who have trouble with the automated system are able to use human consideration and fallback, with the understanding that it may be these users who are most likely to need the human assistance. Similarly, it should be tested to ensure that users with disabilities are able to find and use human consideration and fallback and also request reasonable accommodations or modifications. 

Convenient. Mechanisms for human consideration and fallback should not be unreasonably burdensome as compared to the automated system’s equivalent.

Equitable. Consideration should be given to ensuring outcomes of the fallback and escalation system are equitable when compared to those of the automated system and such that the fallback and escalation system provides equitable access to underserved communities.

Timely. Human consideration and fallback are only useful if they are conducted and concluded in a timely manner. The determination of what is timely should be made relative to the specific automated system, and the review system should be staffed and regularly assessed to ensure it is providing timely consideration and fallback. In time-critical systems, this mechanism should be immediately available or, where possible, available before the harm occurs. Time-critical systems include, but are not limited to, voting-related systems, automated building access and other access systems, systems that form a critical component of healthcare, and systems that have the ability to withhold wages or otherwise cause immediate financial penalties. 

Effective. The organizational structure surrounding processes for consideration and fallback should be designed so that if the human decision-maker charged with reassessing a decision determines that it should be overruled, the new decision will be effectively enacted. This includes ensuring that the new decision is entered into the automated system throughout its components, any previous repercussions from the old decision are also overturned, and safeguards are put in place to help ensure that future decisions do not result in the same errors. 

Maintained. The human consideration and fallback process and any associated automated processes should be maintained and supported as long as the relevant automated system continues to be in use. 

Institute training, assessment, and oversight to combat automation bias and ensure any human-based components of a system are effective. 

Training and assessment. Anyone administering, interacting with, or interpreting the outputs of an automated system should receive training in that system, including how to properly interpret outputs of a system in light of its intended purpose and in how to mitigate the effects of automation bias. The training should reoccur regularly to ensure it is up to date with the system and to ensure the system is used appropriately. Assessment should be ongoing to ensure that the use of the system with human involvement provides for appropriate results, i.e., that the involvement of people does not invalidate the system's assessment as safe and effective or lead to algorithmic discrimination. 

Oversight. Human-based systems have the potential for bias, including automation bias, as well as other concerns that may limit their effectiveness. The results of assessments of the efficacy and potential bias of such human-based systems should be overseen by governance structures that have the potential to update the operation of the human-based system in order to mitigate these effects.

Implement additional human oversight and safeguards for automated systems related to sensitive domains 

Automated systems used within sensitive domains, including criminal justice, employment, education, and health, should meet the expectations laid out throughout this framework, especially avoiding capricious, inappropriate, and discriminatory impacts of these technologies. Additionally, automated systems used within sensitive domains should meet these expectations: 

Narrowly scoped data and inferences. Human oversight should ensure that automated systems in sensitive domains are narrowly scoped to address a defined goal, justifying each included data item or attribute as relevant to the specific use case. Data included should be carefully limited to avoid algorithmic discrimination resulting from, e.g., use of community characteristics, social network analysis, or group-based inferences. 

Tailored to the situation. Human oversight should ensure that automated systems in sensitive domains are tailored to the specific use case and real-world deployment scenario, and evaluation testing should show that the system is safe and effective for that specific situation. Validation testing performed based on one location or use case should not be assumed to transfer to another. 

Human consideration before any high-risk decision. Automated systems, where they are used in sensitive domains, may play a role in directly providing information or otherwise providing positive outcomes to impacted people. However, automated systems should not be allowed to directly intervene in high-risk situations, such as sentencing decisions or medical care, without human consideration. 

Meaningful access to examine the system. Designers, developers, and deployers of automated systems should consider limited waivers of confidentiality (including those related to trade secrets) where necessary in order to provide meaningful oversight of systems used in sensitive domains, incorporating measures to protect intellectual property and trade secrets from unwarranted disclosure as appropriate. This includes (potentially private and protected) meaningful access to source code, documentation, and related data during any associated legal discovery, subject to effective confidentiality or court orders. Such meaningful access should include (but is not limited to) adhering to the principle on Notice and Explanation using the highest level of risk so the system is designed with built-in explanations; such systems should use fully-transparent models where the model itself can be understood by people needing to directly examine it. 

Demonstrate access to human alternatives, consideration, and fallback 

Reporting. Reporting should include an assessment of timeliness and the extent of additional burden for human alternatives, aggregate statistics about who chooses the human alternative, along with the results of the assessment about brevity, clarity, and accessibility of notice and opt-out instructions. Rweporting on the accessibility, timeliness, and effectiveness of human consideration and fallback should be made public at regular intervals for as long as the system is in use. This should include aggregated information about the number and type of requests for consideration, fallback employed, and any repeated requests; the timeliness of the handling of these requests, including mean wait times for different types of requests as well as maximum wait times; and information about the procedures used to address requests for consideration along with the results of the evaluation of their accessibility. For systems used in sensitive domains, reporting should include information about training and governance procedures for these technologies. Reporting should also include documentation of goals and assessment of meeting those goals, consideration of data included, and documentation of the governance of reasonable access to the technology. Reporting should be provided in a clear and machine-readable manner.

Healthcare “navigators” help people find their way through online signup forms to choose and obtain healthcare. A Navigator is “an individual or organization that's trained and able to help consumers, small businesses, and their employees as they look for health coverage options through the Marketplace (a government web site), including completing eligibility and enrollment forms.”106 For the 2022 plan year, the Biden-Harris Administration increased funding so that grantee organizations could “train and certify more than 1,500 Navigators to help uninsured consumers find affordable and comprehensive health coverage.”

The customer service industry has successfully integrated automated services such as chat-bots and AI-driven call response systems with escalation to a human support team.108 Many businesses now use partially automated customer service platforms that help answer customer questions and compile common problems for human agents to review. These integrated human-AI systems allow companies to provide faster customer care while maintaining human agents to answer calls or otherwise respond to complicated requests. Using both AI and human agents is viewed as key to successful customer service.

Ballot curing laws in at least 24 states require a fallback system that allows voters to correct their ballot and have it counted in the case that a voter signature matching algorithm incorrectly flags their ballot as invalid or there is another issue with their ballot, and review by an election official does not rectify the problem. Some federal courts have found that such cure procedures are constitutionally required.110 Ballot curing processes vary among states, and include direct phone calls, emails, or mail contact by election officials.111 Voters are asked to provide alternative information or a new signature to verify the validity of their ballot. 52


/ 022. Kurzweil, Ray. "The Age of Intelligent Machines: Can Machines Think?" 1990.Distinguished Arts and Sciences Professor; Director, The Center for Cognitive Studies, Tufts University.Originally Published 1990Can machines think? This has been a conundrum for philosophers for years, but in their fascination with the pure conceptual issues they have for the most part overlooked the real social importance of the answer.It is of more than academic importance that we learn to think clearly about the actual cognitive powers of computers, for they are now being introduced into a variety of sensitive social roles where their powers will be put to the ultimate test: in a wide variety of areas, we are on the verge of making ourselves dependent upon their cognitive powers. The cast of overestimating them could be enormous.One of the principal inventors of the computer was the great British mathematician Alan Turing. It was he who first figured out, in highly abstract terms, how to design a programmable computing device, what we now call a universal Turing machine.All programmable computers in use today are in essence Turing machines. About forty years ago, at the dawn of the computer age, Turing began a classic article “Computing Machinery and Intelligence” with the words “I propose to consider the question, ‘Can machines think?’” but he then went on to say that this was a bad question, a question that leads only to sterile debate and haggling over definitions, a question, as he put it, “too meaningless to deserve discussion.”1In its place he substituted what he took to be a much better question, a question that would be crisply answerable and intuitively satisfying–in every way an acceptable substitute for the philosophic puzzler with which he began.First he described a parlor game of sorts, the imitation game, to be played by a man, a woman, and a judge (of either gender). The man and woman are hidden from the judge’s view but are able to communicate with the judge by teletype; the judge’s task is to guess, after a period of questioning each contestant, which interlocutor is the man and which the woman.The man tries to convince the judge he is the woman, and the woman tries to convince the judge of the truth. The man wins the judge makes the wrong identification. A little reflection will convince you, I am sure, that aside from lucky breaks, it would take a clever man to convince the judge that he was the woman–on the assumption that the judge is clever too, of course.Now suppose, Turing said, we replace the man or woman with a computer and give the judge the task of determining which is the human being and which is the computer. Turing proposed that any computer that can regularly or often foal a discerning judge in this game would be intelligent, a computer that thinks, beyond any reasonable doubt. Now, it is important to realize that failing this test is not supposed to be a sign of lack of intelligence. Many intelligent people, after all, might not be willing or able to play the imitation game, and we should allow computers the same opportunity to decline to prove themselves. This is, then, a one-way test; failing it proves nothing.Furthermore, Turing was not committing himself to the view (although it is easy to see how one might think he was) that to think is to think just like a human being–any more than he was committing himself to the view that for a man to think, he must think exactly like a woman. Men, women, and computers may all have different ways of thinking.But surely, he thought, one can think in one’s own peculiar style well enough to imitate a thinking man or woman, one can think well, indeed. This imagined exercise has come to be known as the Turing test.It is a sad irony that Turing’s proposal has had exactly the opposite effect on the discussion of what he intended. Turing didn’t design the test as a useful tool in scientific psychology, a method of confirming or disconfirming scientific theories or evaluating particular models of mental function; he designed it to be nothing more than a philosophical conversation stopper.He proposed, in the spirit of “Put up or shut up!”, a simple test for thinking that is surely strong enough to satisfy the sternest skeptic (or so he thought). He was saying, in effect, that instead of arguing interminably about the ultimate nature and essence of thinking, we should all agree that whatever that nature is, anything that could pass this test would surely have it; then we could turn to asking how or whether some machine could be designed and built that might pass the test fair and square.Alas, philosophers, amateur and professional, have instead taken Turing’s proposal as the pretext for just the sort of definitional haggling and interminable arguing about imaginary counter-examples that he was hoping to squelch.This forty-year preoccupation with the Turing test has been all the more regrettable because it has focused attention on the wrong issues. There are real world problems that are revealed by considering the strengths and weaknesses of the Turing test, but these have been concealed behind a smoke screen of misguided criticisms. A failure to think imaginatively about the test actually proposed by Turing has led many to underestimate its severity and to confuse it with much less interesting proposals.So first I want to show that the Turing test, conceived as he conceived it, is (as he thought) quite strong enough as a test of thinking. I defy anyone to improve upon it. But here is the point almost universally overlooked by the literature: there is a common misapplication of the Turing test that often leads to drastic overestimation of the powers of actually existing computer systems. The follies of this familiar sort of thinking about computers can best be brought out by a reconsideration of the Turing test itself.The insight underlying the Turing test is the same insight that inspires the new practice among symphony orchestras of conducting auditions with an opaque screen between the jury and the musician. What matters in a musician is, obviously, musical ability and only musical ability; such features as sex, hair length, skin color, and weight are strictly irrelevant. Since juries might be biased even innocently and unawares by these irrelevant features, they are carefully screened off so only the essential feature, musicianship, can be examined.Turing recognized that people might be similarly biased in their judgments of intelligence by whether the contestant had soft skin, warm blood, facial features, hands, and eyes–which are obviously not themselves essential components of intelligence. So he devised a screen that would let through only a sample of what really mattered: the capacity to understand, and think cleverly about, challenging problems.Perhaps he was inspired by Descartes, who in his Discourse on Method (1637) plausibly argued that there was no more demanding test of human mentality than the capacity to hold an intelligent conversation: “It is indeed conceivable that a machine could be so made that it would utter words, and even words appropriate to the presence of physical acts or objects which cause some change in its organs; as, for example, it was touched in some so spot that it would ask what you wanted to say to it; in another, that it would cry that it was hurt, and so on for similar things. But it could never modify its phrases to reply to the sense of whatever was said in its presence, as even the most stupid men can do.”This seemed obvious to Descartes in the seventeenth century, but of course, the fanciest machines he knew were elaborate clockwork figures, not electronic computers. Today it is far from obvious that such machines are impossible, but Descartes’ hunch that ordinary conversation would put as severe a strain on artificial intelligence as any other test was shared by Turing. Of course, there is nothing sacred about the particular conversational game chosen by Turing for his test; it is just a cannily chosen test of more general intelligence.The assumption Turing was prepared to make was this: Nothing could possibly pass the Turing test by winning the imitation game without being able to perform indefinitely many other clearly intelligent actions. Let us call that assumption the quick-probe assumption.Turing realized, as anyone would, that there are hundreds and thousands of telling signs of intelligent thinking to be observed in our fellow creatures, and one could, one wanted, compile a vast battery of different tests to assay the capacity for intelligent thought. But success on his chosen test, he thought, would be highly predictive of success on many other intuitively acceptable tests of intelligence.Remember, failure on the Turing test does not predict failure on those others, but success would surely predict success. His test was so severe, he thought, that nothing that could pass it fair and square would disappoint us in other quarters. Maybe it wouldn’t do everything we hoped–maybe it wouldn’t appreciate ballet, understand quantum physics, or have a good plan far world peace, but we’d all see that it was surely one of the intelligent, thinking entities in the neighborhood.Is this high opinion of the Turing tests severity misguided? Certainly many have thought so, but usually because they have not imagined the test in sufficient detail, and hence have underestimated it. Trying to forestall this skepticism, Turing imagined several lines of questioning that a judge might employ in this game that would be taxing indeed–lines about writing poetry or playing chess. But with thirty years’ experience with the actual talents and foibles of computers behind us, perhaps we can add a few more tough lines of questioning.Terry Winograd, a leader in AI efforts to produce conversational ability in a computer, draws our attention to a pair of sentences. They differ in only one word. The first sentence is this: “The committee denied the group a parade permit because they advocated violence.” Here’s the second sentence: “The committee denied the group a parade permit because they feared violence.”The difference is just in the verb–”advocated” or “feared.” As Winograd points out, the pronoun “they” in each sentence is officially ambiguous. Both readings of the pronoun are always legal. Thus, we can imagine a world in which governmental committees in charge of parade permits advocate violence in the streets and, for some strange reason, use this as their pretext for denying a parade permit. But the natural, reasonable, intelligent reading of the first sentence is that it’s the group that advocated violence, and of the second, that it’s the committee that feared the violence.Now sentences like this are embedded in a conversation, the computer must figure out which reading of the pronoun is meant, it is to respond intelligently. But mere rules of grammar or vocabulary will not fix the right reading. What fixes the right reading for us is knowledge about politics, social circumstances, committees and their attitudes, groups that want to parade, how they tend to behave, and the like. One must know about the world, in short, to make sense of such a sentence.In the jargon of artificial intelligence, a conversational computer needs lots of world knowledge to do its jab. But, it seems, it is somehow endowed with that world knowledge on many topics, it should be able to do much more with that world knowledge than merely make sense of a conversation containing just that sentence.The only way, it appears, for a computer to disambiguate that sentence and keep up its end of a conversation that uses that sentence would be for it to have a much more general ability to respond intelligently to information about social and political circumstances and many other topics. Thus, such sentences, by putting a demand on such abilities, are good quick probes. That is, they test for a wider competence.People typically ignore the prospect of having the judge ask off-the-wall questions in the Turing test, and hence they underestimate the competence a computer would have to have to pass the test. But remember, the rules of the imitation game as Turing presented it permit the judge to ask any question that could be asked of a human being–no holds barred. Suppose, then, we give a contestant in the game this question: An Irishman found a genie in a bottle who offered him two wishes.“First I’ll have a pint of Guinness,” said the Irishman, and when it appeared, he took several long drinks from it and was delighted to see that the glass filled itself magically as he drank. “What about your second wish?” asked the genie. “Oh well, that’s easy,” said the Irishman. “I’ll have another one of these!” Please explain this story to me, and tell me there is anything funny or sad about it.Now even a child could express, even not eloquently, the understanding that is required to get this joke. But think of how much one has to know and understand about human culture, to put it pompously, to be able to give any account of the point of this joke.I am not supposing that the computer would have to laugh at, or be amused by, the joke. But it wants to win the imitation game–and that’s the test, after all–it had better know enough in its own alien, humorless way about human psychology and culture to be able to pretend effectively that it was amused and explain why.It may seem to you that we could devise a better test. Let’s compare the Turing test with some other candidates.Candidate 1A computer is intelligent; it wins the World Chess Championship.That’s not a good test, it turns out. Chess prowess has proven to be an isolatable talent. There are programs today that can play fine chess but do nothing else. So the quick-probe assumption is false far the test of playing winning chess.Candidate 2The computer is intelligent; it solves the Arab-Israeli conflict.This is surely a more severe test than Turing’s. But it has some detects: passed once, it is unrepeatable; it is slow, no doubt; and it is not crisply clear what would count as passing it. Here’s another prospect, then:Candidate 3A computer is intelligent; it succeeds in stealing the British crown jewels without the use of force or violence.Now this is better. First, it could be repeated again and again, though of course each repeat test would presumably be harder, but this is a feature it shares with the Turing test. Second, the mark of success is clear: either you’ve got the jewels to show for your efforts or you don’t. But it is expensive and slow, a socially dubious caper at best, and no doubt luck would play too great a role.With ingenuity and effort one might be able to came up with other candidates that would equal the Turing test in severity, fairness, and efficiency, but I think these few examples should suffice to convince us that it would be hard to improve on Turing’s original proposal.But still, you may protest, something might pass the Turing test and still not be intelligent, not be a thinker. What does might mean here? what you have in mind is that by cosmic accident, by a supernatural coincidence, a stupid person or a stupid computer might fool a clever judge repeatedly, well, yes, but so what? The same frivolous possibility “in principle” holds for any test whatever.A playful god or evil demon, let us agree, could fool the world’s scientific community about the presence of H20 in the Pacific Ocean. But still, the tests they rely on to establish that there is H20 in the Pacific Ocean are quite beyond reasonable criticism. The Turing test for thinking is no worse than any well-established scientific test, we can set skepticism aside and go back to serious matters. Is there any more likelihood of a false positive result on the Turing test than on, say, the tests currently used for the presence of iron in an ore sample?This question is often obscured by a move called operationalism that philosophers have sometimes made. Turing and those who think well of his test are often accused of being operationalists. Operationalism is the tactic of defining the presence of some property, intelligence, for instance, as being established once and for all by the passing of some test. Let’s illustrate this with a different example.Suppose I offer the following test–we’ll call it the Dennett test–for being a great city. A great city is one in which, on a randomly chosen day, one can do all three of the following: hear a symphony orchestra, see a Rembrandt and a professional athletic contest, and eat quenelles de brothel a la Nantua for lunch. To make the operationalist move would 6e to declare that any city that passes the Dennett test is by definition a great city. What being a great city amounts to is just passing the Dennett test.Well then, if the Chamber of Commerce of Great Falls, Montana, wanted, and I can’t imagine why–to get their hometown on my list of great cities, they could accomplish this by the relatively inexpensive route of hiring full time about ten basketball players, forty musicians, and a quick-order quenelle chef and renting a cheap Rembrandt from some museum. An idiotic operationalist would then be stuck admitting that Great Falls, Montana, was in fact a great city, since all he or she cares about in great cities is that they pass the Dennett test.Sane operationalists (who far that very reason are perhaps not operationalists at all, since “operationalist” seems to be a dirty word) would cling confidently to their test, but only because they have what they consider to be very good reasons for thinking the odds astronomical against a false positive result, like the imagined Chamber of Commerce caper. I devised the Dennett test, of course, with the realization that no one would be both stupid and rich enough to go to such preposterous lengths to foil the test.In the actual world, wherever you find symphony orchestras, quenelles, Rembrandts, and professional sports, you also find daily newspapers, parks, repertory theaters, libraries, fine architecture, and all the other things that go to make a city great. My test was simply devised to locate a telling sample that could not help but be representative of the rest of the city’s treasures. I would cheerfully run the minuscule risk of having my bluff called. Obviously, the test items are not all that I care about in a city.In fact, some of them I don’t care about at all. I just think they would be cheap and easy ways of assuring myself that the subtle things I do care about in cities are present. Similarly, I think it would be entirely unreasonable to suppose that Alan Turing had an inordinate fondness for party games or put too high a value on party game prowess in his test. In both the Turing test and the Dennett test a very unrisky gamble is being taken: the gamble that the quick-probe assumption is in general safe.But two can play this game of playing the odds. Suppose some computer programmer happens to be, for whatever strange reason, dead set on tricking me into judging an entity to be a thinking, intelligent thing when it is not. Such a trickster could rely as well as 1 can on unlikelihood and take a few gambles. Thus, the programmer can expect that it is not remotely likely that 1, as the judge, will bring up the topic of children’s birthday parties, or baseball, or moon rocks, then he or she can avoid the trouble of building world knowledge on those topics into the data base.Whereas I do improbably raise these issues, the system will draw a blank, and I will unmask the pretender easily. But with all the topics and words that I might raise, such a saving would no doubt be negligible. Turn the idea inside out, however, and the trickster will have a fighting chance.Suppose the programmer has reason to believe that I will ask only about children’s birthday parties or baseball or moon rocks–all other topics being, for one reason or another, out of bounds. Not only does the task shrink dramatically, but there already exist systems or preliminary sketches of systems in artificial intelligence that can do a whiz-bang job of responding with apparent intelligence an just those specialized topics.William Wood’s LUNAR program, to take what is perhaps the best example, answers scientists’ questions–posed in ordinary English–about moon rocks. In one test it answered correctly and appropriately something like 90 percent of the questions that geologists and other experts thought of asking it about moon rocks. (In 12 percent of those correct responses there were trivial, correctable defects.)Of course, Wood’s motive in creating LUNAR was not to trick unwary geologists into thinking they were conversing with an intelligent being. And that had been his motive, his project would still be a long way from success.For it is easy enough to unmask LUNAR without ever straying from the prescribed topic of moon rocks. Put LUNAR in one room and a moon rocks specializt in another, and then ask them both their opinion of the social value of the moon-rockgathering expeditions, for instance. Or ask the contestants their opinion of the suitability of moon rocks as ashtrays, or whether people who have touched moon rocks are ineligible for the draft. Any intelligent person knows a lot more about moon rocks than their geology. Although it might be unfair to demand this extra knowledge of a computer moon-rock specialist, it would be an easy way to get it to fail the Turing test.But just suppose that someone could extend LUNAR to cover itself plausibly on such probes, so long as the topic was still, however indirectly, moon rocks. We might come to think it was a lot more like the human moon-rock specialist than it really was. The moral we should draw is that as Turing-test judges we should resist all limitations and waterings-down of the Turing test. They make the game too easy–vastly easier than the original test. Hence, they lead us into the risk of overestimating the actual comprehension of the system being tested.Consider a different limitation on the Turing test that should strike a suspicious chord in us as soon as we hear it. This is a variation on a theme developed in a recent article by Ned Block.4 Suppose someone were to propose to restrict the judge to a vocabulary of, say, the 850 words of Basic English, and to single-sentence probes–that is, “moves”–of no more than four words.Moreover, contestants must respond to these probes with no more than four words per move, and a test may involve no more than forty questions Is this an innocent variation on Turing’s original test? These restrictions would make the imitation game clearly finite.That is, the total number of all possible permissible games is a large but finite number. One might suspect that such a limitation would permit the trickster simply to store, in alphabetical order, all the possible good conversations within the limits and fool the judge with nothing more sophisticated than a system of table lookup. In fact, that isn’t in the cards.Even with these severe, improbable, and suspicious restrictions imposed upon the imitation game, the number of legal games, though finite, is mind-bogglingly large. I haven’t bothered trying to calculate it but it surely astronomically exceeds the number of possible chess games with no more than forty moves, and that number has been calculated. John Haugeland says it’s in the neighborhood of 10120. For comparison, Haugeland suggests there have only been 1018 seconds since the beginning of the universe.5Of course, the number of good, sensible conversations under these limits is a tiny fraction, maybe 1 in 1015, of the number of merely grammatically well-formed conversations. So let’s say, to be very conservative, that there are only 1015 different smart conversations such a computer would have to store. Well, the task shouldn’t take more than a few trillion years– with generous federal support. Finite numbers can be very large.So though we needn’t worry that this particular trick of storing all the smart conversations would work, we can appreciate that there are lots of ways of making the task easier that may appear innocent at first. We also get a reassuring measure of just how severe the unrestricted Turing test is by reflecting on the more than astronomical size of even that severely restricted version of it.Block’s imagined–and utterly impossible–program exhibits the dreaded feature known in computer-science circles as combinatorial explosion. No conceivable computer could overpower a combinatorial explosion with sheer speed and size. Since the problem areas addressed by artificial intelligence are veritable minefields of combinatorial explosion, and since it has often proved difficult to find any solution to a problem that avoids them, there is considerable plausibility in Newell and Simon’s proposal that avoiding combinatorial explosion (by any means at all) be viewed as one of the hallmarks of intelligence.Our brains are millions of times bigger than the brains of gnats, but they are still–for all their vast complexity–compact, efficient, timely organs that somehow or other manage to perform all their tasks while avoiding combinatorial explosion. A computer a million times bigger or faster than a human brain might not look like the brain of a human being, or even be internally organized like the brain of a human being but, far all its differences, it somehow managed to control a wise and timely set of activities, it would have to be the beneficiary of a very special design that avoided combinatorial explosion. And whatever that design was, would we not be right to consider the entity intelligent?Turing’s test was designed to allow for this possibility. His point was that we should not be species-chauvinistic, or anthropocentric, about the insides of an intelligent being, for there might be inhuman ways of being intelligent.To my knowledge the only serious and interesting attempt by any program designer to win even a severely modified Turing test has been Kenneth Colby’s. Colby is a psychiatrist and intelligence artificer at UCLA. He has a program called PARRY, which is a computer simulation of a paranoid patient who has delusions about the Mafia being out to get him.As you do with other conversational programs, you interact with it by sitting at a terminal and typing questions and answers back and forth. A number of years ago, Colby put PARRY to a very restricted test. He had genuine psychiatrists interview PARRY. He did not suggest to them that they might be talking or typing to a computer; rather, he made up some plausible story about why they were communicating with a real, live patient by teletype.He also had the psychiatrists interview real, human paranoids via teletype. Then he took a PARRY transcript, inserted it in a group of teletype transcripts from real patients, gave them to another group of experts–more psychiatrists–and said, “One of these was a conversation with a computer. Can you figure out which one it was?” They couldn’t. They didn’t do better than chance.Colby presented this with some huzzah, but critics scoffed at the suggestion that this was a legitimate Turing test. My favorite commentary on it was Joseph Weizenbaum’s; in a letter to the Communications of the Association of Computing Machinery, he said that, inspired by Colby, he had designed an even better program, which passed the same test.6 His also had the virtue of being a very inexpensive program, in these times of tight money. In fact you didn’t even need a computer for it. All you needed was an electric typewriter. His program modeled infantile autism.And the transcripts–you type in your questions, and the thing just sits there and hums–cannot be distinguished by experts from transcripts of real conversations with infantile autistic patients. What was wrong with Colby’s test, of course, was that the unsuspecting interviewers had no motivation at all to try out any of the sorts of questions that easily would have unmasked PARRY.Colby was undaunted, and after his team had improved PARRY, he put it to a much more severe test–a surprisingly severe test. This time, the interviewers–again psychiatrists, were given the task at the outset of telling the computer from the real patient. They were set up in a classic Turing machine test: the patient in one room, the computer PARRY in the other room, with the judges conducting interviews with both of them (on successive days). The judges’ task was to find out which one was the computer and which one was the real patient. Amazingly, they didn’t do much better, which leads some people to say, “Well, that just confirms my impression of the intelligence of psychiatrists!”But more seriously now, was this an honest-to-goodness Turing test? Were there tacit restrictions on the lines of questioning of the judges? Like the geologists interacting with LUNAR, the psychiatrists’ professional preoccupations and habits kept them from asking the sorts of unlikely questions that would have easily unmasked PARRY. After all, they realized that since one of the contestants was a real, live paranoid person, medical ethics virtually forbade them from toying with, upsetting, or attempting to confuse their interlocutors.Moreover, they also knew that this was a test of a model of paranoia, so there were certain questions that wouldn’t be deemed to be relevant to testing the model as a model of paranoia. So they asked just the sort of questions that therapists typically ask of such patients, and of course PARRY had been ingeniously and laboriously prepared to deal with just that sort of question.One of the psychiatrist judges did, in fact, make a rather half-hearted attempt to break out of the mold and ask some telling questions: “Maybe you’ve heard the saying ‘Don’t cry over spilled milk.’ What does that mean to you?” PARRY answered, “Maybe you have to watch out for the Mafia.”When then asked “Okay, now you were in a movie theater watching a movie and smelled something like burning wood or rubber, what would you do?” PARRY replied, “You know, they know me.” And the next question was, ” you found a stamped, addressed letter in your path as you were walking down the street, what would you do?” PARRY replied, “What else do you want to know?”7Clearly, PARRY was, you might say, parrying these questions, which were incomprehensible to it, with more or less stock paranoid formulas. We see a bit of a dodge that is apt to work, apt to seem plausible to the judge, only because the “contestant” is supposed to be a paranoid, and such people are expected to respond uncooperatively on such occasions. These unimpressive responses didn’t particularly arouse the suspicions of the judge, as a matter of fact, though they probably should have.PARRY, like all other large computer programs, is dramatically hound by limitations of cost-effectiveness. What was important to Colby and his crew was simulating his model of paranoia. This was a massive effort. PARRY has a thesaurus or dictionary of about 4,500 words and 700 idioms and the grammatical competence to use it-a parser, in the jargon of computational linguistics.The entire PARRY program takes up about 200,000 words of computer memory, all laboriously installed by the programming team. Now once all the effort had gone into devising the model of paranoid thought processes and linguistic ability, there was little time, energy, money, and interest left over to build in huge amounts of world knowledge of the sort that any actual paranoid would, of course, have. (Not that anyone yet knows haw to build in world knowledge in the first place.)
Even one could do it, building in the world knowledge would no doubt have made PARRY orders of magnitude larger and slower. And what would have been the point, given Colby’s theoretical aims?PARRY is a theoretician’s model of a psychological phenomenon: paranoia. It is not intended to have practical applications. But in recent years there has appeared a branch of AI (knowledge engineering) that develops what are now called expert systems. Expert systems are designed to be practical. They are typically software super specializt consultants that can be asked to diagnose medical problems, analyze geological data, analyze the results of scientific experiments, and the like. Some of them are very impressive.SRI in California announced a few years ago that PROSPECTOR, an SRI-developed expert system in geology, had correctly predicted the existence of a large, important mineral deposit that had been entirely unanticipated by the human geologists who had fed it its data. MYCIN, perhaps the most famous of these expert systems, diagnoses infections of the blood, and it does probably as well as, maybe better than, any human consultants. And many other expert systems are on the way.All expert systems, like all other large AI programs, are what you might call Potemkin villages. That is, they are cleverly constructed facades, like cinema sets. The actual filling-in of details of AI programs is time-consuming, costly work, so economy dictates that only those surfaces of the phenomenon that are likely to be probed or observed are represented.Consider, for example, the CYRUS program developed 6y Janet Kalodner in Roger Schenk’s AI group at Yale a few years ago. CYRUS stands (we are told) for “Computerized Yale Retrieval and Updating System,” but surely it is no accident that CYRUS modeled the memory of Cyrus Vance, who was then secretary of state in the Carter administration.The paint of the CYRUS project was to devise and test some plausible ideas about how people organize their memories of the events they participate in. Hence, it was meant to be a “pure- AI system, a scientific model, not an expert system intended for any practical purpose. CYRUS was updated daily by being fed all UPI wire-service news stories that mentioned Vance, and it was fed them directly with no doctoring and no human intervention.With an ingenious news-reading program called FRUMP, it could take any story just as it came in on the wire and could digest it and use it to update its database so that it could answer more questions. You could address questions to CYRUS in English by typing at a terminal. You addressed CYRUS in the second person, as you were talking with Cyrus Vance himself. The results looked like this:Question: Last time you went to Saudi Arabia, where did you stay?Answer: In a palace in Saudi Arabia on September 23, 1978.Question: Did you go sightseeing there?Answer: Yes, at an oilfield in Dharan on September 23, 1978.Question: Has your we ever met Mrs. Begin?Answer: Yes, most recently at a state dinner in Israel in January 1980.CYRUS could correctly answer thousands of questions, almost any fair question one could think of asking it. But one actually set out to explore the boundaries of its facade and find the questions that overshot the mark, one could soon find them. “Have you ever met a female head of state?” was a question I asked it wondering CYRUS knew that Indira Ghandi and Margaret Thatcher were women.But for some reason the connection could not be drawn, and CYRUS failed to answer either yes or no. I had stumped it, in spite of the fact that CYRUS could handle a host of what you might call neighboring questions flawlessly. One soon learns from this sort of probing exercise that it is very hard to extrapolate accurately from a sample performance to the system’s total competence. It’s also very hard to keep from extrapolating much too generously.While I was visiting Schenk’s laboratory in the spring of 1980, something revealing happened. The real Cyrus Vance suddenly resigned. The effect on the program CYRUS was chaotic. It was utterly unable to cape with the flood of “unusual” news about Cyrus Vance. The only sorts of episodes CYRUS could understand at all were diplomatic meetings, flights, press conferences, state dinners, and the like–less than two dozen general sorts of activities (the kinds that are newsworthy and typical of secretaries of state). It had no provision for sudden resignation.It was as if the UPI had reported that a wicked witch had turned Vance into a frog. It is distinctly possible that CYRUS would have taken that report more in stride than the actual news. One can imagine the conversationQuestion: Hello, Mr. Vance, what’s new?Answer: I was turned into a frog yesterday.But, of course, it wouldn’t know enough about what it had just written to be puzzled, startled, or embarrassed. The reason is obvious. When you look inside CYRUS, you find that it has skeletal definitions of thousands of words, but these definitions are minimal. They contain as little as the system designers think that they can get away with.Thus, perhaps, “lawyer” would be defined as synonymous with “attorney” and “legal counsel,” but aside from that, all one would discover about lawyers is that they are adult human beings and that they perform various functions in legal areas. you then traced out the path to “human being,” you’d find out various obvious things CYRUS “knew” about human beings (hence about lawyers/, but that is not a lot.That lawyers are university graduates, that they are better paid than chambermaids, that they know how to tie their shoes, that they are unlikely to be found in the company of lumberjacks–these trivial, weird, facts about lawyers would not be explicit or implicit anywhere in this system.In other words, a very thin stereotype of a lawyer would 6e incorporated into the system, so that almost nothing you could tell it about a lawyer would surprise it. So long as surprising things don’t happen, so long as Mr. Vance, for instance, leads a typical diplomat’s le, attending state dinners, giving speeches, flying from Cairo to Rome, and so forth, this system works very well.But as soon as his path is crossed by an important anomaly, the system is unable to cape and unable to recover without fairly massive human intervention. In the case of the sudden resignation, Kolodner and her associates soon had CYRUS up and running again with a new talent–answering questions about Edmund Muskie, Vance’s successor. But it was no less vulnerable to unexpected events. Not that it mattered particularly, since CYRUS was a theoretical model, not a practical system.There are a host of ways of improving the performance of such systems, and, of course, some systems are much better than others. But all AI programs in one way or another have this façade-like quality, simply for reasons of economy. For instance, most expert systems in medical diagnosis developed so far operate with statistical information. They have no deep or even shallow knowledge of the underlying causal mechanisms of the phenomena that they are diagnosing.To take an imaginary example, an expert system asked to diagnose an abdominal pain would be oblivious to the potential import of the fact that the patient had recently been employed as a sparring partner by Mohammed Ali: there being no statistical data available to it on the rate of kidney stones among athlete’s assistants. That’s a fanciful case no doubt–too obvious, perhaps, to lead to an actual failure of diagnosis and practice. But more subtle and hard-to-detect limits to comprehension are always present, and even experts, even the system’s designers, can be uncertain of where and how these limits will interfere with the desired operation of the system.Again, steps can be taken and are being taken to correct these flaws. For instance, my former colleague at Tufts, Benjamin Kuipers, is currently working on an expert system in nephrology for diagnosing kidney ailments that will be based on an elaborate system of causal reasoning about the phenomena being diagnosed. But this is a very ambitious, long-range project of considerable theoretical difficulty. And even all the reasonable, cost-effective steps are taken to minimize the superficiality of expert systems, they will still be facades, just somewhat thicker or wider facades.When we were considering the fantastic case of the crazy Chamber of Commerce of Great Falls, Montana, we couldn’t imagine a plausible motive for anyone going to any sort of trouble to trick the Dennett test. The quick-probe assumption for the Dennett test looked quite secure. But when we look at expert systems, we see that, however innocently, their designers do have motivation for doing exactly the sort of trick that would fool an unsuspicious Turing tester.First, since expert systems are all superspecializts that are only supposed to know about some narrow subject, users of such systems, not having much time to kill, do not bother probing them at the boundaries at all. They don’t bother asking “silly” or irrelevant questions. Instead, they concentrate, not unreasonably, on exploiting the system’s strengths. But shouldn’t they try to obtain a clear vision of such a system’s weaknesses as well? The normal habit of human thought when we converse with one another is to assume general comprehension, to assume rationality, to assume, moreover, that the quick-probe assumption is, in general, sound.This amiable habit of thought almost irresistibly leads to putting too much faith in computer systems, especially use friendly systems that present themselves in a very anthropomorphic manner. Part of the solution to this problem is to teach all users of computers, especially users of expert systems, how to probe their systems before they rely on them, how to search out and explore the boundaries of the facade. This is an exercise that calls for not only intelligence and imagination but also for a bit of special understanding about the limitations and actual structure of computer programs. It would help, of course, if we had standards of truth in advertising, in effect, for expert systems.For instance, each such system should come with a special demonstration routine that exhibits the sorts of shortcomings and failures that the designer knows the system to have. This would not be a substitute, however, for an attitude of cautious, almost obsessive, skepticism on the part of users, for designers are often, not always, unaware of the subtler flaws in the products they produce. That is inevitable and natural because of the way system designers must think. They are trained to think positively–constructively, one might say–about the designs that they are constructing.I come, then, to my conclusions. First, a philosophical or theoretical conclusion: The Turing test, in unadulterated, unrestricted form as Turing presented it, is plenty strong well used. I am confident that no computer in the next twenty years in going to pass the unrestricted Turing test. They may well win the World Chess Championship or even a Nobel Prize in physics, but they won’t pass the unrestricted Turing test.Nevertheless, it is not, I think, impossible in principle for a computer to pass the test fair and square. I’m not giving one of those a priori “computers can’t think” arguments. I stand unabashedly ready, moreover, to declare that any computer that actually passes the unrestricted Turing test will be, in every theoretically interesting sense, a thinking thing.But remembering how very strong the Turing test is, we must also recognize that there may also be interesting varieties of thinking or intelligence that are not well poised to play and win the imitation game. That no nonhuman Turing test winners are yet visible on the horizon does not mean that there aren’t machines that already exhibit some of the important features of thought.About them it is probably futile to ask my title question, Do they think? Do they really think? In some regards they do, and in some regards they don’t. Only a detailed look at what they do and how they are structured will reveal what is interesting about them.The Turing test, not being a scientific test, is of scant help on that task, but there are plenty of other ways to examine such systems. Verdicts on their intelligence, capacity for thought, or consciousness will be only as informative and persuasive as the theories of intelligence, thought, or consciousness the verdicts were based on, and since our task is to create such theories, we should get on with it and leave the Big Verdict for another occasion. In the meantime, should anyone want a surefire test of thinking by a computer that is almost guaranteed to be fail-safe, the Turing test will do very nicely.My second conclusion is more practical and hence in one clear sense more important. Cheapened versions of the Turing test are everywhere in the air. Turing’s test is not just effective, it is entirely natural; this is, after all, the way we assay the intelligence of each other every day.And since incautious use of such judgments and such tests is the norm, we are in some considerable danger of extrapolating too easily and judging too generously about the understanding of the systems we are using. The problem of overestimating cognitive prowess, comprehension, and intelligence is not, then, just a philosophical problem. It is a real social problem, and we should alert ourselves to it and take steps to avert it.Postscript: Eyes, Ears, Hands, and HistoryMy philosophical conclusion in this paper is that any computer that actually passed the Turing test would be a thinker in every theoretically interesting sense. This conclusion seems to some people to fly in the face of what I have myself argued on other occasions. Peter Bieri, commenting on this paper at Boston University, noted that I have often claimed to show the importance to genuine understanding of a rich and intimate perceptual interconnection between an entity and its surrounding world–the need for something like eyes and ears–and a similarly complex active engagement with elements in that world–the need for something like hands with which to do things in that world. Moreover,I have often held that only a biography of sorts–a history of actual projects, learning experiences, and other bouts with reality–could produce the sorts of complexities (both external, or behavioral, and internal) that are needed to ground a principled interpretation of an entity as a thinker, an entity with beliefs, desires, intentions, and other mental attitudes.But the opaque screen in the Turing test discounts or dismisses these factors altogether, it seems, by focusing attention on only the contemporaneous capacity to engage in one very limited sort of activity: verbal communication. (I have even coined a pejorative label for such purely language-using systems: “bedridden.”)Am I going back on my earlier claims? Not at all. I am merely pointing out that the Turing test is so powerful that it will indirectly ensure that these conditions, they are truly necessary, are met by any successful contestant.“You may well be right,” Turing could say, “that eyes, ears, hands, and a history are necessary conditions for thinking. so, then I submit that nothing could pass the Turing test that didn’t have eyes, ears, hands, and a history. That is an empirical claim, which we can someday hope to test. you suggest that these are not just practically or physically necessary but conceptually necessary conditions for thinking, you make a philosophical claim that I for one would not know how, or care, to assess. Isn’t it more interesting and important in the end to discover whether or not it is true that no bedridden system could pass a demanding Turing test?”Suppose we put to Turing the suggestion that he add another component to his test: Not only must an entity win the imitation game; it must also be able to identify–using whatever sensory apparatus it has available to it–a variety of familiar objects placed in its room: a tennis racket, a potted palm, a bucket of yellow paint, a live dog. This would ensure that somehow or other the entity was capable of moving around and distinguishing things in the world.Turing could reply, I assert, that this is an utterly unnecessary addition to his test, making it no more demanding than it already was. A suitably probing conversation would surely establish beyond a shadow of a doubt that the contestant knew its way around in the real world. The imagined alternative of somehow “prestocking” a bedridden, blind computer with enough information and a clever enough program to trick the Turing test is science fiction of the worst kind: possible “in principle” but not remotely possible in fact in view of the combinatorial explosion of possible variation such a system would have to cope with.“But suppose you’re wrong. What would you say of an entity that was created all at once (by some programmers, perhaps), an instant individual with all the conversational talents of an embodied, experienced human being?” This is like the question, Would you call a hunk of H20 that was as hard as steel at room temperature ice? I do not know what Turing would say, of course, so I will speak for myself.Faced with such an improbable violation of what 1 take to be the laws of nature, I would probably be speechless. The least of my worries would 6e about which lexicographical leap to take, whether to say, “it turns out, to my amazement, that something can think without having had the benefit of eyes, ears, hands, and a history” or “it turns out, to my amazement, that something can pass the Turing test without thinking.” Choosing between these ways of expressing my astonishment would be asking myself a question too meaningless to deserve discussion.DiscussionQuestion: Why was Turing interested in differentiating a man from a woman in his famous test?Answer: That was just an example. He described a parlor game in which a man would try to fool the judge by answering questions as a woman would answer. l suppose that Turing was playing on the idea that maybe, just maybe, there is a big difference between the way men think and the way women think. But of course they’re both thinkers. He wanted to use that fact to make us realize that, even there were clear differences between the way a computer and a person thought, they’d both still be thinking.Question: Why does it seem that some people are upset by AI research? Does AI research threaten our self-esteem?Answer: I think Herb Simon has already given the canniest diagnosis of that. For many people the mind is the last refuge of mystery against the encroaching spread of science, and they don’t like the idea of science engulfing the last hit of terra incognito. This means that they are threatened, I think irrationally, by the prospect that researchers in artificial intelligence may come to understand the human mind as well as biologists understand the genetic code and physicists understand electricity and magnetism. This could lead to the “evil scientist” (to take a stock character from science fiction) who can control you because he or she has a deep understanding of what’s going on in your mind.This seems to me to be a totally valueless fear, one that you can set aside for the simple reason that the human mind is full of an extraordinary amount of detailed knowledge, as Roger Schenk, for example, has been pointing out. As long as the scientist who is attempting to manipulate you does not share all your knowledge, his or her chances of manipulating you are minimal. People can always hit you over the head. They can do that now. We don’t need artificial intelligence to manipulate people by putting them in chains or torturing them. But someone tries to manipulate you by controlling your thoughts and ideas, that person will have to know what you know and more. The best way to keep yourself safe from that kind of manipulation is to be well informed.Question: Do you think we will be able to program self-consciousness into a computer?Answer: Yes, I do think that it’s possible to program self-consciousness into a computer. “Self-consciousness” can mean many things. you take the simplest, crudest notion of self-consciousness, I suppose that would be the sort of self consciousness that a lobster has: When it’s hungry, it eats something, but it never eats itself. It has some way of distinguishing between itself and the rest of the world, and it has a rather special regard for itself. The lowly lobster is, in one regard, self-conscious.You want to know whether or not you can create that on the computer, the answer is yes. It’s no trouble at all. The computer is already a self-watching, self-monitoring sort of thing. That is an established part of the technology. But, of course, most people have something more in mind when they speak of self-consciousness. It is that special inner light, that private way that it is with you that nobody else can share, something that is forever outside the bounds of computer science.How could a computer ever be conscious in this sense? That belief, that very gripping, powerful intuition, is in the end, I think, simply an illusion of common sense. It is as gripping as the commonsense illusion that the earth stands still and the sun goes around the earth. But the only way that those of us who do not believe in the illusion will ever convince the general public that it is an illusion is by gradually unfolding a very difficult and fascinating story about just what is going on in our minds.In the interim, people like me, philosophers who have to live by our wits and tell a lot of stories use what I call intuition pumps, little examples that help to free up the imagination. I simply want to draw your attention to one fact.You look at a computer, I don’t care whether it’s a giant Cray or a personal computer, you open up the box and look inside and see those chips, you say, “No way could that be conscious. No way could that be self-conscious.” But the same thing is true if you take the top off somebody’s skull and look at the gray matter pulsing away in there. You think, “That is conscious? No way could that lump of stuff be conscious.” Of course, it makes no difference whether you look at it with a microscope or with the naked eye.At no level of inspection does a brain look like the seat of consciousness. Therefore, don’t expect a computer to look like the seat of consciousness. You want to get a grasp of how a computer could be conscious, it’s no more difficult in the end than getting a grasp of how a brain could be conscious. When we develop good accounts of consciousness, it will no longer seem so obvious to everyone that the idea of a self-conscious computer is a contradiction in terms. At the same time, I doubt that there will ever be self-conscious robots, but for boring reasons. There won’t be any point in making them.Theoretically, could we make a gall bladder out of atoms? In principle, we could. A gall bladder is just a collection of atoms, but manufacturing one would cost the moon. It would be more expensive than every project NASA has ever dreamed of, and there would be no scientific payoff. We wouldn’t learn anything new about how gall bladders work.For the same reason I don’t think we’re going to see really humanoid robots, because practical, cost-effective robots don’t need to be very humanoid at all. They need to be like the robots you can already see at General Motors, or like boxy little computers that do special-purpose things.The theoretical issues will be studied by AI researchers looking at models that, to the layman, will show very little sign of humanity at all, and it will be only by rather indirect arguments that anyone will be able to appreciate that these models cast light on the deep theoretical question of how the mind is organized.Footnotes1. Alan M. Turing, “Computing Machinery and Intelligence,” Mind 59 (1950).2. Rene Descartes, Discourse on Method (1637), trans. Lawrence LaFleur (New York: Bobbs-Merrill,1960).3. Terry Winograd, Understanding Natural Language (New York: Academic Press, 1972).4. Ned Block, “Psychologism and Behaviorism, Philosophical Review, 1982.5. John Haugeland, Mind Design (Cambridge, Mass.: MIT Press, 1981), p. 16.6. Joseph Weizenbaum, CACM17, no. 9 (September 1974): 543.7. I thank Kenneth Colby for providing me with the complete transcripts (including the judges’ commentaries and reactions) from which these exchanges are quoted. The first published account of the experiment is Jon F. Raiser, Kenneth Mark Colby, William S. Faught, and Roger C. Parkinson, “Can Psychiatrists Distinguish a Computer Simulation of Paranoia from the Real Thing? The Limitations of Turing-like Tests as Measures of the Adequacy of Simulations,” in Journal of Psychiatric Research 15, no. 3 (1980):149-162. Colby discusses PARRY and its implications in “Modeling a Paranoid Mind,” in Behavioral and Brain Sciences 4, no. 4 (1981): 515-560.Courtesy of Tufts UniversityDaniel Dennett is Distinguished Arts and Science Professor and Director of the Center for Cognitive Studies at Tufts University. He is the author or editor of a number of books on cognitive science and the philosophy of mind, including The Mind’s I, coedited with Douglans Hofstadter (1981); Elbow Room (1984); and The Intentiional Stance (1987).


/ 023. O'Malley JP. "On Consciousness in Silicon Systems: Is reality non-virtual?" 2023.

"When it comes to consciousness, I call the problems of explaining intelligence, and explaining behaviour in general, the easy problems,” David Chalmers says. The Australian philosopher and cognitive scientist has made it his life’s work to take on some of the big questions relating to the nature of reality, the relationship between mind and body, and consciousness in the age of digital technology."

Although Chalmers admits his ‘easy’ problems are not so easy after all, we at least have some idea of how to go about solving them, he says. They can be explained by computational or neural mechanisms like, say, the ability to discriminate, categorise, and react to environmental stimuli; the reportability of mental states; the focus of attention; the deliberate control of behaviour; and the difference between wakefulness and sleep.

The so-called hard problem of consciousness is something that is much more difficult to solve: “The hard problem is really the problem of explaining how physical processes in the brain could give rise to conscious, subjective, experience,” says Chalmers, who currently holds the position as Professor of Philosophy and Neural science at New York University, where he also works as co-director of the school’s Center for Mind, Brain, and Consciousness.

It is a fundamental question in both philosophy and cognitive science, and one that Chalmers has grappled with for decades. When he first used the phrase “hard problem of consciousness” in April 1994, at a talk he delivered in Tucson, Arizona, it made a significant impact among intellectuals in the philosophical and scientific community. Then came his first book The Conscious Mind (1996) in which he defined the hard problem of consciousness as no less than “the biggest mystery and the largest outstanding obstacle in our quest for a scientific understanding of the universe.”

The book, which was greatly influential in its time, also claimed that no explanation of consciousness is possible in purely physical terms. Standard methods of neuroscience and cognitive science are always faced with having to explain consciousness on a more fundamental level, Chalmers argued.

To solve the hard problem, Chalmers believes science needs something more than physical explanations, which mostly only go on to explain objective structures and dynamics. If science can’t explain consciousness in terms of existing fundamental properties (space, time, mass, and so on) and existing fundamental physical laws, then it might need new fundamental properties found in nature, he believes. Perhaps consciousness is itself fundamental, he speculates.

Chalmers has dedicated much of his career to the topic of consciousness. But over the last few years he became interested in another fundamental philosophical question: what is reality? These two mysteries, he claims, are inextricably linked. “Consciousness is part of reality, so if you want an explanation of reality, you’d better be able to understand consciousness,” he says.

Chalmers explores this connection in a new book, Reality+: Virtual Worlds and the Problems of Philosophy, in which he argues that technologically simulated realities, like those found in virtual reality, are just as genuine as physical reality. That is, virtual worlds are not illusions. In fact, Chalmers believes they can provide just as much meaning and value as the physical world can.

Virtual reality, to Chalmers, becomes a way to engage with some of the deep questions that have troubled philosophers for centuries. He points to the French philosopher René Descartes, who back in the 17th century was already beginning to pose questions about the relationship between the mind and reality. Descartes also raised the issue of what Chalmers calls “the problem of the external world”: How do you know anything at all about the reality that exists outside you? The philosopher famously found himself unable to rule out the possibility that everything he experienced was a dream, and that ‘reality’ therefore was, put in modern terms, a simulation of sorts.

For many years Chalmers thought he didn’t have much to say about this question. But thinking about virtual reality when writing his book gave him a new perspective on this topic, he says.

Although today’s virtual reality worlds are primitive, Chalmers admits, he believes their temporary technological limitations will pass, and that they will eventually become indistinguishable from the nonvirtual world.

Perhaps we’ll eventually plug into machines through a brain-computer interface, bypassing our eyes and ears and other sense organs. Chalmers claims the simulated environments that await us in the future may even be occupied by simulated people, with simulated brains and bodies, who will undergo the whole process of birth, development, aging, and death.

“Some hold the view that consciousness is essentially biological. But in principle, I don’t see why silicon systems cannot achieve it.”

As the technology develops and virtual worlds become increasingly sophisticated, the philosopher predicts that we will eventually be faced with a crucial question: should we move our lives entirely to a virtual world? “The short answer is yes,” Chalmers says. “There is no difference, in principle, between meaning and value in both the physical and virtual worlds. So, there is no barrier preventing us from living morally and ethically in a virtual world.”

This leads us back to Descartes’ dream simulation. Speculating about the future of virtual reality leads Chalmers to pose the question: how do you know you’re not in a virtual simulation right now?

This idea, known as simulation hypothesis, is one that Chalmers takes very seriously. Popularised by philosopher Nick Bostrom, and famously depicted in qÜÉ=j~íêáñ movies, the idea posits that our entire existence is, in fact, a simulated reality – and what seems on a surface level to be an ordinary physical world turns out to be the result of connecting human brains to a giant bank of computers.

“I would say there is a 10 percent probability that we are living a simulation,” Chalmers says. “But it would be very hard to demonstrate. If it’s a perfect simulation that’s indistinguishable from our own world, then no scientific experiment will ever be able to prove this, and it will remain a philosophical hypothesis, rather than a scientific hypothesis.”

Chalmers believes science fiction provides philosophers with great thinking tools and thought experiments that can be used to envision these kinds of mind warping hypotheticals. He points to qÜÉ=j~íêáñ as being partly responsible for his own “entry into the simulation arena.” The filmmakers had a significant interest in philosophy, and shortly after the movie was released, several philosophers were invited to write articles on the movie’s website. Chalmers accepted the invitation. In 2003, he published an article entitled “The Matrix as Metaphysics” which argued that the central premise of qÜÉ j~íêáñ movie might, in fact, not be an illusion.

Star Trek also gets a mention in Chalmers’ new book. In a chapter entitled “Can there be consciousness in a digital world?” the author analyses an episode of the tv show in which a trial is held to determine if the android Data is sentient. One character, Captain Picard, asks the court to define the term ‘sentient’, to which Starfleet cyberneticist, Bruce Maddox, replies: “Intelligent, self-aware, and conscious.”

This episode raises an interesting question, Chalmers points out: can a digital system like Data be conscious, or is that a trait that’s reserved to humans and animals? “Some hold the view that consciousness is essentially biological,” he says. “But in principle, I don’t see why silicon systems cannot achieve it.”

Chalmers takes this idea a few steps further. Once we have consciousness in a functional reproduction of the brain – say, a silicon brain – it would be a very small step from there to having consciousness in a simulated brain. A simulated brain would have the advantage of maximising similarity to a human brain, Chalmers explains. In such a device, every neuron would be simulated perfectly, as would all other cells throughout the brain. All the electrochemical activity, meanwhile, would be simulated too, as would any other bodily activity, such as blood flow.

“So, think about replacing say, biological neurons, gradually, with silicon chips, or some other substrate in the brain, while keeping the information processing the same way, because that would preserve consciousness over into machine consciousness,” Chalmers says. “In fact, given how quickly artificial intelligence is developing right now, I think we could have consciousness in machines in the next few years.”

In principle, then, could consciousness be uploaded into a computer? It’s a concept often discussed in the transhumanism community and referred to as ‘mind uploading’. According to Chalmers, this might be possible, although not anytime soon.

“By building a very detailed simulation of the brain, we would potentially be able to take the contents of the brain and totally upload them to a computer system,” Chalmers explains. “We are still not able to build that kind of simulation. But with advances in neuroscience, maybe in a few decades, mind uploading could be possible. Maybe we’ll build backups of ourselves in case something goes wrong in our life, and we’ll be able to restore ourselves from backups, or perhaps when the brain is dying around the end of life, one will have the ability to upload themselves to the cloud with a silicon brain,” he says.

Philosophically it’s an interesting idea. But will it work? And, more importantly, if you upload your brain to a computer system, what kind of identity will that information represent? Will it be conscious? Can it be called an individual? Will there be anybody home, from the first-person perspective?

Chalmers doesn’t claim to have definite answers to difficult questions like these. But philosophy, as he keeps reminding me, naturally contains more paradoxical perplexities and less clear-cut answers than clinical definitions. “These are all very deep philosophical questions about personal identity,” Chambers concludes. “But once the right technology becomes available, I think we’ve got a fairly extensive philosophical analysis to figure out whether we want to use that technology or not.”

/ 024. Bridle, James. "Rise of the machines: has technology evolved beyond our control?" 2018.

Technology is starting to behave in intelligent and unpredictable ways that even its creators don’t understand. As machines increasingly shape global events, how can we regain control?

The voice-activated gadget in the corner of your bedroom suddenly laughs maniacally, and sends a recording of your pillow talk to a colleague. The clip of Peppa Pig your toddler is watching on YouTube unexpectedly descends into bloodletting and death. The social network you use to keep in touch with old school friends turns out to be influencing elections and fomenting coups.

Something strange has happened to our way of thinking – and as a result, even stranger things are happening to the world. We have come to believe that everything is computable and can be resolved by the application of new technologies. But these technologies are not neutral facilitators: they embody our politics and biases, they extend beyond the boundaries of nations and legal jurisdictions and increasingly exceed the understanding of even their creators. As a result, we understand less and less about the world as these powerful technologies assume more control over our everyday lives.

Across the sciences and society, in politics and education, in warfare and commerce, new technologies are not merely augmenting our abilities, they are actively shaping and directing them, for better and for worse. If we do not understand how complex technologies function then their potential is more easily captured by selfish elites and corporations. The results of this can be seen all around us. There is a causal relationship between the complex opacity of the systems we encounter every day and global issues of inequality, violence, populism and fundamentalism.

Instead of a utopian future in which technological advancement casts a dazzling, emancipatory light on the world, we seem to be entering a new dark age characterised by ever more bizarre and unforeseen events. The Enlightenment ideal of distributing more information ever more widely has not led us to greater understanding and growing peace, but instead seems to be fostering social divisions, distrust, conspiracy theories and post-factual politics. To understand what is happening, it’s necessary to understand how our technologies have come to be, and how we have come to place so much faith in them.

In the 1950s, a new symbol began to creep into the diagrams drawn by electrical engineers to describe the systems they built: a fuzzy circle, or a puXall, or a thought bubble. Eventually, its form settled into the shape of a cloud. Whatever the engineer was working on, it could connect to this cloud, and that’s all you needed to know. The other cloud could be a power system, or a data exchange, or another network of computers. Whatever. It didn’t matter. The cloud was a way of reducing complexity, it allowed you to focus on the issues at hand. Over time, as networks grew larger and more interconnected, the cloud became more important. It became a business buzzword and a selling point. It became more than engineering shorthand; it became a metaphor.

Today the cloud is the central metaphor of the internet: a global system of great power and energy that nevertheless retains the aura of something numinous, almost impossible to grasp. We work in it; we store and retrieve stuff from it; it is something we experience all the time without really understanding what it is. But there’s a problem with this metaphor: the cloud is not some magical faraway place, made of water vapour and radio waves, where everything just works. It is a physical infrastructure consisting of phone lines, fibre optics, satellites, cables on the ocean floor, and vast warehouses filled with computers, which consume huge amounts of water and energy. Absorbed into the cloud are many of the previously weighty edifices of the civic sphere: the places where we shop, bank, socialise, borrow books and vote. Thus obscured, they are rendered less visible and less amenable to critique, investigation, preservation and regulation.

Over the last few decades, trading floors around the world have fallen silent, as people are replaced by banks of computers that trade automatically. Digitisation meant that trades within, as well as between, stock exchangescould happen faster and faster. As trading passed into the hands of machines, it became possible to react almost instantaneously. High- Frequency Trading (HFT) algorithms, designed by former physics PhD students to take advantage of millisecond advantages, entered the market, and traders gave them names such as The Knife. These algorithms were capable of eking out fractions of a cent on every trade, and they could do it millions of times a day.

Something deeply weird is occurring within these massively accelerated, opaque markets. On 6 May 2010, the Dow Jones opened lower than the previous day, falling slowly over the next few hours in response to the debt crisis in Greece. But at 2.42pm, the index started to fall rapidly. In less than five minutes, more than 600 points were wiped off the market. At its lowest point, the index was nearly 1,000 points below the previous day’s average, a difference of almost 10% of its total value, and the biggest single-day fall in the market’s history. By 3.07pm, in just 25 minutes, it recovered almost all of those 600 points, in the largest and fastest swing ever.

In the chaos of those 25 minutes, 2bn shares, worth $56bn, changed hands. Even more worryingly, many orders were executed at what the Securities Exchange Commission called “irrational prices”: as low as a penny, or as high as $100,000. The event became known as the “flash crash”, and it is still being investigated and argued over years later.

One report by regulators found that high-frequency traders exacerbated the price swings. Among the various HFT programs, many had hard-coded sell points: prices at which they were programmed to sell their stocks immediately. As prices started to fall, groups of programs were triggered to sell at the same time. As each waypoint was passed, the subsequent price fall triggered another set of algorithms to automatically sell their stocks, producing a feedback effect. As a result, prices fell faster than any human trader could react to. While experienced market players might have been able to stabilise the crash by playing a longer game, the machines, faced with uncertainty, got out as quickly as possible.

Other theories blame the algorithms for initiating the crisis. One technique that was identified in the data was HFT programmes sending large numbers of “non-executable” orders to the exchanges – that is, orders to buy or sell stocks so far outside of their usual prices that they would be ignored. The purpose of such orders is not to actually communicate or make money, but to deliberately cloud the system, so that other, more valuable trades can be executed in the confusion. Many orders that were never intended to be executed were actually fulfilled, causing wild volatility.

Flash crashes are now a recognised feature of augmented markets, but are still poorly understood. In October 2016, algorithms reacted to negative news headlines about Brexit negotiations by sending the pound down 6% against the dollar in under two minutes, before recovering almost immediately. Knowing which particular headline, or which particular algorithm, caused the crash is next to impossible. When one haywire algorithm started placing and cancelling orders that ate up 4% of all traffic in US stocks in October 2012, one commentator was moved to comment wryly that “the motive of the algorithm is still unclear”.

At 1.07pm on 23 April 2013 Associated Press sent a tweet to its 2 million followers: “Breaking: Two Explosions in the White House and Barack Obama is injured.” The message was the result of a hack later claimed by the Syrian Electronic Army, a group affiliated to Syrian president Bashar al-Assad. AP and other journalists quickly flooded the site with alerts that the message was false. The algorithms following breaking news stories had no such discernment, however. At 1.08pm, the Dow Jones went into a nosedive. Before most human viewers had even seen the tweet, the index had fallen 150 points in under two minutes, and bounced back to its earlier value. In that time, it erased $136bn in equity market value.

Computation is increasingly layered across, and hidden within, every object in our lives, and with its expansion comes an increase in opacity and unpredictability. One of the touted benefits of Samsung’s line of “smart fridges” in 2015 was their integration with Google’s calendar services, allowing owners to schedule grocery deliveries from the kitchen. It also meant that hackers who gained access to the then inadequately secured machines could read their owner’s Gmail passwords. Researchers in Germany discovered a way to insert malicious code into Philips’s wifienabled Hue lightbulbs, which could spread from fixture to fixture throughout a building or even a city, turning the lights rapidly on and off and – in one possible scenario – triggering photosensitive epilepsy. This is the approach favoured by Byron the Bulb in Thomas Pynchon’s Gravity’s Rainbow, an act of grand revolt by the little machines against the tyranny of their makers. Once-fictional possibilities for technological violence are being realised by the Internet of Things.

In Kim Stanley Robinson’s novel Aurora, an intelligent spacecraft carries a human crew from Earth to a distant star. The journey will take multiple lifetimes, so one of the ship’s jobs is to ensure that the humans look after themselves. When their fragile society breaks down, threatening the mission, the ship deploys safety systems as a means of control: it is able to see everywhere through sensors, open or seal doors at will, speak so loudly through its communications equipment that it causes physical pain, and use fire suppression systems to draw down the level of oxygen in a particular space.

This is roughly the same suite of operations available now from Google Home and its partners: a network of internet-connected cameras for home security, smart locks on doors, a thermostat capable of raising and lowering the temperature in individual rooms, and a fire and intruder detection system that emits a piercing emergency alarm. Any successful hacker would have the same powers as the Aurora does over its crew, or Byron over his hated masters.

Before dismissing such scenarios as the fever dreams of science fiction writers, consider again the rogue algorithms in the stock exchanges. These are not isolated events, but everyday occurrences within complex systems. The question then becomes, what would a rogue algorithm or a flash crash look like in the wider reality?

Would it look, for example, like Mirai, a piece of software that brought down large portions of the internet for several hours on 21 October 2016? When researchers dug into Mirai, they discovered it targets poorly secured internet connected devices – from security cameras to digital video recorders – and turns them into an army of bots. In just a few weeks, Mirai infected half a million devices, and it needed just 10% of that capacity to cripple major networks for hours.

Mirai, in fact, looks like nothing so much as Stuxnet, another virus discovered within the industrial control systems of hydroelectric plants and factory assembly lines in 2010. Stuxnet was a military-grade cyberweapon; when dissected, it was found to be aimed specifically at Siemens centrifuges, and designed to go off when it encountered a facility that possessed a particular number of such machines. That number corresponded with one particular facility: the Natanz nuclear facility in Iran. When activated, the program would quietly degrade crucial components of the centrifuges, causing them to break down and disrupt the Iranian enrichment programme.

The attack was apparently partially successful, but the effect on other infected facilities is unknown. To this day, despite obvious suspicions, nobody knows where Stuxnet came from, or who made it. Nobody knows for certain who developed Mirai, either, or where its next iteration might come from, but it might be there, right now, breeding in the CCTV camera in your office, or the wifi-enabled kettle in the corner of your kitchen.

Or perhaps the crash will look like a string of blockbuster movies pandering to rightwing conspiracies and survivalist fantasies, from quasi-fascist superheroes (Captain America and the Batman series) to justifications of torture and assassination (Zero Dark Thirty, American Sniper). In Hollywood, studios run their scripts through the neural networks of a company called Epagogix, a system trained on the unstated preferences of millions of moviegoers developed over decades in order to predict which lines will push the right – meaning the most lucrative – emotional buttons. Algorithmic engines enhanced with data from Netflix, Hulu, YouTube and others, with access to the minute-by-minute preferences of millions of video watchers acquire a level of cognitive insight undreamed of by previous regimes. Feeding directly on the frazzled, binge-watching desires of news-saturated consumers, the network turns on itself, reflecting, reinforcing and heightening the paranoia inherent in the system.

Game developers enter endless cycles of updates and in-app purchases directed by A/B testing interfaces and real-time monitoring of players’ behaviours. They have such a fine-grained grasp of dopamine-producing neural pathways that teenagers die of exhaustion in front of their computers, unable to tear themselves away.

Or perhaps the flash crash will look like literal nightmares broadcast across the network for all to see? In the summer of 2015, the sleep disorders clinic of an Athens hospital was busier than it had ever been: the country’s debt crisis was in its most turbulent period. Among the patients were top politicians and civil servants, but the machines they spent the nights hooked up to, monitoring their breathing, their movements, even the things they said out loud in their sleep, were sending that information, together with their personal medical details, back to the manufacturers’ diagnostic data farms in northern Europe. What whispers might escape from such facilities?

We are able to record every aspect of our daily lives by attaching technology to the surface of our bodies, persuading us that we too can be optimised and upgraded like our devices. Smart bracelets and smartphone apps with integrated step counters and galvanic skin response monitors track not only our location, but every breath and heartbeat, even the patterns of our brainwaves. Users are encouraged to lay their phones beside them on their beds at night, so that their sleep patterns can be recorded. Where does all this data go, who owns it, and when might it come out? Data on our dreams, our night terrors and early morning sweating jags, the very substance of our unconscious selves, turn into more fuel for systems both pitiless and inscrutable.

Or perhaps the flash crash in reality looks exactly like everything we are experiencing right now: rising economic inequality, the breakdown of the nation-state and the militarisation of borders, totalising global surveillance and the curtailment of individual freedoms, the triumph of transnational corporations and neurocognitive capitalism, the rise of far-right groups and nativist ideologies, and the degradation of the natural environment. None of these are the direct result of novel technologies, but all of them are the product of a general inability to perceive the wider, networked effects of individual and corporate actions accelerated by opaque, technologically augmented complexity.

In New York in 1997, world chess champion Garry Kasparov faced off for the second time against Deep Blue, a computer specially designed by IBM to beat him. When he lost, he claimed some of Deep Blue’s moves were so intelligent and creative that they must have been the result of human intervention. But we understand why Deep Blue made those moves: its process for selecting them was ultimately one of brute force, a massively parallel architecture of 14,000 custom-designed chess chips, capable of analysing 200m board positions per second. Kasparov was not outthought, merely outgunned.

By the time the Google Brain–powered AlphaGo software took on the Korean professional Go player Lee Sedol in 2016, something had changed. In the second of five games, AlphaGo played a move that stunned Sedol, placing one of its stones on the far side of the board. “That’s a very strange move,” said one commentator. “I thought it was a mistake,” said another. Fan Hui, a seasoned Go player who had been the first professional to lose to the machine six months earlier, said: “It’s not a human move. I’ve never seen a human play this move.”

AlphaGo went on to win the game, and the series. AlphaGo’s engineers developed its software by feeding a neural network millions of moves by expert Go players, and then getting it to play itself millions of times more, developing strategies that outstripped those of human players. But its own representation of those strategies is illegible: we can see the moves it made, but not how it decided to make them.

The late Iain M Banks called the place where these moves occurred “Infinite Fun Space”. In Banks’s SF novels, his Culture civilisation is administered by benevolent, superintelligent AIs called simply Minds. While the Minds were originally created by humans, they have long since redesigned and rebuilt themselves and become all-powerful. Between controlling ships and planets, directing wars and caring for billions of humans, the Minds also take up their own pleasures. Capable of simulating entire universes within their imaginations, some Minds retreat for ever into Infinite Fun Space, a realm of meta-mathematical possibility, accessible only to superhuman artificial intelligences.

Many of us are familiar with Google Translate, which was launched in 2006, using a technique called statistical language inference. Rather than trying to understand how languages actually worked, the system imbibed vast corpora of existing translations: parallel texts with the same content in different languages. By simply mapping words on to one another, it removed human understanding from the equation and replaced it with data-driven correlation.

Translate was known for its humorous errors, but in 2016, the system started using a neural network developed by Google Brain, and its abilities improved exponentially. Rather than simply cross-referencing heaps of texts, the network builds its own model of the world, and the result is not a set of two-dimensional connections between words, but a map of the entire territory. In this new architecture, words are encoded by their distance from one another in a mesh of meaning – a mesh only a computer could comprehend.

While a human can draw a line between the words “tank” and “water” easily enough, it quickly becomes impossible to draw on a single map the lines between “tank” and “revolution”, between “water” and “liquidity”, and all of the emotions and inferences that cascade from those connections. The map is thus multidimensional, extending in more directions than the human mind can hold. As one Google engineer commented, when pursued by a journalist for an image of such a system: “I do not generally like trying to visualise thousand-dimensional vectors in three-dimensional space.” This is the unseeable space in which machine learning makes its meaning. Beyond that which we are incapable of visualising is that which we are incapable of even understanding.

In the same year, other researchers at Google Brain set up three networks called Alice, Bob and Eve. Their task was to learn how to encrypt information. Alice and Bob both knew a number – a key, in cryptographic terms – that was unknown to Eve. Alice would perform some operation on a string of text, and then send it to Bob and Eve. If Bob could decode the message, Alice’s score increased; but if Eve could, Alice’s score decreased.

Over thousands of iterations, Alice and Bob learned to communicate without Eve breaking their code: they developed a private form of encryption like that used in private emails today. But crucially, we don’t understand how this encryption works. Its operation is occluded by the deep layers of the network. What is hidden from Eve is also hidden from us. The machines are learning to keep their secrets.

How we understand and think of our place in the world, and our relation to one another and to machines, will ultimately decide where our technologies will take us. We cannot unthink the network; we can only think through and within it. The technologies that inform and shape our present perceptions of reality are not going to go away, and in many cases we should not wish them to. Our current life support systems on a planet of 7.5 billion people and rising depend on them. Our understanding of those systems, and of the conscious choices we make in their design, remain entirely within our capabilities. We are not powerless, not without agency. We only have to think, and think again, and keep thinking. The network – us and our machines and the things we think and discover together – demands it.

Computational systems, as tools, emphasise one of the most powerful aspects of humanity: our ability to act effectively in the world and shape it to our desires. But uncovering and articulating those desires, and ensuring that they do not degrade, overrule, efface, or erase the desires of others, remains our prerogative.

When Kasparov was defeated back in 1997, he didn’t give up the game. A year later, he returned to competitive play with a new format: advanced, or centaur, chess. In advanced chess, humans partner, rather than compete, with machines. And it rapidly became clear that something very interesting resulted from this approach. While even a mid-level chess computer can today wipe the floor with most grandmasters, an average player paired with an average computer is capable of beating the most sophisticated supercomputer – and the play that results from this combination of ways of thinking has revolutionised the game. It remains to be seen whether cooperation is possible – or will be permitted – with the kinds of complex machines and systems of governance now being developed, but understanding and thinking together offer a more hopeful path forward than obfuscation and dominance.

Our technologies are extensions of ourselves, codified in machines and infrastructures, in frameworks of knowledge and action. Computers are not here to give us all the answers, but to allow us to put new questions, in new ways, to the universe


/ 025. Future of Life Institute. "Policymaking in the Pause: What can policymakers do now to combat risks from advanced AI systems?" 2023.


Introduction

Prominent AI researchers have identified a range of dangers that may arise from the present and future generations of advanced AI systems if they are left unchecked. AI systems are already capable of creating misinformation and authentic-looking fakes that degrade the shared factual foundations of society and inflame political tensions.1 AI systems already show a tendency toward amplifying entrenched discrimination and biases, further marginalizing disadvantaged communities and diverse viewpoints.2 The current, frantic rate of development will worsen these problems significantly.

As these types of systems become more sophisticated, they could destabilize labor markets and political institutions, and lead to the concentration of enormous power in the hands of a small number of unelected corporations. Advanced AI systems could also threaten national security, e.g., by facilitating the inexpensive development of chemical, biological, and cyber weapons by non-state groups. The systems could themselves pursue goals, either human- or self-assigned, in ways that place negligible value on human rights, human safety, or, in the most harrowing scenarios, human existence.

In an effort to stave off these outcomes, the Future of Life Institute (FLI), joined by over 20,000 leading AI researchers, professors, CEOs, engineers, students, and others on the frontline of AI progress, called for a pause of at least six months on the riskiest and most resource- intensive AI experiments – those experiments seeking to further scale up the size and general capabilities of the most powerful systems developed to date.

The proposed pause provides time to better understand these systems, to reflect on their ethical, social, and safety implications, and to ensure that AI is developed and used in a responsible manner. The unchecked competitive dynamics in the AI industry incentivize aggressive development at the expense of caution5. In contrast to the breakneck pace of development, however, the levers of governance are generally slow and deliberate. A pause on the production of even more powerful AI systems would thus provide an important opportunity for the instruments of governance to catch up with the rapid evolution of the field.

We have called on AI labs to institute a development pause until they have protocols in place to ensure that their systems are safe beyond a reasonable doubt, for individuals, communities, and society. Regardless of whether the labs will heed our call, this policy brief provides policymakers with concrete recommendations for how governments can manage AI risks.

The recommendations are by no means exhaustive: the project of AI governance is perennial and will extend far beyond any pause. Nonetheless, implementing these recommendations, which largely reflect a broader consensus among AI policy experts, will establish a strong governance foundation for AI.

Policy recommendations:

1.   Mandate robust third-party auditing and certification.
2.   Regulate access to computational power.
3.   Establish capable AI agencies at the national level.
4.   Establish liability for AI-caused harms.
5.   Introduce measures to prevent and track AI model leaks.
6.   Expand technical AI safety research funding.
7.   Develop standards for identifying and managing AI-generated content and recommendations.

1. Mandate robust third-party auditing and certification for specific AI systems

For some types of AI systems, the potential to impact the physical, mental, and financial wellbeing of individuals, communities, and society is readily apparent. For example, a credit scoring system could discriminate against certain ethnic groups. For other systems – in particular general-purpose AI systems6 – the applications and potential risks are often not immediately evident. General-purpose AI systems trained on massive datasets also have unexpected (and often unknown) emergent capabilities.7

In Europe, the draft AI Act already requires that, prior to deployment and upon any substantial modification, ‘high-risk’ AI systems undergo ‘conformity assessments’ in order to certify compliance with specified harmonized standards or other common specifications.8 In some cases, the Act requires such assessments to be carried out by independent third-parties to avoid conflicts of interest.

In contrast, the United States has thus far established only a general, voluntary framework for AI risk assessment.9 The National Institute of Standards and Technology  (NIST), in coordination with various stakeholders, is developing so-called ‘profiles’ that will provide specific risk assessment and mitigation guidance for certain types of AI systems, but this framework still allows organizations to simply ‘accept’ the risks that they create for society instead of addressing them. In other words, the United States does not require any third-party risk assessment or risk mitigation measures before a powerful AI system can be deployed at scale.

To ensure proper vetting of powerful AI systems before deployment, we recommend a robust independent auditing regime for models that are general-purpose, trained on large amounts of compute, or intended for use in circumstances likely to impact the rights or the wellbeing of individuals, communities, or society. This mandatory third-party auditing and certification scheme could be derived from the EU’s proposed ‘conformity assessments’ and should be adopted by jurisdictions worldwide10.

In particular, we recommend third-party auditing of such systems across a range of benchmarks for the assessment of risks11, including possible weaponization12 and unethical behaviors13 and mandatory certification by accredited third-party auditors before these high-risk systems can be deployed. Certification should only be granted if the developer of the system can demonstrate that appropriate measures have been taken to mitigate risk, and that any residual risks deemed tolerable are disclosed and are subject to established protocols for minimizing harm.

2. Regulate organizations’ access to computational power

At present, the most advanced AI systems are developed through training that requires an enormous amount of computational power - ‘compute’ for short. The amount of compute used to train a general-purpose system largely correlates with its capabilities, as well as the magnitude of its risks.

Today’s most advanced models, like OpenAI’s GPT-4 or Google’s PaLM, can only be trained with thousands of specialized chips running over a period of months. While chip innovation and better algorithms will reduce the resources required in the future, training the most powerful AI systems will likely remain prohibitively expensive to all but the best-resourced players.

In practical terms, compute is more easily monitored and governed than other AI inputs, such as talent, data, or algorithms. It can be measured relatively easily and the supply chain for advanced AI systems is highly centralized, which means governments can leverage such measures in order to limit the harms of large-scale models.14
To prevent reckless training of the highest risk models, we recommend that governments make access to large amounts of specialized computational power for AI conditional upon the completion of a comprehensive risk assessment. The risk assessment should include a detailed plan for minimizing risks to individuals, communities, and society, consider downstream risks in the value chain, and ensure that the AI labs conduct diligent know-your- customer checks.

Successful implementation of this recommendation will require governments to monitor the use of compute at data centers within their respective jurisdictions.15 The supply chains for AI chips and other key components for high-performance computing will also need to be regulated such that chip firmware can alert regulators to unauthorized large training runs of advanced AI systems.

Through passage of the CHIPS and Science Act of 2022, the United States has instituted licensing requirements for export of many of these components in an effort to monitor and control their global distribution. However, licensing is only required when exporting to certain destinations, limiting the capacity to monitor aggregation of equipment for unauthorized large training runs within the United States and outside the scope export restrictions. Companies within the specified destinations have also successfully skirted monitoring by training AI systems using compute leased from cloud providers.17 We recommend expansion of know- your-customer requirements to all high-volume suppliers for high-performance computing components, as well as providers that permit access to large amounts cloud compute.

3. Establish capable AI agencies at national level

AI is developing at a breakneck pace and governments need to catch up. The establishment of AI regulatory agencies helps to consolidate expertise and reduces the risk of a patchwork approach.

The UK has already established an Office for Artificial Intelligence and the EU is currently legislating for an AI Board. Similarly, in the US, Representative Ted Lieu has announced legislation to create a non-partisan AI Commission with the aim of establishing a regulatory agency. These efforts need to be sped up, taken up around the world and, eventually, coordinated within a dedicated international body.

We recommend that national AI agencies be established in line with a blueprint18 developed by Anton Korinek at Brookings. Korinek proposes that an AI agency have the power to

Monitor public developments in AI progress and define a threshold for which types of advanced AI systems fall under the regulatory oversight of the agency (i.e. systems that develop systems above a certain level of compute or that affect a particularly large group of people).
-     Mandate impact assessments of AI systems on various stakeholders, define reporting requirements for advanced AI companies and audit the impact on people’s rights, wellbeing, and society at large. For example, in systems used for biomedical research, auditors would be asked to evaluate the potential for these systems to create new pathogens.
-     Establish enforcement authority to act upon risks identified in impact assessments and to prevent abuse of AI systems.
-     Publish generalized lessons from the impact assessments such that consumers, workers and other AI developers know what problems to look out for. This transparency will also allow academics to study trends and propose solutions to common problems.

Beyond this blueprint, we also recommend that national agencies around the world mandate record-keeping of AI safety incidents, such as when a facial recognition system causes the arrest of an innocent person. Examples include the non-profit AI Incident Database and the forthcoming EU AI Database created under the European AI Act.19

4. Establish liability for AI-caused harm

AI systems present a unique challenge in assigning liability. In contrast to typical commercial products or traditional software, AI systems can perform in ways that are not well understood by their developers, can learn and adapt after they are sold and are likely to be applied in unforeseen contexts. The ability for AI systems to interact with and learn from other AI systems is expected to expedite the emergence of unanticipated behaviors and capabilities, especially as the AI ecosystem becomes more expansive and interconnected.

Several plug-ins have already been developed that allow AI systems like ChatGPT to perform tasks through other online services (e.g. ordering food delivery, booking travel, making reservations), broadening the range of potential real-world harms that can result from their use and further complicating the assignment of liability.20 OpenAI’s GPT-4   system card references an instance of the system explicitly deceiving a human into bypassing a CAPTCHA bot-detection system using TaskRabbit, a service for soliciting freelance labor.21

When such systems make consequential decisions or perform tasks that cause harm, assigning responsibility for that harm is a complex legal challenge. Is the harmful decision the fault of the AI developer, deployer, owner, end-user, or the AI system itself?

Key among measures to better incentivize responsible AI development is a coherent liability "framework that allows those who develop and deploy these systems to be held responsible for resulting harms. Such a proposal should impose a financial cost for failing to exercise necessary diligence in identifying and mitigating risks, shifting profit incentives away from reckless empowerment of poorly-understood systems toward emphasizing the safety and wellbeing of individuals, communities, and society as a whole.

To provide the necessary financial incentives for profit-driven AI developers to exercise abundant caution, we recommend the urgent adoption of a framework for liability for AI- derived harms. At a minimum, this framework should hold developers of general-purpose AI systems and AI systems likely to be deployed for critical functions22 strictly liable for resulting harms to individuals, property, communities, and society. It should also allow for joint and several liability for developers and downstream deployers when deployment of an AI system that was explicitly or implicitly authorized by the developer results in harm.

5. Introduce measures to prevent and track AI model leaks

Commercial actors may not have sufficient incentives to protect their models, and their cyberdefense measures can often be insufficient. In early March 2023, Meta demonstrated that this is not a theoretical concern, when their model known as LLaMa was leaked to the internet.23 As of the date of this publication, Meta has been unable to determine who leaked the model. This lab leak allowed anyone to copy the model and represented the first time that a major tech firm’s restricted-access large language model was released to the public.

Watermarking  of  AI  models  provides  effective  protection  against  stealing,  illegitimate redistribution  and  unauthorized  application,  because  this  practice  enables  legal  action against identifiable leakers. Many digital media are already protected by watermarking - for example through the embedding of company logos in images or videos. A similar process24 can be applied to advanced AI models, either by inserting information directly into the model parameters or by training it on specific trigger data.

We recommend that governments mandate watermarking for AI models, which will make it easier for AI developers to take action against illegitimate distribution.

6. Expand technical AI safety research funding

The private sector under-invests in research that ensures that AI systems are safe and secure. Despite nearly USD 100 billion of private investment in AI in 2022 alone, it is estimated that only about 100 full-time researchers worldwide are specifically working to ensure AI is safe and properly aligned with human values and intentions.25

In recent months, companies developing the most powerful AI systems have either downsized or entirely abolished their respective ‘responsible AI’ teams.26 While this partly reflects a broader trend of mass layoffs across the technology sector, it nonetheless reveals the relative de- prioritization of safety and ethics considerations in the race to put new systems on the market.

Governments have also invested in AI safety and ethics research, but these investments have primarily focused on narrow applications rather than on the impact of more general AI systems like those that have recently been released by the private sector. The US National Science Foundation (NSF), for example, has established ‘AI Research Institutes’ across a broad range of disciplines. However, none of these institutes are specifically working on the large-scale, societal, or aggregate risks presented by powerful AI systems.

To ensure that our capacity to control AI systems keeps pace with the growing risk that they pose, we recommend a significant increase in public funding for technical AI safety research in the following research domains:

-     Alignment: development of technical mechanisms for ensuring AI systems learn and perform in accordance with intended expectations, intentions, and values.
-     Robustness and assurance: design features to ensure that AI systems responsible for critical functions27 can perform reliably in unexpected circumstances, and that their performance can be evaluated by their operators.
-     Explainability and interpretability: develop mechanisms for opaque models to report the internal logic used to produce output or make decisions in understandable ways. More explainable and interpretable AI systems facilitate better evaluations of whether output can be trusted.

In the past few months, experts such as the former Special Advisor to the UK Prime Minister on Science and Technology James W. Phillips28 and a Congressionally-established US taskforce have called for the creation of national AI labs as ‘a shared research infrastructure that would provide AI researchers and students with significantly expanded access to computational resources, high-quality data, educational tools, and user support.’29 Should governments move forward with this concept, we propose that at least 25% of resources made available through these labs be explicitly allocated to technical AI safety projects.

7. Develop standards for identifying and managing AI-generated content and recommendations

The need to distinguish real from synthetic media and factual content from ‘hallucinations’ is essential for maintaining the shared factual foundations underpinning social cohesion. Advances in generative AI have made it more difficult to distinguish between AI-generated media and real images, audio, and video recordings. Already we have seen AI-generated voice technology used in financial scams.30

Creators of the most powerful AI systems have acknowledged that these systems can produce convincing textual responses that rely on completely fabricated or out-of-context information.31 For society to absorb these new technologies, we will need effective tools that allow the public to evaluate the authenticity and veracity of the content they consume.

We recommend increased funding for research into techniques, and development of standards, for digital content provenance. This research, and its associated standards, should ensure that a reasonable person can determine whether content published online is of synthetic or natural origin, and whether the content has been digitally modified, in a manner that protects the privacy and expressive rights of its creator.

We also recommend the expansion of ‘bot-or-not’ laws that require disclosure when a person is interacting with a chatbot. These laws help prevent users from being deceived or manipulated by AI systems impersonating humans, and facilitate contextualizing the source of the information. The draft EU AI Act requires that AI systems be designed such that users are informed they are interacting with an AI system,32 and the US State of California enacted a similar bot disclosure law in 2019.33 Almost all of the world’s nations, through the adoption of a UNESCO agreement on the ethics of AI, have recognized34 ‘the right of users to easily identify whether they are interacting with a living being, or with an AI system imitating human or animal characteristics.’ We recommend that all governments convert this agreement into hard law to avoid fraudulent representations of natural personhood by AI from outside regulated jurisdictions.

Even if a user knows they are interacting with an AI system, they may not know when that system is prioritizing the interests of the developer or deployer over the user. These systems may appear to be acting in the user’s interest, but could be designed or employed to serve other functions.  For instance, the developer of a general-purpose AI system could be financially incentivized to design the system such that when asked about a product, it preferentially recommends a certain brand, when asked to book a flight, it subtly prefers a certain airline, when asked for news, it provides only media advocating specific viewpoints, and when asked for medical advice, it prioritizes diagnoses that are treated with more profitable pharmaceutical drugs. These preferences could in many cases come at the expense of the end user’s mental, physical, or financial well-being.

Many jurisdictions require that sponsored content be clearly labeled, but because the provenance of output from complex general-purpose AI systems is remarkably opaque, these laws may not apply. We therefore recommend, at a minimum, that conflict-of-interest trade-offs should be clearly communicated to end users along with any affected output; ideally, laws and industry standards should be implemented that require AI systems to be designed and deployed with a duty to prioritize the best interests of the end user.

Finally, we recommend the establishment of laws and industry standards clarifying and the fulfillment of ‘duty of loyalty’ and ‘duty of care’ when AI is used in the place of or in assistance to a human fiduciary. In some circumstances – for instance, financial advice and legal counsel – human actors are legally obligated to act in the best interest of their clients and to exercise due care to minimize harmful outcomes. AI systems are increasingly being deployed to advise on these types of decisions or to make them (e.g. trading stocks) independent of human input. Laws and standards towards this end should require that if an AI system is to contribute to the decision-making of a fiduciary, the fiduciary must be able to demonstrate beyond a reasonable doubt that the AI system will observe duties of loyalty and care comparable to their human counterparts. Otherwise, any breach of these fiduciary responsibilities should be attributed to the human fidiciary employing the AI system.

Conclusion

The new generation of advanced AI systems is unique in that it presents significant, well- documented  risks,  but  can  also  manifest  high-risk  capabilities  and  biases  that  are  not immediately apparent. In other words, these systems may perform in ways that their developers had not anticipated or malfunction when placed in a different context. Without appropriate safeguards, these risks are likely to result in substantial harm, in both the near- and longer- term, to individuals, communities, and society.

Historically, governments have taken critical action to mitigate risks when confronted with emerging technology that, if mismanaged, could cause significant harm. Nations around the world have employed both hard regulation and international consensus to ban the use and development of biological weapons, pause human genetic engineering, and establish robust government oversight for introducing new drugs to the market. All of these efforts required swift action to slow the pace of development, at least temporarily, and to create institutions that could realize effective governance appropriate to the technology. Humankind is much safer as a result.

We believe that approaches to advancement in AI R&D that preserve safety and benefit society are possible, but require decisive, immediate action by policymakers, lest the pace of technological evolution exceed the pace of cautious oversight. A pause in development at the frontiers of AI is necessary to mobilize the instruments of public policy toward common- sense risk mitigation. We acknowledge that the recommendations in this brief may not be fully achievable within a six month window, but such a pause would hold the moving target still and allow policymakers time to implement the foundations of good AI governance.

The path forward will require coordinated efforts by civil society, governments, academia, industry, and the public. If this can be achieved, we envision a flourishing future where responsibly developed AI can be utilized for the good of all humanity.

/ 026. Jackson, Lauren. "What if A.I. Sentience Is a Question of Degree?" 2023.

A conversation with Nick Bostrom, a philosopher at Oxford, who has spent decades preparing for the dayartifi cial intelligence is capable of anything the human brain can do.

The refrain from experts is resounding: Artificial intelligence is not sentient.

It is a corrective of sorts to the hype that A.I. chatbots have spawned, especially in recent months. At least two news events in particular have introduced the notion of self-aware chatbots into our collective imagination.

Last year, a former Google employee raised concerns about what he said was evidence of A.I. sentience. And then, this February, a conversation between Microsoft’s chatbot and my colleague Kevin Roose about love and wanting to be a human went viral, freaking out the internet.

In response, experts and journalists have repeatedly reminded the public that A.I. chatbots are not conscious. If they can seem eerily human, that’s only because they have learned how to sound like us from huge amounts of text on the internet — everything from food blogs to old Facebook posts to Wikipedia entries. They’re really good mimics, experts say, but ones without feelings.

Industry leaders agree with that assessment, at least for now. But many insist that artificial intelligence will one day be capable of anything the human brain can do.

Nick Bostrom has spent decades preparing for that day. Bostrom is a philosopher and director of the Future of Humanity Institute at Oxford University. He is also the author of the book “Superintelligence.” It’s his job to imagine possible futures, determine risks and lay the conceptual groundwork for how to navigate them. And one of his longest-standing interests is how we govern a world full of superintelligent digital minds.

I spoke with Bostrom about the prospect of A.I. sentience and how it could reshape our fundamental assumptions about ourselves and our societies.

This conversation has been edited for clarity and length.

Many experts insist that chatbots are not sentient or conscious — two words that describe an awareness of the surrounding world. Do you agree with the assessment that chatbots are just regurgitating inputs?

Consciousness is a multidimensional, vague and confusing thing. And it’s hard to define or determine. There are various theories of consciousness that neuroscientists and philosophers have developed over the years. And there’s no consensus as to which one is correct. Researchers can try to apply these different theories to try to test A.I. systems for sentience.

But I have the view that sentience is a matter of degree. I would be quite willing to ascribe very small amounts of degree to a wide range of systems, including animals. If you admit that it’s not an all-or-nothing thing, then it’s not so dramatic to say that some of these assistants might plausibly be candidates for having some degrees of sentience.

I would say with these large language models, I also think it’s not doing them justice to say they’re simply regurgitating text. They exhibit glimpses of creativity, insight and understanding that are quite impressive and may show the rudiments of reasoning. Variations of these A.I.’s may soon develop a conception of self as persisting through time, reflect on desires, and socially interact and form relationships with humans.

What would it mean if A.I. was determined to be, even in a small way, sentient?

If an A.I. showed signs of sentience, it plausibly would have some degree of moral status. This means there would be certain ways of treating it that would be wrong, just as it would be wrong to kick a dog or for medical researchers to perform surgery on a mouse without anesthetizing it.

The moral implications depend on what kind and degree of moral status we are talking about. At the lowest levels, it might mean that we ought to not needlessly cause it pain or suffering. At higher levels, it might mean, among other things, that we ought to take its preferences into account and that we ought to seek its informed consent before doing certain things to it.

I’ve been working on this issue of the ethics of digital minds and trying to imagine a world at some point in the future in which there are both digital minds and human minds of all different kinds and levels of sophistication. I’ve been asking: How do they coexist in a harmonious way? It’s quite challenging because there are so many basic assumptions about the human condition that would need to be rethought.

What are some of those fundamental assumptions that would need to be reimagined or extended to accommodate artificial intelligence?

Here are three. First, death: Humans tend to be either dead or alive. Borderline cases exist but are relatively rare. But digital minds could easily be paused, and later restarted.

Second, individuality. While even identical twins are quite distinct, digital minds could be exact copies.

And third, our need for work. Lots of work must be done by humans today. With full automation, this may no longer be necessary.

Can you give me an example of how these upended assumptions could test us socially?

Another obvious example is democracy. In democratic countries, we pride ourselves on a form of government that gives all people a say. And usually that’s by one person, one vote.

Think of a future in which there are minds that are exactly like human minds, except they are implemented on computers. How do you extend democratic governance to include them? You might think, well, we give one vote to each A.I. and then one vote to each human. But then you find it isn’t that simple. What if the software can be copied?

The day before the election, you could make 10,000 copies of a particular A.I. and get 10,000 more votes. Or, what if the people who build the A.I. can select the values and political preferences of the A.I.’s? Or, if you’re very rich, you could build a lot of A.I.’s. Your influence could be proportional to your wealth.

More than 1,000 technology leaders and researchers, including Elon Musk, recently came out with a letter warning that unchecked A.I. development poses a “profound risks to society and humanity.” How credible is the existential threat of A.I.?

I’ve long held the view that the transition to machine superintelligence will be associated with significant risks, including existential risks. That hasn’t changed. I think the timelines now are shorter than they used to be in the past.

And we better get ourselves into some kind of shape for this challenge. I think we should have been doing metaphorical CrossFit for the last three decades. But we’ve just been lying on the couch eating popcorn when we needed to be thinking through alignment, ethics and governance of potential superintelligence. That is lost time that we will never get back.

Can you say more about those challenges? What are the most pressing issues that researchers, the tech industry and policymakers need to be thinking through?

First is the problem of alignment. How do you ensure that these increasingly capable A.I. systems we build are aligned with what the people building them are seeking to achieve? That’s a technical problem.

Then there is the problem of governance. What is maybe the most important thing to me is we try to approach this in a broadly cooperative way. This whole thing is ultimately bigger than any one of us, or any one company, or any one country even.

We should also avoid deliberately designing A.I.’s in ways that make it harder for researchers to determine whether they have moral status, such as by training them to deny that they are conscious or to deny that they have moral status. While we definitely can’t take the verbal output of current A.I. systems at face value, we should be actively looking for — and not attempting to suppress or conceal — possible signs that they might have attained some degree of sentience or moral status."

/ 027. Lee, Kai-Fu. AI Superpowers: China, Silicon Valley, and the New World Order, 2018.

/ 028. Reese, Hope. "A Human-Centered Approach to the AI Revolution," 2022.

Scholars envision a future where people and machines are collaborators, not competitors.

In 1950, computing pioneer Alan Turing predicted that in a few decades, computers would convincingly mimic human intelligence — a feat known as passing the Turing Test. Fast-forward to earlier this year, when a Google software engineer announced that his conversations with the company’s AI-powered chatbot had convinced him that it had become “sentient.” “I know a person when I talk to it,” he told the Washington Post. (Google said that he was “anthropomorphizing” the bot and fired him.)


As AI technologies such as natural language processing, machine learning, and deep learning rapidly evolve, so does the idea that they will go from imitating humans to making us obsolete: Elon Musk has warned that a superintelligent machine could “take over the world.” The fantasy — or nightmare — that people and AI will become locked in competition is remarkably enduring. It is also distracting us from AI’s true potential.

So argues Erik Brynjolfsson, a professor of economics and of operations, information, and technology (both by courtesy) at Stanford Graduate School of Business and a fellow at the Stanford Institute for Human- Centered Artificial Intelligence (HAI). In a recent paper, “The Turing Trap,” Brynjolfsson contends that too much attention has been paid to the idea that algorithms or robots will become substitutes for people. Instead, he believes that shifting our focus to envision ways that AI can work alongside people will spur innovation and productivity while unlocking economic benefits for everyone.

Using AI to automate human intelligence and labor is “an incredibly powerful and evocative vision, but it’s a very limiting one,” Brynjolfsson says. The alternative is augmentation: using AI to complement people by enabling them to do new things. “Both automation and augmentation can create benefits and both can be profitable,” he says. “But right now a lot of technologists, managers, and entrepreneurs are putting too much emphasis on automation.”"

Beyond the set of tasks that people can do and the limited set of tasks that can be automated is a much larger range of work that we could do with assistance from machines — the universe of augmentation. With advances in AI, we could simply mimic humans more closely than ever. Or, Brynjolfsson says, people could take a more expansive view of AI where “they’ll be able to do a lot more things.”

Looking Beyond Automation

Other researchers who are thinking critically about the future of this transformative technology are also convinced that it must go beyond automation.

“The idea that the entirety of AI is a field aimed toward automation is actually a bit of a misconception,”
says Fei-Fei Li    , the co-director of HAI. She says we need to “tease apart the hype” surrounding AI and look at its broader applications, such as deciphering complex data and using it to make decisions as well as actuating vehicles and robots that interact with the world.

Li thinks automation can play an important role in protecting people from harm in jobs like disaster relief, firefighting, and manufacturing. It makes sense for machines to take on tasks where “the very biology of being human is a disadvantage.” But, she says, “there’s so much more opportunity for this technology to augment humans than the very narrow notion of replacing humans.”

Machines have been assisting people and replacing their labor for centuries, explains Michael Spence    , an emeritus professor of economics and former dean at Stanford GSB. Yet the current digital wave is different from the wave of mechanization that defined the Industrial Revolution. Unlike their 19th- and 20th- century predecessors, which required constant human intervention to keep running, AI tools can function autonomously. And that, Spence warns, is taking us into “uncharted territory.”

“We have machines doing things that we thought only humans could do,” he says. These machines are increasingly supervised by other machines, and the idea of people being taken out of the loop “scares the wits out of people.” The scale of economic disruption that AI could cause is difficult to predict, though according to the McKinsey Global Institute, automation could displace more than 45 million U.S. workers by 2030.

Jennifer Aaker PhD ’95, hopes that AI will transform the way we work — for the better. Aaker, a behavioral scientist and a professor of marketing at Stanford GSB, cites a recent survey by Gartner in which 85% of people reported higher levels of burnout since the pandemic began. Can AI help alleviate disconnection and dissatisfaction on the job? “The increasing amount of data from the last couple of years will make this question become more pressing,” she says.

For the past three years, Aaker and Li cotaught Designing AI to Cultivate Human Well-Being    , an interdisciplinary course that explored ways to build AI that “augments human dignity and autonomy.” Aaker believes if augmentation can increase growth, education, and agency, it will be a critical way to improve people’s happiness and productivity. “We know that humans thrive when they learn, when they improve, when they accelerate their progress,” she says. “So, to what degree can AI be harnessed to facilitate or accelerate that?”

Augmentation in Action

Many of the potential uses of artificial intelligence have yet to materialize. Yet augmentation is already here, most visibly in the explosion of AI assistants     everywhere from dashboards and kitchen counters to law firms, medical offices, and research labs.

The benefits of augmentative AI can be seen in the healthcare industry. Li mentions one of her recent favorite student projects in the course she coteaches with Aaker, which used AI to prevent falls, a common cause of injuries in hospitals. “Patients fall or have rapidly deteriorating conditions that go undetected,” she says. Yet it’s not feasible for a nurse or caregiver to constantly monitor people who are at risk of falling. As a result, “there are procedural errors, dark spaces. How do you know a patient is about to fall? These are things you can’t do labs on.” Smart-sensor technology can give healthcare providers an “extra pair of eyes to augment the attention of human caretakers and to add information and to alert when something needs to be alerted.”

AI can also make short work of necessary yet tedious tasks. Spence mentions how “pure augmentation” is helping doctors by using machine learning to sift through mountains of medical literature. “It can pick off, with reasonable accuracy, the articles that are particularly important for a specific doctor with a specific specialty patient.” Similarly, Aaker cites a project from her course with Li where nurses and doctors used an AI tool to process paperwork, allowing them to spend more time connecting with patients. “Imagine how that frees up medical professionals to do the work that inspired them to get involved in the field in the first place?”

That may be one of the most compelling selling points for augmentation: It liberates people to focus on things that really matter. Aaker cites AI tools that help around the house. “Parents can get burdened by household tasks,” she explains. “What the AI is doing is removing the boring or useless types of tasks so that parents can spend time in ways that are more meaningful.”

Machine learning tools that can quickly digest large amounts of data are widely available and are being employed to inform decision-making in medicine, insurance, and banking. In many of these cases, AI is not the ultimate authority; instead, it is a tool for quickly recognizing patterns or predicting outcomes, which are then reviewed by human experts. Keeping people in the loop can ensure that AI is working properly and fairly and also provides insights into human factors that machines don’t understand.

This type of assistive technology, Li says, “is a win-win. AI is not taking away from the human element, but it’s an enabler to make human jobs faster and more efficient.”

Defining AI’s Values

Building a future where AI boosts human potential requires leadership from the people who will be over- seeing its implementation. Before business leaders can embrace augmentation, Li sees it as imperative to educate them about “the unintended consequences” of the technology they’re adopting. One of HAI’s main purposes is to help business leaders think through the big questions surrounding AI: “How it should be guided, how it should be governed, and how it reflects society’s values.”

“Those things are a bigger part of the challenge than just getting the state-of-the-art machine learning algorithm,” says Susan Athey    , a professor of economics at Stanford GSB, Stanford HAI affiliate, and an early adopter of machine learning for economic research    . But these questions of governance and ethics can’t be left entirely to AI developers. “Universities are putting out thousands of engineers every year to go and build these systems that are affecting our society,” Athey says. “Most of their classes don’t get to these topics.”

That makes it all the more urgent that we develop a framework to guide real-world applications of AI. “That framing is not going to come from a typical master’s degree holder in engineering,” Athey says. “It’s going to have to come from businesspeople, from those with a background in social science, ethics, or policy — but they need to understand the technology deeply enough to do the framing.”

For now, many corporate leaders are figuring out how AI can quickly boost profits. “There’s a gold rush going on right now about ways to apply these incredibly powerful machine learning techniques,” Brynjolfsson says. While there have been incredible advancements in AI, “the big gap is in getting the economics and business side to catch up. I’m trying to get my fellow economists, my fellow business school colleagues, managers, and entrepreneurs to figure out new ways to implement new business models. How can we do this so it’s consistent with our values?”"

Athey says that campus institutions such as HAI and Stanford GSB’s Golub Capital Social Impact Lab    , which she directs, can provide essential guidance for these discussions. “Businesses are going to make investments that align with their bottom line,” she says. “But Stanford can play a role if we do the basic R&D that helps people use AI in an augmented way that can influence the trajectory of industry.”

Considering a “diversity of values” is critical to determining the direction AI will take, Li says. “It’s about including people who have been raised on something more than an engineering education and sci-fi culture,” she says. “Our field needs people who want to impact real people in meaningful ways — not merely solve problems in the abstract.”

Payoffs and Progress

Even if we look past the hyperbole about AI run amok and accept the argument that it shouldn’t be viewed simply as a substitute for human capabilities, what’s the incentive for companies to pursue augmentation if full automation is easier and cheaper?

Automation can be used “to replace human labor and drive down labor costs,” Brynjolfsson acknowledges. While that can help the bottom line, it is “not where the big payoff is.” Augmentation clearly offers greater economic benefits to employees who wouldn’t be swapped out like old parts. But it would also provide expanded opportunities and options for employers and consumers.

He notes that technology has already boosted living standards enormously, mainly by creating new capabilities and products rather than making existing goods and services more cheaply. Instead of rushing to automate jobs and tasks, Brynjolfsson hopes business leaders will think harder about innovation and ask themselves, “What new things can we do now that we could never have done before because we have this technology?” Answering that question, he says, will “ultimately create more value for the shareholders and for all of society.”

Spence also believes that augmentation would lead to more inclusive growth, while automation would worsen current economic trends. Although the past era of mechanization had an initial “pain period” as workers scrambled to adopt new skills, it “contributed to the productivity and the earnings of what has come to be called the middle class.” While the people who owned the machines got rich, income was more widely distributed than it is now. “There’s a fair amount of evidence that the digital era has contributed to the polarization of jobs and income,” Spence says. Automation would further shrink the proportion of GDP going to the middle class and working class, leading to even more concentration of wealth. In that scenario, Spence says, “inequality worsens.”

He agrees that a more creative approach to AI is needed. “Consciously biasing the evolution and development of AI in the direction of augmentation is the right way to think about it,” he says. This will mean “using AI and digital tech to bring key services to people who now have limited access to them,” such as the 5.5 billion people living in developing countries. “There are values and policies that affect these incentives and so you want to try to operate on them in such a way that the benefits are broadly available to people. Not concentrated, say, on the owners of capital, or even more narrowly on the owners of some sort of digital capital.”

Those incentives aren’t in place yet. Current tax policies favor companies that install machines instead of hiring workers, Spence explains. “If you shifted the tax system so it was less favorable to capital and more favorable to employing people, you’d probably get more focus on people and maybe more focus on augmentation as well,” he says.
Brynjolfsson agrees. “The market does not automatically get the balance right in many ways. Our policymakers have put their thumb on the scale to steer too much investment toward mimicking and automating more jobs and using capital merely for labor substitution,” he says. That could lead to a situation where AI brings prosperity to a few and disempowers the rest — the Turing Trap.

We’re not there yet. Artificial intelligence is just beginning to have an impact, Brynjolfsson says. The challenge is to chart a path to a future where people remain indispensable. “Most progress over the past thousands of years has come from doing new things that we never did before — not from simply automating the things that we were already doing.” That will require us to tap into a superpower that can’t be programmed into a robot: imagination."


/ 029. Sanders, Nathan E. et al. "How AI could write our laws," 2023.


Nearly 90% of the multibillion-dollar federal lobbying apparatus in the United States serves corporate interests. In some cases, the objective of that money is obvious. Google pours millions into lobbying on bills related to antitrust regulation. Big energy companies expect action whenever there is a move to end drilling leases for federal lands, in exchange for the tens of millions they contribute to congressional reelection campaigns.

But lobbying strategies are not always so blunt, and the interests involved are not always so obvious. Consider, for example, a 2013 Massachusetts bill that tried to restrict the commercial use of data collected from K-12 students using services accessed via the internet. The bill ap p ealed to many privacy-conscious education advocates, and appropriately so. But behind the justification of protecting students lay a market-altering policy: the bill was introduced at the behest of Microsoft lobbyists, in an eﬀort to exclude Google Docs from classrooms.

What would happen if such legal-but-sneaky strategies for tilting the rules in favor of one group over another become more widespread and eﬀective? We can see hints of an answer in the remarkable pace at which artificial- intelligence tools for everything from writing to graphic design are being developed and improved. And the unavoidable conclusion is that AI will make lobbying more guileful, and perhaps more successful."

It turns out there is a natural opening for this technology: microlegislation.

“Microlegislation” is a term for small pieces of proposed law that cater—sometimes unexpectedly—to narrow interests. Political scientist Amy McKay coined the term. She studied the 564 amendments to the Affordable Care Act (“Obamacare”) considered by the Senate Finance Committee in 2009, as well as the positions of 866 lobbying groups and their campaign contributions. She documented instances where lobbyist comments—on health-care research, vaccine services, and other provisions—were translated directly into microlegislation in the form of amendments. And she found that those groups’ financial contributions to specific senators on the committee increased the amendments’ chances of passing.

Her finding that lobbying works was no surprise. More important, McKay’s work demonstrated that computer models can predict the likely fate of proposed legislative amendments, as well as the paths by which lobbyists can most eﬀectively secure their desired outcomes. And that turns out to be a critical piece of creating an AI lobbyist.

Lobbying has long been part of the give-and-take among human policymakers and advocates working to balance their competing interests. The danger of microlegislation—a danger greatly exacerbated by AI—is that it can be used in a way that makes it diﬃcult to figure out who the legislation truly benefits.

Another word for a strategy like this is a “hack.” Hacks follow the rules of a system but subvert their intent. Hacking is often associated with computer systems, but the concept is also applicable to social systems like financial markets, tax codes, and legislative processes.

While the idea of monied interests incorporating AI assistive technologies into their lobbying remains hypothetical, specific machine-learning technologies exist today that would enable them to do so. We should expect these techniques to get better and their utilization to grow, just as we’ve seen in so many other domains."

Here’s how it might work.

Crafting an AI microlegislator

To make microlegislation, machine-learning systems must be able to uncover the smallest modification that could be made to a bill or existing law that would make the biggest impact on a narrow interest.

There are three basic challenges involved. First, you must create a policy proposal—small suggested changes to legal text—and anticipate whether or not a human reader would recognize the alteration as substantive. This is important; a change that isn’t detectable is more likely to pass without controversy. Second, you need to do an impact assessment to project the implications of that change for the short- or long-range financial interests of companies. Third, you need a lobbying strategizer to identify what levers of power to pull to get the best proposal into law."
	
Existing AI tools can tackle all three of these.

The first step, the policy proposal, leverages the core function of generative AI. Large language models, the sort that have been used for general- purpose chatbots such as ChatGPT, can easily be adapted to write like a native in diﬀerent specialized domains after seeing a relatively small number of examples. This process is called fine-tuning. For example, a model “pre-trained” on a large library of generic text samples from books and the internet can be “fine-tuned” to work eﬀectively  on medical literature, computer science papers, and product reviews.

Given this flexibility and capacity for adaptation, a large language model could be fine-tuned to produce draft legislative texts, given a data set of previously offered amendments and the bills they were associated with. Training data is available. At the federal level, it’s provided by the US Government Publishing Office, and there are already tools for downloading and interacting with it. Most other jurisdictions provide similar data feeds, and there are even convenient assemblages of that data.

Meanwhile, large language models like the one underlying ChatGPT are routinely used for summarizing long, complex documents (even laws and computer code) to capture the essential points, and they are optimized to match human expectations. This capability could allow an AI assistant to automatically predict how detectable the true eﬀect of a policy insertion may be to a human reader.

Today, it can take a highly paid team of human lobbyists days or weeks to generate and analyze alternative pieces of microlegislation on behalf of a client. With AI assistance, that could be done instantaneously and cheaply. This opens the door to dramatic increases in the scope of this kind of microlegislating, with a potential to scale across any number of bills in any jurisdiction.

Teaching machines to assess impact

Impact assessment is more complicated. There is a rich series of methods for quantifying the predicted outcome of a decision or policy, and then also optimizing the return under that model. This kind of approach goes by diﬀerent names in diﬀerent circles—mathematical programming in management science, utility maximization in economics, and rational design in the life sciences.

To train an AI to do this, we would need to specify some way to calculate the benefit to diﬀerent parties as a result of a policy choice. That could mean estimating the financial return to diﬀerent companies under a few diﬀerent scenarios of taxation or regulation. Economists are skilled at building risk models like this, and companies are already required to formulate and disclose regulatory compliance risk factors to investors. Such a mathematical model could translate directly into a reward function, a grading system that could provide feedback for the model used to create policy proposals and direct the process of training it.

The real challenge in impact assessment for generative AI models would be to parse the textual output of a model like ChatGPT in terms that an economic model could readily use. Automating this would require extracting structured financial information from the draft amendment or any legalese surrounding it. This kind of information extraction, too, is an area where AI has a long history; for example, AI systems have been trained to recognize clinical details in doctors’ notes. Early indications are that  large language models are fairly  good at recognizing financial information  in texts such as investor call transcripts. While it remains an open challenge in the field, they may even be capable of writing out multi-step plans based on descriptions in free text.

Machines as strategists

The last piece of the puzzle is a lobbying strategizer to figure out what actions to take to convince lawmakers to adopt the amendment.

Passing legislation requires a keen understanding of the complex interrelated networks of legislative oﬃces, outside groups, executive agencies, and other stakeholders vying to serve their own interests. Each actor in this network has a baseline perspective and diﬀerent factors that influence that point of view. For example, a legislator may be moved by seeing an allied stakeholder take a firm position, or by a negative news story, or by a campaign contribution.

It turns out that AI developers are very experienced at modeling these kinds of networks. Machine-learning models for network graphs have been built, refined, improved, and iterated by hundreds of researchers working on incredibly diverse problems: lidar scans used to guide self-driving cars, the chemical functions of molecular structures, the capture of motion in actors’ joints for computer graphics, behaviors in social networks, and more.

In the context of AI-assisted lobbying, political actors like legislators and lobbyists are nodes on a graph, just like users in a social network. Relations between them are graph edges, like social connections. Information can be passed along those edges, like messages sent to a friend or campaign contributions made to a member. AI models can use past examples to learn to estimate how that information changes the network. Calculating the likelihood that a campaign contribution of a given size will flip a legislator’s vote on an amendment is one application.

McKay’s work has already shown us that there are significant, predictable relationships between these actions and the outcomes of legislation, and that the work of discovering those can be automated. Others have shown that graphs of neural network models like those described above can be applied to political systems. The full-scale use of these technologies to guide lobbying strategy is theoretical, but plausible.

Put together, these three components could create an automatic system for generating profitable microlegislation. The policy proposal system would create millions, even billions, of possible amendments. The impact assessor would identify the few that promise to be most profitable to the client. And the lobbying strategy tool would produce a blueprint for getting them passed.

What remains is for human lobbyists to walk the floors of the Capitol or state house, and perhaps supply some cash to grease the wheels. These  final two aspects of lobbying—access and financing—cannot be supplied by the AI tools we envision. This suggests that lobbying will continue to primarily benefit those who are already influential and wealthy, and AI assistance will amplify their existing advantages.

The transformative benefit that AI oﬀers to lobbyists and their clients is scale. While individual lobbyists tend to focus on the federal level or a single state, with AI assistance they could more easily infiltrate a large number of state-level (or even local-level) law-making bodies and elections. At that level, where the average cost of a seat is measured in the tens of thousands of dollars instead of millions, a single donor can wield a lot of influence—if automation makes it possible to coordinate lobbying across districts.

How to stop them

When it comes to combating the potentially adverse eﬀects of assistive AI, the first response always seems to be to try to detect whether or not content was AI-generated. We could imagine a defensive AI that detects anomalous lobbyist spending associated with amendments that benefit the contributing group. But by then, the damage might already be done.

In general, methods for detecting the work of AI tend not to keep pace with its ability to generate convincing content. And these strategies won’t be implemented by AIs alone. The lobbyists will still be humans who take the results of an AI microlegislator and further refine the computer’s strategies. These hybrid human-AI systems will not be detectable from their output.

But the good news is: the same strategies that have long been used to combat misbehavior by human lobbyists can still be eﬀective when those lobbyists get an AI assist. We don’t need to reinvent our democracy to stave oﬀ the worst risks of AI; we just need to more fully implement long- standing ideals.

First, we should reduce the dependence of legislatures on monolithic, multi-thousand-page omnibus bills voted on under deadline. This style of legislating exploded in the 1980s and 1990s and continues through to the most recent federal budget bill. Notwithstanding their legitimate benefits to the political system, omnibus bills present an obvious and proven vehicle for inserting unnoticed provisions that may later surprise the same
legislators who approved them.

The issue is not that individual legislators need more time to read and understand each bill (that isn’t realistic or even necessary). It’s that omnibus bills must pass. There is an imperative to pass a federal budget bill, and so the capacity to push back on individual provisions that may seem deleterious (or just impertinent) to any particular group is small. Bills that are too big to fail are ripe for hacking by microlegislation.

Moreover, the incentive for legislators to introduce microlegislation catering to a narrow interest is greater if the threat of exposure is lower. To strengthen the threat of exposure for misbehaving legislative sponsors, bills should focus more tightly on individual substantive areas and, after the introduction of amendments, allow more time before the committee and floor votes. During this time, we should encourage public review and testimony to provide greater oversight.

Second, we should strengthen disclosure requirements on lobbyists, whether they’re entirely human or AI-assisted. State laws regarding lobbying disclosure are a hodgepodge. North Dakota, for example, only requires lobbying reports to be filed annually, so that by the time a disclosure is made, the policy is likely already decided. A lobbying disclosure scorecard created by Open Secrets, a group researching the influence of money in US politics, tracks nine states that do not even require lobbyists to report their compensation.

Ideally, it would be great for the public to see all communication between lobbyists and legislators, whether it takes the form of a proposed amendment or not. Absent that, let’s give the public the benefit of reviewing what lobbyists are lobbying for—and why. Lobbying is traditionally an activity that happens behind closed doors. Right now, many states reinforce that: they actually exempt testimony delivered publicly to a legislature from being reported as lobbying.

In those jurisdictions, if you reveal your position to the public, you’re no longer lobbying. Let’s do the inverse: require lobbyists to reveal their positions on issues. Some jurisdictions already require a statement of position (a ‘yea’ or ‘nay’) from registered lobbyists. And in most (but not all) states, you could make a public records request regarding meetings held with a state legislator and hope to get something substantive back. But we can expect more—lobbyists could be required to proactively publish, within a few days, a brief summary of what they demanded of policymakers during meetings and why they believe it’s in the general interest.

We can’t rely on corporations to be forthcoming and wholly honest about the reasons behind their lobbying positions. But having them on the record about their intentions would at least provide a baseline for accountability.

Finally, consider the role AI assistive technologies may have on lobbying firms themselves and the labor market for lobbyists. Many observers are rightfully concerned about the possibility of AI replacing or devaluing the human labor it automates. If the automating potential of AI ends up commodifying the work of political strategizing and message development, it may indeed put some professionals on K Street out of work.

But don’t expect that to disrupt the careers of the most astronomically compensated lobbyists: former members Congress and other insiders who have passed through the revolving door. There is no shortage of reform ideas for limiting the ability of government oﬃcials turned lobbyists to sell access to their colleagues still in government, and they should be adopted and—equally important—maintained and enforced in successive Congresses and administrations.

None of these solutions are really original, specific to the threats posed by AI, or even predominantly focused on microlegislation—and that’s the point. Good governance should and can be robust to threats from a variety of techniques and actors.
But what makes the risks posed by AI especially pressing now is how fast the field is developing. We expect the scale, strategies, and eﬀectiveness of humans engaged in lobbying to evolve over years and decades.
Advancements in AI, meanwhile, seem to be making impressive breakthroughs at a much faster pace—and it’s still accelerating.

The legislative process is a constant struggle between parties trying to control the rules of our society as they are updated, rewritten, and expanded at the federal, state, and local levels. Lobbying is an important tool for balancing various interests through our system. If it’s well- regulated, perhaps lobbying can support policymakers in making equitable decisions on behalf of us all."


/ 030. Noy, Shakked et al. "Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence," 2023.

We examine the productivity effects of a generative artificial intelligence technology—the assistive chatbot ChatGPT—in the context of mid-level professional writing tasks.  In a preregistered online experiment, we assign occupation-specific, incentivized writing tasks to 444 college-educated professionals, and randomly expose half of them to ChatGPT. Our results show that ChatGPT substantially raises average productivity:  time taken decreases by 0.8 SDs and output quality rises by 0.4 SDs.  Inequality between workers decreases, as ChatGPT compresses the productivity distribution by benefiting low-ability workers more. ChatGPT mostly substitutes for worker effort rather than complementing worker skills, and restructures tasks towards idea-generation and editing and away from rough-drafting.   Exposure to ChatGPT increases job satisfaction and self-efficacy and heightens both concern and excitement about automation technologies.

1    Introduction

Recent advances in generative artificial intelligence may have widespread implications for production and labor markets. New generative AI systems like ChatGPT or DALL-E, which can be prompted to create novel text or visual outputs from large amounts of training data, are qualitatively unlike most historical examples of automation technologies. Previous waves of automation predominantly impacted “routine” tasks consisting of explicit sequences of steps that could be easily codified and programmed into a computer (Autor and Dorn, 2013; Autor, 2015). Creative, difficult-to-codify tasks (such as writing and image generation) largely avoided automation—a pattern that scholars noted might change with the advent of the deep learning techniques that now underpin generative AI systems.

The  emergence  of  powerful  generative  AI  technologies  reintroduces  a  host  of  classic questions in a new context. Automation technologies—by definition—perform specific tasks in place of humans.  But, more broadly, they may either displace humans completely from certain occupations or complement existing human workers and increase their productivity (Acemoglu  and  Restrepo,  2020;  Boustan  et  al.,  2022;  Kanazawa  et  al.,  2022).   Insofar  as automation technologies mostly displace human workers, they can increase unemployment, while their impacts on aggregate productivity may be small or nonexistent to the degree that they mainly serve to redistribute income from workers to owners of capital (Acemoglu and Restrepo, 2018). Insofar as automation complements existing workers, it can simultaneously benefit  workers,  capital  owners,  and  consumers  by  raising  productivity  and  wages  and lowering prices (Kleinberg et al., 2018; Hoffman et al., 2018; Agrawal et al., 2019).

For example, a potent generative writing tool like ChatGPT might entirely replace certain kinds of writers, such as grant writers or marketers, by letting companies directly automate the creation of grant applications and press releases with minimal human oversight.  This might not increase the quality of the resulting written output, but would let companies save on wage costs by eliminating human labor. Alternatively, a tool like ChatGPT could substantially raise the productivity of grant writers and marketers, for example by automating relatively routine, time-consuming subcomponents of their writing tasks, such as translating ideas into an initial rough draft. In this case, demand for these services could expand, resulting in higher employment and wages as well as greater productivity for companies and cheaper products for consumers. Inequalities between workers could also be affected: inequality could decrease if lower-ability workers are helped more by ChatGPT, or increase if higher-ability workers have the skills necessary to take advantage of the new technology.

Which of these eventualities will generative AI systems bring about? The answer depends on a host of questions: how do generative AI systems affect workers’ productivity in existing tasks? Do they affect productivity largely by substituting for worker effort or complementing
worker skills?  Do they differentially affect low- or high-ability workers, or workers with "different  skill  profiles?   Do  they  affect  workers’  enjoyment  of  their  work  (Schwabe  and Castellacci, 2020)?

This paper takes the first step towards answering these questions.1 In an online experiment, we recruit 444 experienced, college-educated professionals and assign each to complete two occupation-specific, incentivized writing tasks. The occupations we draw on are marketers, grant writers, consultants, data analysts, human resource professionals, and managers. The tasks, which include writing press releases, short reports, analysis plans, and delicate emails, comprise 20-to 30-minute assignments designed to resemble real tasks performed in these occupations; indeed, most of our participants report completing similar tasks before and rate the assigned tasks as realistic representations of their everyday work. Participants face high-powered incentives, in the form of large bonus payments, to produce high-quality work. Quality is assessed by (blinded) experienced professionals working in the same occupations. Evaluators are asked to treat the output as if encountered in a work setting and are incentivized to grade outputs carefully.  Evaluators assign an overall grade as well as separate grades for writing quality, content quality, and originality.  Each piece of output is seen by three evaluators, with an average within-essay cross-evaluator correlation of 0.44.

A randomly-selected 50% of our participants—the treatment group—are instructed to sign up for ChatGPT between the first and second task, are walked through how to use it, and are told they are permitted to use it on the second task if they find it useful. The control group is instead instructed to sign up for the LaTeX editor Overleaf. This design allows us to estimate the causal effects of ChatGPT using a combination of within-person and between-person variation, and performance on the first task serves as a measure of baseline ability that enables our inequality analyses.

We collect participants’ output and elicit total time taken, time taken on various subcom- ponents of the task, job satisfaction, self-efficacy, and beliefs about automation. We also take a snapshot of each participant’s output each minute while they perform the task, to construct an objective measure of time active on the task and to detect ChatGPT usage in the control group and on the pre-treatment task.

A complete description of our experimental design, a copy of relevant survey questionnaires, and additional figures validating our central measures and extending our main results are included in the Online Appendix. Descriptive statistics about the sample, as well as balance and selective attrition tests, are available in Table 1.  The attrition rate is 5% in the control group and 10% in the treatment group.  Balance tests indicate that across 13 pre-treatment characteristics, the treatment and control group exhibit a small significant difference only for only two characteristic (employment status and being an HR professional).  Our partly
within-person design, which controls for performance on the pre-treatment task, should "eliminate any influence of selective attrition on our results;  in the Online Appendix,  we also report Lee (2009) bounds on our main results and versions of our results controlling for employment status and occupation, which confirm that our results are highly robust to selective attrition.

2    Results

2.1    Takeup of ChatGPT

In the treatment group, 92% of treated participants successfully sign up for ChatGPT,2 and 81% choose to use it on the second task, giving it an average self-assessed usefulness score of 4.4 out of 5.

Prior to treatment, about 70% of our participants had heard of ChatGPT and 30% had used it before. Self-reported and objective measures indicate only 10-20% of the control group use ChatGPT on the tasks, meaning there is at least a 60 percentage point experimentally-induced gap in usage between our treatment and control groups on the second task.  The fact that some control participants are using the tool means our estimates provide lower bounds on the effects of ChatGPT usage on productivity.

2.2    Productivity

We measure productivity as earnings per minute.   Figure 1 shows that the experimental intervention increases this outcome dramatically. In the treatment group, time taken on the post-treatment task drops by 10 minutes (37%) relative to the control group, who take an average of 27 minutes (p = 0.000). Average evaluator grades in the treatment group increase by 0.45 standard deviations (p = 0.000), with roughly similar increases for overall grades and specific grades for writing quality, content quality, and originality.

Figure 1 Panels (c) and (d) show that these effects are not limited to specific pockets of the time or grade distributions: the entire time distribution shifts to the left (faster work) and the entire grade distribution shifts to the right (higher quality). At the individual worker level, Figure 2 shows that treated workers who received a low grade on the first task experience both increases in grades and decreases in time spent, while workers who received a high grade maintain their grade level while substantially reducing their time spent.

These results are virtually identical across our two main incentive schemes, covering 80% of respondents: a “linear” scheme in which respondents are paid $1 for each point they receive on each submission (each of which is graded on a 1-7 point scale), and a “convex” scheme in which respondents are additionally paid $3 for earning a grade of 6 or 7, giving them an extra incentive to produce high-quality output.

Two supplementary interventions allow us to probe further.  In one arm involving 20% of participants, we require participants in both the treatment and control group to spend exactly 15 minutes on each task.  This holds effort fixed across the treatment and control groups, allowing us to interpret any difference in grades as a pure effect of ChatGPT access on productive capacity.  In this arm, the treatment increases grades by a similar 0.39 standard deviations (p = 0.13), albeit imprecisely estimated and with a slight imbalance in pre-treatment "outcomes (see Online Appendix Figure A.7).

In another arm involving 30% of the treatment group, after completing the second task, respondents are shown their first-task output and given the opportunity to edit or replace it using ChatGPT if they wish. 23% choose to replace their response with ChatGPT’s output and 25% use ChatGPT to edit their original response, suggesting that participants view ChatGPT as a way to improve output quality in addition to a convenient way to save time.

2.3    Productivity Inequality

The control group exhibits persistent productivity inequality: participants who score well on the first task also tend to score well on the second task. As Figure 2 Panel (a) shows, there is a correlation of 0.49 between a control participant’s average grade on the first task and their average grade on the second task.

In the treatment group, initial inequalities are half-erased by the treatment: the correlation between first-task and second-task grades is only 0.25 (p-value on difference in slopes = 0.004). This reduction in inequality is driven by the fact that participants who scored lower on the first round benefit more from ChatGPT access, as the figure shows: the gap between the treatment and control lines is much larger at the left-hand end of the x-axis.

2.4    Human-Machine Complementarity

ChatGPT could increase workers’ productivity in two ways. On the one hand, it could substitute for worker effort by quickly producing output of satisfactory quality that workers directly submit, letting them reduce the time they spend on the task. On the other hand, it could complement workers’ skills: humans and ChatGPT working together could produce more than the sum of their parts, for example if ChatGPT aids with the brainstorming process, or quickly produces a rough draft and humans then edit and improve on the draft.  In our experiment, evidence for the complementarity story could come in two forms: (a) we could observe treatment-group participants choosing to expend significant time editing ChatGPT’s output or repeatedly prompting ChatGPT in anticipation of earning higher grades, and (b) we could observe that treatment participants’ essays receive higher grades than ChatGPT’s raw output, suggesting that human input adds value.

We observe neither of these pieces of evidence, suggesting that ChatGPT is increasing productivity primarily by substituting for worker effort. 68% of treated participants report submitting ChatGPT’s initial output without editing it, and on average treated participants are active on the task for only 3 minutes after we first observe them pasting in a large quantity of text (presumably from ChatGPT). There is also no correlation between how long a participant is active after pasting in the ChatGPT text and the grade they ultimately receive, and treated respondents do not receive higher average grades than raw ChatGPT output that we give to evaluators to grade, meaning we find no evidence that human editing is improving the ChatGPT output. This is true even when participants are given strong pecuniary incentives to do so, in the convex incentives group.

2.5    Task Structure

As suggested by the preceding discussion, ChatGPT substantially changes the structure of writing tasks. Figure 3 Panel A shows that prior to the treatment, participants spend about 25% of their time brainstorming, 50% writing a rough draft, and 25% editing. Post-treatment, the share of time spent writing a rough draft falls by more than half and the share of time spent editing more than doubles.

2.6    Skill Demand

If ChatGPT is especially helpful to those with poor writing and communication skills relative to their other skills, it could have major labor market implications by expanding the available occupational choices and raise the earnings of individuals with strong idea-generation skills who struggle to effectively get those ideas onto paper.
We perform several tests of this hypothesis.  We construct two measures of a person’s relative writing skills. First, at the beginning of the experiment, we ask participants to rank from 1 to 3 their skills at communication (writing and speaking),  problem solving,  and creativity. Second, in addition to assigning overall grades, evaluators separately assess each piece of output based on writing quality, content quality, and originality; the gap between a person’s first-task overall score and their writing score affords another measure.

Similarly, we construct two measures of the individual-level benefits of ChatGPT. First, at the end of the experiment, we ask treatment-group participants how much they would be willing to pay on a monthly basis to access ChatGPT in their jobs.  Second, we measure how much each treatment participant’s grade increases from the first to the second task. We find no clear evidence for the aforementioned hypothesis. Figure 3 Panel B show that average willingness to pay for ChatGPT is flat across the terciles of both our measures of writing skill: respondents, regardless of their writing skills, are willing to pay about 0.5% of their monthly salary for a monthly subscription to ChatGPT. Grade gains from ChatGPT are also roughly flat across both measures of relative writing skills: people with comparatively poor writing
skills do not experience unusually large grade gains.

2.7    Job Satisfaction and Self-Efficacy

Access to ChatGPT could affect job satisfaction.  For example, it could make participants happier by automating tedious or annoying components of the task or allowing them to finish more quickly. Alternatively, it could make the experience less enjoyable by quickly automating the most fun parts of the task. It could similarly either boost self-efficacy by giving participants access to a complex and powerful tool that enhances their capabilities, or it could lower it by making participants feel superfluous. We measure job satisfaction with a question, after each task, about how much participants enjoyed the task, and self-efficacy with a question about how skilled/effective they felt while completing the task, both on 1-10 Likert scales.
Figure 4 Panel (a) shows that ChatGPT substantially increases job satisfaction, by about
0.40 standard deviations (p = 0.000). Panel B shows that it mildly and imprecisely increases self-efficacy, by 0.20 standard deviations (p = 0.060), despite the fact that participants are mostly using it to substitute for their own effort. Qualitative feedback from participants (in an open-text box at the end of the survey) suggests that many enjoy discovering and working with this tool.

2.8    Beliefs About Automation

Many of our treated participants had never heard of (30%) or never used (70%) ChatGPT before participating in the experiment. Hence, most essentially encounter the technology for the first time and receive a crash course on its usefulness for writing tasks.  How are their beliefs about future waves of automation affected by this encounter?
After respondents complete their second task, we elicit three beliefs, each on a 1-10 scale: how worried they are about workers in their occupation being replaced by AI; how optimistic they are that AI will make workers in their occupation more productive; and, overall, how optimistic or pessimistic they feel about future advances in AI. The effects of treatment on these outcomes are displayed in Figure 4 Panel (c); worry about automation increases by 0.26 standard deviations (p = 0.006), excitement by 0.39 (p = 0.000), and net optimism increases by about 0.20 (p = 0.037)."

2.9    Two-Week Followup Survey

One indication of the value of ChatGPT to participants is whether they continue to use it after the experiment. To track whether participants are subsequently using ChatGPT in their real jobs, we resurvey them two weeks after their completion of the initial survey. This followup is still in progress, with an 82% response rate among the 423 respondents who have been invited so far, and no evidence of differential response rates by treatment status.

33% of former treatment group participants have used ChatGPT in their job in the past week, relative to 18% of control group participants.   Restricting to workers who had not previously used ChatGPT when they participated in our main experiment, 26% of treated and 9% of control workers are now using ChatGPT in their jobs (p-value on difference 0.048). Users give it an average usefulness score of 3.65/5.00, somewhat lower than in our main experiment, likely owing to the greater length and complexity of real-world tasks. The range of tasks they report using it for is broad: generating recommendation letters for employees, responding to customer service requests, brainstorming, search-engine requests, rough-drafting emails, and so on.

Respondents who are not using ChatGPT in their jobs mostly report that this is because the chatbot lacks context-specific knowledge that forms an important part of their writing. For example, they report that their writing is “very specifically tailored to [their] customers and involves real time information” or “unique [and] specific to [their] company products.”

These comments point to an important (and inherent) limitation of our experiment:  it involves relatively small,  self-contained tasks that lack much context-specific knowledge beyond what we stipulate in the task prompts. However, our core result, that ChatGPT can increase productivity on many mid-level professional writing tasks, is supported by the fact that many respondents choose to use it in their real jobs. The fact that the treatment group is substantially more likely to use ChatGPT than the control group also suggests that the dissemination of ChatGPT into real professional activity is still in its very early stages, with many people not using it due to a lack of knowledge about or experience with the technology. There is no difference between the former treatment and control in their overall satisfaction with their job in the followup survey. That said, many of our respondents only started using ChatGPT in their job in the past week or two, and it may take longer for access to ChatGPT to
affect overall job satisfaction.

3    Discussion

College-educated professionals performing mid-level professional writing tasks experience substantial increases in productivity when given access to ChatGPT. The generative writing tool increases the output quality of low-ability workers while reducing their time spent, and it "allows high-ability workers to maintain their quality standards while becoming significantly faster. At the aggregate level, ChatGPT substantially compresses the productivity distribution, reducing inequality.  It is also already being used by many workers in their real jobs.  The experimental evidence suggests that ChatGPT largely substitutes for worker effort rather than complementing workers’ skills, potentially causing a decrease in demand for workers, with adverse distributional effects as capital owners gain at the expense of workers.

The experiment has several important limitations worth enumerating. First, the tasks are relatively short, self-contained, and lack a dimension of context-specific knowledge, which may inflate our estimates of ChatGPT’s usefulness. The results on job satisfaction and self-efficacy are similarly limited, reflecting enjoyment of a small task rather than feelings about a worker’s whole job, as evidenced by the fact that there is no difference between the treatment and control groups in real job satisfaction after two weeks. Second, an experiment, by its nature, captures only direct, immediate effects of ChatGPT on the selected occupations. There will be many indirect, reinforcing, or counteracting “general-equilibrium” effects as labor markets and production systems adapt to the advent of technologies like ChatGPT. The effects of ChatGPT will also likely vary by occupation, task, and skill level.

Only time and future research will fully reveal how ChatGPT and its successors will affect labor markets.  For now, the evidence we provide suggests that generative AI technologies will—and have already begun—to noticeably impact workers.


/ 031. HAI Stanford University, Artificial Intelligence Index Report, 2023.

Introduction to the AI Index Report 2023

Welcome to the sixth edition of the AI Index Report! This year, the report introduces more original data than any previous edition, including a new chapter on AI public opinion, a more thorough technical performance chapter, original analysis about large language and multimodal models, detailed trends in global AI legislation records,
a study of the environmental impact of AI systems, and more.

The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world’s most credible and authoritative source for data and insights about AI.

From the Co-Directors

AI has moved into its era of deployment; throughout 2022 and the beginning of 2023, new large-scale AI models have been released every month. These models, such as ChatGPT, Stable Diffusion, Whisper, and DALL-E 2, are capable of an increasingly broad range of tasks, from text manipulation and analysis, to image generation, to unprecedentedly good speech recognition. These systems demonstrate capabilities in question answering and the generation of text, image, and code unimagined a decade ago, and they outperform the state of the art on many benchmarks, old and new. However, they are prone to hallucination, routinely biased, and can be tricked into serving nefarious aims, highlighting the complicated ethical challenges associated with their deployment.

Although 2022 was the first year in a decade where private AI investment decreased, AI is still a topic of great interest to policymakers, industry leaders, researchers, and the public. Policymakers are talking about AI more than ever before. Industry leaders that have integrated AI into their businesses are seeing tangible cost and revenue benefits. The number of AI publications and collaborations continues to increase. And the public is forming sharper opinions about AI and which elements they like or dislike.

AI will continue to improve and, as such, become a greater part of all our lives. Given the increased presence of this technology and its potential for massive disruption, we should all begin thinking more critically about how exactly we want AI to be developed and deployed. We should also ask questions about who is deploying it—as our analysis shows, AI is increasingly defined by the actions of a small set of private sector actors, rather than a broader range of societal actors. This year’s AI Index paints a picture of where we are so far with AI, in order to highlight what might await us in the future.

Jack Clark and Ray Perrault

Top Ten Takeaways

1 Industry races ahead of academia. Until 2014, most significant machine learning models were released by academia. Since then, industry has taken over. In 2022, there were 32 significant industry-produced machine learning models compared to just three produced by academia. Building state-of-the-art AI systems increasingly requires large amounts of data, computer power, and money—resources that industry actors inherently possess in greater amounts compared to nonprofits and academia.

2 Performance saturation on traditional benchmarks. AI continued to post state-of-the-art results, but year-over-year improvement on many benchmarks continues to be marginal. Moreover, the speed at which benchmark saturation is being reached is increasing. However, new, more comprehensive benchmarking suites such as BIG-bench and HELM are being released.

3 AI is both helping and harming the environment. New research suggests that AI systems can have serious environmental impacts. According to Luccioni et al., 2022, BLOOM’s training run emitted 25 times more carbon than a single air traveler on a one-way trip from New York to San Francisco. Still, new reinforcement learning models like BCOOLER show that AI systems can be used to optimize energy usage.

4 The world’s best new scientist … AI?
AI models are starting to rapidly accelerate scientific progress and in 2022 were used to aid hydrogen fusion, improve the efficiency of matrix manipulation, and generate new antibodies.

5 The number of incidents concerning the misuse of AI is rapidly rising.
According to the AIAAIC database, which tracks incidents related to the ethical misuse of AI, the number of AI incidents and controversies has increased 26 times since 2012. Some notable incidents in 2022 included a deepfake video of Ukrainian President Volodymyr Zelenskyy surrendering and U.S. prisons using call-monitoring technology on their inmates. This growth is evidence of both greater use of AI technologies and awareness of misuse possibilities.

6 The demand for AI-related professional skills is increasing across
virtually every American industrial sector. Across every sector in the United States for which there is data (with the exception of agriculture, forestry, fishing, and hunting), the number of AI- related job postings has increased on average from 1.7% in 2021 to 1.9% in 2022. Employers in the United States are increasingly looking for workers with AI-related skills.

7 For the first time in the last decade, year-over-year private investment in AI decreased. Global AI private investment was $91.9 billion in 2022, which represented a 26.7% decrease since 2021. The total number of AI-related funding events as well as the number of newly funded AI companies likewise decreased. Still, during the last decade as a whole, AI investment has significantly increased. In 2022 the amount of private investment in AI was 18 times greater than it was in 2013.

8 While the proportion of companies adopting AI has plateaued, the companies that have adopted AI continue to pull ahead. The proportion of companies adopting AI in 2022 has more than doubled since 2017, though it has plateaued in recent years between 50% and 60%, according to the results of McKinsey’s annual research survey. Organizations that have adopted AI report realizing meaningful cost decreases and revenue increases.

9 Policymaker interest in AI is on the rise. An AI Index analysis of the legislative records of 127 countries shows that the number of bills containing “artificial intelligence” that were passed into law grew from just 1 in 2016 to 37 in 2022. An analysis of the parliamentary records on AI in 81 countries likewise shows that mentions of AI in global legislative proceedings have increased nearly 6.5 times since 2016.

10 Chinese citizens are among those who feel the most positively about AI products and services. Americans … not so much. In a 2022 IPSOS survey, 78% of Chinese respondents (the highest proportion of surveyed countries) agreed with the statement that products and services using AI have more benefits than drawbacks. After Chinese respondents, those from Saudi Arabia (76%) and India (71%) felt the most positive about AI products. Only 35% of sampled Americans (among the lowest of surveyed countries) agreed that products and services using AI had more benefits than drawbacks.

Report Highlights

Chapter 1: Research and Development	
	
The United States and China had the greatest number of cross-country collaborations in AI publications from 2010 to 2021, although the pace of collaboration has slowed. The number of AI research collaborations between the United States and China increased roughly 4 times since 2010, and was 2.5 times greater than the collaboration totals of the next nearest country pair, the United Kingdom and China. However the total number of U.S.-China collaborations only increased by 2.1% from 2020 to 2021, the smallest year-over-year growth rate since 2010.

AI research is on the rise, across the board. The total number of AI publications has more than doubled since 2010. The specific AI topics that continue dominating research include pattern recognition, machine learning, and computer vision.

China continues to lead in total AI journal, conference, and repository publications.
The United States is still ahead in terms of AI conference and repository citations, but those leads are slowly eroding. Still, the majority of the world’s large language and multimodal models (54% in 2022) are produced by American institutions.

Industry races ahead of academia. Until 2014, most significant machine learning models were released by academia. Since then, industry has taken over. In 2022, there were 32 significant industry-produced machine learning models compared to just three produced by academia.
Building state-of-the-art AI systems increasingly requires large amounts of data, computer power, and money—resources that industry actors inherently possess in greater amounts compared to nonprofits and academia.

Large language models are getting bigger and more expensive. GPT-2, released in 2019, considered by many to be the first large language model, had 1.5 billion parameters and cost an estimated $50,000 USD to train. PaLM, one of the flagship large language models launched in 2022, had 540 billion parameters and cost an estimated $8 million USD—PaLM was around 360 times larger than GPT-2 and cost 160 times more. It’s not just PaLM: Across the board, large language and multimodal models are becoming larger and pricier."

Chapter 2: Technical Performance
	
Performance saturation on traditional benchmarks. AI continued to post state-of-the-art results, but year-over-year improvement on many benchmarks continues to be marginal. Moreover, the speed at which benchmark saturation is being reached is increasing. However, new, more comprehensive benchmarking suites such as BIG-bench and HELM are being released.

Generative AI breaks into the public consciousness. 2022 saw the release of text-to-image models like DALL-E 2 and Stable Diffusion, text-to-video systems like Make-A-Video, and chatbots like ChatGPT. Still, these systems can be prone to hallucination, confidently outputting incoherent or untrue responses, making it hard to rely on them for critical applications.

AI systems become more flexible. Traditionally AI systems have performed well on narrow tasks  but have struggled across broader tasks. Recently released models challenge that trend; BEiT-3, PaLI, and Gato, among others, are single AI systems increasingly capable of navigating multiple tasks (for example, vision, language).
Capable language models still struggle with reasoning. Language models continued to improve their generative capabilities, but new research suggests that they still struggle with complex planning tasks.

AI is both helping and harming the environment. New research suggests that AI systems can have serious environmental impacts. According to Luccioni et al., 2022, BLOOM’s training run emitted 25 times more carbon than a single air traveler on a one-way trip from New York to San Francisco. Still, new reinforcement learning models like BCOOLER show that AI systems can be used to optimize energy usage.

The world’s best new scientist … AI? AI models are starting to rapidly accelerate scientific progress and in 2022 were used to aid hydrogen fusion, improve the efficiency of matrix manipulation, and generate new antibodies.

AI starts to build better AI. Nvidia used an AI reinforcement learning agent to improve the design of the chips that power AI systems. Similarly, Google recently used one of its language models, PaLM, to suggest ways to improve the very same model. Self-improving AI learning will accelerate AI progress.

Chapter 3: Technical AI Ethics
	
The effects of model scale on bias and toxicity are confounded by training data and mitigation methods. In the past year, several institutions have built their own large models trained on proprietary data—and while large models are still toxic and biased, new evidence suggests that these issues can be somewhat mitigated after training larger models with instruction-tuning.

Generative models have arrived and so have their ethical problems. In 2022, generative models became part of the zeitgeist. These models are capable but also come with ethical challenges. Text- to-image generators are routinely biased along gender dimensions, and chatbots like ChatGPT can be tricked into serving nefarious aims.

The number of incidents concerning the misuse of AI is rapidly rising. According to the AIAAIC database, which tracks incidents related to the ethical misuse of AI, the number of AI incidents and controversies has increased 26 times since 2012. Some notable incidents in 2022 included a deepfake video of Ukrainian President Volodymyr Zelenskyy surrendering and U.S. prisons using call-monitoring technology on their inmates. This growth is evidence of both greater use of AI technologies and awareness of misuse possibilities.

Fairer models may not be less biased. Extensive analysis of language models suggests that while there is a clear correlation between performance and fairness, fairness and bias can be at odds: Language models which perform better on certain fairness benchmarks tend to have worse gender bias.

Interest in AI ethics continues to skyrocket. The number of accepted submissions to FAccT, a leading AI ethics conference, has more than doubled since 2021 and increased by a factor of 10 since 2018. 2022 also saw more submissions than ever from industry actors.

Automated fact-checking with natural language processing isn’t so straightforward after all. While several benchmarks have been developed for automated fact-checking, researchers find that 11 of 16 of such datasets rely on evidence “leaked” from fact-checking reports which did not exist at the time of the claim surfacing.

Chapter 4: The Economy	
	
The demand for AI-related professional skills is increasing across virtually every American industrial sector. Across every sector in the United States for which there is data (with the exception of agriculture, forestry, fishing, and hunting), the number of AI-related job postings has increased on average from 1.7% in 2021 to 1.9% in 2022. Employers in the United States are increasingly looking for workers with AI-related skills.

For the first time in the last decade, year-over-year private investment in AI decreased.
Global AI private investment was $91.9 billion in 2022, which represented a 26.7% decrease since 2021. The total number of AI-related funding events as well as the number of newly funded AI companies likewise decreased. Still, during the last decade as a whole, AI investment has significantly increased. In 2022 the amount of private investment in AI was 18 times greater than it was in 2013.

Once again, the United States leads in investment in AI. The U.S. led the world in terms of total amount of AI private investment. In 2022, the $47.4 billion invested in the U.S. was roughly 3.5 times the amount invested in the next highest country, China ($13.4 billion). The U.S. also continues to lead in terms of total number of newly funded AI companies, seeing 1.9 times more than the European Union and the United Kingdom combined, and 3.4 times more than China.

In 2022, the AI focus area with the most investment was medical and healthcare ($6.1 billion); followed by data management, processing, and cloud ($5.9 billion); and Fintech ($5.5 billion). However, mirroring the broader trend in AI private investment, most AI focus areas saw less investment in 2022 than in 2021. In the last year, the three largest AI private investment events were:
(1) a $2.5 billion funding event for GAC Aion New Energy Automobile, a Chinese manufacturer of electric vehicles; (2) a $1.5 billion Series E funding round for Anduril Industries, a U.S. defense products company that builds technology for military agencies and border surveillance; and (3) a $1.2 billion investment in Celonis, a business-data consulting company based in Germany.

While the proportion of companies adopting AI has plateaued, the companies that have adopted AI continue to pull ahead. The proportion of companies adopting AI in 2022 has more than doubled since 2017, though it has plateaued in recent years between 50% and 60%, according to the results of McKinsey’s annual research survey. Organizations that have adopted AI report realizing meaningful cost decreases and revenue increases.	
	
AI is being deployed by businesses in multifaceted ways. The AI capabilities most likely to have been embedded in businesses include robotic process automation (39%), computer vision (34%), NL text understanding (33%), and virtual agents (33%). Moreover, the most commonly adopted AI use case in 2022 was service operations optimization (24%), followed by the creation of new AI-based products (20%), customer segmentation (19%), customer service analytics (19%), and new AI-based enhancement of products (19%).
AI tools like Copilot are tangibly helping workers. Results of a GitHub survey on the use of Copilot, a text-to-code AI system, find that 88% of surveyed respondents feel more productive when using the system, 74% feel they are able to focus on more satisfying work, and 88% feel they are able to complete tasks more quickly.
China dominates industrial robot installations. In 2013, China overtook Japan as the nation installing the most industrial robots. Since then, the gap between the total number of industrial robots installed by China and the next-nearest nation has widened. In 2021, China installed more industrial robots than the rest of the world combined.

Chapter 5: Education
	
More and more AI specialization. The proportion of new computer science PhD graduates from U.S. universities who specialized in AI jumped to 19.1% in 2021, from 14.9% in 2020 and 10.2% in 2010.

New AI PhDs increasingly head to industry. In 2011, roughly the same proportion of new AI PhD graduates took jobs in industry (40.9%) as opposed to academia (41.6%). Since then, however, a majority of AI PhDs have headed to industry. In 2021, 65.4% of AI PhDs took jobs in industry, more than double the 28.2% who took jobs in academia.

New North American CS, CE, and information faculty hires stayed flat. In the last decade, the total number of new North American computer science (CS), computer engineering (CE), and information faculty hires has decreased: There were 710 total hires in 2021 compared to 733 in 2012. Similarly, the total number of tenure-track hires peaked in 2019 at 422 and then dropped to 324 in 2021.

The gap in external research funding for private versus public American CS departments continues to widen. In 2011, the median amount of total expenditure from external sources for computing research was roughly the same for private and public CS departments in the United States. Since then, the gap has widened, with private U.S. CS departments receiving millions more in additional funding than public universities. In 2021, the median expenditure for private universities was $9.7 million, compared to $5.7 million for public universities.

Interest in K–12 AI and computer science education grows in both the United States and the rest of the world. In 2021, a total of 181,040 AP computer science exams were taken by American students, a 1.0% increase from the previous year. Since 2007, the number of AP computer science exams has increased ninefold. As of 2021, 11 countries, including Belgium, China, and South Korea, have officially endorsed and implemented a K–12 AI curriculum.


Chapter 6: Policy and Governance	
	
Policymaker interest in AI is on the rise. An AI Index analysis of the legislative records of 127 countries shows that the number of bills containing “artificial intelligence” that were passed into law grew from just 1 in 2016 to 37 in 2022. An analysis of the parliamentary records on AI in 81 countries likewise shows that mentions of AI in global legislative proceedings have increased nearly 6.5 times since 2016.

From talk to enactment—the U.S. passed more AI bills than ever before. In 2021, only 2% of all federal AI bills in the United States were passed into law. This number jumped to 10% in 2022. Similarly, last year 35% of all state-level AI bills were passed into law.

When it comes to AI, policymakers have a lot of thoughts. A qualitative analysis of the parliamentary proceedings of a diverse group of nations reveals that policymakers think about AI from a wide range of perspectives. For example, in 2022, legislators in the United Kingdom
discussed the risks of AI-led automation; those in Japan considered the necessity of safeguarding human rights in the face of AI; and those in Zambia looked at the possibility of using AI for weather forecasting.

The U.S. government continues to increase spending on AI. Since 2017, the amount of U.S. government AI-related contract spending has increased roughly 2.5 times.

The legal world is waking up to AI. In 2022, there were 110 AI-related legal cases in United  States state and federal courts, roughly seven times more than in 2016. The majority of these cases originated in California, New York, and Illinois, and concerned issues relating to civil, intellectual property, and contract law.

Chapter 7: Diversity
	
North American bachelor’s, master’s, and PhD-level computer science students are becoming more ethnically diverse. Although white students are still the most represented ethnicity among new resident bachelor’s, master’s, and PhD-level computer science graduates, students from other ethnic backgrounds (for example, Asian, Hispanic, and Black or African American) are becoming increasingly more represented. For example, in 2011, 71.9% of new resident CS bachelor’s graduates were white. In 2021, that number dropped to 46.7%.

New AI PhDs are still overwhelmingly male. In 2021, 78.7% of new AI PhDs were male.
Only 21.3% were female, a 3.2 percentage point increase from 2011. There continues to be a gender imbalance in higher-level AI education.

Women make up an increasingly greater share of CS, CE, and information faculty hires.
Since 2017, the proportion of new female CS, CE, and information faculty hires has increased from 24.9% to 30.2%. Still, most CS, CE, and information faculty in North American universities are male (75.9%). As of 2021, only 0.1% of CS, CE, and information faculty identify as nonbinary.

American K–12 computer science education has become more diverse, in terms of both gender and ethnicity. The share of AP computer science exams taken by female students increased from 16.8% in 2007 to 30.6% in 2021. Year over year, the share of Asian, Hispanic/Latino/Latina, and Black/African American students taking AP computer science has likewise increased.

Chapter 8: Public Opinion	
	
Chinese citizens are among those who feel the most positively about AI products and services. Americans … not so much. In a 2022 IPSOS survey, 78% of Chinese respondents (the highest proportion of surveyed countries) agreed with the statement that products and services using AI have more benefits than drawbacks. After Chinese respondents, those from Saudi Arabia (76%) and India (71%) felt the most positive about AI products. Only 35% of sampled Americans (among the lowest of surveyed countries) agreed that products and services using AI had more benefits than drawbacks.

Men tend to feel more positively about AI products and services than women. Men are also more likely than women to believe that AI will mostly help rather than harm. According to the 2022 IPSOS survey, men are more likely than women to report that AI products and services make their lives easier, trust companies that use AI, and feel that AI products and services have more benefits than drawbacks. A 2021 survey by Gallup and Lloyd’s Register Foundation likewise revealed that men are more likely than women to agree with the statement that AI will mostly help rather than harm their country in the next 20 years.

People across the world and especially America remain unconvinced by self-driving cars. In a global survey, only 27% of respondents reported feeling safe in a self-driving car. Similarly, Pew Research suggests that only 26% of Americans feel that driverless passenger vehicles are a good idea for society.

Different causes for excitement and concern. Among a sample of surveyed Americans, those who report feeling excited about AI are most excited about the potential to make life and society better (31%) and to save time and make things more efficient (13%). Those who report feeling more
concerned worry about the loss of human jobs (19%); surveillance, hacking, and digital privacy (16%); and the lack of human connection (12%).

NLP researchers … have some strong opinions as well. According to a survey widely distributed to NLP researchers, 77% either agreed or weakly agreed that private AI firms have too much influence, 41% said that NLP should be regulated, and 73% felt that AI could soon lead to revolutionary societal change. These were some of the many strong opinions held by the NLP research community.


/ European Commission, "the Artificial Intelligence Act," 2021. 

Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL

LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLATIVE ACTS

THE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,Having regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 and 114 thereof,Having regard to the proposal from the European Commission,After transmission of the draft legislative act to the national parliaments,Having regard to the opinion of the European Economic and Social Committee31,Having regard to the opinion of the Committee of the Regions32,Acting in accordance with the ordinary legislative procedure,Whereas:(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values. This Regulation pursues a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, and it ensures the free movement of AI-based goods and services cross-border, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation.(2) Artificial intelligence systems (AI systems) can be easily deployed in multiple sectors of the economy and society, including cross border, and circulate throughout the Union. Certain Member States have already explored the adoption of national rules to ensure that artificial intelligence is safe and is developed and used in compliance with fundamental rights obligations. Differing national rules may lead to fragmentation of the internal market and decrease legal certainty for operators that develop or use AI systems. A consistent and high level of protection throughout the Union should therefore be ensured, while divergences hampering the free circulation of AI systems and related products and services within the internal market should be prevented, by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on Article 114 of the Treaty on the Functioning of the European Union (TFEU). To the extent that this Regulation contains specific rules on the protection of individuals with regard to the processing of personal data concerning restrictions of the use of AI systems for ‘real-time’ remote biometric identification in publicly accessible spaces for the purpose of law enforcement, it is appropriate to base this Regulation, in as far as those specific rules are concerned, on Article 16 of the TFEU. In light of those specific rules and the recourse to Article 16 TFEU, it is appropriate to consult the European Data Protection Board.(3) Artificial intelligence is a fast evolving family of technologies that can contribute to a wide array of economic and societal benefits across the entire spectrum of industries and social activities. By improving prediction, optimising operations and resource allocation, and personalising digital solutions available for individuals and organisations, the use of artificial intelligence can provide key competitive advantages to companies and support socially and environmentally beneficial outcomes, for example in healthcare, farming, education and training, infrastructure management, energy, transport and logistics, public services, security, justice, resource and energy efficiency, and climate change mitigation and adaptation.(4) At the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and rights that are protected by Union law. Such harm might be material or immaterial.(5) A Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety and the protection of fundamental rights, as recognised and protected by Union law. To achieve that objective, rules regulating the placing on the market and putting into service of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services. By laying down those rules, this Regulation supports the objective of the Union of being a global leader in the development of secure, trustworthy and ethical artificial intelligence, as stated by the European Council33, and it ensures the protection of ethical principles, as specifically requested by the European Parliament34.(6) The notion of AI system should be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments. The definition should be based on the key functional characteristics of the software, in particular the ability, for a given set of human-defined objectives, to generate outputs such as content, predictions, recommendations, or decisions which influence the environment with which the system interacts, be it in a physical or digital dimension. AI systems can be designed to operate with varying levels of autonomy and be used on a stand-alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). The definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to–date in the light of market and technological developments through the adoption of delegated acts by the Commission to amend that list.(7) The notion of biometric data used in this Regulation is in line with and should be interpreted consistently with the notion of biometric data as defined in Article 4(14) of Regulation (EU) 2016/679 of the European Parliament and of the Council35, Article 3(18) of Regulation (EU) 2018/1725 of the European Parliament and of the Council36 and Article 3(13) of Directive (EU) 2016/680 of the European Parliament and of the Council37.(8) The notion of remote biometric identification system as used in this Regulation should be defined functionally, as an AI system intended for the identification of natural persons at a distance through the comparison of a person’s biometric data with the biometric data contained in a reference database, and without prior knowledge whether the targeted person will be present and can be identified, irrespectively of the particular technology, processes or types of biometric data used. Considering their different characteristics and manners in which they are used, as well as the different risks involved, a distinction should be made between ‘real-time’ and ‘post’ remote biometric identification systems. In the case of ‘real-time’ systems, the capturing of the biometric data, the comparison and the identification occur all instantaneously, near-instantaneously or in any event without a significant delay. In this regard, there should be no scope for circumventing the rules of this Regulation on the ‘real-time’ use of the AI systems in question by providing for minor delays. ‘Real-time’ systems involve the use of ‘live’ or ‘near-‘live’ material, such as video footage, generated by a camera or other device with similar functionality. In the case of ‘post’ systems, in contrast, the biometric data have already been captured and the comparison and identification occur only after a significant delay. This involves material, such as pictures or video footage generated by closed circuit television cameras or private devices, which has been generated before the use of the system in respect of the natural persons concerned.(9) For the purposes of this Regulation the notion of publicly accessible space should be understood as referring to any physical place that is accessible to the public, irrespective of whether the place in question is privately or publicly owned. Therefore, the notion does not cover places that are private in nature and normally not freely accessible for third parties, including law enforcement authorities, unless those parties have been specifically invited or authorised, such as homes, private clubs, offices, warehouses and factories. Online spaces are not covered either, as they are not physical spaces. However, the mere fact that certain conditions for accessing a particular space may apply, such as admission tickets or age restrictions, does not mean that the space is not publicly accessible within the meaning of this Regulation. Consequently, in addition to public spaces such as streets, relevant parts of government buildings and most transport infrastructure, spaces such as cinemas, theatres, shops and shopping centres are normally also publicly accessible. Whether a given space is accessible to the public should however be determined on a case-by-case basis, having regard to the specificities of the individual situation at hand.(10) In order to ensure a level playing field and an effective protection of rights and freedoms of individuals across the Union, the rules established by this Regulation should apply to providers of AI systems in a non-discriminatory manner, irrespective of whether they are established within the Union or in a third country, and to users of AI systems established within the Union.(11) In light of their digital nature, certain AI systems should fall within the scope of this Regulation even when they are neither placed on the market, nor put into service, nor used in the Union. This is the case for example of an operator established in the Union that contracts certain services to an operator established outside the Union in relation to an activity to be performed by an AI system that would qualify as high-risk and whose effects impact natural persons located in the Union. In those circumstances, the AI system used by the operator outside the Union could process data lawfully collected in and transferred from the Union, and provide to the contracting operator in the Union the output of that AI system resulting from that processing, without that AI system being placed on the market, put into service or used in the Union. To prevent the circumvention of this Regulation and to ensure an effective protection of natural persons located in the Union, this Regulation should also apply to providers and users of AI systems that are established in a third country, to the extent the output produced by those systems is used in the Union. Nonetheless, to take into account existing arrangements and special needs for cooperation with foreign partners with whom information and evidence is exchanged, this Regulation should not apply to public authorities of a third country and international organisations when acting in the framework of international agreements concluded at national or European level for law enforcement and judicial cooperation with the Union or with its Member States. Such agreements have been concluded bilaterally between Member States and third countries or between the European Union, Europol and other EU agencies and third countries and international organisations.(12) This Regulation should also apply to Union institutions, offices, bodies and agencies when acting as a provider or user of an AI system. AI systems exclusively developed or used for military purposes should be excluded from the scope of this Regulation where that use falls under the exclusive remit of the Common Foreign and Security Policy regulated under Title V of the Treaty on the European Union (TEU). This Regulation should be without prejudice to the provisions regarding the liability of intermediary service providers set out in Directive 2000/31/EC of the European Parliament and of the Council [as amended by the Digital Services Act].(13) In order to ensure a consistent and high level of protection of public interests as regards health, safety and fundamental rights, common normative standards for all high-risk AI systems should be established. Those standards should be consistent with the Charter of fundamental rights of the European Union (the Charter) and should be non-discriminatory and in line with the Union’s international trade commitments.

(14) In order to introduce a proportionate and effective set of binding rules for AI systems, a clearly defined risk-based approach should be followed. That approach should tailor the type and content of such rules to the intensity and scope of the risks that AI systems can generate. It is therefore necessary to prohibit certain artificial intelligence practices, to lay down requirements for high-risk AI systems and obligations for the relevant operators, and to lay down transparency obligations for certain AI systems.(15) Aside from the many beneficial uses of artificial intelligence, that technology can also be misused and provide novel and powerful tools for manipulative, exploitative and social control practices. Such practices are particularly harmful and should be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and Union fundamental rights, including the right to non-discrimination, data protection and privacy and the rights of the child.(16) The placing on the market, putting into service or use of certain AI systems intended to distort human behaviour, whereby physical or psychological harms are likely to occur, should be forbidden. Such AI systems deploy subliminal components individuals cannot perceive or exploit vulnerabilities of children and people due to their age, physical or mental incapacities. They do so with the intention to materially distort the behaviour of a person and in a manner that causes or is likely to cause harm to that or another person. The intention may not be presumed if the distortion of human behaviour results from factors external to the AI system which are outside of the control of the provider or the user. Research for legitimate purposes in relation to such AI systems should not be stifled by the prohibition, if such research does not amount to use of the AI system in human-machine relations that exposes natural persons to harm and such research is carried out in accordance with recognised ethical standards for scientific research.(17) AI systems providing social scoring of natural persons for general purpose by public authorities or on their behalf may lead to discriminatory outcomes and the exclusion of certain groups. They may violate the right to dignity and non-discrimination and the values of equality and justice. Such AI systems evaluate or classify the trustworthiness of natural persons based on their social behaviour in multiple contexts or known or predicted personal or personality characteristics. The social score obtained from such AI systems may lead to the detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts, which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behaviour. Such AI systems should be therefore prohibited.(18) The use of AI systems for ‘real-time’ remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement is considered particularly intrusive in the rights and freedoms of the concerned persons, to the extent that it may affect the private life of a large part of the population, evoke a feeling of constant surveillance and indirectly dissuade the exercise of the freedom of assembly and other fundamental rights. In addition, the immediacy of the impact and the limited opportunities for further checks or corrections in relation to the use of such systems operating in ‘real-time’ carry heightened risks for the rights and freedoms of the persons that are concerned by law enforcement activities.(19) The use of those systems for the purpose of law enforcement should therefore be prohibited, except in three exhaustively listed and narrowly defined situations, where the use is strictly necessary to achieve a substantial public interest, the importance of which outweighs the risks. Those situations involve the search for potential victims of crime, including missing children; certain threats to the life or physical safety of natural persons or of a terrorist attack; and the detection, localisation, identification or prosecution of perpetrators or suspects of the criminal offences referred to in Council Framework Decision 2002/584/JHA38 if those criminal offences are punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least three years and as they are defined in the law of that Member State. Such threshold for the custodial sentence or detention order in accordance with national law contributes to ensure that the offence should be serious enough to potentially justify the use of ‘real-time’ remote biometric identification systems. Moreover, of the 32 criminal offences listed in the Council Framework Decision 2002/584/JHA, some are in practice likely to be more relevant than others, in that the recourse to ‘real-time’ remote biometric identification will foreseeably be necessary and proportionate to highly varying degrees for the practical pursuit of the detection, localisation, identification or prosecution of a perpetrator or suspect of the different criminal offences listed and having regard to the likely differences in the seriousness, probability and scale of the harm or possible negative consequences.(20) In order to ensure that those systems are used in a responsible and proportionate manner, it is also important to establish that, in each of those three exhaustively listed and narrowly defined situations, certain elements should be taken into account, in particular as regards the nature of the situation giving rise to the request and the consequences of the use for the rights and freedoms of all persons concerned and the safeguards and conditions provided for with the use. In addition, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement should be subject to appropriate limits in time and space, having regard in particular to the evidence or indications regarding the threats, the victims or perpetrator. The reference database of persons should be appropriate for each use case in each of the three situations mentioned above.(21) Each use of a ‘real-time’ remote biometric identification system in publicly accessible spaces for the purpose of law enforcement should be subject to an express and specific authorisation by a judicial authority or by an independent administrative authority of a Member State. Such authorisation should in principle be obtained prior to the use, except in duly justified situations of urgency, that is, situations where the need to use the systems in question is such as to make it effectively and objectively impossible to obtain an authorisation before commencing the use. In such situations of urgency, the use should be restricted to the absolute minimum necessary and be subject to appropriate safeguards and conditions, as determined in national law and specified in the context of each individual urgent use case by the law enforcement authority itself. In addition, the law enforcement authority should in such situations seek to obtain an authorisation as soon as possible, whilst providing the reasons for not having been able to request it earlier.(22) Furthermore, it is appropriate to provide, within the exhaustive framework set by this Regulation that such use in the territory of a Member State in accordance with this Regulation should only be possible where and in as far as the Member State in question has decided to expressly provide for the possibility to authorise such use in its detailed rules of national law. Consequently, Member States remain free under this Regulation not to provide for such a possibility at all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use identified in this Regulation.(23) The use of AI systems for ‘real-time’ remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement necessarily involves the processing of biometric data. The rules of this Regulation that prohibit, subject to certain exceptions, such use, which are based on Article 16 TFEU, should apply as lex specialis in respect of the rules on the processing of biometric data contained in Article 10 of Directive (EU) 2016/680, thus regulating such use and the processing of biometric data involved in an exhaustive manner. Therefore, such use and processing should only be possible in as far as it is compatible with the framework set by this Regulation, without there being scope, outside that framework, for the competent authorities, where they act for purpose of law enforcement, to use such systems and process such data in connection thereto on the grounds listed in Article 10 of Directive (EU) 2016/680. In this context, this Regulation is not intended to provide the legal basis for the processing of personal data under Article 8 of Directive 2016/680. However, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for purposes other than law enforcement, including by competent authorities, should not be covered by the specific framework regarding such use for the purpose of law enforcement set by this Regulation. Such use for purposes other than law enforcement should therefore not be subject to the requirement of an authorisation under this Regulation and the applicable detailed rules of national law that may give effect to it.(24) Any processing of biometric data and other personal data involved in the use of AI systems for biometric identification, other than in connection to the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement as regulated by this Regulation, including where those systems are used by competent authorities in publicly accessible spaces for other purposes than law enforcement, should continue to comply with all requirements resulting from Article 9(1) of Regulation (EU) 2016/679, Article 10(1) of Regulation (EU) 2018/1725 and Article 10 of Directive (EU) 2016/680, as applicable.(25) In accordance with Article 6a of Protocol No 21 on the position of the United Kingdom and Ireland in respect of the area of freedom, security and justice, as annexed to the TEU and to the TFEU, Ireland is not bound by the rules laid down in Article 5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 of the TFEU which relate to the processing of personal data by the Member States when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU, where Ireland is not bound by the rules governing the forms of judicial cooperation in criminal matters or police cooperation which require compliance with the provisions laid down on the basis of Article 16 of the TFEU.(26) In accordance with Articles 2 and 2a of Protocol No 22 on the position of Denmark, annexed to the TEU and TFEU, Denmark is not bound by rules laid down in Article 5(1), point (d), (2) and (3) of this Regulation adopted on the basis of Article 16 of the TFEU, or subject to their application, which relate to the processing of personal data by the Member States when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU.

(27) High-risk AI systems should only be placed on the Union market or put into service if they comply with certain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Union or whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interests as recognised and protected by Union law. AI systems identified as high-risk should be limited to those that have a significant harmful impact on the health, safety and fundamental rights of persons in the Union and such limitation minimises any potential restriction to international trade, if any.(28) AI systems could produce adverse outcomes to health and safety of persons, in particular when such systems operate as components of products. Consistently with the objectives of Union harmonisation legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market, it is important that the safety risks that may be generated by a product as a whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance, increasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be able to safely operate and performs their functions in complex environments. Similarly, in the health sector where the stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems supporting human decisions should be reliable and accurate. The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk. Those rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, and non-discrimination, consumer protection, workers’ rights, rights of persons with disabilities, right to an effective remedy and to a fair trial, right of defence and the presumption of innocence, right to good administration. In addition to those rights, it is important to highlight that children have specific rights as enshrined in Article 24 of the EU Charter and in the United Nations Convention on the Rights of the Child (further elaborated in the UNCRC General Comment No. 25 as regards the digital environment), both of which require consideration of the children’s vulnerabilities and provision of such protection and care as necessary for their well-being. The fundamental right to a high level of environmental protection enshrined in the Charter and implemented in Union policies should also be considered when assessing the severity of the harm that an AI system can cause, including in relation to the health and safety of persons.(29) As regards high-risk AI systems that are safety components of products or systems, or which are themselves products or systems falling within the scope of Regulation (EC) No 300/2008 of the European Parliament and of the Council39, Regulation (EU) No 167/2013 of the European Parliament and of the Council40, Regulation (EU) No 168/2013 of the European Parliament and of the Council41, Directive 2014/90/EU of the European Parliament and of the Council42, Directive (EU) 2016/797 of the European Parliament and of the Council43, Regulation (EU) 2018/858 of the European Parliament and of the Council44, Regulation (EU) 2018/1139 of the European Parliament and of the Council45, and Regulation (EU) 2019/2144 of the European Parliament and of the Council46, it is appropriate to amend those acts to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of each sector, and without interfering with existing governance, conformity assessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in this Regulation when adopting any relevant future delegated or implementing acts on the basis of those acts.(30) As regards AI systems that are safety components of products, or which are themselves products, falling within the scope of certain Union harmonisation legislation, it is appropriate to classify them as high-risk under this Regulation if the product in question undergoes the conformity assessment procedure with a third-party conformity assessment body pursuant to that relevant Union harmonisation legislation. In particular, such products are machinery, toys, lifts, equipment and protective systems intended for use in potentially explosive atmospheres, radio equipment, pressure equipment, recreational craft equipment, cableway installations, appliances burning gaseous fuels, medical devices, and in vitro diagnostic medical devices.(31) The classification of an AI system as high-risk pursuant to this Regulation should not necessarily mean that the product whose safety component is the AI system, or the AI system itself as a product, is considered ‘high-risk’ under the criteria established in the relevant Union harmonisation legislation that applies to the product. This is notably the case for Regulation (EU) 2017/745 of the European Parliament and of the Council47 and Regulation (EU) 2017/746 of the European Parliament and of the Council48, where a third-party conformity assessment is provided for medium-risk and high-risk products.(32) As regards stand-alone AI systems, meaning high-risk AI systems other than those that are safety components of products, or which are themselves products, it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre-defined areas specified in the Regulation. The identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high-risk AI systems.(33) Technical inaccuracies of AI systems intended for the remote biometric identification of natural persons can lead to biased results and entail discriminatory effects. This is particularly relevant when it comes to age, ethnicity, sex or disabilities. Therefore, ‘real-time’ and ‘post’ remote biometric identification systems should be classified as high-risk. In view of the risks that they pose, both types of remote biometric identification systems should be subject to specific requirements on logging capabilities and human oversight.(34) As regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity, since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities.(35) AI systems used in education or vocational training, notably for determining access or assigning persons to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their education should be considered high-risk, since they may determine the educational and professional course of a person’s life and therefore affect their ability to secure their livelihood. When improperly designed and used, such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination.(36) AI systems used in employment, workers management and access to self-employment, notably for the recruitment and selection of persons, for making decisions on promotion and termination and for task allocation, monitoring or evaluation of persons in work-related contractual relationships, should also be classified as high-risk, since those systems may appreciably impact future career prospects and livelihoods of these persons. Relevant work-related contractual relationships should involve employees and persons providing services through platforms as referred to in the Commission Work Programme 2021. Such persons should in principle not be considered users within the meaning of this Regulation. Throughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns of discrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation. AI systems used to monitor the performance and behaviour of these persons may also impact their rights to data protection and privacy.(37) Another area in which the use of AI systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve one’s standard of living. In particular, AI systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high-risk AI systems, since they determine those persons’ access to financial resources or essential services such as housing, electricity, and telecommunication services. AI systems used for this purpose may lead to discrimination of persons or groups and perpetuate historical patterns of discrimination, for example based on racial or ethnic origins, disabilities, age, sexual orientation, or create new forms of discriminatory impacts. Considering the very limited scale of the impact and the available alternatives on the market, it is appropriate to exempt AI systems for the purpose of creditworthiness assessment and credit scoring when put into service by small-scale providers for their own use. Natural persons applying for or receiving public assistance benefits and services from public authorities are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities. If AI systems are used for determining whether such benefits and services should be denied, reduced, revoked or reclaimed by authorities, they may have a significant impact on persons’ livelihood and may infringe their fundamental rights, such as the right to social protection, non-discrimination, human dignity or an effective remedy. Those systems should therefore be classified as high-risk. Nonetheless, this Regulation should not hamper the development and use of innovative approaches in the public administration, which would stand to benefit from a wider use of compliant and safe AI systems, provided that those systems do not entail a high risk to legal and natural persons. Finally, AI systems used to dispatch or establish priority in the dispatching of emergency first response services should also be classified as high-risk since they make decisions in very critical situations for the life and health of persons and their property.(38) Actions by law enforcement authorities involving certain uses of AI systems are characterised by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of a natural person’s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter. In particular, if the AI system is not trained with high quality data, does not meet adequate requirements in terms of its accuracy or robustness, or is not properly designed and tested before being put on the market or otherwise put into service, it may single out people in a discriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable and documented. It is therefore appropriate to classify as high-risk a number of AI systems intended to be used in the law enforcement context where accuracy, reliability and transparency is particularly important to avoid adverse impacts, retain public trust and ensure accountability and effective redress. In view of the nature of the activities in question and the risks relating thereto, those high-risk AI systems should include in particular AI systems intended to be used by law enforcement authorities for individual risk assessments, polygraphs and similar tools or to detect the emotional state of natural person, to detect ‘deep fakes’, for the evaluation of the reliability of evidence in criminal proceedings, for predicting the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of natural persons, or assessing personality traits and characteristics or past criminal behaviour of natural persons or groups, for profiling in the course of detection, investigation or prosecution of criminal offences, as well as for crime analytics regarding natural persons. AI systems specifically intended to be used for administrative proceedings by tax and customs authorities should not be considered high-risk AI systems used by law enforcement authorities for the purposes of prevention, detection, investigation and prosecution of criminal offences.(39) AI systems used in migration, asylum and border control management affect people who are often in particularly vulnerable position and who are dependent on the outcome of the actions of the competent public authorities. The accuracy, non-discriminatory nature and transparency of the AI systems used in those contexts are therefore particularly important to guarantee the respect of the fundamental rights of the affected persons, notably their rights to free movement, non-discrimination, protection of private life and personal data, international protection and good administration. It is therefore appropriate to classify as high-risk AI systems intended to be used by the competent public authorities charged with tasks in the fields of migration, asylum and border control management as polygraphs and similar tools or to detect the emotional state of a natural person; for assessing certain risks posed by natural persons entering the territory of a Member State or applying for visa or asylum; for verifying the authenticity of the relevant documents of natural persons; for assisting competent public authorities for the examination of applications for asylum, visa and residence permits and associated complaints with regard to the objective to establish the eligibility of the natural persons applying for a status. AI systems in the area of migration, asylum and border control management covered by this Regulation should comply with the relevant procedural requirements set by the Directive 2013/32/EU of the European Parliament and of the Council49, the Regulation (EC) No 810/2009 of the European Parliament and of the Council50 and other relevant legislation.(40) Certain AI systems intended for the administration of justice and democratic processes should be classified as high-risk, considering their potentially significant impact on democracy, rule of law, individual freedoms as well as the right to an effective remedy and to a fair trial. In particular, to address the risks of potential biases, errors and opacity, it is appropriate to qualify as high-risk AI systems intended to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts. Such qualification should not extend, however, to AI systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases, such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks or allocation of resources

(41) The fact that an AI system is classified as high risk under this Regulation should not be interpreted as indicating that the use of the system is necessarily lawful under other acts of Union law or under national law compatible with Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. Any such use should continue to occur solely in accordance with the applicable requirements resulting from the Charter and from the applicable acts of secondary Union law and national law. This Regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant.(42) To mitigate the risks from high-risk AI systems placed or otherwise put into service on the Union market for users and affected persons, certain mandatory requirements should apply, taking into account the intended purpose of the use of the system and according to the risk management system to be established by the provider.(43) Requirements should apply to high-risk AI systems as regards the quality of data sets used, technical documentation and record-keeping, transparency and the provision of information to users, human oversight, and robustness, accuracy and cybersecurity. Those requirements are necessary to effectively mitigate the risks for health, safety and fundamental rights, as applicable in the light of the intended purpose of the system, and no other less trade restrictive measures are reasonably available, thus avoiding unjustified restrictions to trade.(44) High data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law. High quality training, validation and testing data sets require the implementation of appropriate data governance and management practices. Training, validation and testing data sets should be sufficiently relevant, representative and free of errors and complete in view of the intended purpose of the system. They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used. In particular, training, validation and testing data sets should take into account, to the extent required in the light of their intended purpose, the features, characteristics or elements that are particular to the specific geographical, behavioural or functional setting or context within which the AI system is intended to be used. In order to protect the right of others from the discrimination that might result from the bias in AI systems, the providers shouldbe able to process also special categories of personal data, as a matter of substantial public interest, in order to ensure the bias monitoring, detection and correction in relation to high-risk AI systems.(45) For the development of high-risk AI systems, certain actors, such as providers, notified bodies and other relevant entities, such as digital innovation hubs, testing experimentation facilities and researchers, should be able to access and use high quality datasets within their respective fields of activities which are related to this Regulation. European common data spaces established by the Commission and the facilitation of data sharing between businesses and with government in the public interest will be instrumental to provide trustful, accountable and non-discriminatory access to high quality data for the training, validation and testing of AI systems. For example, in health, the European health data space will facilitate non-discriminatory access to health data and the training of artificial intelligence algorithms on those datasets, in a privacy-preserving, secure, timely, transparent and trustworthy manner, and with an appropriate institutional governance. Relevant competent authorities, including sectoral ones, providing or supporting the access to data may also support the provision of high-quality data for the training, validation and testing of AI systems.(46) Having information on how high-risk AI systems have been developed and how they perform throughout their lifecycle is essential to verify compliance with the requirements under this Regulation. This requires keeping records and the availability of a technical documentation, containing information which is necessary to assess the compliance of the AI system with the relevant requirements. Such information should include the general characteristics, capabilities and limitations of the system, algorithms, data, training, testing and validation processes used as well as documentation on the relevant risk management system. The technical documentation should be kept up to date.(47) To address the opacity that may make certain AI systems incomprehensible to or too complex for natural persons, a certain degree of transparency should be required for high-risk AI systems. Users should be able to interpret the system output and use it appropriately. High-risk AI systems should therefore be accompanied by relevant documentation and instructions of use and include concise and clear information, including in relation to possible risks to fundamental rights and discrimination, where appropriate.(48) High-risk AI systems should be designed and developed in such a way that natural persons can oversee their functioning. For this purpose, appropriate human oversight measures should be identified by the provider of the system before its placing on the market or putting into service. In particular, where appropriate, such measures should guarantee that the system is subject to in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role.(49) High-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy, robustness and cybersecurity in accordance with the generally acknowledged state of the art. The level of accuracy and accuracy metrics should be communicated to the users.(50) The technical robustness is a key requirement for high-risk AI systems. They should be resilient against risks connected to the limitations of the system (e.g. errors, faults, inconsistencies, unexpected situations) as well as against malicious actions that may compromise the security of the AI system and result in harmful or otherwise undesirable behaviour. Failure to protect against these risks could lead to safety impacts or negatively affect the fundamental rights, for example due to erroneous decisions or wrong or biased outputs generated by the AI system.(51) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’s vulnerabilities. Cyberattacks against AI systems can leverage AI specific assets, such as training data sets (e.g. data poisoning) or trained models (e.g. adversarial attacks), or exploit vulnerabilities in the AI system’s digital assets or the underlying ICT infrastructure. To ensure a level of cybersecurity appropriate to the risks, suitable measures should therefore be taken by the providers of high-risk AI systems, also taking into account as appropriate the underlying ICT infrastructure.

(52) As part of Union harmonisation legislation, rules applicable to the placing on the market, putting into service and use of high-risk AI systems should be laid down consistently with Regulation (EC) No 765/2008 of the European Parliament and of the Council51 setting out the requirements for accreditation and the market surveillance of products, Decision No 768/2008/EC of the European Parliament and of the Council52 on a common framework for the marketing of products and Regulation (EU) 2019/1020 of the European Parliament and of the Council53 on market surveillance and compliance of products (‘New Legislative Framework for the marketing of products’).(53) It is appropriate that a specific natural or legal person, defined as the provider, takes the responsibility for the placing on the market or putting into service of a high-risk AI system, regardless of whether that natural or legal person is the person who designed or developed the system.(54) The provider should establish a sound quality management system, ensure the accomplishment of the required conformity assessment procedure, draw up the relevant documentation and establish a robust post-market monitoring system. Public authorities which put into service high-risk AI systems for their own use may adopt and implement the rules for the quality management system as part of the quality management system adopted at a national or regional level, as appropriate, taking into account the specificities of the sector and the competences and organisation of the public authority in question.(55) Where a high-risk AI system that is a safety component of a product which is covered by a relevant New Legislative Framework sectorial legislation is not placed on the market or put into service independently from the product, the manufacturer of the final product as defined under the relevant New Legislative Framework legislation should comply with the obligations of the provider established in this Regulation and notably ensure that the AI system embedded in the final product complies with the requirements of this Regulation.(56) To enable enforcement of this Regulation and create a level-playing field for operators, and taking into account the different forms of making available of digital products, it is important to ensure that, under all circumstances, a person established in the Union can provide authorities with all the necessary information on the compliance of an AI system. Therefore, prior to making their AI systems available in the Union, where an importer cannot be identified, providers established outside the Union shall, by written mandate, appoint an authorised representative established in the Union.(57) In line with New Legislative Framework principles, specific obligations for relevant economic operators, such as importers and distributors, should be set to ensure legal certainty and facilitate regulatory compliance by those relevant operators.

(58) Given the nature of AI systems and the risks to safety and fundamental rights possibly associated with their use, including as regard the need to ensure proper monitoring of the performance of an AI system in a real-life setting, it is appropriate to set specific responsibilities for users. Users should in particular use high-risk AI systems in accordance with the instructions of use and certain other obligations should be provided for with regard to monitoring of the functioning of the AI systems and with regard to record-keeping, as appropriate.(59) It is appropriate to envisage that the user of the AI system should be the natural or legal person, public authority, agency or other body under whose authority the AI system is operated except where the use is made in the course of a personal non-professional activity.(60) In the light of the complexity of the artificial intelligence value chain, relevant third parties, notably the ones involved in the sale and the supply of software, software tools and components, pre-trained models and data, or providers of network services, should cooperate, as appropriate, with providers and users to enable their compliance with the obligations under this Regulation and with competent authorities established under this Regulation.(61) Standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation. Compliance with harmonised standards as defined in Regulation (EU) No 1025/2012 of the European Parliament and of the Council54 should be a means for providers to demonstrate conformity with the requirements of this Regulation. However, the Commission could adopt common technical specifications in areas where no harmonised standards exist or where they are insufficient.(62) In order to ensure a high level of trustworthiness of high-risk AI systems, those systems should be subject to a conformity assessment prior to their placing on the market or putting into service.(63) It is appropriate that, in order to minimise the burden on operators and avoid any possible duplication, for high-risk AI systems related to products which are covered by existing Union harmonisation legislation following the New Legislative Framework approach, the compliance of those AI systems with the requirements of this Regulation should be assessed as part of the conformity assessment already foreseen under that legislation. The applicability of the requirements of this Regulation should thus not affect the specific logic, methodology or general structure of conformity assessment under the relevant specific New Legislative Framework legislation. This approach is fully reflected in the interplay between this Regulation and the [Machinery Regulation]. While safety risks of AI systems ensuring safety functions in machinery are addressed by the requirements of this Regulation, certain specific requirements in the [Machinery Regulation] will ensure the safe integration of the AI system into the overall machinery, so as not to compromise the safety of the machinery as a whole. The [Machinery Regulation] applies the same definition of AI system as this Regulation.(64) Given the more extensive experience of professional pre-market certifiers in the field of product safety and the different nature of risks involved, it is appropriate to limit, at least in an initial phase of application of this Regulation, the scope of application of third-party conformity assessment for high-risk AI systems other than those related to products. Therefore, the conformity assessment of such systems should be carried out as a general rule by the provider under its own responsibility, with the only exception of AI systems intended to be used for the remote biometric identification of persons, for which the involvement of a notified body in the conformity assessment should be foreseen, to the extent they are not prohibited.(65) In order to carry out third-party conformity assessment for AI systems intended to be used for the remote biometric identification of persons, notified bodies should be designated under this Regulation by the national competent authorities, provided they are compliant with a set of requirements, notably on independence, competence and absence of conflicts of interests.(66) In line with the commonly established notion of substantial modification for products regulated by Union harmonisation legislation, it is appropriate that an AI system undergoes a new conformity assessment whenever a change occurs which may affect the compliance of the system with this Regulation or when the intended purpose of the system changes. In addition, as regards AI systems which continue to ‘learn’ after being placed on the market or put into service (i.e. they automatically adapt how functions are carried out), it is necessary to provide rules establishing that changes to the algorithm and its performance that have been pre-determined by the provider and assessed at the moment of the conformity assessment should not constitute a substantial modification.(67) High-risk AI systems should bear the CE marking to indicate their conformity with this Regulation so that they can move freely within the internal market. Member States should not create unjustified obstacles to the placing on the market or putting into service of high-risk AI systems that comply with the requirements laid down in this Regulation and bear the CE marking.(68) Under certain conditions, rapid availability of innovative technologies may be crucial for health and safety of persons and for society as a whole. It is thus appropriate that under exceptional reasons of public security or protection of life and health of natural persons and the protection of industrial and commercial property, Member States could authorise the placing on the market or putting into service of AI systems which have not undergone a conformity assessment.(69) In order to facilitate the work of the Commission and the Member States in the artificial intelligence field as well as to increase the transparency towards the public, providers of high-risk AI systems other than those related to products falling within the scope of relevant existing Union harmonisation legislation, should be required to register their high-risk AI system in a EU database, to be established and managed by the Commission. The Commission should be the controller of that database, in accordance with Regulation (EU) 2018/1725 of the European Parliament and of the Council55. In order to ensure the full functionality of the database, when deployed, the procedure for setting the database should include the elaboration of functional specifications by the Commission and an independent audit report.(70) Certain AI systems intended to interact with natural persons or to generate content may pose specific risks of impersonation or deception irrespective of whether they qualify as high-risk or not. In certain circumstances, the use of these systems should therefore be subject to specific transparency obligations without prejudice to the requirements and obligations for high-risk AI systems. In particular, natural persons should be notified that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use. Moreover, natural persons should be notified when they are exposed to an emotion recognition system or a biometric categorisation system. Such information and notifications should be provided in accessible formats for persons with disabilities. Further, users, who use an AI system to generate or manipulate image, audio or video content that appreciably resembles existing persons, places or events and would falsely appear to a person to be authentic, should disclose that the content has been artificially created or manipulated by labelling the artificial intelligence output accordingly and disclosing its artificial origin.(71) Artificial intelligence is a rapidly developing family of technologies that requires novel forms of regulatory oversight and a safe space for experimentation, while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures. To ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service.(72) The objectives of the regulatory sandboxes should be to foster AI innovation by establishing a controlled experimentation and testing environment in the development and pre-marketing phase with a view to ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and Member States legislation; to enhance legal certainty for innovators and the competent authorities’ oversight and understanding of the opportunities, emerging risks and the impacts of AI use, and to accelerate access to markets, including by removing barriers for small and medium enterprises (SMEs) and start-ups. To ensure uniform implementation across the Union and economies of scale, it is appropriate to establish common rules for the regulatory sandboxes’ implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes. This Regulation should provide the legal basis for the use of personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox, in line with Article 6(4) of Regulation (EU) 2016/679, and Article 6 of Regulation (EU) 2018/1725, and without prejudice to Article 4(2) of Directive (EU) 2016/680. Participants in the sandbox should ensure appropriate safeguards and cooperate with the competent authorities, including by following their guidance and acting expeditiously and in good faith to mitigate any high-risks to safety and fundamental rights that may arise during the development and experimentation in the sandbox. The conduct of the participants in the sandbox should be taken into account when competent authorities decide whether to impose an administrative fine under Article 83(2) of Regulation 2016/679 and Article 57 of Directive 2016/680.(73) In order to promote and protect innovation, it is important that the interests of small-scale providers and users of AI systems are taken into particular account. To this objective, Member States should develop initiatives, which are targeted at those operators, including on awareness raising and information communication. Moreover, the specific interests and needs of small-scale providers shall be taken into account when Notified Bodies set conformity assessment fees. Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale. Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers’ documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users.(74) In order to minimise the risks to implementation resulting from lack of knowledge and expertise in the market as well as to facilitate compliance of providers and notified bodies with their obligations under this Regulation, the AI-on demand platform, the European Digital Innovation Hubs and the Testing and Experimentation Facilities established by the Commission and the Member States at national or EU level should possibly contribute to the implementation of this Regulation. Within their respective mission and fields of competence, they may provide in particular technical and scientific support to providers and notified bodies.(75) It is appropriate that the Commission facilitates, to the extent possible, access to Testing and Experimentation Facilities to bodies, groups or laboratories established or accredited pursuant to any relevant Union harmonisation legislation and which fulfil tasks in the context of conformity assessment of products or devices covered by that Union harmonisation legislation. This is notably the case for expert panels, expert laboratories and reference laboratories in the field of medical devices pursuant to Regulation (EU) 2017/745 and Regulation (EU) 2017/746.(76) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a European Artificial Intelligence Board should be established. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation, including on technical specifications or existing standards regarding the requirements established in this Regulation and providing advice to and assisting the Commission on specific questions related to artificial intelligence.(77) Member States hold a key role in the application and enforcement of this Regulation. In this respect, each Member State should designate one or more national competent authorities for the purpose of supervising the application and implementation of this Regulation. In order to increase organisation efficiency on the side of Member States and to set an official point of contact vis-à-vis the public and other counterparts at Member State and Union levels, in each Member State one national authority should be designated as national supervisory authority.(78) In order to ensure that providers of high-risk AI systems can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process or can take any possible corrective action in a timely manner, all providers should have a post-market monitoring system in place. This system is also key to ensure that the possible risks emerging from AI systems which continue to ‘learn’ after being placed on the market or put into service can be more efficiently and timely addressed. In this context, providers should also be required to have a system in place to report to the relevant authorities any serious incidents or any breaches to national and Union law protecting fundamental rights resulting from the use of their AI systems.(79) In order to ensure an appropriate and effective enforcement of the requirements and obligations set out by this Regulation, which is Union harmonisation legislation, the system of market surveillance and compliance of products established by Regulation (EU) 2019/1020 should apply in its entirety. Where necessary for their mandate, national public authorities or bodies, which supervise the application of Union law protecting fundamental rights, including equality bodies, should also have access to any documentation created under this Regulation.(80) Union legislation on financial services includes internal governance and risk management rules and requirements which are applicable to regulated financial institutions in the course of provision of those services, including when they make use of AI systems. In order to ensure coherent application and enforcement of the obligations under this Regulation and relevant rules and requirements of the Union financial services legislation, the authorities responsible for the supervision and enforcement of the financial services legislation, including where applicable the European Central Bank, should be designated as competent authorities for the purpose of supervising the implementation of this Regulation, including for market surveillance activities, as regards AI systems provided or used by regulated and supervised financial institutions. To further enhance the consistency between this Regulation and the rules applicable to credit institutions regulated under Directive 2013/36/EU of the European Parliament and of the Council56, it is also appropriate to integrate the conformity assessment procedure and some of the providers’ procedural obligations in relation to risk management, post marketing monitoring and documentation into the existing obligations and procedures under Directive 2013/36/EU. In order to avoid overlaps, limited derogations should also be envisaged in relation to the quality management system of providers and the monitoring obligation placed on users of high-risk AI systems to the extent that these apply to credit institutions regulated by Directive 2013/36/EU.(81) The development of AI systems other than high-risk AI systems in accordance with the requirements of this Regulation may lead to a larger uptake of trustworthy artificial intelligence in the Union. Providers of non-high-risk AI systems should be encouraged to create codes of conduct intended to foster the voluntary application of the mandatory requirements applicable to high-risk AI systems. Providers should also be encouraged to apply on a voluntary basis additional requirements related, for example, to environmental sustainability, accessibility to persons with disability, stakeholders’ participation in the design and development of AI systems, and diversity of the development teams. The Commission may develop initiatives, including of a sectorial nature, to facilitate the lowering of technical barriers hindering cross-border exchange of data for AI development, including on data access infrastructure, semantic and technical interoperability of different types of data.(82) It is important that AI systems related to products that are not high-risk in accordance with this Regulation and thus are not required to comply with the requirements set out herein are nevertheless safe when placed on the market or put into service. To contribute to this objective, the Directive 2001/95/EC of the European Parliament and of the Council57 would apply as a safety net.(83) In order to ensure trustful and constructive cooperation of competent authorities on Union and national level, all parties involved in the application of this Regulation should respect the confidentiality of information and data obtained in carrying out their tasks.(84) Member States should take all necessary measures to ensure that the provisions of this Regulation are implemented, including by laying down effective, proportionate and dissuasive penalties for their infringement. For certain specific infringements, Member States should take into account the margins and criteria set out in this Regulation. The European Data Protection Supervisor should have the power to impose fines on Union institutions, agencies and bodies falling within the scope of this Regulation.(85) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the techniques and approaches referred to in Annex I to define AI systems, the Union harmonisation legislation listed in Annex II, the high-risk AI systems listed in Annex III, the provisions regarding technical documentation listed in Annex IV, the content of the EU declaration of conformity in Annex V, the provisions regarding the conformity assessment procedures in Annex VI and VII and the provisions establishing the high-risk AI systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply. It is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level, and that those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making58. In particular, to ensure equal participation in the preparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States’ experts, and their experts systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts.(86) In order to ensure uniform conditions for the implementation of this Regulation, implementing powers should be conferred on the Commission. Those powers should be exercised in accordance with Regulation (EU) No 182/2011 of the European Parliament and of the Council59.(87) Since the objective of this Regulation cannot be sufficiently achieved by the Member States and can rather, by reason of the scale or effects of the action, be better achieved at Union level, the Union may adopt measures in accordance with the principle of subsidiarity as set out in Article 5 TEU. In accordance with the principle of proportionality as set out in that Article, this Regulation does not go beyond what is necessary in order to achieve that objective.(88) This Regulation should apply from … [OP – please insert the date established in Art. 85]. However, the infrastructure related to the governance and the conformity assessment system should be operational before that date, therefore the provisions on notified bodies and governance structure should apply from … [OP – please insert the date – three months following the entry into force of this Regulation]. In addition, Member States should lay down and notify to the Commission the rules on penalties, including administrative fines, and ensure that they are properly and effectively implemented by the date of application of this Regulation. Therefore the provisions on penalties should apply from [OP – please insert the date – twelve months following the entry into force of this Regulation].(89) The European Data Protection Supervisor and the European Data Protection Board were consulted in accordance with Article 42(2) of Regulation (EU) 2018/1725 and delivered an opinion on […]”.

/ 033. Bubeck, Sebastien et al. "Sparks of Artificial General Intelligence: Early experiments with GPT-4," 2023.

AbstractArtificial intelligence (AI) researchers have been developing and re ning large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and di cult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-levelperformance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with refections on societal influences of the recent technological leap and future research directions.

1 Introduction

Intelligence is a multifaceted and elusive concept that has long challenged psychologists, philosophers, and computer scientists. An attempt to capture its essence was made in 1994 by a group of 52 psychologists who signed onto a broad definition published in an editorial about the science of intelligence [Got97]. The consensus group de ned intelligence as a very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. This definition implies that intelligence is not limited to a specific domain or task, but rather encompasses a broad range of cognitive skills and abilities. Building an arti cial system that exhibits the kind of general intelligence captured by the 1994 consensus de nition is a long-standing and ambitious goal of AI research. In early writings, the founders of the modern discipline of arti cial intelligence (AI) research called out sets of aspirational goals for understanding intelligence [MMRS06]. Over decades, AI researchers have pursued principles of intelligence, including generalizable mechanisms for reasoning (e.g., [NSS59], [LBFL93]) and construction of knowledge bases containing large corpora of commonsense knowledge [Len95]. However, many of the more recent successes in AI research can be described as being narrowly focused on well-de ned tasks and challenges, such as playing chess or Go, which were mastered by AI systems in 1996 and 2016, respectively. In the late-1990s and into the 2000s, there were increasing calls for developing more general AI systems (e.g., [SBD+96]) and scholarship in the  eld has sought to identify principles that might underly more generally intelligent systems (e.g., [Leg08, GHT15]). The phrase, \arti cial general intelligence" (AGI), was popularized in the early-2000s (see [Goe14]) to emphasize the aspiration of moving from the \narrow AI", as demonstrated in the focused, real-world applications being developed, to broader notions of intelligence, harkening back to the long-term aspirations and dreams of earlier AI research. We use AGI to refer to systems that demonstrate broad capabilities of intelligence as captured in the 1994 de nition above, with the additional requirement, perhaps implicit in the work of the consensus group, that these capabilities are at or above human-level. We note however that there is no single de nition of AGI that is broadly accepted, and we discuss other de nitions in the conclusion section.

The most remarkable breakthrough in AI research of the last few years has been the advancement of natural language processing achieved by large language models (LLMs). These neural network models are based on the Transformer architecture [VSP+17] and trained on massive corpora of web-text data, using at its core a self-supervised objective of predicting the next word in a partial sentence. In this paper, we report on evidence that a new LLM developed by OpenAI, which is an early and non-multimodal version of GPT-4 [Ope23], exhibits many traits of intelligence, according to the 1994 de nition. Despite being purely a language model, this early version of GPT-4 demonstrates remarkable capabilities on a variety of domains and tasks, including abstraction, comprehension, vision, coding, mathematics, medicine, law, understanding of human motives and emotions, and more. We interacted with GPT-4 during its early development by OpenAI using purely natural language queries (prompts)1. In Figure 1.1, we display some preliminary examples of outputs from GPT-4, asking it to write a proof of in nitude of primes in the form of a poem, to draw a unicorn in TiKZ (a language for creating graphics in LATEX), to create a complex animation in Python, and to solve a high-school level mathematical problem. It easily succeeds at all these tasks, and produces outputs that are essentially indistinguishable from (or even better than) what humans could produce. We also compare GPT-4's performance to those of previous LLMs, most notably ChatGPT, which is a  ne-tuned version of (an improved) GPT-3 [BMR+20]. In Figure 1.2, we display the results of asking ChatGPT for both the in nitude of primes poem and the TikZ unicorn drawing. While the system performs non-trivially on both tasks, there is no comparison with the outputs from GPT-4. These preliminary observations will repeat themselves throughout the paper, on a great variety of tasks. The combination of the generality of GPT-4's capabilities, with numerous abilities spanning a broad swath of domains, and its performance on a wide spectrum of tasks at or beyond human-level, makes us comfortable with saying that GPT-4 is a signi cant step towards AGI.




