/ 000. Turing, Alan M. "Computing Machinery and Intelligence," 1950.


1. The Imitation Game

I propose to consider the question, "Can machines think?" This should begin with definitions of the meaning of the terms "machine" and "think." The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words "machine" and "think" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, "Can machines think?" is to be sought in a statistical survey such as a Gallup poll. But this is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.

The new form of the problem can be described in terms of a game which we call the 'imitation game." It is played with three people, a man (A), a woman (B), and an interrogator (C) who may be of either sex. The interrogator stays in a room apart front the other two. The object of the game for the interrogator is to determine which of the other two is the man and which is the woman. He knows them by labels X and Y, and at the end of the game he says either "X is A and Y is B" or "X is B and Y is A." The interrogator is allowed to put questions to A and B thus:

C: Will X please tell me the length of his or her hair?

Now suppose X is actually A, then A must answer. It is A's object in the game to try and cause C to make the wrong identification. His answer might therefore be:

"My hair is shingled, and the longest strands are about nine inches long."

In order that tones of voice may not help the interrogator the answers should be written, or better still, typewritten. The ideal arrangement is to have a teleprinter communicating between the two rooms. Alternatively the question and answers can be repeated by an intermediary. The object of the game for the third player (B) is to help the interrogator. The best strategy for her is probably to give truthful answers. She can add such things as "I am the woman, don't listen to him!" to her answers, but it will avail nothing as the man can make similar remarks.

We now ask the question, "What will happen when a machine takes the part of A in this game?" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, "Can machines think?"

2. Critique of the New Problem
As well as asking, "What is the answer to this new form of the question," one may ask, "Is this new question a worthy one to investigate?" This latter question we investigate without further ado, thereby cutting short an infinite regress.
The new problem has the advantage of drawing a fairly sharp line between the physical and the intellectual capacities of a man. No engineer or chemist claims to be able to produce a material which is indistinguishable from the human skin. It is possible that at some time this might be done, but even supposing this invention available we should feel there was little point in trying to make a "thinking machine" more human by dressing it up in such artificial flesh. The form in which we have set the problem reflects this fact in the condition which prevents the interrogator from seeing or touching the other competitors, or hearing -their voices. Some other advantages of the proposed criterion may be shown up by specimen questions and answers. Thus:
Q: Please write me a sonnet on the subject of the Forth Bridge.
A : Count me out on this one. I never could write poetry.
Q: Add 34957 to 70764.
A: (Pause about 30 seconds and then give as answer) 105621.
Q: Do you play chess?
A: Yes.
Q: I have K at my K1, and no other pieces. You have only K at K6 and R at R1. It is your move. What do you play?
A: (After a pause of 15 seconds) R-R8 mate.
The question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include. We do not wish to penalise the machine for its inability to shine in beauty competitions, nor to penalise a man for losing in a race against an aeroplane. The conditions of our game make these disabilities irrelevant. The "witnesses" can brag, if they consider it advisable, as much as they please about their charms, strength or heroism, but the interrogator cannot demand practical demonstrations.
The game may perhaps be criticised on the ground that the odds are weighted too heavily against the machine. If the man were to try and pretend to be the machine he would clearly make a very poor showing. He would be given away at once by slowness and inaccuracy in arithmetic. May not machines carry out something which ought to be described as thinking but which is very different from what a man does? This objection is a very strong one, but at least we can say that if, nevertheless, a machine can be constructed to play the imitation game satisfactorily, we need not be troubled by this objection.

It might be urged that when playing the "imitation game" the best strategy for the machine may possibly be something other than imitation of the behaviour of a man. This may be, but I think it is unlikely that there is any great effect of this kind. In any case there is no intention to investigate here the theory of the game, and it will be assumed that the best strategy is to try to provide answers that would naturally be given by a man.
3. The Machines Concerned in the GameThe question which we put in 1 will not be quite definite until we have specified what we mean by the word "machine." It is natural that we should wish to permit every kind of engineering technique to be used in our machines. We also wish to allow the possibility than an engineer or team of engineers may construct a machine which works, but whose manner of operation cannot be satisfactorily described by its constructors because they have applied a method which is largely experimental. Finally, we wish to exclude from the machines men born in the usual manner. It is difficult to frame the definitions so as to satisfy these three conditions. One might for instance insist that the team of engineers should be all of one sex, but this would not really be satisfactory, for it is probably possible to rear a complete individual from a single cell of the skin (say) of a man. To do so would be a feat of biological technique deserving of the very highest praise, but we would not be inclined to regard it as a case of "constructing a thinking machine." This prompts us to abandon the requirement that every kind of technique should be permitted. We are the more ready to do so in view of the fact that the present interest in "thinking machines" has been aroused by a particular kind of machine, usually called an "electronic computer" or "digital computer." Following this suggestion we only permit digital computers to take part in our game.
This restriction appears at first sight to be a very drastic one. I shall attempt to show that it is not so in reality. To do this necessitates a short account of the nature and properties of these computers.
It may also be said that this identification of machines with digital computers, like our criterion for "thinking," will only be unsatisfactory if (contrary to my belief), it turns out that digital computers are unable to give a good showing in the game.
There are already a number of digital computers in working order, and it may be asked, "Why not try the experiment straight away? It would be easy to satisfy the conditions of the game. A number of interrogators could be used, and statistics compiled to show how often the right identification was given." The short answer is that we are not asking whether all digital computers would do well in the game nor whether the computers at present available would do well, but whether there are imaginable computers which would do well. But this is only the short answer. We shall see this question in a different light later.

4. Digital Computers
The idea behind digital computers may be explained by saying that these machines are intended to carry out any operations which could be done by a human computer. The human computer is supposed to be following fixed rules; he has no authority to deviate from them in any detail. We may suppose that these rules are supplied in a book, which is altered whenever he is put on to a new job. He has also an unlimited supply of paper on which he does his calculations. He may also do his multiplications and additions on a "desk machine," but this is not important.
If we use the above explanation as a definition we shall be in danger of circularity of argument. We avoid this by giving an outline. of the means by which the desired effect is achieved. A digital computer can usually be regarded as consisting of three parts:
(i) Store.
(ii) Executive unit.
(iii) Control.
The store is a store of information, and corresponds to the human computer's paper, whether this is the paper on which he does his calculations or that on which his book of rules is printed. In so far as the human computer does calculations in his bead a part of the store will correspond to his memory.
The executive unit is the part which carries out the various individual operations involved in a calculation. What these individual operations are will vary from machine to machine. Usually fairly lengthy operations can be done such as "Multiply 3540675445 by 7076345687" but in some machines only very simple ones such as "Write down 0" are possible.
We have mentioned that the "book of rules" supplied to the computer is replaced in the machine by a part of the store. It is then called the "table of instructions." It is the duty of the control to see that these instructions are obeyed correctly and in the right order. The control is so constructed that this necessarily happens.
The information in the store is usually broken up into packets of moderately small size. In one machine, for instance, a packet might consist of ten decimal digits. Numbers are assigned to the parts of the store in which the various packets of information are stored, in some systematic manner. A typical instruction might say-
"Add the number stored in position 6809 to that in 4302 and put the result back into the latter storage position."
Needless to say it would not occur in the machine expressed in English. It would more likely be coded in a form such as 6809430217. Here 17 says which of various possible operations is to be performed on the two numbers. In this case the)e operation is that described above, viz., "Add the number. . . ." It will be noticed that the instruction takes up 10 digits and so forms one packet of information, very conveniently. The control will normally take the instructions to be obeyed in the order of the positions in which they are stored, but occasionally an instruction such as"Now obey the instruction stored in position 5606, and continue from there"may be encountered, or again"If position 4505 contains 0 obey next the instruction stored in 6707, otherwise continue straight on."Instructions of these latter types are very important because they make it possible for a sequence of operations to be replaced over and over again until some condition is fulfilled, but in doing so to obey, not fresh instructions on each repetition, but the same ones over and over again. To take a domestic analogy. Suppose Mother wants Tommy to call at the cobbler's every morning on his way to school to see if her shoes are done, she can ask him afresh every morning. Alternatively she can stick up a notice once and for all in the hall which he will see when he leaves for school and which tells him to call for the shoes, and also to destroy the notice when he comes back if he has the shoes with him.
The reader must accept it as a fact that digital computers can be constructed, and indeed have been constructed, according to the principles we have described, and that they can in fact mimic the actions of a human computer very closely.
The book of rules which we have described our human computer as using is of course a convenient fiction. Actual human computers really remember what they have got to do. If one wants to make a machine mimic the behaviour of the human computer in some complex operation one has to ask him how it is done, and then translate the answer into the form of an instruction table. Constructing instruction tables is usually described as "programming." To "programme a machine to carry out the operation A" means to put the appropriate instruction table into the machine so that it will do A.
An interesting variant on the idea of a digital computer is a "digital computer with a random element." These have instructions involving the throwing of a die or some equivalent electronic process; one such instruction might for instance be, "Throw the die and put the-resulting number into store 1000." Sometimes such a machine is described as having free will (though I would not use this phrase myself), It is not normally possible to determine from observing a machine whether it has a random element, for a similar effect can be produced by such devices as making the choices depend on the digits of the decimal for .
Most actual digital computers have only a finite store. There is no theoretical difficulty in the idea of a computer with an unlimited store. Of course only a finite part can have been used at any one time. Likewise only a finite amount can have been constructed, but we can imagine more and more being added as required. Such computers have special theoretical interest and will be called infinitive capacity computers.
The idea of a digital computer is an old one. Charles Babbage, Lucasian Professor of Mathematics at Cambridge from 1828 to 1839, planned such a machine, called the Analytical Engine, but it was never completed. Although Babbage had all the essential ideas, his machine was not at that time such a very attractive prospect. The speed which would have been available would be definitely faster than a human computer but something like I 00 times slower than the Manchester machine, itself one of the slower of the modern machines, The storage was to be purely mechanical, using wheels and cards.
The fact that Babbage's Analytical Engine was to be entirely mechanical will help us to rid ourselves of a superstition. Importance is often attached to the fact that modern digital computers are electrical, and that the nervous system also is electrical. Since Babbage's machine was not electrical, and since all digital computers are in a sense equivalent, we see that this use of electricity cannot be of theoretical importance. Of course electricity usually comes in where fast signalling is concerned, so that it is not surprising that we find it in both these connections. In the nervous system chemical phenomena are at least as important as electrical. In certain computers the storage system is mainly acoustic. The feature of using electricity is thus seen to be only a very superficial similarity. If we wish to find such similarities we should took rather for mathematical analogies of function.
5. Universality of Digital ComputersThe digital computers considered in the last section may be classified amongst the "discrete-state machines." These are the machines which move by sudden jumps or clicks from one quite definite state to another. These states are sufficiently different for the possibility of confusion between them to be ignored. Strictly speaking there, are no such machines. Everything really moves continuously. But there are many kinds of machine which can profitably be thought of as being discrete-state machines. For instance in considering the switches for a lighting system it is a convenient fiction that each switch must be definitely on or definitely off. There must be intermediate positions, but for most purposes we can forget about them. As an example of a discrete-state machine we might consider a wheel which clicks round through 120 once a second, but may be stopped by a ]ever which can be operated from outside; in addition a lamp is to light in one of the positions of the wheel. This machine could be described abstractly as follows. The internal state of the machine (which is described by the position of the wheel) may be q1, q2 or q3. There is an input signal i0. or i1 (position of ]ever). The internal state at any moment is determined by the last state and input signal according to the table
(TABLE DELETED)The output signals, the only externally visible indication of the internal state (the light) are described by the table

State q1 q2 q3
output o0 o0 o1This example is typical of discrete-state machines. They can be described by such tables provided they have only a finite number of possible states.It will seem that given the initial state of the machine and the input signals it is always possible to predict all future states, This is reminiscent of Laplace's view that from the complete state of the universe at one moment of time, as described by the positions and velocities of all particles, it should be possible to predict all future states. The prediction which we are considering is, however, rather nearer to practicability than that considered by Laplace. The system of the "universe as a whole" is such that quite small errors in the initial conditions can have an overwhelming effect at a later time. The displacement of a single electron by a billionth of a centimetre at one moment might make the difference between a man being killed by an avalanche a year later, or escaping. It is an essential property of the mechanical systems which we have called "discrete-state machines" that this phenomenon does not occur. Even when we consider the actual physical machines instead of the idealised machines, reasonably accurate knowledge of the state at one moment yields reasonably accurate knowledge any number of steps later.
As we have mentioned, digital computers fall within the class of discrete-state machines. But the number of states of which such a machine is capable is usually enormously large. For instance, the number for the machine now working at Manchester is about 2 165,000, i.e., about 10 50,000. Compare this with our example of the clicking wheel described above, which had three states. It is not difficult to see why the number of states should be so immense. The computer includes a store corresponding to the paper used by a human computer. It must be possible to write into the store any one of the combinations of symbols which might have been written on the paper. For simplicity suppose that only digits from 0 to 9 are used as symbols. Variations in handwriting are ignored. Suppose the computer is allowed 100 sheets of paper each containing 50 lines each with room for 30 digits. Then the number of states is 10 100x50x30 i.e., 10 150,000 . This is about the number of states of three Manchester machines put together. The logarithm to the base two of the number of states is usually called the "storage capacity" of the machine. Thus the Manchester machine has a storage capacity of about 165,000 and the wheel machine of our example about 1.6. If two machines are put together their capacities must be added to obtain the capacity of the resultant machine. This leads to the possibility of statements such as "The Manchester machine contains 64 magnetic tracks each with a capacity of 2560, eight electronic tubes with a capacity of 1280. Miscellaneous storage amounts to about 300 making a total of 174,380."
Given the table corresponding to a discrete-state machine it is possible to predict what it will do. There is no reason why this calculation should not be carried out by means of a digital computer. Provided it could be carried out sufficiently quickly the digital computer could mimic the behavior of any discrete-state machine. The imitation game could then be played with the machine in question (as B) and the mimicking digital computer (as A) and the interrogator would be unable to distinguish them. Of course the digital computer must have an adequate storage capacity as well as working sufficiently fast. Moreover, it must be programmed afresh for each new machine which it is desired to mimic.
This special property of digital computers, that they can mimic any discrete-state machine, is described by saying that they are universal machines. The existence of machines with this property has the important consequence that, considerations of speed apart, it is unnecessary to design various new machines to do various computing processes. They can all be done with one digital computer, suitably programmed for each case. It 'ill be seen that as a consequence of this all digital computers are in a sense equivalent.
We may now consider again the point raised at the end of ยง3. It was suggested tentatively that the question, "Can machines think?" should be replaced by "Are there imaginable digital computers which would do well in the imitation game?" If we wish we can make this superficially more general and ask "Are there discrete-state machines which would do well?" But in view of the universality property we see that either of these questions is equivalent to this, "Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?"
6. Contrary Views on the Main Question
We may now consider the ground to have been cleared and we are ready to proceed to the debate on our question, "Can machines think?" and the variant of it quoted at the end of the last section. We cannot altogether abandon the original form of the problem, for opinions will differ as to the appropriateness of the substitution and we must at least listen to what has to be said in this connexion.
It will simplify matters for the reader if I explain first my own beliefs in the matter. Consider first the more accurate form of the question. I believe that in about fifty years' time it will be possible, to programme computers, with a storage capacity of about 109, to make them play the imitation game so well that an average interrogator will not have more than 70 per cent chance of making the right identification after five minutes of questioning. The original question, "Can machines think?" I believe to be too meaningless to deserve discussion. Nevertheless I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted. I believe further that no useful purpose is served by concealing these beliefs. The popular view that scientists proceed inexorably from well-established fact to well-established fact, never being influenced by any improved conjecture, is quite mistaken. Provided it is made clear which are proved facts and which are conjectures, no harm can result. Conjectures are of great importance since they suggest useful lines of research.

I now proceed to consider opinions opposed to my own.
(1) The Theological ObjectionThinking is a function of man's immortal soul. God has given an immortal soul to every man and woman, but not to any other animal or to machines. Hence no animal or machine can think.I am unable to accept any part of this, but will attempt to reply in theological terms. I should find the argument more convincing if animals were classed with men, for there is a greater difference, to my mind, between the typical animate and the inanimate than there is between man and the other animals. The arbitrary character of the orthodox view becomes clearer if we consider how it might appear to a member of some other religious community. How do Christians regard the Moslem view that women have no souls? But let us leave this point aside and return to the main argument. It appears to me that the argument quoted above implies a serious restriction of the omnipotence of the Almighty. It is admitted that there are certain things that He cannot do such as making one equal to two, but should we not believe that He has freedom to confer a soul on an elephant if He sees fit? We might expect that He would only exercise this power in conjunction with a mutation which provided the elephant with an appropriately improved brain to minister to the needs of this sort[. An argument of exactly similar form may be made for the case of machines. It may seem different because it is more difficult to "swallow." But this really only means that we think it would be less likely that He would consider the circumstances suitable for conferring a soul. The circumstances in question are discussed in the rest of this paper. In attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing .mansions for the souls that He creates.
However, this is mere speculation. I am not very impressed with theological arguments whatever they may be used to support. Such arguments have often been found unsatisfactory in the past. In the time of Galileo it was argued that the texts, "And the sun stood still . . . and hasted not to go down about a whole day" (Joshua x. 13) and "He laid the foundations of the earth, that it should not move at any time" (Psalm cv. 5) were an adequate refutation of the Copernican theory. With our present knowledge such an argument appears futile. When that knowledge was not available it made a quite different impression.
(2) The "Heads in the Sand" ObjectionThe consequences of machines thinking would be too dreadful. Let us hope and believe that they cannot do so."This argument is seldom expressed quite so openly as in the form above. But it affects most of us who think about it at all. We like to believe that Man is in some subtle way superior to the rest of creation. It is best if he can be shown to be necessarily superior, for then there is no danger of him losing his commanding position. The popularity of the theological argument is clearly connected with this feeling. It is likely to be quite strong in intellectual people, since they value the power of thinking more highly than others, and are more inclined to base their belief in the superiority of Man on this power.I do not think that this argument is sufficiently substantial to require refutation. Consolation would be more appropriate: perhaps this should be sought in the transmigration of souls.
(3) The Mathematical Objection
There are a number of results of mathematical logic which can be used to show that there are limitations to the powers of discrete-state machines. The best known of these results is known as Godel's theorem ( 1931 ) and shows that in any sufficiently powerful logical system statements can be formulated which can neither be proved nor disproved within the system, unless possibly the system itself is inconsistent. There are other, in some respects similar, results due to Church (1936), Kleene (1935), Rosser, and Turing (1937). The latter result is the most convenient to consider, since it refers directly to machines, whereas the others can only be used in a comparatively indirect argument: for instance if Godel's theorem is to be used we need in addition to have some means of describing logical systems in terms of machines, and machines in terms of logical systems. The result in question refers to a type of machine which is essentially a digital computer with an infinite capacity. It states that there are certain things that such a machine cannot do. If it is rigged up to give answers to questions as in the imitation game, there will be some questions to which it will either give a wrong answer, or fail to give an answer at all however much time is allowed for a reply. There may, of course, be many such questions, and questions which cannot be answered by one machine may be satisfactorily answered by another. We are of course supposing for the present that the questions are of the kind to which an answer "Yes" or "No" is appropriate, rather than questions such as "What do you think of Picasso?" The questions that we know the machines must fail on are of this type, "Consider the machine specified as follows. . . . Will this machine ever answer 'Yes' to any question?" The dots are to be replaced by a description of some machine in a standard form, which could be something like that used in ยง5. When the machine described bears a certain comparatively simple relation to the machine which is under interrogation, it can be shown that the answer is either wrong or not forthcoming. This is the mathematical result: it is argued that it proves a disability of machines to which the human intellect is not subject.
The short answer to this argument is that although it is established that there are limitations to the Powers If any particular machine, it has only been stated, without any sort of proof, that no such limitations apply to the human intellect. But I do not think this view can be dismissed quite so lightly. Whenever one of these machines is asked the appropriate critical question, and gives a definite answer, we know that this answer must be wrong, and this gives us a certain feeling of superiority. Is this feeling illusory? It is no doubt quite genuine, but I do not think too much importance should be attached to it. We too often give wrong answers to questions ourselves to be justified in being very pleased at such evidence of fallibility on the part of the machines. Further, our superiority can only be felt on such an occasion in relation to the one machine over which we have scored our petty triumph. There would be no question of triumphing simultaneously over all machines. In short, then, there might be men cleverer than any given machine, but then again there might be other machines cleverer again, and so on.
Those who hold to the mathematical argument would, I think, mostly he willing to accept the imitation game as a basis for discussion, Those who believe in the two previous objections would probably not be interested in any criteria.
(4) The Argument from Consciousness
This argument is very, well expressed in Professor Jefferson's Lister Oration for 1949, from which I quote. "Not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain-that is, not only write it but know that it had written it. No mechanism could feel (and not merely artificially signal, an easy contrivance) pleasure at its successes, grief when its valves fuse, be warmed by flattery, be made miserable by its mistakes, be charmed by sex, be angry or depressed when it cannot get what it wants."
This argument appears to be a denial of the validity of our test. According to the most extreme form of this view the only way by which one could be sure that machine thinks is to be the machine and to feel oneself thinking. One could then describe these feelings to the world, but of course no one would be justified in taking any notice. Likewise according to this view the only way to know that a man thinks is to be that particular man. It is in fact the solipsist point of view. It may be the most logical view to hold but it makes communication of ideas difficult. A is liable to believe "A thinks but B does not" whilst B believes "B thinks but A does not." instead of arguing continually over this point it is usual to have the polite convention that everyone thinks.
I am sure that Professor Jefferson does not wish to adopt the extreme and solipsist point of view. Probably he would be quite willing to accept the imitation game as a test. The game (with the player B omitted) is frequently used in practice under the name of viva voce to discover whether some one really understands something or has "learnt it parrot fashion." Let us listen in to a part of such a viva voce:
Interrogator: In the first line of your sonnet which reads "Shall I compare thee to a summer's day," would not "a spring day" do as well or better?
Witness: It wouldn't scan.
Interrogator: How about "a winter's day," That would scan all right.
Witness: Yes, but nobody wants to be compared to a winter's day.

Interrogator: Would you say Mr. Pickwick reminded you of Christmas?
Witness: In a way.
Interrogator: Yet Christmas is a winter's day, and I do not think Mr. Pickwick would mind the comparison.
Witness: I don't think you're serious. By a winter's day one means a typical winter's day, rather than a special one like Christmas.
And so on, What would Professor Jefferson say if the sonnet-writing machine was able to answer like this in the viva voce? I do not know whether he would regard the machine as "merely artificially signalling" these answers, but if the answers were as satisfactory and sustained as in the above passage I do not think he would describe it as "an easy contrivance." This phrase is, I think, intended to cover such devices as the inclusion in the machine of a record of someone reading a sonnet, with appropriate switching to turn it on from time to time.
In short then, I think that most of those who support the argument from consciousness could be persuaded to abandon it rather than be forced into the solipsist position. They will then probably be willing to accept our test.
I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.
(5) Arguments from Various Disabilities
These arguments take the form, "I grant you that you can make machines do all the things you have mentioned but you will never be able to make one to do X." Numerous features X are suggested in this connexion I offer a selection:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humour, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make some one fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.
No support is usually offered for these statements. I believe they are mostly founded on the principle of scientific induction. A man has seen thousands of machines in his lifetime. From what he sees of them he draws a number of general conclusions. They are ugly, each is designed for a very limited purpose, when required for a minutely different purpose they are useless, the variety of behaviour of any one of them is very small, etc., etc. Naturally he concludes that these are necessary properties of machines in general. Many of these limitations are associated with the very small storage capacity of most machines. (I am assuming that the idea of storage capacity is extended in some way to cover machines other than discrete-state machines. The exact definition does not matter as no mathematical accuracy is claimed in the present discussion,) A few years ago, when very little had been heard of digital computers, it was possible to elicit much incredulity concerning them, if one mentioned their properties without describing their construction. That was presumably due to a similar application of the principle of scientific induction. These applications of the principle are of course largely unconscious. When a burnt child fears the fire and shows that he fears it by avoiding it, f should say that he was applying scientific induction. (I could of course also describe his behaviour in many other ways.) The works and customs of mankind do not seem to be very suitable material to which to apply scientific induction. A very large part of space-time must be investigated, if reliable results are to be obtained. Otherwise we may (as most English 'Children do) decide that everybody speaks English, and that it is silly to learn French.
There are, however, special remarks to be made about many of the disabilities that have been mentioned. The inability to enjoy strawberries and cream may have struck the reader as frivolous. Possibly a machine might be made to enjoy this delicious dish, but any attempt to make one do so would be idiotic.
The claim that "machines cannot make mistakes" seems a curious one. One is tempted to retort, "Are they any the worse for that?" But let us adopt a more sympathetic attitude, and try to see what is really meant. I think this criticism can be explained in terms of the imitation game. It is claimed that the interrogator could distinguish the machine from the man simply by setting them a number of problems in arithmetic. The machine would be unmasked because of its deadly accuracy. The reply to this is simple. The machine (programmed for playing the game) would not attempt to give the right answers to the arithmetic problems. It would deliberately introduce mistakes in a manner calculated to confuse the interrogator. A mechanical fault would probably show itself through an unsuitable decision as to what sort of a mistake to make in the arithmetic. Even this interpretation of the criticism is not sufficiently sympathetic. But we cannot afford the space to go into it much further. It seems to me that this criticism depends on a confusion between two kinds of mistake, We may call them "errors of functioning" and "errors of conclusion." Errors of functioning are due to some mechanical or electrical fault which causes the machine to behave otherwise than it was designed to do. In philosophical discussions one likes to ignore the possibility of such errors; one is therefore discussing "abstract machines." These abstract machines are mathematical fictions rather than physical objects. By definition they are incapable of errors of functioning. In this sense we can truly say that "machines can never make mistakes." Errors of conclusion can only arise when some meaning is attached to the output signals from the machine. The machine might, for instance, type out mathematical equations, or sentences in English. When a false proposition is typed we say that the machine has committed an error of conclusion. There is clearly no reason at all for saying that a machine cannot make this kind of mistake. It might do nothing but type out repeatedly "O = I." To take a less perverse example, it might have some method for drawing conclusions by scientific induction. We must expect such a method to lead occasionally to erroneous results.
The claim that a machine cannot be the subject of its own thought can of course only be answered if it can be shown that the machine has some thought with some subject matter. Nevertheless, "the subject matter of a machine's operations" does seem to mean something, at least to the people who deal with it. If, for instance, the machine was trying to find a solution of the equation x2 - 40x - 11 = 0 one would be tempted to describe this equation as part of the machine's subject matter at that moment. In this sort of sense a machine undoubtedly can be its own subject matter. It may be used to help in making up its own programmes, or to predict the effect of alterations in its own structure. By observing the results of its own behaviour it can modify its own programmes so as to achieve some purpose more effectively. These are possibilities of the near future, rather than Utopian dreams.
The criticism that a machine cannot have much diversity of behaviour is just a way of saying that it cannot have much storage capacity. Until fairly recently a storage capacity of even a thousand digits was very rare.
The criticisms that we are considering here are often disguised forms of the argument from consciousness, Usually if one maintains that a machine can do one of these things, and describes the kind of method that the machine could use, one will not make much of an impression. It is thought that tile method (whatever it may be, for it must be mechanical) is really rather base. Compare the parentheses in Jefferson's statement quoted on page 22.
(6) Lady Lovelace's ObjectionOur most detailed information of Babbage's Analytical Engine comes from a memoir by Lady Lovelace ( 1842). In it she states, "The Analytical Engine has no pretensions to originate anything. It can do whatever we know how to order it to perform" (her italics). This statement is quoted by Hartree ( 1949) who adds: "This does not imply that it may not be possible to construct electronic equipment which will 'think for itself,' or in which, in biological terms, one could set up a conditioned reflex, which would serve as a basis for 'learning.' Whether this is possible in principle or not is a stimulating and exciting question, suggested by some of these recent developments But it did not seem that the machines constructed or projected at the time had this property."
I am in thorough agreement with Hartree over this. It will be noticed that he does not assert that the machines in question had not got the property, but rather that the evidence available to Lady Lovelace did not encourage her to believe that they had it. It is quite possible that the machines in question had in a sense got this property. For suppose that some discrete-state machine has the property. The Analytical Engine was a universal digital computer, so that, if its storage capacity and speed were adequate, it could by suitable programming be made to mimic the machine in question. Probably this argument did not occur to the Countess or to Babbage. In any case there was no obligation on them to claim all that could be claimed.
This whole question will be considered again under the heading of learning machines.
A variant of Lady Lovelace's objection states that a machine can "never do anything really new." This may be parried for a moment with the saw, "There is nothing new under the sun." Who can be certain that "original work" that he has done was not simply the growth of the seed planted in him by teaching, or the effect of following well-known general principles. A better variant of the objection says that a machine can never "take us by surprise." This statement is a more direct challenge and can be met directly. Machines take me by surprise with great frequency. This is largely because I do not do sufficient calculation to decide what to expect them to do, or rather because, although I do a calculation, I do it in a hurried, slipshod fashion, taking risks. Perhaps I say to myself, "I suppose the Voltage here ought to he the same as there: anyway let's assume it is." Naturally I am often wrong, and the result is a surprise for me for by the time the experiment is done these assumptions have been forgotten. These admissions lay me open to lectures on the subject of my vicious ways, but do not throw any doubt on my credibility when I testify to the surprises I experience.
I do not expect this reply to silence my critic. He will probably say that h surprises are due to some creative mental act on my part, and reflect no credit on the machine. This leads us back to the argument from consciousness, and far from the idea of surprise. It is a line of argument we must consider closed, but it is perhaps worth remarking that the appreciation of something as surprising requires as much of a "creative mental act" whether the surprising event originates from a man, a book, a machine or anything else.
The view that machines cannot give rise to surprises is due, I believe, to a fallacy to which philosophers and mathematicians are particularly subject. This is the assumption that as soon as a fact is presented to a mind all consequences of that fact spring into the mind simultaneously with it. It is a very useful assumption under many circumstances, but one too easily forgets that it is false. A natural consequence of doing so is that one then assumes that there is no virtue in the mere working out of consequences from data and general principles.
(7) Argument from Continuity in the Nervous System
The nervous system is certainly not a discrete-state machine. A small error in the information about the size of a nervous impulse impinging on a neuron, may make a large difference to the size of the outgoing impulse. It may be argued that, this being so, one cannot expect to be able to mimic the behaviour of the nervous system with a discrete-state system.
It is true that a discrete-state machine must be different from a continuous machine. But if we adhere to the conditions of the imitation game, the interrogator will not be able to take any advantage of this difference. The situation can be made clearer if we consider sonic other simpler continuous machine. A differential analyser will do very well. (A differential analyser is a certain kind of machine not of the discrete-state type used for some kinds of calculation.) Some of these provide their answers in a typed form, and so are suitable for taking part in the game. It would not be possible for a digital computer to predict exactly what answers the differential analyser would give to a problem, but it would be quite capable of giving the right sort of answer. For instance, if asked to give the value of (actually about 3.1416) it would be reasonable to choose at random between the values 3.12, 3.13, 3.14, 3.15, 3.16 with the probabilities of 0.05, 0.15, 0.55, 0.19, 0.06 (say). Under these circumstances it would be very difficult for the interrogator to distinguish the differential analyser from the digital computer.
(8) The Argument from Informality of BehaviourIt is not possible to produce a set of rules purporting to describe what a man should do in every conceivable set of circumstances. One might for instance have a rule that one is to stop when one sees a red traffic light, and to go if one sees a green one, but what if by some fault both appear together? One may perhaps decide that it is safest to stop. But some further difficulty may well arise from this decision later. To attempt to provide rules of conduct to cover every eventuality, even those arising from traffic lights, appears to be impossible. With all this I agree.
From this it is argued that we cannot be machines. I shall try to reproduce the argument, but I fear I shall hardly do it justice. It seems to run something like this. "if each man had a definite set of rules of conduct by which he regulated his life he would be no better than a machine. But there are no such rules, so men cannot be machines." The undistributed middle is glaring. I do not think the argument is ever put quite like this, but I believe this is the argument used nevertheless. There may however be a certain confusion between "rules of conduct" and "laws of behaviour" to cloud the issue. By "rules of conduct" I mean precepts such as "Stop if you see red lights," on which one can act, and of which one can be conscious. By "laws of behaviour" I mean laws of nature as applied to a man's body such as "if you pinch him he will squeak." If we substitute "laws of behaviour which regulate his life" for "laws of conduct by which he regulates his life" in the argument quoted the undistributed middle is no longer insuperable. For we believe that it is not only true that being regulated by laws of behaviour implies being some sort of machine (though not necessarily a discrete-state machine), but that conversely being such a machine implies being regulated by such laws. However, we cannot so easily convince ourselves of the absence of complete laws of behaviour as of complete rules of conduct. The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, "We have searched enough. There are no such laws."
We can demonstrate more forcibly that any such statement would be unjustified. For suppose we could be sure of finding such laws if they existed. Then given a discrete-state machine it should certainly be possible to discover by observation sufficient about it to predict its future behaviour, and this within a reasonable time, say a thousand years. But this does not seem to be the case. I have set up on the Manchester computer a small programme using only 1,000 units of storage, whereby the machine supplied with one sixteen-figure number replies with another within two seconds. I would defy anyone to learn from these replies sufficient about the programme to be able to predict any replies to untried values.
(9) The Argument from Extrasensory PerceptionI assume that the reader is familiar with the idea of extrasensory perception, and the meaning of the four items of it, viz., telepathy, clairvoyance, precognition and psychokinesis. These disturbing phenomena seem to deny all our usual scientific ideas. How we should like to discredit them! Unfortunately the statistical evidence, at least for telepathy, is overwhelming. It is very difficult to rearrange one's ideas so as to fit these new facts in. Once one has accepted them it does not seem a very big step to believe in ghosts and bogies. The idea that our bodies move simply according to the known laws of physics, together with some others not yet discovered but somewhat similar, would be one of the first to go.
This argument is to my mind quite a strong one. One can say in reply that many scientific theories seem to remain workable in practice, in spite of clashing with ESP; that in fact one can get along very nicely if one forgets about it. This is rather cold comfort, and one fears that thinking is just the kind of phenomenon where ESP may be especially relevant.A more specific argument based on ESP might run as follows: "Let us play the imitation game, using as witnesses a man who is good as a telepathic receiver, and a digital computer. The interrogator can ask such questions as 'What suit does the card in my right hand belong to?' The man by telepathy or clairvoyance gives the right answer 130 times out of 400 cards. The machine can only guess at random, and perhaps gets 104 right, so the interrogator makes the right identification." There is an interesting possibility which opens here. Suppose the digital computer contains a random number generator. Then it will be natural to use this to decide what answer to give. But then the random number generator will be subject to the psychokinetic powers of the interrogator. Perhaps this psychokinesis might cause the machine to guess right more often than would be expected on a probability calculation, so that the interrogator might still be unable to make the right identification. On the other hand, he might be able to guess right without any questioning, by clairvoyance. With ESP anything may happen.
If telepathy is admitted it will be necessary to tighten our test up. The situation could be regarded as analogous to that which would occur if the interrogator were talking to himself and one of the competitors was listening with his ear to the wall. To put the competitors into a "telepathy-proof room" would satisfy all requirements.
7. Learning MachinesThe reader will have anticipated that I have no very convincing arguments of a positive nature to support my views. If I had I should not have taken such pains to point out the fallacies in contrary views. Such evidence as I have I shall now give.

Let us return for a moment to Lady Lovelace's objection, which stated that the machine can only do what we tell it to do. One could say that a man can "inject" an idea into the machine, and that it will respond to a certain extent and then drop into quiescence, like a piano string struck by a hammer. Another simile would be an atomic pile of less than critical size: an injected idea is to correspond to a neutron entering the pile from without. Each such neutron will cause a certain disturbance which eventually dies away. If, however, the size of the pile is sufficiently increased, tire disturbance caused by such an incoming neutron will very likely go on and on increasing until the whole pile is destroyed. Is there a corresponding phenomenon for minds, and is there one for machines? There does seem to be one for the human mind. The majority of them seem to be "subcritical," i.e., to correspond in this analogy to piles of subcritical size. An idea presented to such a mind will on average give rise to less than one idea in reply. A smallish proportion are supercritical. An idea presented to such a mind that may give rise to a whole "theory" consisting of secondary, tertiary and more remote ideas. Animals minds seem to be very definitely subcritical. Adhering to this analogy we ask, "Can a machine be made to be supercritical?"
The "skin-of-an-onion" analogy is also helpful. In considering the functions of the mind or the brain we find certain operations which we can explain in purely mechanical terms. This we say does not correspond to the real mind: it is a sort of skin which we must strip off if we are to find the real mind. But then in what remains we find a further skin to be stripped off, and so on. Proceeding in this way do we ever come to the "real" mind, or do we eventually come to the skin which has nothing in it? In the latter case the whole mind is mechanical. (It would not be a discrete-state machine however. We have discussed this.)
These last two paragraphs do not claim to be convincing arguments. They should rather be described as "recitations tending to produce belief."
The only really satisfactory support that can be given for the view expressed at the beginning of ยง6, will be that provided by waiting for the end of the century and then doing the experiment described. But what can we say in the meantime? What steps should be taken now if the experiment is to be successful?
As I have explained, the problem is mainly one of programming. Advances in engineering will have to be made too, but it seems unlikely that these will not be adequate for the requirements. Estimates of the storage capacity of the brain vary from 1010 to 1015 binary digits. I incline to the lower values and believe that only a very small fraction is used for the higher types of thinking. Most of it is probably used for the retention of visual impressions, I should be surprised if more than 109 was required for satisfactory playing of the imitation game, at any rate against a blind man. (Note: The capacity of the Encyclopaedia Britannica, 11th edition, is 2 X 109) A storage capacity of 107, would be a very practicable possibility even by present techniques. It is probably not necessary to increase the speed of operations of the machines at all. Parts of modern machines which can be regarded as analogs of nerve cells work about a thousand times faster than the latter. This should provide a "margin of safety" which could cover losses of speed arising in many ways, Our problem then is to find out how to programme these machines to play the game. At my present rate of working I produce about a thousand digits of progratiirne a day, so that about sixty workers, working steadily through the fifty years might accomplish the job, if nothing went into the wastepaper basket. Some more expeditious method seems desirable.
In the process of trying to imitate an adult human mind we are bound to think a good deal about the process which has brought it to the state that it is in. We may notice three components.
(a) The initial state of the mind, say at birth,
(b) The education to which it has been subjected,
(c) Other experience, not to be described as education, to which it has been subjected.
Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain. Presumably the child brain is something like a notebook as one buys it from the stationer's. Rather little mechanism, and lots of blank sheets. (Mechanism and writing are from our point of view almost synonymous.) Our hope is that there is so little mechanism in the child brain that something like it can be easily programmed. The amount of work in the education we can assume, as a first approximation, to be much the same as for the human child.
We have thus divided our problem into two parts. The child programme and the education process. These two remain very closely connected. We cannot expect to find a good child machine at the first attempt. One must experiment with teaching one such machine and see how well it learns. One can then try another and see if it is better or worse. There is an obvious connection between this process and evolution, by the identifications
Structure of the child machine = hereditary material
Changes of the child machine = mutation,
Natural selection = judgment of the experimenter
One may hope, however, that this process will be more expeditious than evolution. The survival of the fittest is a slow method for measuring advantages. The experimenter, by the exercise of intelligence, should he able to speed it up. Equally important is the fact that he is not restricted to random mutations. If he can trace a cause for some weakness he can probably think of the kind of mutation which will improve it.
It will not be possible to apply exactly the same teaching process to the machine as to a normal child. It will not, for instance, be provided with legs, so that it could not be asked to go out and fill the coal scuttle. Possibly it might not have eyes. But however well these deficiencies might be overcome by clever engineering, one could not send the creature to school without the other children making excessive fun of it. It must be given some tuition. We need not be too concerned about the legs, eyes, etc. The example of Miss Helen Keller shows that education can take place provided that communication in both directions between teacher and pupil can take place by some means or other.
We normally associate punishments and rewards with the teaching process. Some simple child machines can be constructed or programmed on this sort of principle. The machine has to be so constructed that events which shortly preceded the occurrence of a punishment signal are unlikely to be repeated, whereas a reward signal increased the probability of repetition of the events which led up to it. These definitions do not presuppose any feelings on the part of the machine, I have done some experiments with one such child machine, and succeeded in teaching it a few things, but the teaching method was too unorthodox for the experiment to be considered really successful.The use of punishments and rewards can at best be a part of the teaching process. Roughly speaking, if the teacher has no other means of communicating to the pupil, the amount of information which can reach him does not exceed the total number of rewards and punishments applied. By the time a child has learnt to repeat "Casabianca" he would probably feel very sore indeed, if the text could only be discovered by a "Twenty Questions" technique, every "NO" taking the form of a blow. It is necessary therefore to have some other "unemotional" channels of communication. If these are available it is possible to teach a machine by punishments and rewards to obey orders given in some language, e.g., a symbolic language. These orders are to be transmitted through the "unemotional" channels. The use of this language will diminish greatly the number of punishments and rewards required.
Opinions may vary as to the complexity which is suitable in the child machine. One might try to make it as simple as possible consistently with the general principles. Alternatively one might have a complete system of logical inference "built in."' In the latter case the store would be largely occupied with definitions and propositions. The propositions would have various kinds of status, e.g., well-established facts, conjectures, mathematically proved theorems, statements given by an authority, expressions having the logical form of proposition but not belief-value. Certain propositions may be described as "imperatives." The machine should be so constructed that as soon as an imperative is classed as "well established" the appropriate action automatically takes place. To illustrate this, suppose the teacher says to the machine, "Do your homework now." This may cause "Teacher says 'Do your homework now' " to be included amongst the well-established facts. Another such fact might be, "Everything that teacher says is true." Combining these may eventually lead to the imperative, "Do your homework now," being included amongst the well-established facts, and this, by the construction of the machine, will mean that the homework actually gets started, but the effect is very satisfactory. The processes of inference used by the machine need not be such as would satisfy the most exacting logicians. There might for instance be no hierarchy of types. But this need not mean that type fallacies will occur, any more than we are bound to fall over unfenced cliffs. Suitable imperatives (expressed within the systems, not forming part of the rules of the system) such as "Do not use a class unless it is a subclass of one which has been mentioned by teacher" can have a similar effect to "Do not go too near the edge."
The imperatives that can be obeyed by a machine that has no limbs are bound to be of a rather intellectual character, as in the example (doing homework) given above. important amongst such imperatives will be ones which regulate the order in which the rules of the logical system concerned are to be applied, For at each stage when one is using a logical system, there is a very large number of alternative steps, any of which one is permitted to apply, so far as obedience to the rules of the logical system is concerned. These choices make the difference between a brilliant and a footling reasoner, not the difference between a sound and a fallacious one. Propositions leading to imperatives of this kind might be "When Socrates is mentioned, use the syllogism in Barbara" or "If one method has been proved to be quicker than another, do not use the slower method." Some of these may be "given by authority," but others may be produced by the machine itself, e.g. by scientific induction.
The idea of a learning machine may appear paradoxical to some readers. How can the rules of operation of the machine change? They should describe completely how the machine will react whatever its history might be, whatever changes it might undergo. The rules are thus quite time-invariant. This is quite true. The explanation of the paradox is that the rules which get changed in the learning process are of a rather less pretentious kind, claiming only an ephemeral validity. The reader may draw a parallel with the Constitution of the United States.
An important feature of a learning machine is that its teacher will often be very largely ignorant of quite what is going on inside, although he may still be able to some extent to predict his pupil's behavior. This should apply most strongly to the later education of a machine arising from a child machine of well-tried design (or programme). This is in clear contrast with normal procedure when using a machine to do computations one's object is then to have a clear mental picture of the state of the machine at each moment in the computation. This object can only be achieved with a struggle. The view that "the machine can only do what we know how to order it to do,"' appears strange in face of this. Most of the programmes which we can put into the machine will result in its doing something that we cannot make sense (if at all, or which we regard as completely random behaviour. Intelligent behaviour presumably consists in a departure from the completely disciplined behaviour involved in computation, but a rather slight one, which does not give rise to random behaviour, or to pointless repetitive loops. Another important result of preparing our machine for its part in the imitation game by a process of teaching and learning is that "human fallibility" is likely to be omitted in a rather natural way, i.e., without special "coaching." (The reader should reconcile this with the point of view on pages 23 and 24.) Processes that are learnt do not produce a hundred per cent certainty of result; if they did they could not be unlearnt.


/ 001. Benanav, Aaron. "Automation and the Future of Work," 2019.


From the New Left Review 119 September October 2019.

The world is abuzz with talk of automation. Rapid advances in artificial intelligence, machine learning and robotics seem set to transform the world of work. In the most advanced factories, companies like Tesla have been aiming for โlights-outโ production, in which fully automated work processes, no longer needing human hands, can run in the dark. Meanwhile, in the illuminated halls of robotics conventions, machines are on display that can play ping-pong, cook food, have sex and even hold conversations. Computers are not only developing new strategies for playing Go, but are said to be writing symphonies that bring audiences to tears. Dressed in white lab coats or donning virtual suits, computers are learning to identify cancers and will soon be developing legal strategies. Trucks are already barrelling across the us without drivers; robotic dogs are carrying military-grade weapons across desolate plains. Are we living in the last days of human toil? Is what Edward Bellamy once called the โedict of Edenโ about to be revoked, as โmenโโor at least, the wealthiest among themโbecome like gods?

There are many reasons to doubt the hype. For one thing, machines remain comically incapable of opening doors or, alas, folding laundry. Robotic security guards are toppling into mall fountains. Computerized digital assistants can answer questions and translate documents, but not well enough to do the job without human intervention; the same is true of self-driving cars. In the midst of the American โFight for Fifteenโ movement, billboards went up in San Francisco threatening to replace fast-food workers with touchscreens if a law raising the minimum wage were passed. The Wall Street Journal dubbed the bill the โrobot employment actโ. Yet many fast-food workers in Europe already work alongside touchscreens and often earn better pay than in the us. Is the talk of automation overdone?

1. THE AUTOMATION DISCOURSE

In the pages of newspapers and popular magazines, scare stories about automation may remain just idle chatter. However, over the past decade, this talk has crystalized into an influential social theory, which purports not only to analyse current technologies and predict their future, but also to explore the consequences of technological change for society at large. This automation discourse rests on four main propositions. First, workers are already being displaced by ever-more advanced machines, resulting in rising levels of โtechnological unemploymentโ. Second, this displacement is a sign that we are on the verge of achieving a largely automated society, in which nearly all work will be performed by self-moving machines and intelligent computers. Third: automation should entail humanityโs collective liberation from toil, but because we live in a society where most people must work in order to live, this dream may well turn out to be a nightmare. Fourth, therefore, the only way to prevent a mass-unemployment catastrophe is to provide a universal basic income (ubi), breaking the connection between the incomes people earn and the work they do, as a way to inaugurate a new society.

This argument has been put forward by a number of self-described futurists. In the widely read Second Machine Age (2014), Erik Brynjolfsson and Andrew McAfee argue that we find ourselves โat an inflection pointโa bend in the curve where many technologies that used to be found only in science fiction are becoming everyday reality.โ New technologies promise an enormous โbountyโ, but Brynjolfsson and McAfee caution that โthere is no economic law that says that all workers, or even a majority of workers, will benefit from these advances.โ On the contrary: as the demand for labour falls with the adoption of more advanced technologies, wages are stagnating; a rising share of annual income is therefore being captured by capital rather than by labour. The result is growing inequality, which could โslow our journeyโ into what they call โthe second machine ageโ by generating a โfailure mode of capitalismโ in which rentier extraction crowds out technological innovation. In Rise of the Robots (2015), Martin Ford similarly claims that we are pushing โtowards a tipping pointโ that is poised to โmake the entire economy less labour-intensive.โ Again, โthe most frightening long-term scenario of all might be if the global economic system eventually manages to adapt to the new realityโ, leading to the creation of an โautomated feudalismโ in which the โpeasants would be largely superfluousโ and the elite impervious to economic demands. For these authors, education and retraining will not be enough to stabilize the demand for labour in an automated economy; some form of guaranteed non-wage income, such as a negative income tax, must be put in place.

The automation discourse has been enthusiastically adopted by the jeans-wearing elite of Silicon Valley. Bill Gates is advocating for a tax on robots. Mark Zuckerberg told Harvard undergraduate inductees that they should โexplore ideas like universal basic incomeโ, a policy Elon Musk also thinks will become increasingly โnecessaryโ over time, as robots outcompete humans across a growing range of jobs. Musk has been naming his SpaceX drone vessels after spaceships from Iain M. Banksโs Culture Series, a set of ambiguously utopian science-fiction novels depicting a post-scarcity world in which human beings live fulfilling lives alongside intelligent robots, called โmindsโ, without the need for markets or states.

Politicians and their advisors have equally identified with the automation discourse, which has become one of the leading perspectives on our โdigital futureโ. In his farewell presidential address, Obama suggested that the โnext wave of economic dislocationsโ will come not from overseas trade, but rather from โthe relentless pace of automation that makes a lot of good, middle-class jobs obsolete.โ Robert Reich, former Labour Secretary under Bill Clinton, expressed similar fears: we will soon reach a point โwhere technology is displacing so many jobs, not just menial jobs but also professional jobs, that weโre going to have to take seriously the notion of a universal basic income.โ Clintonโs former Treasury Secretary, Lawrence Summers, made the same admission: once-โstupidโ ideas about technological unemployment now seem increasingly smart, he said, as workersโ wages stagnate and economic inequality rises. The discourse has become the basis of a long-shot presidential campaign for 2020: Andrew Yang, Obamaโs former โAmbassador of Global Entrepreneurshipโ, has penned his own tome on automation, The War on Normal People, and is now running a futuristic campaign on a โHumanity Firstโ, ubi platform. Among Yangโs vocal supporters is Andy Stern, former head of the seiu, whose Raising the Floor is yet another example of the discourse.10

Yang and Sternโlike all of the other writers named so farโtake pains to assure readers that some variant of capitalism is here to stay, even if it must jettison its labour markets; however, they admit to the influence of figures on the far left who offer a more radical version of the automation discourse. In Inventing the Future, Nick Srnicek and Alex Williams argue that the โmost recent wave of automation is poisedโ to transform the labour market โdrastically, as it comes to encompass every aspect of the economyโ.11 They claim that only a socialist government would actually be able to fulfil the promise of full automation by creating a post-work or post-scarcity society. In Four Futures, Peter Frase thoughtfully explores the alternative outcomes for such a post-scarcity society, depending on whether it still had private property and still suffered from resource scarcity, which could persist even if labour scarcity were overcome. Like the liberal proponents of the automation discourse, these left-wing writers stress that, even if the coming of advanced robotics is inevitable, โthere is no necessary progression into a post-work worldโ. Srnicek, Williams and Frase are all proponents of ubi, but in a left-wing variant. For them, ubi serves as a bridge to โfully automated luxury communismโ, a term originally coined in 2014 by Aaron Bastani to name a possible goal of socialist politics, and which flourished for five years as a meme on the internet before his bookโoutlining an automated future in which artificial intelligence, solar power, gene-editing, asteroid mining and lab-grown meat generate a world of limitless leisure and self-inventionโfinally appeared.

Recurrent fears

These futurist visions, from all points of the political spectrum, depend upon a common prediction of the trajectory of technological change. Have they got this right? To answer this question, it is helpful to have a couple of working definitions. Automation may be distinguished as a specific form of labour-saving technical innovation: automation technologies fully substitute for human labour, rather than merely augmenting human-productive capacities. With labour-augmenting technologies, a given job category will continue to exist, but each worker in that category will be more productive. For example, adding new machines to an assembly-line producing cars may make line workers more productive without abolishing line work as such. However, fewer workers will be needed in total to produce any given number of automobiles. Whether that results in fewer jobs will then depend on how much outputโthe total number of carsโalso increases.

By contrast, automation may be defined as what Kurt Vonnegut describes in Player Piano: it takes place whenever an entire โjob classification has been eliminated. Poof.โ No matter how much production might increase, another telephone-switchboard operator or hand-manipulator of rolled steel will never be hired. In these cases, machines have fully substituted for human labour. Much of the debate around the future of workplace automation turns on an evaluation of the degree to which present or near-future technologies are labour-substituting or labour-augmenting in character. Distinguishing between these two types of technical change turns out to be incredibly difficult in practice. One famous study from the Oxford Martin School suggested that 47 per cent of jobs in the us are at high risk of automation; a more recent study from the oecd predicts that 14 per cent of oecd jobs are at high risk, with another 32 per cent at risk of significant change in the way they are carried out (due to labour-augmenting rather than substituting innovations).

It is unclear, however, whether even the highest of these estimates suggests that a qualitative break with the past has taken place. By one count, โ57 per cent of the jobs workers did in the 1960s no longer exist todayโ.16 Automation, in fact, turns out to be a constant feature of the history of capitalism. By contrast, the discourse around automation, which extrapolates from instances of technological change to a broader social theory, is not constant; it periodically recurs in modern history. Excitement about a coming age of automation can be traced back to at least the mid-19th century. Charles Babbage published On the Economy of Machinery and Manufactures in 1832; John Adolphus Etzlerโs The Paradise Within the Reach of All Men, Without Labour appeared in 1833, Andrew Ureโs The Philosophy of Manufactures in 1835. These books presaged the imminent emergence of largely or fully automated factories, run with minimal or merely supervisory human labour. This vision was a major influence on Marx, whose Capital, Volume One argued that a complex world of interacting machines was in the process of displacing labour at the centre of economic life.

Visions of automated factories then appeared again in the 1930s, 1950s and 1980s, before their re-emergence in the 2010s. Each time, they were accompanied or shortly followed by predictions of a coming age of โcatastrophic unemployment and social breakdownโ, which could be prevented only if society were reorganized.17 To point out the periodicity of this discourse is not to say that its accompanying social visions should be dismissed. For one thing, the technological breakthroughs presaged by automation discourse could still be achieved at any time: just because they were wrong in the past does not necessarily mean that they will always be wrong in the future. More than that, these visions of automation have clearly been generative in social terms: they point to certain utopian possibilities latent within modern capitalist societies. The error in their approach is merely to suppose that, via ongoing technological shifts, these utopian possibilities will imminently be revealed via a catastrophe of mass unemployment.

The basic insight on which automation theory relies was described, most succinctly, by the Harvard economist Wassily Leontief. He pointed out that the โeffective operation of the automatic price mechanismโ at the core of capitalist societies โdepends criticallyโ on a peculiar feature of modern technology, namely that in spite of bringing about โan unprecedented rise in total outputโ, it nevertheless โstrengthened the dominant role of human labour in most kinds of productive processesโ.18 At any time, a breakthrough could destroy this fragile pin, annihilating the social preconditions of functioning market economies. Drawing on this insightโand adding only that such a technological breakthrough now existsโthe automation prognosticators often argue that capitalism must be a transitory mode of production, which will eventually give way to a new form of life that does not organize itself around work for wages and monetary exchange.

Taking its periodicity into account, automation theory may be described as a spontaneous discourse of capitalist societies, which, for a mixture of structural and contingent reasons, reappears in those societies time and again as a way of thinking through their limits. What summons the automation discourse periodically into being is a deep anxiety about the functioning of the labour market: there are simply too few jobs for too many people. Proponents of the automation discourse consistently explain the problem of a low demand for labour in terms of runaway technological change.

Declining labour demand
If automation discourse appeals so widely again today, it is because, whatever their causes, the ascribed consequences of automation are all around us: global capitalism clearly is failing to provide jobs for many of the people who need them. There is, in other words, a persistently low demand for labour, reflected not only in higher spikes of unemployment and increasingly jobless recoveriesโboth frequently cited by automation theoristsโbut also in a phenomenon with more generic consequences: declining labour shares of income. Many studies have now confirmed that the labour share, whose steadiness was held to be a stylized fact of economic growth, has been falling for decades (Figure 1).
These shifts signal a radical decline in workersโ bargaining power. Realities for the typical worker are worse than these statistics suggest, since wage growth has become increasingly skewed towards the highest earners: the infamous top one per cent. A growing gap has opened up not only between the growth of labour productivity and average wage-incomes, but also between the growth of average wages and that of median wages, with the result that many workers see a vanishingly thin slice of economic growth (Figure 2).20 Under these conditions, rising inequality is contained only by the strength of redistributive programmes. Even critics of automation discourse such as David Autor and Robert Gordon are disturbed by these trends: something has gone wrong with the economy, leading to a low demand for labour.

Is automation the cause of the low demand for labour? I will join the critics of automation discourse in arguing that it is not. However, along the way, I will also criticize the criticsโboth for producing explanations of low labour demand that only apply in high-income countries and for failing to produce anything like a radical vision of social change that is adequate to the scale of the problems we now confront. Indeed, it should be said from the outset that I am more sympathetic to the left automation theorists than to their critics.

Even if the explanation they offer turns out to be inadequate, the automation theorists have at least focused the worldโs attention on the problem of a persistently low demand for labour. They have also excelled in actually trying to imagine solutions to this problem that are broadly emancipatory in character. In Jamesonโs terms, the automation theorists are our late capitalist utopians. In a world reeling from the โperfect stormโ of climate change, rising inequality, recalcitrant neoliberalism and resurgent ethno-nationalism, the automation theorists are the ones pushing through the catastrophe with a vision of an emancipated future, in which humanity advances to the next stage in our history, whatever that might mean (or whatever we want to make it mean), and technology helps to free us all to discover and follow our passions. That is true in spite of the fact thatโlike many of the utopians of the pastโthe actual visions these latest utopians offer need to be freed from their largely technocratic fantasies of how social change to a better future might take place.

Major shifts in the forms of government intervention in the economy are adopted only under massive social pressure, such as, in the course of the 20th century, the threat of communism or of civilizational collapse. Today, policy reforms could emerge in response to pressure coming from a new mass movement, aiming to change the basic makeup of the social order. Instead of fearing that movement, we should see ourselves as part of it, helping articulate its goals and paths forward. If that movement is defeated, maybe the best we will get is basic income, but that should not be our goal. We should be reaching towards a post-scarcity world, which advanced technologies will certainly help us realize, even if full automation is not achievableโor even desirable.The return of automation discourse is a symptom of our era, as it was in times past: it arises when the global economyโs failure to create enough jobs causes people to question its fundamental viability. The breakdown of this market mechanism today is more extreme than at any time in the past. This is because a greater share of the worldโs population than ever before depends on selling its labour or the simple products of its labour to survive, in the context of weakening global economic growth. Our present reality is better described by near-future science-fiction dystopias than by standard economic analysis; ours is a hot planet, with micro-drones flying over the heads of the street hawkers and rickshaw pullers, where the rich live in guarded, climate-controlled communities while the rest of us wile away our time in dead-end jobs, playing video games on smartphones. We need to slip out of this timeline and into another.
Reaching towards a post-scarcity worldโin which all individuals are guaranteed access to whatever they need to make a life, without exceptionโcan become the basis on which humanity mounts a battle against climate change. It can also be the foundation on which we remake the world, creating the conditions in which, as James Boggs once put it, โfor the first time in human history, great masses of people will be free to explore and reflect, to question and to create, to learn and to teach, unhampered by the fear of where the next meal is coming fromโ. Finding our way forward requires a break between work and income, as the automation theorists recognize, but also between profit and income, as many do not.
In responding to the automation discourse, then, I will argue that the decline in the demand for labour is due not to an unprecedented leap in technological innovation, but to ongoing technical change in an environment of deepening economic stagnation. In the second part of this contribution, to be published in nlr 120, I contend that this fall in labour demand manifests not as mass unemployment, but rather as mass under-employment, not necessarily a problem for the elites. On this basis, I mount a critique of technocratic solutions, like basic income. I offer a thought-experiment of how we might imagine a post-scarcity society that centres on humans, not machines, and project a path of how we might get there through social struggle, rather than administrative intervention. But first, in Part One, I provide a diagnosis of the underlying causes of the decline in demand for labour. This involves a detour to consider the fortunes of the global manufacturing sector and the competitive dynamics at work in labourโs โdeindustrializationโ.
2. LABOUR'S GLOBAL DEINDUSTRALIZATION
Automation-discourse theorists recognize that, if technologically induced job-destruction is to have widespread social ramifications, it will have to eliminate employment in the vast and variegated service sector, which absorbs 74 per cent of workers in high-income countries and 52 per cent worldwide. They therefore focus on โnew forms of service-sector automationโ in retail, transportation and food services, where โrobotizationโ is said to be โgathering steamโ with a growing army of machines that take orders, stock shelves, drive cars and flip burgers. Many more service-sector jobs, including some that require years of education and training, will supposedly be rendered obsolete in the coming years due to advances in artificial intelligence. Of course, these claims are mostly predictions about the effects that technologies will have on future patterns of employment. Such predictions can go wrongโas for example when Eatsa, an automated fast-food company which employed neither cashiers nor waiters, was forced to close most of its stores in 2017.
In making their case, automation theorists often point to the manufacturing sector as the precedent for what they imagine is beginning to happen in servicesโfor in manufacturing, the employment-apocalypse has already taken place. To evaluate the theoristsโ claims, it therefore makes sense to begin by looking at what role automation has played in that sectorโs fate. After all, manufacturing is the area most amenable to automation, since on the shop floor it is possible to โradically simplify the environment in which machines work, to enable autonomous operationโ. Industrial robotics has been around for a long time: the first robot, the โunimateโ, was installed in a General Motors plant in 1961. Still, until the 1960s, scholars studying this sector were able to dismiss Luddite fears of long-term technological unemployment out of hand. Manufacturing employment in fact grew most rapidly in those lines where technical innovation was happening at the fastest pace, because it was in those lines that prices fell the fastest, stoking the growth of demand for the products.
Industrialization has long since given way to deindustrialization, and not just in any one line but across the manufacturing sectors of most countries. The share of workers employed in manufacturing fell first across the high-income world: manufacturing employed 22 per cent of all workers in the us in 1970; that share declined to just 8 per cent in 2017. Over the same period, manufacturing employment shares fell from 23 per cent to 9 per cent in France, and from 30 per cent to 8 per cent in the uk. Japan, Germany and Italy have experienced smaller but still substantial declines: in Japan from 25 per cent to 15 per cent, in Germany from 29 per cent to 17 per cent, and in Italy from 25 per cent to 15 per cent. In all cases, the declines were eventually associated with substantial falls in the total number of people employed in manufacturing. In the us, Germany, Italy and Japan, the overall number of manufacturing jobs fell by approximately a third from postwar peaks; in France, by 50 per cent and in the uk, by 67 per cent.

It is commonly assumed that deindustrialization must be the result of production facilities moving offshore. Yet in none of the countries named above has manufacturing job loss been associated with declines in manufacturing output. Real value added in manufacturing more than doubled in the us, France, Germany, Japan and Italy between 1970 and 2017. Even the uk, whose manufacturing sector fared worst of all among this group, saw a 25 per cent increase in manufacturing real value added over this period. To be sure, low- and middle-income countries are producing more and more goods for import into high-income countries; however, deindustrialization in the latter cannot simply be the result of productive capacity moving to the former. In the scholarly literature, deindustrialization is therefore โmost commonly defined as a decline in the share of manufacturing in total employmentโ, regardless of corresponding trends in levels of manufactured output. This definition moves in step with automation theoristsโ core expectations: more goods are being produced but by fewer workers.

It is on this basis that commentators typically cite rapidly rising labour productivity, rather than an influx of low-cost imports from abroad, as the primary cause of industrial-job loss in advanced economies. On closer inspection, however, this explanation turns out to be inadequate: no upward leap has taken place in manufacturing productivity levels. On the contrary, manufacturing productivity has been growing at a sluggish pace for decades, leading Robert Solow to quip, โWe see the computer age everywhere, except in the productivity statistics.โ Automation theorists discuss this โproductivity paradoxโ as a problem for their accountโexplaining it in terms of weak demand for products, or the persistent availability of low-wage workersโbut they understate its true significance. This is partly due to the appearance of steady labour-productivity growth in us manufacturing, at an average rate of around 3 per cent per year since 1950. On that basis, Brynjolfsson and McAfee suggest, automation could show up in the compounding effects of exponential growth, rather than an uptick in the growth rate.

However, official us manufacturing growth-rate statistics are overinflated, for example in logging the production of computers with higher processing speeds as equivalent to the production of more computers. On that basis, government statistics claim that productivity levels in the computers and electronics sub-sector rose at an average rate of over 10 per cent per year between 1987 and 2011, even as productivity growth rates outside of that sub-sector fell to around 2 per cent per year over the same period. Since 2011, trends across the manufacturing sector have worsened: real output per hour in the sector as a whole was lower in 2017 than at its peak in 2010. Productivity growth rates in manufacturing collapsed precisely when they were supposed to be rising rapidly due to industrial automation.

Correcting manufacturing-productivity statistics in the us brings them more into line with trends visible in the statistics of other countries. In Germany and Japan, manufacturing-productivity growth rates have fallen dramatically since their postwar peaks. In Germany, for example, manufacturing productivity grew at an average annual rate of 6.3 per cent per year in the 1950s and 60s, falling to 2.4 per cent since 2000. This downward trend is to some extent an expected result of the end of an era of rapid, catch-up growth. However, it should still be surprising to the automation theorists, since Germany and Japan have raced ahead of the us in the field of industrial robotics. Indeed, the robots used in Teslaโs largely automated car factory in California were made by a German robotics company. German and Japanese firms deploy about 60 per cent more industrial robots per 10,000 manufacturing workers, compared to the US.

Yet deindustrialization continues to take place in all these countries, despite lacklustre manufacturing-productivity growth rates: that is, it is taking place as the automation theorists expect, but not for the reasons they offer. To explore the causes of deindustrialization in more detail, I use the following accounting identity. For any given industry, the rate of growth of output (ฮO) minus the rate of growth of labour productivity (ฮP) equals the rate of growth of employment (ฮE). Thus, ฮO โ ฮP = ฮE. So, for example, if the output of automobiles grows by 3 per cent per year, and productivity in the automobile industry grows by 2 per cent per year, then employment in that industry must necessarily rise by one per cent per year (3 โ 2 = 1). Contrariwise, if output grows by 3 per cent per year and productivity grows by 4 per cent per year, employment will contract by 1 per cent per year (3 โ 4 = -1).

Disaggregating manufacturing-output growth rates in France provides us with a sense of the typical pattern playing out across the high-income countries (Figure 3). During the so-called Golden Age of postwar capitalism, productivity growth rates in French manufacturing were much higher than they are todayโ5.2 per cent per year, on average, between 1950 and 1973โbut output growth rates were even higher than thatโ5.9 per cent per yearโas a result of a steady increase in employment of 0.7 per cent per year. Since 1973, both output and productivity rates have declined, but output rates fell much more sharply than productivity rates. By the early years of the 21st century, productivity growth ratesโalthough much slower, at 2.7 per cent per yearโwere now faster than their corresponding output growth ratesโat 0.9 per centโas manufacturing employment contracted rapidly, by 1.7 per cent per year.

This disaggregation helps explain why automation theorists falsely perceive productivity to be growing at a rapid pace in manufacturing: in fact, productivity growth has been rapid only relative to extremely sluggish output growth. The same pattern can be seen in the statistics of other countries: no absolute decline in levels of manufacturing production has taken place, but there has been a decline in the output growth rate, with the result that output is growing more slowly than productivity (Table 1, overleaf). The simultaneity of limited technological dynamism and worsening economic stagnation combines to generate a progressive decline in industrial employment levels.

As such, โoutput-ledโ deindustrialization is impossible to explain in purely technological terms. In searching for alternative perspectives, economists have mostly preferred to describe it as a harmless evolutionary feature of advanced economies. However, that perspective is itself at a loss in explaining extreme variations in the gdp per capita levels at which this supposedly evolutionary economic shift has taken place. Deindustrialization unfolded first in high-income countries in the late 1960s and early 1970s, at the tail-end of a period in which levels of income per person had converged across the us, Europe and Japan. In the decades that followed, deindustrialization then spread โprematurelyโ to middle- and low-income countries, with larger variations in incomes per capita (Figure 4). In the late 1970s, deindustrialization arrived in southern Europe; much of Latin America, parts of East and Southeast Asia, and southern Africa followed in the 1980s and 1990s. Peak industrialization levels in many poorer countries were so low that it may be more accurate to say that they never industrialized in the first place.

By the end of the 20th century, it was possible to describe deindustrialization as a kind of global epidemic: worldwide manufacturing employment rose in absolute terms by 0.4 per cent per year between 1991 and 2016, but that was much slower than the overall growth of the global labour force, with the result that the manufacturing share of total employment declined by 3 percentage points over the same period. China is a key exception, but only a partial one (Figure 5, overleaf). In the mid 1990s, Chinese state-owned enterprises shed large numbers of workers, sending manufacturing-employment shares on a steady downward trajectory. China re-industrialized, starting in the early 2000s, but then began to deindustrialize once again in the mid 2010s: its manufacturing-employment share has since dropped from 19.3 per cent in 2013 to 17.5 per cent in 2017, with further falls likely. If deindustrialization cannot be explained by either automation or the internal evolution of advanced economies, what could be its source?

3. BLIGHT OF MANUFACTURING OVERCAPACITY

What the economistsโ accounts fail to register in explaining deindustrialization is also what is missing from the automation theoristsโ accounts. The truth is that rates of output growth in manufacturing have tended to decline, not only in this or that country, but worldwide (Figure 6). In the 1950s and 60s, global manufacturing production expanded at an average annual rate of 7.1 per cent per year, in real terms. That rate fell progressively to 4.8 per cent in the 1970s, and to 3.0 per cent between 1980 and 2007. Since the 2008 crisis and up to 2014, manufacturing output expanded at just 1.6 per cent per year, on a world scaleโthat is, at less than a quarter of the pace achieved during the so-called postwar Golden Age. It is worth noting that these figures include the dramatic expansion of manufacturing productive capacity in China. Again, it is the incredible degree of slowdown or even stagnation in manufacturing-output growth, visible on the world scale, that explains why manufacturing-productivity growth appears to be advancing at a rapid clip, even though it is actually much slower than before. More and more is produced with fewer workers, as the automation theorists claim, but not because technological change is giving rise to high rates of productivity growth. On the contrary, productivity growth in manufacturing appears rapid today only because the yardstick of output growth, against which it is measured, is shrinking.Seen from this perspective, the global wave of deindustrialization can be said to find its origins not in runaway technical change but rather in worsening overcapacity in world markets for manufactured goods. The rise in overcapacity developed stepwise after World War Two. In the immediate postwar period, the us hosted the most dynamic economy in the world, with the most advanced technologies. Under the threat of communist expansion within Europe, as well as in East and Southeast Asia, the us proved willing to share its technological largesse with its former imperial competitors Germany and Japan, as well as other โfrontlineโ countries, in order to bring them all under the us security umbrella. In the first few decades of the post-wwii era, these technology transfers were a major boost to economic growth in Europe and Japan, opening up opportunities for export-led expansion. This strategy was also supported by the devaluation of European and Japanese currencies against the dollar. However, as Robert Brenner has argued, rising manufacturing capacity across the globe quickly generated overcapacity, issuing in a โlong downturnโ in manufacturing output growth rates.

What mattered here was not only the later building out of manufacturing capacity in the global South, but the earlier creation of such capacity in countries like Germany, Italy and Japan, which hosted the first low-cost producers in the postwar era who succeeded in taking shares in global markets for industrial goods, and then invading the previously impenetrable us domestic market. That competition caused rates of industrial-output growth in the us to decline in the late 1960s, issuing in deindustrialization in employment terms. As the us responded to heightened import penetration in the 1970s by breaking up the Bretton Woods order and devaluing the dollar, these same problems spread from the highest wage countries in North America and northern Europe to Japan and the rest of Europe. Thereafter, as more and more countries built up manufacturing capacity, adopted export-led growth strategies and entered global markets for manufactured goods, falling rates of manufacturing-output growth and consequent labour deindustrialization also spread to Latin America, the Middle East, Asia and Africa, as well as to the global economy taken as a whole.

Deindustrialization is not only a matter of technological advance, but also of a global redundancy of technological capacities, creating more crowded markets in which rapid rates of industrial-output expansion become more difficult to achieve. The mechanism transmitting this problem across the globe was severely depressed prices in global markets for manufactured goods. That led to falling income-per-unit capital ratios, then to falling rates of profit, then to lower rates of investment, and hence lower rates of output growth. In this environment, firms have faced heightened competition for market share: as overall growth rates slow, the only way to grow quickly is to steal market shares from other firms. Each firm has to do everything it can to keep up with its competitors. Overcapacity explains why, since the early 1970s, productivity-growth rates have fallen less severely than output-growth rates: firms have continued to raise their productivity levels as best they can despite falling rates of output growth (or else have gone under, disappearing from statistical averages). As manufacturing-output growth rates slipped below productivity-growth rates in one country after another, deindustrialization spread worldwide.

Driving globalization

Explaining global waves of deindustrialization in terms of global overcapacity rather than industrial automation allows us to understand a number of features of this phenomenon that otherwise appear paradoxical. For example, rising overcapacity explains why deindustrialization has been accompanied not only by ongoing efforts to develop new labour-saving technologies, but also by the building out of gigantic, labour-using supply chainsโusually with a more damaging environmental impact. A key turning point in that story came in the 1960s, when low-cost Japanese and German products invaded the us domestic market, sending the us industrial-import penetration ratio soaring from less than 7 per cent in the mid-60s to 16 per cent in the early 1970s. From that point forward, it became clear that high levels of labour productivity would no longer serve as a shield against competition from lower-wage countries. The us firms that did best in this context were the ones that responded by globalizing production. Facing competition on prices, us multinational firms built international supply chains, shifting the more labour-intensive components of their production processes abroad and playing suppliers off against one another to achieve the best prices. In the mid-60s the first export-processing zones opened in Taiwan and South Korea. Even Silicon Valley, which formerly produced its computer chips locally in the San Jose area, shifted its production to low-wage areas, using lower grades of technology (and also benefitting from laxer laws around pollution and workersโ safety). mncs in Germany and Japan adopted similar strategies, which were everywhere supported by new infrastructures of transportation and communication technologies.

The globalization of production allowed the worldโs wealthiest economies to retain manufacturing capacity, but it did not reverse the overall trend towards labour deindustrialization. As supply chains were built out across the world, firms in more and more countries were pulled into the swirl of world-market competition. In some countries, this move was accompanied by shifts in the location of new plants: rustbelts oriented towards production for domestic markets went into decline, while sunbelts integrated into global supply networks expanded dramatically. Chattanooga grew at the expense of Detroit, Ciudad Juรกrez at the expense of Mexico City, Guangdong at the expense of Dongbei. Yet given the overall slowdown in rates of world manufacturing-market expansion, this re-orientation towards the world market could only result in lacklustre outcomes: the rise of sunbelts failed to balance out the decline of rustbelts, resulting in global deindustrialization.

At the same time, global manufacturing overcapacity explains why the countries that have succeeded in attaining a high degree of robotization are not those that have seen the worst degree of deindustrialization. In the context of intense global competition, high degrees of robotization have given firms competitive advantages, allowing them to take market share from firms in other countries. Thus Germany, Japan and South Korea have some of the highest levels of robotization; they also have the largest trade surpluses in the world. Workers in European and East Asian firms know that automation helps preserve their jobs. China is also a top-four country in terms of trade surpluses, providing its manufacturing sector with a gigantic boost in terms of both output and employment growth. China has advanced on this front not due to high levels of robotization, but rather due to a mix of low wages, moderate to advanced technologies, and strong infrastructural capacities. Yet the result was the same: in spite of system-wide overcapacity and slow growth rates, the prc has industrialized rapidly because Chinese firms have been able to take market share away from other firmsโnot only in the us, but also in countries like Mexico and Brazilโwhich lost market share as Chinese firms expanded. It could not have been otherwise, since in an environment where average growth rates are low, firms can only achieve high rates of growth by taking market share from their competitors. Whether China will be able to retain its competitive position as its wage levels rise remains an open question; Chinese firms are now racing to robotize in order to head off this possibility.

4. BEYOND MANUFACTURING

The evidence I have cited so far to explain job loss in the manufacturing sector through worsening overcapacity may appear to have little purchase on the larger, economy-wide patternsโof stagnant wages, falling labour shares of income, declining labour-force participation rates and jobless recoveries after recessionsโthat the automation theorists have sought to explain by growing technological dynamism. Automation may therefore still seem a good explanation for the decline in demand for labour across the service sectors of each countryโs economy, and so across the world economy as a whole. Yet this broader problem of declining labour demand also turns out to be better explained by the worsening industrial stagnation I have described than by widespread technological dynamism.

This is because, as rates of manufacturing-output growth stagnated in one country after another from the 1970s onward, no other sector appeared on the scene to replace industry as a major economic-growth engine. Instead, the slowdown in manufacturing-output growth rates was accompanied by a slowdown in overall growth rates. This trend is visible in the economic statistics of high-income countries. France is again a striking example (Figure 7). In France, real manufacturing value added (mva) rose at 5.9 per cent per year between 1950 and 1973, while real value added in the total economy (gdp) rose at 5.1 per cent per year. Since 1973, both growth measures have declined significantly: by the 2001โ17 period, mva was rising at only 0.9 per cent per year, while gdp was rising at a faster but still sluggish pace of 1.2 per cent per year. Note that during the 1950s and 60s, mva growth generally led the overall economy: manufacturing served as the major engine of overall growth. Since 1973, mva growth rates have trailed overall economic growth. Similar patterns can be seen in other high-income countries (Table 2, overleaf).Their export-led growth engines sputtered and slowed to a crawl; and as they did so, overall rates of economic growth slowed considerably.

Economists studying deindustrialization often point out that while manufacturing has declined as a share of nominal gdp, it has maintained, until recently, a more or less steady share of real gdp, which is to say that, between 1973 and 2000, real mva grew at approximately the same pace as real gdp. What that has meant in practice is that, as manufacturing has become less dynamic, so has the overall economy. There was no significant shift in demand from industry to services. Instead, as capital accumulation slowed down in manufacturing, the expansion of aggregate output also slowed significantly across the economy as a whole.

This tendency to economy-wide stagnation, associated with the decline in manufacturing dynamism, then explains the system-wide decline in the demand for labour, and so also the problems that the automation theorists cite: stagnant real wages, falling labour shares of income and so on. This economy-wide pattern of declining labour demand is not the result of rising productivity-growth rates, associated with automation in the service sector. On the contrary, productivity is growing even more slowly outside of the manufacturing sector than inside of it: in France, for example, while productivity in the manufacturing sector was rising at an average annual rate of 2.7 per cent per year between 2001โ17, productivity in the service sector was rising at just 0.6 per cent per year. Similar gaps exist in other countries. Once again, the mistake of the automation theorists is to focus on rising productivity growth rather than falling output growth. The environment of slower economic growth explains the low demand for labour all by itself. Workers, and especially workers who are not protected by powerful unions or labour laws, find it difficult to pressure employers to raise their wages when there is so much slack in the labour market.

These trends are as visible in the world economyโincluding Chinaโas they are in the high-income countries (Figure 8, overleaf). In the 1950s and 60s, global mva growth and gdp growth were expanding at rapid clips of 7.1 per cent and 5.0 per cent respectively, with mva growth leading gdp growth by a significant margin. From the 1970s onward, as global mva growth slowed, so did global gdp growth. In most of the decades that followed, global mva growth continued to lead gdp growth but by a much smaller margin. Since 2008, both rates have been growing at the exceptionally slow pace of 1.6 per cent per year. Again, the implication is that, as manufacturing growth rates declined, nothing emerged to replace industry as a growth engine. Not all regions of the world economy are experiencing this slowdown in the same way or to the same extent, but even countries like China that have grown quickly have to contend with this global slowdown and its consequences. Since the 2008 crisis, Chinaโs economic growth rate has slowed considerably; its economy is deindustrializing.

The clear conclusion is that manufacturing turned out to be a unique engine of overall economic growth. Industrial production tends to be amenable to incremental increases in productivity, achieved via technologies that can be repurposed across numerous lines. Industry also benefits from static and dynamic economies of scale. Meanwhile, there is no necessary boundary to industrial expansion: industry consists of all economic activities that are capable of being rendered via an industrial process. The reallocation of workers from low-productivity jobs in agriculture, domestic industry and domestic services to high-productivity jobs in factories raises levels of income per worker and hence overall economic growth rates. The countries that have caught up with the West in terms of incomeโsuch as Japan, South Korea and Taiwanโmostly did so by industrializing: they exploited opportunities to produce for the world market, at increasing scale and using advanced technologies, allowing them to grow at speeds that would have been unachievable had they depended on domestic-market demand alone.

When the growth engine of industrialization sputtersโdue to the replication of technical capacities, international redundancy and fierce competition for marketsโthere has been no replacement for it as a source of rapid growth. Instead of workers reallocating from low-productivity jobs to high-productivity ones, the reverse of this process takes place, as workers pool increasingly in low-productivity jobs in the service sector. As countries have deindustrialized, they have also seen a massive build-up of financialized capital, chasing returns to the ownership of relatively liquid assets, rather than investment in new fixed capital. In spite of the high degree of overcapacity in industry, there is nowhere more profitable in the real economy for capital to invest itself. Indeed, if there had been, we would have evidence of it in higher rates of investment and hence higher gdp growth rates. This helps explain why firms have reacted to over-accumulation by trying to make their existing manufacturing capacity more flexible and efficient, rather than ceding territory to lower-cost, higher-productivity firms from other countries.

The lack of an alternative growth engine also explains why governments in poorer countries have encouraged domestic producers to try to break into already oversupplied international markets for manufactures. Nothing has replaced those markets as a major source of globally accessible demand. Overcapacity exists in agriculture, too, and is even worse there than in industry; meanwhile services, which are mostly non-tradable, make up only a tiny share of global exports. If countries are to retain any dependable link to the international market under these conditions, they must find some way to insert themselves into industrial lines, however oversupplied. System-wide overcapacity and the generalized slowdown in economic growth have therefore been devastating for most poorer countries: the amount of foreign exchange they have captured through liberalization has been pitiful; so, too, has been the number of jobs created.

Indeed, global economic downshifts have been particularly devastating for low- and middle-income countries, not only because they are poorer, but also because those downshifts have taken place in an era of rapid labour-force expansion: between 1980 and the present, the worldโs waged workforce grew by about 75 per cent, adding more than 1.5 billion people to the worldโs labour markets. These labour market entrants, living mostly in poorer countries, had the misfortune of growing up and looking for work at a time when global industrial overcapacity began to shape patterns of economic growth in post-colonial countries: declining rates of manufactured export growth into the us and Europe in the late 1970s and early 1980s ignited the 1982 debt crisis, followed by imf-led structural adjustment, which pushed countries to deepen their imbrications in global markets at a time of ever slower global growth and rising competition from China. In spite of shocks to the demand for labour generated by slowing global growth rates and rising economic turmoil, huge numbers of workers were still forced to seek employment in order to live.

Some may respond that the present low rates of global growth are in fact nothing out of the ordinary, if only we shift our baseline from the exceptional postwar โGolden Ageโ to previous periods, such as the pre-WWI era. But a global perspective on the decline in the demand for labour provides the answer to this objection. It is true that, during the Belle Epoque, average rates of economic growth were more comparable to growth rates today. However, in that period, large sections of the population still lived in the countryside and produced much of what they needed to live. European empires still overran the globe, not only limiting the diffusion of new manufacturing technologies to a few regions, but also actively deindustrializing the rest of the world economy. Yet in spite of the much more limited sphere in which labour markets were activeโand in which industrialization took placeโthe pre-wwi era, as also the inter-war period, was marked by a persistently low demand for labour, making for employment insecurity, rising inequality and tumultuous social movements aimed at transforming economic relations. In this respect, the world of today does look like the world of the Belle Epoque. The difference is that today, a much larger share of the worldโs population depends on finding work in labour markets in order to live.What automation theorists describe as the result of rising technological dynamism is actually the consequence of worsening economic stagnation: productivity-growth rates appear to rise when, in reality, output-growth rates are falling. This mistake is not without reason. The demand for labour is determined by the gap between productivity and output growth rates. Reading the shrinking of this gap the wrong way aroundโthat is, as due to rising productivity rather than falling output ratesโis what generates the upside-down world of the automation discourse. Proponents of this discourse then search for the technological evidence that supports their view of the causes for the declining demand for labour. In making this leap, the automation theorists miss the true story of overcrowded markets and economic slowdown that actually explains the decline in labour demand.

Yet even if automation is not itself the primary cause of a low demand for labour, it is nevertheless the case that, in a slow-growing world economy, technological changes within a near-future horizon may still threaten large numbers of jobs with destruction, in a context of economic stagnation and slower rates of job creation. Technological change then acts as a secondary cause of a low labour demand, operating within the context of the first. The concluding section of this essay in nlr 120 will address these technological dynamics, as well as the socio-political problemsโand opportunitiesโgenerated by a persistently low demand for labour in late-capitalist societies.


/ 002. Purcell, Conor. "Beyond Human Intelligence," Farsight, 2023.


Anyone with personal experience taking LSD, psilocybin, ayahuasca, or any other mindaltering psychedelic will truly recognise the worldโs interconnected nature. Fractalising into white diamonds, the air shatters, trees breathe, and animals speak your language. Some suggest these drugs dismantle an evolved human filter, revealing nature for what it truly is: a connected intelligence.

James Bridleโs new book Ways of Being: Beyond Human Intelligence is an exploration of different forms of intelligence, both biological and artificial. Itโs also, as the author says, a call for us humans to start forming new relationships with non-human intelligence. Throughout the book Bridle argues that our common future demands less industrial hubris, and more cooperation with existing and deeply knowledgeable biological systems.

A writer and artist known for coining the term โNew Aestheticโ โ used to refer to the increasing appearance of the visual language of digital technology in the physical world, and the combination of the virtual and physical โ James Bridle advocates for a future characterised by human, animal, and plant reconnectivity for the sake of achieving a better planetary balance.

Our regular contributor, Conor Purcell, PhD, had the opportunity to interview Bridle for FARSIGHT, speaking by video call between Purcellโs home in County Donegal, Ireland, and the intervieweeโs in Greece.

What inspired you to write this book?

I studied artificial intelligence almost twenty years ago when it was kind of fading from the curriculum because it wasnโt going anywhere. Since then, there havenโt been any kind of major discoveries. But what has happened is that vast amounts of data have become available, which have been harvested largely by social media giants and governments. At the same time, processing power has massively increased. Weโre now seeing how AI is revealing itself to be something not quite human in that it thinks and approaches the world in a very different way than we do. Weโre also starting to realise, thanks to decades of research, that intelligence is something much more interesting and greater than our very narrow human idea of it.

With the book, I wanted to understand how we can better accommodate ourselves with everything else that we share the planet with. For me, this question is central to achieving environmental justice and progress. I now see an opportunity with AI for reimagining, firstly, what intelligence is, and secondly, how we impact other forms of intelligence beyond the human.

How do you think people have become so disconnected from these other ways of being, specifically the intelligences of animals and plants in nature?

A good example to demonstrate this is how in medieval times there were cases when animals were accused of committing a crime and tried in courtrooms. There were lawyers there, and the animals were presented to juries. This wasnโt pantomime, but a deeply serious undertaking because non-humans were part of the community. That meant they had rights and responsibilities.

Over time, and especially with industrialisation and urbanisation, attitudes towards nonhuman life in all its forms changed. We started to view them essentially as machines โ unfeeling automatons who didnโt have the kind of inner life or higher importance which we ascribe to humans. This became the dominant mode of thought within Western, postenlightenment societies. Thatโs when the abattoirs began. And now the environmental mess that we find ourselves in is all related to how weโre out of balance with our deeply entangled and interdependent relationships with all other species.

What do we now know about the intelligent behavior of plants?

Recent research has shown several surprising behavioural qualities in plants. I write in the book about scientists who subjected certain plants to repeated shocks and found that quite quickly they learned essentially to ignore the shock and move away from its source. Whatโs more is that they remembered patterns and continued to avoid the source of the shock in the future.

This is an extraordinary finding that completely changes our understanding of plant behaviour. Even the idea that plants have a thing that we might call behaviour is astonishing because the traditional kind of botanical approach mostly involves cutting them up into small pieces and studying them as if they were machines. Whatโs interesting too is that these researchers write about working with plant spirits, and their work is informed by both the knowledge that has come from the plants themselves and by treating the plant as already having its own personhood.

This is real science published in legitimate scientific journals. Itโs peer reviewed. Itโs reproducible. It conforms to all the structures of the scientific method. What that tells me is that there are multiple ways of approaching these intelligences and to do that via a kind of synthesis of these different ways of knowing is incredibly powerful. We can explore the world by observing and connecting with these behaviours, as long as our goal is to truly understand. Ultimately, it all depends on admitting the possibility in the first place that these kinds of alternative intelligences are real.

In a chapter called "Non-Binary Machines" you talk about the fields of cybernetics- which has a long history dating back to the mid-20th century- and how this shows a future alternative to what you call 'corporate artificial intelligence'. Can you explain what you mean by this?

It has to do with thinking of intelligence as a process, rather than as a machine that thinks like a kind of brain in a box. Particularly in Britain, cybernetic researchers โ those involved in the science of communication and automation in machines and other living beings โ envisioned a kind of intelligence that is active in the world, which is connected to the world around it, which is learning, and which is defined by what it does, rather than what it is. This is different to the corporate artificial intelligence of today which is currently being developed to increase profits.

Cybernetic research continues in various ways. There is very interesting new research around soft robotics, which essentially tries to make robotic systems more adaptive to the world around them. Programmes like the Unconventional Computing Lab at the University of the West of England is a good example. One of the things they study is the computational abilities of various plants and animals. They are doing very interesting things like redesigning computer logic based on the movement of crabs, for example. This points to the fact that what we understand as computation is not something that can only be performed within machines, but in fact is conducted by biological organisms too.

It also appears that biological systems can be calibrated to test variable abilities and to solve mathematical problems โ they might even be more efficient than our fastest supercomputers. These abilities exist across the natural world, but since we usually only see the things that we know how to test for, there remains the possibility of a whole range of intelligences which far exceed our own. The problem is that we donโt even know how to ask the appropriate questions yet.

How can we reconnect with non-human intelligence in the future?

Towards the end of the book, I write about the need to provide more shared territory for human and non-human lives. I mean this both in the form of animal reserves, conservation areas, wildlife corridors and shared spaces that allow animals to move in ways that they currently cannot. But I also think that the notion of animal intelligence compels us to think politically.

In the book, I discuss the Irish experience with the introduction of the so-called citizen assemblies, made up of 33 representatives chosen by political parties and 66 randomly chosen citizens, to make recommendations on societyโs biggest challenges. One of the things we learned from the citizen assemblies, not just in Ireland, but in other places, is that this is an extraordinary mechanism for mobilising what are essentially multiple forms of intelligence. The assemblies didnโt use animal, plant or artificial intelligence, but by branching out beyond the traditional domain of experts, the range of human intelligence โ and personality types โ was enlarged. So instead of selecting a very narrow definition of domain experts, itโs acknowledging that what you need for complex thorny problems, particularly novel ones, is a wider diversity of life experiences and ways of thinking.

The same principle can apply to including intelligences beyond the human in our decision making. I believe that only by bringing in diverse ways of thinking and forms of life experience can we address the kind of extraordinary global and pan-species problems that we face.


/ 003. Mogensen, Klaus ร. "Is Artificial Intelligence a Myth?" Farsight, 2023.


Erik Larson is a tech entrepreneur and pioneering research scientist working at the forefront of natural language processing. He recently published a book called The Myth of Artificial Intelligence: Why Computers Can't Think the Way We Do. FARSIGHT met him online for an interview about the future of AI, and why he believes the fieldโs current path of development will not lead us to human-level intelligence in machines anytime soon.

Erik, what made you decide to write this book?

My specialty is natural language processing, and I wrote the book from the perspective of understanding the many practical challenges and difficulties there are in making computers understand human language on a deep level. Early in my career, I read a book by Ray Kurzweil, The Age of Intelligent Machines, where he proposed 2029 as the year when computers become as smart as humans. I thought, maybe โ itโs 30 years, after all. By 2005, when his book The Singularity is Near came out, I thought that it could not happen in 20 years without some major unexpected scientific breakthrough that we couldnโt anticipate yet. Instead of acting like weโre on an inevitable path to general AI, we should tell the broader public that achieving true computer intelligence is a lot more difficult than many assume. Thatโs why I wrote the book. 

You argue that we are very far from developing general artificial intelligence. In fact, you believe that the approach we are currently pursuing can never lead us there. Why is that?

The main framework that I use in the book is inference. In AI, the problem is that weโre using the wrong type of inference to ever get to general or commonsense intelligence. Right now, the field is almost exclusively dominated by machine learning using inductive inference, learning from prior examples. Human beings use induction all the time, but itโs not the most important type of inference for us. It canโt handle novelty because itโs based on prior observation. Without a novelty mechanism, you canโt get to certain kinds of intelligence. I donโt mean to say that itโs impossible. Nature has developed general intelligence, so we should be able to eventually do the same thing. However, thereโs something currently missing, and thatโs why itโs been so difficult to make certain kinds of progress in the field.

Arthur C. Clarke famously thought that to get something like intelligence in a computer, we would need heuristic logic- finding and using solutions that aren't precise, but just good enough, which is how we think. We don't measure the distance across the street with a measuring tape; we guesstimate how far it is This method is a lot faster and works well for everyday stuff. Do you think we could program that kind of heuristic logic into computers?

We do that already. Before deep learning became the dominant paradigm in AI development, classic AI design was more rule-based. One of the great challenges in the classic rules-based paradigm was in fact to find these rules of thumb, or heuristics. Herbert Simon, a pioneer in AI and Nobel Prize winner in economics, has said that people who favour adequacy and efficiency over optimisation generally make better, more responsible, and quicker decisions than those who want to make every decision perfect. Precision can be a barrier. However, the classic AI approach based on common-sense heuristics also failed when the domain wasnโt sufficiently constrained. Even if you have a rule that doesnโt need precision, you have so much context in an unconstrained real-world environment that you need rules to tell other rules when they are relevant. It quickly becomes intractable to try to get intelligent behaviour from such a system.

There are two major, unsolved problems in AI. One is robotics, especially when the robot is not in a very specific environment. A robot arm in an industrial setting with few degrees of freedom works well, but if we have a robot walking down the street in Manhattan, there are just so many peripheral problems that can occur in such a complex environment. Somebody walks in front of the robot; something unexpected happens. If you took the best, smartest robot in the world and set it loose on any city street, within a few minutes it would cause a traffic accident. Thatโs why you donโt see robots on the street.

The other major problem is having a real conversation with an AI system where it truly understands what youโre saying and responds with understanding. I mentioned inference before, and in addition to deduction and induction, thereโs a third type of inference called abduction that people generally arenโt aware of, but which we use all the time. Deduction is, โItโs raining; therefore, the streets are wet.โ Abduction is, โI know rain makes streets wet. I see the streets are wet. Perhaps itโs raining.โ You generate a hypothesis that explains an observation. Itโs not certain knowledge โ you could be wrong. Maybe a fire hydrant broke. However, you keep correcting your hypothesis with further observation. The streets are wet, my hypothesis is that itโs raining, and then I confirm it or form another. Thatโs abduction โ hypothesis generation.

You mentioned novelty. A human who has not been in a certain situation before can think it through and still handle it. If you introduce a chess master to Shogi, Japanese chess, which has slightly different rules, they would very quickly be able to adapt their experience with chess to be able to play it well. A chess-playing AI, however, would have to learn from scratch โ its inductive deep learning of chess would be useless.

I believe game-playing AIs still use some version of a min-max algorithm, deducing what would be the best move given that it has watched a million games play out before. This is very different from a human, who doesnโt play a million games and then computes the probability. Iโm not a neuroscientist, so I couldnโt tell you whatโs happening in the brain of chess masters โ but Iโm pretty sure they donโt mindlessly play a million games before becoming masters.

I've observed that as computers get better than us at something, like chess or trivia knowledge, we tend to move the goalpost and say that this has nothing to do with intelligence. Will we keep redefining intelligence as being whatever we can do that computers can't, or are there some markers of intelligence that we can't explain away?

My response is to go back to Alan Turingโs original 1950 paper, when he said that if a person can converse with a computer and be convinced that it is a real human, then it must be intelligent. I would say that this test still holds. Of course, you can converse with a chatbot that just continues to deflect questions, but to have a conversation thatโs empathetic and understanding with the computer โ we still canโt do that.

During the summer of 2022, a big news story surfaced of a Google engineer becoming convinced that a program he was developing had gained real sentience and warranted rights akin to human rights. Could we not say that it is passed the Turing test?

The latest language models are quite good, but you can trip them up very easily if you know how. Language has a property called compositionality, how sentences are put together to provide meaning. Thereโs a big difference between me riding a horse and a horse riding me, but an AI language model is not going to get that because it doesnโt have a sense of compositionality. Natural language is a barrier for artificial intelligence โ one of the biggest. A legitimate test of language understanding would convince me that an AI was intelligent.

Another test would be navigation in dynamic environments by autonomous vehicles or robots. Getting to fully autonomous driving will be a lot harder than people think. The small city of Palo Alto, California, is mapped out on a grid, and you get pretty good performance from the vehicles there. But if youโre driving on a rural road and the AI must rely on sensor data, weโre a long way from vehicles being able to autonomously navigate that. Fully capable robotics in openended dynamic environments and fully understanding natural language; those are the two big frontiers.

Could an AI not develop its own language, very different from human language, that is uses to understand its environment and gets around some of the current limitations? We could compare it to communicating with dolphins, which seem to have a complex language that we haven't come close to understanding. They cannot understand our questions, and we cannot understand theirs; yet they are doubtless sentient beings.

I suppose is it possible for a creative AI to somehow achieve a way to frame the world that doesnโt require natural language. I donโt know the answer to that, but the immediate practical problem I see is how do we then interact with those systems? That might create some very, very strange human-machine interactions. I almost completely avoided the question of sentience in my book because, frankly, I donโt have a lot to say about it. Itโs an issue that very quickly becomes philosophical. It could be that computers right now have some low level of sentience, like insects, and we just canโt detect it because we donโt know how. As an engineer, I donโt know the entry point into that argument, so I leave it alone.

You argue that we canโt achieve general AI the way we try to do it now, with machine learning and adding more components to computers. However, in physics thereโs the phenomenon of โemergenceโ, where new traits develop when the complexity is high enough. One water molecule doesnโt have surface tension, but put enough together, and you get it. A single neuron isnโt sentient, but enough produce human sentience. Would it not be possible, if we add complexity and more components to supercomputers, that they could achieve intelligence and sentience as an emergent trait?

I think itโs an interesting question. Itโs like a pile of sand: if you keep adding grains of sand, you get a nice conical shape, until at one point adding just one more grain of sand gets you a cascading effect. We have these thresholds in emergence where something isnโt happening, and then at some level of complexity, a completely different phenomenon emerges. I think itโs interesting whether that could apply to technology or computers, but I donโt have any strong scientific position on that.

Isn't there a danger if we have, say, self-driving cars who all think the same way because we have copied the same machine learning into all of them? If there are several routes from a suburb to the city, they will all choose the same route because that's what the system says they should do, whereas humans might imagine that the main route will probably be too busy and choose another one instead?

I think weโll solve those sorts of problems. We already have systems where you can see traffic flow. The problems that I worry about are more practical. There have been cases where selfdriving cars donโt stop because a stop sign is slightly damaged and is perceived as something else. Thereโs a famous example of a system that tried to drive underneath a school bus because it thought it was an overpass. We just canโt eliminate all problems because the natural world is so messy. A bunch of leaves that the wind blows across the street might be interpreted as a solid object, and the AI will slam on the brakes.

We have people worrying that if we achieve general intelligence in computers, they are going to take over the world, or follow some order, like maximizing the production of paperclips, to such extremes that the AI will wipe out humanity to do it more efficiently. Do you think there is any real danger of such things happening, or are we just projecting our own faults onto artificial intelligence?

Thereโs an interesting contradiction in the paperclip scenario. The system is supposed to have general intelligence, which you would think included common sense, but on the other hand, itโs so narrow and computational that it thinks it can maximise the sale of paperclips by turning all humans into paperclips. Real computer intelligence would realise that itโs not intended to wipe us out. Thereโs another option, though, which is that it becomes malevolent and actively desires to rid the world of human beings. That gets us into the question of whether something like malevolence could possibly emerge in an AI.

We have AIs today that looks at x-rays of patients, trying to determine if they have cancer. They can be very good at this, but they don't know anything about cancer or what it means to a human being. They lack an understanding of what their task really is about. Do you think we can achieve intelligence in computers without true understanding of what they do?

Thatโs a great question, but I donโt have a great answer for it. It raises the whole issue, in this case of medical science, of whether an AI can provide proper diagnoses when it doesnโt understand care. Someone should write a PhD about how medicine is best administered and what the role of technology is and can be.

Research shows that even when an AI is better than any doctor at diagnosing cancer, it is even more efficient when it works with a human doctor. They approach the problem in different ways - one with a human understanding, the other from being trained on millions of x-rays. Human-AI partnerships seem to work best.

I think thatโs right. In terms of something we care about, like medicine, it sounds like this kind of collaboration may work best. To me thatโs a good use of technology. Thatโs why we make technology โ because it furthers human goals. Whether we will have autonomous systems that will replace humans in all domains, that is a completely different question. Whether we get fully sentient AI or not, weโre heading in this direction in the future. Thatโs for sure.โ


/ 004. Brown, J. Dakota. "Typography, Automation, and the Division of Labor: A Brief History," 2019.


Typography was born in the mass-production mechanism of the printing press. It has thus always been implicated in automation and, thereby, in the distinctly modern dynamics of overwork, underemployment, and runaway production. Transformations of labor and technology, however, have received scant attention in graphic design historiography. Philip Meggs' landmark textbook A History of Graphic Design, for example, offers only the briefest hints of the social dislocations that accompanied automation in the printing trades. One reads, for example, that the first steam press in England was operated in a secret location to guard against sabotage, or that vaguely-defined "strikes and violence" greeted the first installations of typesetting machines. Otherwise, such histories tend to treat innovations in print technology as a politically neutral process of technical refinement. But the new machines and methods did not just drop from the heavens: their development was often materially supported by employers who aimed to speed up production, capture control over the work process, and even break strikes.

Modernity, Modernism, and the Graphic Designer

As the design historian Adrian Forty has documented, industrial and graphic design emerged with the capitalist division of labor; both professions, in turn, catalyzed further divisions and fragmentations of work. In eighteenth-century crafts like ceramics and printed fabrics, the erosion of trade knowledge was accompanied by the rise of a new role in production: that of the "modeller. Usually hired from outside of the trade, these early designers were more dependably in touch with bourgeois taste than craftspeople were. In Josiah Wedgwood's ceramic works, stylistic concerns were conditioned by a need to simplify production into a rigid series of straightforward tasks, in which there was little occasion for variation between workers. The contemporaneous vogue for Neoclassicism, with its simplified geometry and restrained ornament, provided an ideal opportunity to streamline production - with the express goal, in Wedgwood's words, of making "such Machines of the Men as cannot err."

As critical historians from Karl Marx to Henry Braverman and David Noble have shown, the progress of capitalism's division of labor entails a gradual transfer of control and planning from the factory floor to management. But the resulting degradation and cheapening of work was noticed almost from the beginning: notably by Wedgwood's contemporary Adam Smith. In the opening chapter to The Wealth of Nations, Smith explains the production process in a new type of pin factory. Here, the capitalist has not simply gathered formerly-independent artisans to practice their trade side-by-side - instead, he has exploded the pin-making process into a line along which each laborer only cuts, sharpens, or polishes. Smith notes the miraculous extension of productivity in a process thus rationalized; elsewhere, however, he worries that the "great body of the people" will increasingly fill their days repeating the same handful of tasks.

The man whose whole life is spent performing a few simple operations, of which the effects too are, perhaps, always the same โฆ has no occasion to exert his understanding, or to exercise his invention.... He naturally loses, therefore, the habit of such exertion, and generally becomes as stupid and ignorant as it is possible for a human creature to become.

This same image was on John Ruskin's mind in 1853, as he formulated what would become a central text for the Arts & Crafts movement's antiindustrial critique. In "The Nature of the Gothic," Ruskin mourns "the little piece of intelligence" rationed out to the factory worker, which "exhausts itself in making the point of a pin." For Ruskin, the division of labor is more accurately the division of the laborers themselves: these abundant pins, he writes, are polished with mere "crumbs" of human capacities. The essay was republished by William Morris' Kelmscott Press in 1892. Arriving at the end of a career rich in the paradoxes of an anti-capitalist design practice, Kelmscott was Morris' attempt to restore aesthetic unity to the book while keeping skilled craftspeople employed at higher-thanaverage wages. Though Morris believed that the form of the book had been betrayed by industrial shoddiness, the scale of his undertaking still necessitated some modernization of the traditional work process. Kelmscott books nonetheless remained so expensive to produce that Morris was trapped, as he lamented, "ministering to the swinish luxury of the rich."

In the early twentieth-century United States, Morris' legacy would be refashioned in a context of accelerating industrial transformation. Frank Lloyd Wright came to believe that Morris' desired reconciliation between art, labor, and leisure was likely to be delivered by mechanization itself. For Wright, the machineโcentrally illustrated by the printing pressโhad to be grasped for what it had become: "intellect mastering the drudgery of the earth." The "meaningless torture" inflicted on workers and materials alike could now be swept away, Wright argued, as long as designers could part with anachronistic practices of ornamentation. Meanwhile, American commercial artists were falling "under the Arts & Crafts spell" and emerging as freelance specialists in book typography. Bruce Rogers and Frederic Goudy, for example, took on Morris' aesthetic standards while largely ignoring his concerns about the social contradictions of large-scale production. It was Goudy's student W.A. Dwiggins who would later popularize the phrase "graphic design" to describe this emerging position in print's division of labor. The workshops of these early graphic designers were characterized by a clarified managerial role for the designer, a more rationalized division of labor below and, finally, an embrace of labor-saving technology in typesetting and printing.

Across the ocean, meanwhile, a more explicitly socialist embrace of industry had produced the modernist "machine aesthetic." Echoing and radicalizing Wright, the Constructivist manifesto of 1922 declared war on traditional art and pledged a conditional allegiance to the machine: Constructivists would be both technology's "first fighting and punitive force" and its "last slave-workers."[17] At the same time, the Bauhaus was moving away from its Arts & Crafts roots. The school had initially been organized along guild lines: composed not of students and professors, but of masters, journeymen, and apprentices.[18] In transitioning to an emphasis on industrial production, Bauhaus designers synthesized compositional lessons from Futurism, Dada, and Constructivism. One uniting theme of these movements had been a desire to alter the experience of reading by exploding the strictures of the letterpress. In each case, photomechanical techniques promised a way out. In the Bauhaus graphics studios, Lรกszlรณ Moholy-Nagy continued in this vein, developing a a montage practice he termed "Typophoto." 

In the USSR, El Lissitzky theorized the epistemological and technical aspects of this blurring of text and image. The text-image, he surmised, could be put to work perfecting the human sensorium: revolutionized book forms would yield a "perpetual sharpening of the optic nerve." Lissitzky also read early attempts at phototypography in the context of a historical tendency toward lightness and mobility: "The amount of material used is decreasing, we are dematerializing, cumbersome masses of material are being supplanted by released energies."[20] Such a process would culminate, as he cryptically wrote in 1926, in a final transcendence of print itself: "the electro-library." After the midpoint of the century, metal type would indeed be supplanted by photographic media, in systems that were increasingly directed by "electro-libraries" of "dematerialized" data. By the 1990s, the convergences and displacements predicted by Lissitzky had yielded the digital hybridization of writing, typesetting, and imaging. In the end, however, these transformations owed more to the bottom lines of print capitalists than to the efforts of the radical modernists.

Modernism in design began with a vision of socialist industrial transformation -but by mid-century, it had become welded to the public image of the great capitalist conglomerates. Corporate boosters of modernism like the paperboard magnate Walter Paepke, a benefactor of the postwar "New Bauhaus" in Chicago, prophesied a world in which design would meld with management.[22] Paepke argued that design could improve market competition between large firms tied to uniform machinery and wage agreements; internally, it could even be put to work on such "problems" as worker morale. Though modernism's trajectory from utopian potential to capitalist instrumentalization is familiar to design history, parallel themes in the history of the printing trades have received less attention. The initial promise of technological innovationsโto end, in Wright's words, the "meaningless torture" of repetitive and inefficient labor -soon gave way to the sobering realities of deskilling and displacement. The contours of twentieth-century print technologies would be shaped in large part by struggles over automation and employment. The "released energies" of print's dematerialization increasingly took the form of outmoded workers.

Industrial Rationalization and the Printer

The growing coherence and confidence of the graphic design profession is accompanied historically by the gradual fragmentation and decline of the printing trades.[24] The job description of "printing" originally encompassed a set of knowledges that extended far beyond the point of contact between ink and paper. Early printers were often also typefounders, publishers, and booksellers. Even as the craft became more specialized, printing still involved typesetting and composing pages, which often extended to a role in writing. According to union typesetter and historian Henry Rosemont, newspaper printers in the mid-nineteenth century relied on a broad but informal education in "language, history, geography and other subjects," which enabled them to produce entire articles from telegrams consisting of little more than the relevant nouns, verbs, and modifiers. 

Print workers thus held a strategic position in the circulation of public discourse, which was simply not possible without them. They often took advantage of this position to educate themselves and to advocate for the interests of their trade. In addition to their obligatory literacy, they had access to the press as an organizing tool-an extreme rarity for manufacturing workers of the early industrial era. Journeyman printers became the first group of workers to go on strike in the United States, just a year after the Revolutionary War. As Rรฉgis Debray has documented, print workers would go on to play prominent roles in revolutionary movements around the world during the next two centuries.

The printed book was, if not the first, then certainly the clearest early example of the world of standardized, mass-produced commodities to come. And as with any other commodity, print production required "labor-saving" innovations to keep competing firms profitable. More efficient inventions often rendered the work less taxing and dangerous, but the primary motivation for their adoption was always the reduction of labor costsโresulting in layoffs or slashed hours. Print workers often found themselves fighting technologies that promised to ease the burden of their labor. Rosemont offers the example of a more efficient ink roller, invented in 1814 and vigorously resisted by American printers. The existing standard was a more rudimentary instrument that required periodic soakings in animal urine to keep it from hardening. Printers worked in consistently squalid, poorly- ventilated shops that contributed to lower-than-average life expectanciesโbut such conditions were clearly preferable to unemployment.

The printed book was, if not the first, then certainly the clearest early example of the world of standardized, mass-produced commodities to come. And as with any other commodity, print production required "labor-saving" innovations to keep competing firms profitable. More efficient inventions often rendered the work less taxing and dangerous, but the primary motivation for their adoption was always the reduction of labor costsโresulting in layoffs or slashed hours. Print workers often found themselves fighting technologies that promised to ease the burden of their labor. Rosemont offers the example of a more efficient ink roller, invented in 1814 and vigorously resisted by American printers. The existing standard was a more rudimentary instrument that required periodic soakings in animal urine to keep it from hardening. Printers worked in consistently squalid, poorly- ventilated shops that contributed to lower-than-average life expectanciesโbut such conditions were clearly preferable to unemployment.

Though Johannes Gutenberg's fifteenth-century press had scarcely changed in the intervening years, the first decades of the nineteenth century brought transformations far beyond the humble roller. Iron construction and steam power fundamentally changed not only the shape of the machine, but the entire work process that fed and maintained it. However, a a major production bottleneck remained: the reproduction of writing still required the manual assembly of each word and line. Printing firms grew to be heavily reliant upon a workforce of typesetters who were both rigorously trained and militantly organized. During the 1880s, several attempts were made to mechanize the typesetting process. One particularly spectacular failure was the Paige Compositor, which bankrupted its primary investor Mark Twain. As Twain is said to have boasted shortly before the invention proved unfeasible, the Paige could "work like six men and do everything but drink, swear, and go out on strike."

Then in 1886 Ottmar Mergenthaler, a German engineer hired by an investors' group that represented the major New York newspapers, presented the first working model of the Linotype machine. Like type composition by the old method, the Linotype utilized thin bits of metal; here, however, each bit carried the negative impression of a character. Operators typed the characters into a line; justification was then carried out by a spacing mechanism that sealed the channel into which the characters had been set. Molten metal was then injected into the channel to form a full line in positive relief. After cooling, each "line o' type" was stacked into columns and locked into page layouts for the press. While the Linotype was an expensive and somewhat risky investment, it delivered on promises of labor-cost savings, and in time it contributed to a dramatic enlargement of the size and circulation of the periodical press.

The trade at first dismissed these developments. As one printer's newspaper reassuringly put it in 1891, Linotypes were mere "toys" for print capitalists with no practical background in the trade. Over the next decade, however, the threat became palpable. As one unemployed typesetter wrote in 1900, his place at the typecase had been usurped by a "monster" that eerily replicated his movements without need for food or human dignity. While a steady demand for the manual composition of headlines, advertisements, and other display applications muffled the effect slightly, the new work process soon touched off an employment crisis. Younger compositors scrambled to learn machine composition, while thousands of older or more narrowly-trained workers fell through the cracks. However, the International Typographical Union (ITU), which represented manual typesetters, was able to establish jurisdiction over the machines in strategic industrial centers around the turn of the century. In order to stave off the effects of the transformation, the ITU pushed for shorter workdays and encouraged early retirements. 

As the popular press grew during the teens and twenties, typesetting employment stabilized and even expanded. The ITU grew in tandem, and soon became one of the most powerful unions in the United States. However, this position was soon threatened by a number of mutually-reinforcing technical innovations. First, teletypesetting enabled Linotypes to be driven like player pianos; encoded tape was poised to replace human typists. Second, a slew of phototypesetting inventions sought to replace the cumbersome "hot metal" process with typefaces stored on film. Giving typography a photochemical basis, in turn, allowed a more seamless integration of text and image, while also making typesetting more readily compatible with letterpress printing's longtime competitor, offset lithography. 

Teletypesetting evolved from Morse code and the stock ticker; it would go on to form the basis for early computing.

Like the beginnings of mechanized typesetting itself, efforts at moving beyond hot metal were piloted by newspapers. Early experiments with "cold type" were explicitly undertaken to break a wave of ITU strikes following the passage of the Taft-Hartley Act of 1947, which stripped organized labor of many of the bargaining rights it had won over the preceding decades.[36] During a citywide pressroom strike that lasted from November 1947 to September 1949, the Chicago Tribune put its existing clerical staff to work on a new model of justifying typewriter whose output could be "pasted up" as camera-ready paper layouts, as opposed to being "locked up" in countless pieces of backward-reading metal.[37] The Tribune's infamous "Dewey Defeats Truman" edition of November 3, 1948 was typeset by strikebreakers, in a work process that now moved faster than the official ballot counts.38 While the quality of typewriter paste-up left something to be desired, these experiments strongly hinted at the possibility of producing a newspaper without the union. 

The ITU was able to keep these challenges at bay throughout the mid-twentieth century. New contracts forbade machines like the teletypesetter, even though this meant that print-ready stories from the wire services had to be retyped by an ITU member on the premises.[39] It wasn't until 1964 that the New York City local signed a contract allowing Linotypes to be run on "outside tape" on the condition, however, that employers paid 100% of the profits deriving from the new machinery into an "automation fund."[40] While this price was prohibitively steep for many firms, it opened the door to similar agreements on phototypesetting and, eventually, to computer systems. During the 1970s, the ITU began to draw down in exchange for the job and pension security of existing members.[41] In the meantime, the new machines had already crept into areas of the industry with low union representation. A paradoxical result was that capital-intensive metropolitan papers like the New York Times were among last to make the transition. The final night of Linotype composition at the TimesโJuly 1, 1978โis memorialized in the documentary Farewell Etaoin Shrdlu, directed by ITU proofreader David Loeb Weiss. Among the film's interviewees is a compositor who reflects on his 26 years in the industry: 

[T]hat's six years apprenticeship, 20 years journeyman. And these are words that aren't just tossed around. ... All the knowledge I've acquired over these 26 years is all locked up in a little box now called a computer. And I think probably most jobs are gonna end up the same way.

Once more, the newspaper industry led the way in automation, and again the ITU attempted to train people in the new processes or encourage early retirements. In the earlier transformation, the work lost to Linotype composition was compensated by a gradual but decisive expansion of print production. This time, however, the further rationalization of typesetting destroyed older forms of work while narrowing the number of jobs in the new lines. As Lissitzky had predicted, metal gave way to film and paper; the material footprint of typography was shrinking. But as long as each text needed to be retyped to be typeset, labor-time savings were minimal. The widespread adoption of teletypesetting technology, however, allowed the storage and transmission of coded texts and, eventually, their formatting directions as well. By the 1970s, computer systems were beginning to dissolve typesetting into word processing. A centuries-old gap separating writing and printing was beginning to closeโand this gap had been the very ground on which the ITU stood. The union suffered a long decline and finally dissolved in 1986, just as the personal computer was completing typography's process of dematerialization. It was, at that time, the longest continuously-running union in U.S. history.

The End of Modernism and the Last Typesetters

In the 1970s, print production involved a complex hierarchy of work processes, the final product of which was never fully visible until it had been printed. Designers could only approximate typographical treatments; directions on spacing, size, and weight were then handed off to phototypesetting shops to interpret in detail. A separate group of prepress specialists followed designers' directions on variables like color density and image placement, and then "stripped" together disparate negatives to create a print-ready master. But despite the many hands through which such work passed, much of the period's modernist-influenced design left the impression that it was the product a singular, detached mind. 

Though there was still a high degree of churn in new machines and processes, this division of labor held stable until the arrival of Apple's Macintosh computer in 1984. The personal computer centralized capacities formerly bound up in massive metal-founding operations, delicate apparatuses of type on film, or astronomically expensive, room-filling computersโto say nothing of the highly specialized workers that attended these machines, or of the systems of education and apprenticeship that such a workforce presupposed. Tasks that were once contracted out with some combination of strict direction and trust were now fully under the control of the individual designerโfrom the smallest details of letterforms to the organization of entire books. The Macintosh would soon offer image-editing capacities with no existing analogue, which in turn put pressure on commercial photographers and illustrators. The century since the invention of the Linotype had been one of "creative destruction" in the print industry: novel forms of work appeared suddenly and disruptively, only to be rendered obsolete in their turn. Once the brake provided by ITU contracts was removed, this process could accelerate unabated.

By the mid-1980s, typographical technology had reached a height of modernized seamlessness which, ironically, contributed to the decline of modernism's hegemony in graphic design. New design software facilitated effects like layering and distortion, which were quickly put to use in visual polemics against modernist clarity. Formal complexity and semantic confusion in graphic design had a long pre-Macintosh history โstretching at least as far back as the late-1960s letterpress experiments of Wolfgang Weingart. In the 1980s, however, graphic designers raised the stakes of these experiments by linking them to contemporaneous developments in the academy: in particular, to the "linguistic" and "cultural turns" in the humanities.[45] Terms like "deconstruction" and "post-structuralism" were applied to the printed page in ways that often required little familiarity with the theories in question. The gridโincreasingly understood as a symbol of authoritarian and, perhaps, Eurocentric rationalityโwas parodied, skewed, or thrown aside entirely. Designers arranged texts into ambiguous formations, and designed new typefaces that intentionally thwarted legibility.

By the 1990s, the postmodernist critique of modern rationality and power had grown more rigorous. However, the movement's theorists showed little interest in grasping capitalism as a determining context for their theory and practice; transformations in the political economy of print were thus largely ignored. When, in 1997, Emigre published a rare acknowledgment that entire industries were collapsing next door, it was with a heavy dose of schadenfreude 

[M]any of the printers who have gone out of business over the last quarter century deserved their fate. The grassroots of the printing trade is, after all, notoriously conservative, protectionist, and sexist.

While prepress and printing-like most American trades-tended toward a narrowly white male membership and self-image, the heaviest losses in the industry from the 1980s forward were in fact suffered by the largely non-unionized workforce of the cold type shops. Compared to the membership of the ITU, these workers were disproportionately women and people of color.

The postmodernists' focus on cultural intervention often neglected the material contingencies of the practice. Semiotic theory and cultural studies opened vistas to broad contexts of symbolic circulation, but often at the cost of such bare facts as design's own relationship to waged work. It is perhaps not surprising, then, that a new generation of practitioners has taken a more archaeological approach to the labor of design. In the recent documentaries Linotype: The Film (2012) and Graphic Means: A History of Graphic Design Production (2016)-both directed by practicing graphic designers. histories of print production expose deeper issues of deskilling, unemployment, and deindustrialization. These documentaries elegantly organize a complex history of print technology, and the present essay would admittedly have been impossible without them. However, both ultimately elide the capitalist labor dynamics that would explain their own narratives. 

Douglas Wilson's Linotype stirringly evokes the lost world of hot motel through humanizing portraits of the workers who kept it running. For example, in a near-reprise of his role as the narrator of Farewell Etaoin Shrdlu, the late Carl Schlesinger makes frequent appearances. The filmmakers include footage of him singing and tap dancing, and they indulge him as he tells a long-winded story about the time he met Marilyn Monroe. A casual viewer would never know that Schlesinger was also a lifelong member of the ITU, or that he coauthored an important book on the union's automation strategy. Despite its exhaustiveness, in fact, Linotype manages to bracket the union's existence altogether. Briar Levit's Graphic Means takes up where Linotype leaves off-impressively condensing the jumble of machines that bridged the hot type and digital eras. Graphic Means directly addresses the role of the ITU and, further, the gendered division that arose between unionized hot type shops and "open" cold type shops. However, the decline of the union is presented as a technical inevitability and even as a refutation of male privilege; the phototypesetting bosses interviewed seem to be speaking as feminists when they say that "the girls" did equally admirable work for half the wages. The vulnerability of non-unionized women to the next wave of automation, meanwhile, is never addressed. 

While typesetting has disappeared as a distinct job, it would be too simple to say that it was automated out of existence. Rather, since the late twentieth century the job description of the graphic designer has expanded to include tasks once carried out by the earliest printers. Now that we have considered the standpoints of both modernist radicals and extinct print workers, the contemporary situation of the graphic designer should appear somewhat absurd. Capitalist technological development has rendered texts and images almost infinitely reproducibleโand has built unfathomable electro-libraries in the process. But despite this gigantic aggregation of productive force, it is still necessary to put people to work moving words and pictures around, most often in the service of brand competition among otherwise identical commodities. What confronts us is not a world in which machines have freed people from work, but one of mass unemployment, in which some of the most celebrated "innovations" are apps that facilitate short-term, low-wage, benefit-less contracts.

If graphic designers became typesetters, they may turn out to be the last typesetters. The design software that repackaged the knowledge and skill of the printing trades seemed at first to deliver a dreamed-of autonomy to graphic design as a profession. But because these technologies were off-the-shelf consumer products, trained and credentialed designers have less and less of a monopoly on the medium. A general facility with image and text has bled into general literacyโdue in no small part to the ease of pirating such "immaterial" commodities as Photoshop. In the contemporary design press, articles on apps like TaskRabbit and Fiverr, or a future role for Al in the automation of design decisions, recall the mix of anxiety and reassurance that characterized coverage of the Linotype nearly 130 years ago.[53] These projected "disruptions" may well turn out to be empty hype. But whatever is in store for graphic design in the coming decades, it will be impossible to understand without accounting for the capitalist constraints and imperatives that have shaped the practice from the beginning.


/ 005. Gefter, Amanda and Quanta Magazine. "The Case Against Reality," 2016.


A professor of cognitive science argues that the world is nothing like the one we experience through ur senses.

As we go about our daily lives, we tend to assume that our perceptionsโ sights, sounds, textures, tastesโare an accurate portrayal of the real world. Sure, when we stop and think about itโor when we find ourselves fooled by a perceptual illusionโwe realize with a jolt that what we perceive is never the world directly, but rather our brainโs best guess at what that world is like, a kind of internal simulation of an external reality. Still, we bank on the fact that our simulation is a reasonably decent one. If it wasnโt, wouldnโt evolution have weeded us out by now? The true reality might be forever beyond our reach, but surely our senses give us at least an inkling of what itโs really like.

Not so, says Donald D. Hoffman, a professor of cognitive science at the University of California, Irvine. Hoffman has spent the past three decades studying perception, artificial intelligence, evolutionary game theory and the brain, and his conclusion is a dramatic one: The world presented to us by our perceptions is nothing like reality. Whatโs more, he says, we have evolution itself to thank for this magnificent illusion, as it maximizes evolutionary fitness by driving truth to extinction.

Getting at questions about the nature of reality, and disentangling the observer from the observed, is an endeavor that straddles the boundaries of neuroscience and fundamental physics. On one side youโll find researchers scratching their chins raw trying to understand how a three-pound lump of gray matter obeying nothing more than the ordinary laws of physics can give rise to first-person conscious experience. This is the aptly named โhard problem.โ

On the other side are quantum physicists, marveling at the strange fact that quantum systems donโt seem to be definite objects localized in space until we come along to observe them. Experiment after experiment has shownโ defying common senseโthat if we assume that the particles that make up ordinary objects have an objective, observer-independent existence, we get the wrong answers. The central lesson of quantum physics is clear: There are no public objects sitting out there in some preexisting space. As the physicist John Wheeler put it, โUseful as it is under ordinary circumstances to say that the world exists โout thereโ independent of us, that view can no longer be upheld.โ

So while neuroscientists struggle to understand how there can be such a thing as a first-person reality, quantum physicists have to grapple with the mystery of how there can be anything but a first-person reality. In short, all roads lead back to the observer. And thatโs where you can find Hoffmanโstraddling the boundaries, attempting a mathematical model of the observer, trying to get at the reality behind the illusion. Quanta Magazine caught up with him to find out more.

Gefter: People often use Darwinian evolution as an argument that our perceptions accurately reflect reality. They say, โObviously we must be latching onto reality in some way because otherwise we would have been wiped out a long time ago. If I think Iโm seeing a palm tree but itโs really a tiger, Iโm in trouble.โ

Hoffman: Right. The classic argument is that those of our ancestors who saw more accurately had a competitive advantage over those who saw less accurately and thus were more likely to pass on their genes that coded for those more accurate perceptions, so after thousands of generations we can be quite confident that weโre the offspring of those who saw accurately, and so we see accurately. That sounds very plausible. But I think it is utterly false. It misunderstands the fundamental fact about evolution, which is that itโs about fitness functionsโmathematical functions that describe how well a given strategy achieves the goals of survival and reproduction. The mathematical physicist Chetan Prakash proved a theorem that I devised that says: According to evolution by natural selection, an organism that sees reality as it is will never be more fit than an organism of equal complexity that sees none of reality but is just tuned to fitness. Never.

Gefter: Youโve done computer simulations to show this. Can you give an example?

Hoffman: Suppose in reality thereโs a resource, like water, and you can quantify how much of it there is in an objective orderโvery little water, medium amount of water, a lot of water. Now suppose your fitness function is linear, so a little water gives you a little fitness, medium water gives you medium fitness, and lots of water gives you lots of fitnessโin that case, the organism that sees the truth about the water in the world can win, but only because the fitness function happens to align with the true structure in reality. Generically, in the real world, that will never be the case. Something much more natural is a bell curveโsay, too little water you die of thirst, but too much water you drown, and only somewhere in between is good for survival. Now the fitness function doesnโt match the structure in the real world. And thatโs enough to send truth to extinction. For example, an organism tuned to fitness might see small and large quantities of some resource as, say, red, to indicate low fitness, whereas they might see intermediate quantities as green, to indicate high fitness. Its perceptions will be tuned to fitness, but not to truth. It wonโt see any distinction between small and largeโit only sees redโ even though such a distinction exists in reality.

Gefter: But how can seeing a false reality be beneficial to an organismโs survival?

Hoffman: Thereโs a metaphor thatโs only been available to us in the past 30 or 40 years, and thatโs the desktop interface. Suppose thereโs a blue rectangular icon on the lower right corner of your computerโs desktop โ does that mean that the file itself is blue and rectangular and lives in the lower right corner of your computer? Of course not. But those are the only things that can be asserted about anything on the desktop โ it has color, position, and shape. Those are the only categories available to you, and yet none of them are true about the file itself or anything in the computer. They couldnโt possibly be true. Thatโs an interesting thing. You could not form a true description of the innards of the computer if your entire view of reality was confined to the desktop. And yet the desktop is useful. That blue rectangular icon guides my behavior, and it hides a complex reality that I donโt need to know. Thatโs the key idea. Evolution has shaped us with perceptions that allow us to survive. They guide adaptive behaviors. But part of that involves hiding from us the stuff we donโt need to know. And thatโs pretty much all of reality, whatever reality might be. If you had to spend all that time figuring it out, the tiger would eat you.

Gefter: So everything we see is one big illusion?

Hoffman: Weโve been shaped to have perceptions that keep us alive, so we have to take them seriously. If I see something that I think of as a snake, I donโt pick it up. If I see a train, I donโt step in front of it. Iโve evolved these symbols to keep me alive, so I have to take them seriously. But itโs a logical flaw to think that if we have to take it seriously, we also have to take it literally.

Gefter: If snakes arenโt snakes and trains arenโt trains, what are they?

Hoffman: Snakes and trains, like the particles of physics, have no objective, observer-independent features. The snake I see is a description created by my sensory system to inform me of the fitness consequences of my actions. Evolution shapes acceptable solutions, not optimal ones. A snake is an acceptable solution to the problem of telling me how to act in a situation. My snakes and trains are my mental representations; your snakes and trains are your mental representations.

Gefter: How did you first become interested in these ideas?

Hoffman: As a teenager, I was very interested in the question โAre we machines?โ My reading of the science suggested that we are. But my dad was a minister, and at church they were saying weโre not. So I decided I needed to figure it out for myself. Itโs sort of an important personal questionโif Iโm a machine, I would like to find that out! And if Iโm not, Iโd like to know, what is that special magic beyond the machine? So eventually in the 1980s I went to the artificial-intelligence lab at MIT and worked on machine perception. The field of vision research was enjoying a newfound success in developing mathematical models for specific visual abilities. I noticed that they seemed to share a common mathematical structure, so I thought it might be possible to write down a formal structure for observation that encompassed all of them, perhaps all possible modes of observation. I was inspired in part by Alan Turing. When he invented the Turing machine, he was trying to come up with a notion of computation, and instead of putting bells and whistles on it, he said, Letโs get the simplest, most pared down mathematical description that could possibly work. And that simple formalism is the foundation for the science of computation. So I wondered, could I provide a similarly simple formal foundation for the science of observation?

Gefter: A mathematical model of consciousness.

Hoffman: Thatโs right. My intuition was, there are conscious experiences. I have pains, tastes, smells, all my sensory experiences, moods, emotions and so forth. So Iโm just going to say: One part of this consciousness structure is a set of all possible experiences. When Iโm having an experience, based on that experience I may want to change what Iโm doing. So I need to have a collection of possible actions I can take and a decision strategy that, given my experiences, allows me to change how Iโm acting. Thatโs the basic idea of the whole thing. I have a space X of experiences, a space G of actions, and an algorithm D that lets me choose a new action given my experiences. Then I posited a W for a world, which is also a probability space. Somehow the world affects my perceptions, so thereโs a perception map P from the world to my experiences, and when I act, I change the world, so thereโs a map A from the space of actions to the world. Thatโs the entire structure. Six elements. The claim is: this is the structure of consciousness. I put that out there so people have something to shoot at.

Gefter: But if thereโs a W, are you saying there is an external world?

Hoffman: Hereโs the striking thing about that. I can pull the W out of the model and stick a conscious agent in its place and get a circuit of conscious agents. In fact, you can have whole networks of arbitrary complexity. And thatโs the world.

Gefter: The world is just other conscious agents?

Hoffman: I call it conscious realism: Objective reality is just conscious agents, just points of view. Interestingly, I can take two conscious agents and have them interact, and the mathematical structure of that interaction also satisfies the definition of a conscious agent. This mathematics is telling me something. I can take two minds, and they can generate a new, unified single mind. Hereโs a concrete example. We have two hemispheres in our brain. But when you do a split-brain operation, a complete transection of the corpus callosum, you get clear evidence of two separate consciousnesses. Before that slicing happened, it seemed there was a single unified consciousness. So itโs not implausible that there is a single conscious agent. And yet itโs also the case that there are two conscious agents there, and you can see that when theyโre split. I didnโt expect that, the mathematics forced me to recognize this. It suggests that I can take separate observers, put them together and create new observers, and keep doing this ad infinitum. Itโs conscious agents all the way down.

Gefter: If itโs conscious agents all the way down, all first-person points of view, what happens to science? Science has always been a third-person description of the world.

Hoffman: The idea that what weโre doing is measuring publicly accessible objects, the idea that objectivity results from the fact that you and I can measure the same object in the exact same situation and get the same results โ itโs very clear from quantum mechanics that that idea has to go. Physics tells us that there are no public physical objects. So whatโs going on? Hereโs how I think about it. I can talk to you about my headache and believe that I am communicating effectively with you, because youโve had your own headaches. The same thing is true as apples and the moon and the sun and the universe. Just like you have your own headache, you have your own moon. But I assume itโs relevantly similar to mine. Thatโs an assumption that could be false, but thatโs the source of my communication, and thatโs the best we can do in terms of public physical objects and objective science.

Gefter: It doesnโt seem like many people in neuroscience or philosophy of mind are thinking about fundamental physics. Do you think thatโs been a stumbling block for those trying to understand consciousness?

Hoffman: I think it has been. Not only are they ignoring the progress in fundamental physics, they are often explicit about it. Theyโll say openly that quantum physics is not relevant to the aspects of brain function that are causally involved in consciousness. They are certain that itโs got to be classical properties of neural activity, which exist independent of any observersโ spiking rates, connection strengths at synapses, perhaps dynamical properties as well. These are all very classical notions under Newtonian physics, where time is absolute and objects exist absolutely. And then [neuroscientists] are mystified as to why they donโt make progress. They donโt avail themselves of the incredible insights and breakthroughs that physics has made. Those insights are out there for us to use, and yet my field says, โWeโll stick with Newton, thank you. Weโll stay 300 years behind in our physics.โ

Gefter: I suspect theyโre reacting to things like Roger Penrose and Stuart Hameroffโs model, where you still have a physical brain, itโs still sitting in space, but supposedly itโs performing some quantum feat. In contrast, youโre saying, โLook, quantum mechanics is telling us that we have to question the very notions of โphysical thingsโ sitting in โspace.โโ

Hoffman: I think thatโs absolutely true. The neuroscientists are saying, โWe donโt need to invoke those kind of quantum processes, we donโt need quantum wave functions collapsing inside neurons, we can just use classical physics to describe processes in the brain.โ Iโm emphasizing the larger lesson of quantum mechanics: Neurons, brains, space โฆ these are just symbols we use, theyโre not real. Itโs not that thereโs a classical brain that does some quantum magic. Itโs that thereโs no brain! Quantum mechanics says that classical objectsโincluding brainsโdonโt exist. So this is a far more radical claim about the nature of reality and does not involve the brain pulling off some tricky quantum computation. So even Penrose hasnโt taken it far enough. But most of us, you know, weโre born realists. Weโre born physicalists. This is a really, really hard one to let go of.

Gefter: To return to the question you started with as a teenager, are we machines?

Hoffman: The formal theory of conscious agents Iโve been developing is computationally universalโin that sense, itโs a machine theory. And itโs because the theory is computationally universal that I can get all of cognitive science and neural networks back out of it. Nevertheless, for now I donโt think we are machinesโin part because I distinguish between the mathematical representation and the thing being represented. As a conscious realist, I am postulating conscious experiences as ontological primitives, the most basic ingredients of the world. Iโm claiming that experiences are the real coin of the realm. The experiences of everyday lifeโmy real feeling of a headache, my real taste of chocolateโthat really is the ultimate nature of reality.


/ 006. "Pause Against AI Experiments: An Open Letter," 2023.


AI systems with human-competitive intelligence can pose profound risks to society and humanity, as shown by extensive research and acknowledged by top AI labs. As stated in the widely-endorsed Asilomar AI Principles, Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources. Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one โ not even their creators โ can understand, predict, or reliably control.

Contemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable. This confidence must be well justified and increase with the magnitude of a system's potential effects. OpenAI's recent statement regarding artificial general intelligence, states that "At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models." We agree. That point is now.

Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT@4. This pause should be public and verifiable, and include all key actors. If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.

AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts. These protocols should ensure that systems adhering to them are safe beyond a reasonable doubt. This does not mean a pause on AI development in general, merely a stepping back from the dangerous race to ever-larger unpredictable black-box models with emergent capabilities.

AI research and development should be refocused on making today's powerful, state-of-the-art systems more accurate, safe, interpretable, transparent, robust, aligned, trustworthy, and loyal.

In parallel, AI developers must work with policymakers to dramatically accelerate development of robust AI governance systems. These should at a minimum include: new and capable regulatory authorities dedicated to AI; oversight and tracking of highly capable AI systems and large pools of computational capability; provenance and watermarking systems to help distinguish real from synthetic and to track model leaks; a robust auditing and certification ecosystem; liability for AI-caused harm; robust public funding for technical AI safety research; and well-resourced institutions for coping with the dramatic economic and political disruptions (especially to democracy) that AI will cause.

Humanity can enjoy a flourishing future with AI. Having succeeded in creating powerful AI systems, we can now enjoy an "AI summer" in which we reap the rewards, engineer these systems for the clear benefit of all, and give society a chance to adapt. Society has hit pause on other technologies with potentially catastrophic effects on society. We can do so here. Let's enjoy a long AI summer, not rush unprepared into a fall.


/ 007. Metz, Cade. "What's the Future for A.I.?" 2023.


Where weโre heading tomorrow, next year and beyond.

In todayโs A.I. newsletter, the last in our five-part series, I look at where artificial intelligence may be headed in the years to come.

In early March, I visited OpenAIโs San Francisco offices for an early look at GPT-4, a new version of the technology that underpins its ChatGPT chatbot. The most eye-popping moment arrived when Greg Brockman, OpenAIโs president and co-founder, showed off a feature that is still unavailable to the public: He gave the bot a photograph from the Hubble Space Telescope and asked it to describe the image โin painstaking detail.โ

The description was completely accurate, right down to the strange white line created by a satellite streaking across the heavens. This is one look at the future of chatbots and other A.I. technologies: A new wave of multimodal systems will juggle images, sounds and videos as well as text.

Yesterday, my colleague Kevin Roose told you about what A.I. can do now. Iโm going to focus on the opportunities and upheavals to come as it gains abilities and skills.

AI. In the near term

Generative A.I.s can already answer questions, write poetry, generate computer code and carry on conversations. As โchatbotโ suggests, they are first being rolled out in conversational formats like ChatGPT and Bing.

But thatโs not going to last long. Microsoft and Google have already announced plans to incorporate these A.I. technologies into their products. Youโll be able to use them to write a rough draft of an email, automatically summarize a meeting and pull off many other cool tricks.

OpenAI also offers an A.P.I., or application programming interface, that other tech companies can use to plug GPT-4 into their apps and products. And it has created a series of plug-ins from companies like Instacart, Expedia and Wolfram Alpha that expand ChatGPTโs abilities.

A.I. in the medium term

Many experts believe A.I. will make some workers, including doctors, lawyers and computer programmers, more productive than ever. They also believe some workers will be replaced.

โThis will affect tasks that are more repetitive, more formulaic, more generic,โ said Zachary Lipton, a professor at Carnegie Mellon who specializes in artificial intelligence and its impact on society. โThis can liberate some people who are not good at repetitive tasks. At the same time, there is a threat to people who specialize in the repetitive part.โ

Human-performed jobs could disappear from audio-to-text transcription and translation. In the legal field, GPT-4 is already proficient enough to ace the bar exam, and the accounting firm PricewaterhouseCoopers plans to roll out an OpenAI-powered legal chatbot to its staff.

At the same time, companies like OpenAI, Google and Meta are building systems that let you instantly generate images and videos simply by describing what you want to see.

Other companies are building bots that can actually use websites and software applications as a human does. In the next stage of the technology, A.I. systems could shop online for your Christmas presents, hire people to do small jobs around the house and track your monthly expenses.

All that is a lot to think about. But the biggest issue may be this: Before we have a chance to grasp how these systems will affect the world, they will get even more powerful.

A.I. in the long term

For companies like OpenAI and DeepMind, a lab thatโs owned by Googleโs parent company, the plan is to push this technology as far as it will go. They hope to eventually build what researchers call artificial general intelligence, or A.G.I. โ a machine that can do anything the human brain can do.

As Sam Altman, OpenAIโs chief executive, told me three years ago: โMy goal is to build broadly beneficial A.G.I. I also understand this sounds ridiculous.โ Today, it sounds less ridiculous. But it is still easier said than done.

For an A.I. to become an A.G.I., it will require an understanding of the physical world writ large. And it is not clear whether systems can learn to mimic the length and breadth of human reasoning and common sense using the methods that have produced technologies like GPT-4. New breakthroughs will probably be necessary.

The question is, do we really want artificial intelligence to become that powerful? A very important related question: Is there any way to stop it from happening?

The risks of A.I.

Many A.I. executives believe the technologies they are creating will improve our lives. But some have been warning for decades about a darker scenario, where our creations donโt always do what we want them to do, or they follow our instructions in unpredictable ways, with potentially dire consequences.

A.I. experts talk about โalignmentโ โ that is, making sure A.I. systems are in line with human values and goals.

Before GPT-4 was released, OpenAI handed it over to an outside group to imagine and test dangerous uses of the chatbot.

The group found that the system was able to hire a human online to defeat a Captcha test. When the human asked if it was โa robot,โ the system, unprompted by the testers, lied and said it was a person with a visual impairment.

Testers also showed that the system could be coaxed into suggesting how to buy illegal firearms online and into describing ways to make dangerous substances from household items. After changes by OpenAI, the system no longer does these things.

But itโs impossible to eliminate all potential misuses. As a system like this learns from data, it develops skills that its creators never expected. It is hard to know how things might go wrong after millions of people start using it.

โEvery time we make a new A.I. system, we are unable to fully characterize all its capabilities and all of its safety problems โ and this problem is getting worse over time rather than better,โ said Jack Clark, a founder and the head of policy of Anthropic, a San Francisco start-up building this same kind of technology.

And OpenAI and giants like Google are hardly the only ones exploring this technology. The basic methods used to build these systems are widely understood, and other companies, countries, research labs and bad actors may be less careful.

The remedies for A.I. 

Ultimately, keeping a lid on dangerous A.I. technology will require far-reaching oversight. But experts are not optimistic.

โWe need a regulatory system that is international,โ said Aviv Ovadya, a researcher at the Berkman Klein Center for Internet & Society at Harvard who helped test GPT-4 before its release. โBut I do not see our existing government institutions being about to navigate this at the rate that is necessary.โ

As we told you earlier this week, more than 1,000 technology leaders and researchers, including Elon Musk, have urged artificial intelligence labs to pause development of the most advanced systems, warning in an open letter that A.I. tools present โprofound risks to society and humanity.โ

A.I. developers are โlocked in an out-of-control race to develop and deploy ever more powerful digital minds that no one โ not even their creators โ can understand, predict or reliably control,โ according to the letter.

Some experts are mostly concerned about near-term dangers, including the spread of disinformation and the risk that people would rely on these systems for inaccurate or harmful medical and emotional advice.

But other critics are part of a vast and influential online community called rationalists or effective altruists, who believe that A.I could eventually destroy humanity. This mind-set is reflected in the letter.


/ 008. Ryan-Mosley, Tate. "An early guide to policymaking on generative AI," 2023.


Earlier this week, I was chatting with a policy professor in Washington, DC, who told me that students and colleagues alike are asking about GPT-4 and generative AI: What should they be reading? How much attention should they be paying?

She wanted to know if I had any suggestions, and asked what I thought all the new advances meant for lawmakers. Iโve spent a few days thinking, reading, and chatting with the experts about this, and my answer morphed into this newsletter. So here goes!

Though GPT-4 is the standard bearer, itโs just one of many high-profile generative AI releases in the past few months: Google, Nvidia, Adobe, and Baidu have all announced their own projects. In short, generative AI is the thing that everyone is talking about. And though the tech is not new, its policy implications are months if not years from being understood.

GPT-4, released by OpenAI last week, is a multimodal large language model that uses deep learning to predict words in a sentence. It generates remarkably fluent text, and it can respond to images as well as word-based prompts. For paying customers, GPT-4 will now power ChatGPT, which has already been incorporated into commercial applications.

The newest iteration has made a major splash, and Bill Gates called it โrevolutionaryโ in a letter this week. However, OpenAI has also been criticized for a lack of transparency about how the model was trained and evaluated for bias.

Despite all the excitement, generative AI comes with significant risks. The models are trained on the toxic repository that is the internet, which means they often produce racist and sexist output. They also regularly make things up and state them with convincing confidence. That could be a nightmare from a misinformation standpoint and could make scams more persuasive and prolific.

Generative AI tools are also potential threats to peopleโs security and privacy, and they have little regard for copyright laws. Companies using generative AI that has stolen the work of others are already being sued.

Alex Engler, a fellow in governance studies at the Brookings Institution, has considered how policymakers should be thinking about this and sees two main types of risks: harms from malicious use and harms from commercial use. Malicious uses of the technology, like disinformation, automated hate speech, and scamming, โhave a lot in common with content moderation,โ Engler said in an email to me, โand the best way to tackle these risks is likely platform governance.โ (If you want to learn more about this, Iโd recommend listening to this weekโs Sunday Show from Tech Policy Press, where Justin Hendrix, an editor and a lecturer on tech, media, and democracy, talks with a panel of experts about whether generative AI systems should be regulated similarly to search and recommendation algorithms. Hint: Section 230.)

Policy discussions about generative AI have so far focused on that second category: risks from commercial use of the technology, like coding or advertising. So far, the US government has taken small but notable actions, primarily through the Federal Trade Commission (FTC). The FTC issued a warning statement to companies last month urging them not to make claims about technical capabilities that they canโt substantiate, such as overstating what AI can do. This week, on its business blog, it used even stronger language about risks companies should consider when using generative AI.

โIf you develop or offer a synthetic media or generative AI product, consider at the design stage and thereafter the reasonably foreseeableโand often obviousโways it could be misused for fraud or cause other harm. Then ask yourself whether such risks are high enough that you shouldnโt offer the product at all,โ the blog post reads.

The US Copyright Office also launched a new initiative intended to deal with the thorny policy questions around AI, attribution, and intellectual property.

The EU, meanwhile, is sticking true to its reputation as the world leader in tech policy. At the start of this year my colleague Melissa Heikkilรค wrote about the EUโs efforts to try to pass the AI Act. Itโs a set of rules that would prevent companies from releasing models into the wild without disclosing their inner workings, which is precisely what some critics are accusing OpenAI of with the GPT-4 release.

The EU intends to separate high-risk uses of AI, like hiring, legal, or financial applications, from lower-risk uses like video games and spam filters, and require more transparency around the more sensitive uses. OpenAI has acknowledged some of the concerns about the speed of adoption. In fact, its own CEO, Sam Altman, told ABC News he shares many of the same fears. However, the company is still not disclosing key data about GPT-4.

For policy folks in Washington, Brussels, London, and offices everywhere else in the world, itโs important to understand that generative AI is here to stay. Yes, thereโs significant hype, but the recent advances in AI are as real and important as the risks that they pose.

Yesterday, the United States Congress called Shou Zi Chew, the CEO of TikTok, to a hearing about privacy and security concerns raised by the popular social media app. His appearance came after the Biden administration threatened a national ban if its parent company, ByteDance, didnโt sell off the majority of its shares.

There were lots of headlines, most using a temporal pun, and the hearing laid bare the depths of the new technological cold war between the US and China. For many watching, the hearing was both important and disappointing, with some legislators displaying poor technical understanding and hypocrisy about how Chinese companies handle privacy when American companies collect and trade data in much the same ways.

It also revealed how deeply American lawmakers distrust Chinese tech. Here are some of the spicier takes and helpful articles to get up to speed:

Key takeaways from TikTok hearing in Congress โ and the uncertain road ahead - Kari Paul and Johana Bhuiyan, The Guardian
What to Know About the TikTok Security Concerns - Billy Perrigo, Time
Americaโs online privacy problems are much bigger than TikTok - Will Oremus, Washington Post
Thereโs a Problem With Banning TikTok. Itโs Called the First
Amendment - Jameel Jaffer (Executive Director of the Knight First Amendment Institute), NYT Opinion

AI is able to persuade people to change their minds about hot-button political issues like an assault weapon ban and paid parental leave, according to a study by a team at Stanfordโs Polarization and Social Change Lab. The researchers compared peopleโs political opinions on a topic before and after reading an AI-generated argument, and found that these arguments can be as effective as human-written ones in persuading the readers: โAI ranked consistently as more factual and logical, less angry, and less reliant upon storytelling as a persuasive technique.โ

The teams point to concerns about the use of generative AI in a political context, such as in lobbying or online discourse. (For more on the use of generative AI in politics, do please read this recent piece by Nathan Sanders and Bruce Schneier.)


/ 009. North, Geoffrey. "A celebration of connectionism," 1987.


New developments in neural network theory have excited both psychologists and neurobiologists. Practitioners of the new art displayed their wares last week.

WHEN David Rumelhart, Geoffrey Hinton and Ronald Williams described for neural networks a powerful new learning procedure called back-propagation (Nature 323, 533; 1986), they noted that theirs was not a plausible model of how brains learn. Yet the generality of their approach, and the several intriguing features of network learning by backpropagation which have come to light, have stimulated a resurgence of interest in neural network models among neuroscientists, theoreticians and experimentalists alike. Last week, at a meeting organized by the Society of Experimental Psychology, a packed audience heard Geoffrey Hinton describe a new learning algorithm that seems a better model of biological learning than is back-propagation by parallel networks which nevertheless seems to retain much of the power of its predecessor.

In parallel distributed processing, a network can be thought of as embodying a mathematical function mapping vectors in 'input space' to vectors in 'output space', much as matrices effect linear transformations between vector spaces. A vector in the neural context is simply the pattern of excitation of some set of units taken to be the input or the output of the network. The processing is thus distributed in the pattern of the connections between units of the network and their strengths. Corresponding to the real physiological task of, say, pattern recognition, will be some kind of network function mapping inputs (patterns) onto outputs (interpretations). The all-important question is what kind of network is needed for a particular task.

Simple networks developed in the 1960s, known as perceptrons, involved only two layers of units, an input and an output, with direct connections between them. Such networks are very limited in the range of tasks they can carry out. The versatility of a network can be greatly increased by the introduction of intermediate layers of 'hidden' units, but this raises the problem of how it can be trained.

Back-propagation provides an elegant way of training a multi-layered network. During learning, the output vectors generated by the network for a given input are compared with the desired output, giving an error calculated from the difference between the two. Back-propagation calculates the dependence of this error on all the connection weights, simply by using the chain-rule for differentiation, and the weights are adjusted to reduce the error, so that the network converges by gradient descent on the required structure.

During learning, the network comes to capture certain general features which are characteristic of its task. The hidden units, in particular, develop features that seem especially significant to neuroscientists who record the properties of single neurons in brains. For example, in some cases they are reminiscent of the way in which some neurons in the brain are found to be specific for different aspects of the representation of the visual field.

Even so, this system of learning by back-propagation has not seemed very biologically realistic. Hinton (CarnegieMellon University) and his colleagues have been looking for a more plausible system oflearning.

The new development is known as a 'recirculation' algorithm, and works as follows. In a network learning by backpropagation, there is a linear flow of activity (via the hidden intermediate units) from the input units to the output units.In the new system, the hidden units connect back to the single layer of 'visible' input units. Activity thus recirculates through the network; during training, the connection weights are adjusted to minimize the rate of change of activity in each unit. Thus, when trained, the network is set up so as to stabilize on certain states, and so can work as a kind of 'content addressable memory' with the property that degraded or incomplete forms of the training inputs can regenerate the correct version.

It has been shown that, under certain conditions, the new algorithm is equivalent to gradient descent, and it has been found empirically that the system still works when these conditions are relaxed.

A number of interesting applications of back-propagation were reported at the meeting. Hinton described a network for recognizing one-dimensional shapes on a one-dimensional retina independently of position: the hidden units learn to respond to shapes in different positions. Hinton also described a speech-recognition network which learns to recognize spoken consonants given very noisy corrupted data. It appears to perform almost as well as people, and better than the previous best system of automated speech recognition.

Several speakers described analogies between the behaviour of their networks during training or after 'damage' and what is known of human learning and cognitive disorders. For example, J. L. McClelland (Carnegie-Mellon University) described how a network for learning a balancebeam task progressed through stages of competence similar to those of children given the same task. The problem is to decide which way a balance-beam will tip, depending on the position and size of weights on either side of the fulcrum. With an appropriately biased learning environment, such as children might well experience, the network, like children, initially bases its decisions purely on weight information; gradually the network learns to use the position of the weights. M.S. Seidenberg (McGill University) described a network for word recognition and pronunciation that, when 'damaged' by the removal of hidden units, displayed behaviour reminiscent of some human disorders, such as dyslexia.

Parallel distributed processing is not without its critics, and S. Pinker (MIT) reported that a linguistic analysis of Rumelhart and McClelland's network for changing the tense of verbs in sentences shows that the system is not 'descriptively adequate' as a model for human language, in that it abandons certain symbolic rules and principles that linguistic studies suggest are crucial in human language.

David Willshaw (Edinburgh) asked whether parallel distributed processing networks might, like perceptrons, similarly cease to make significant progress and fade in interest after a period of development and excitement. McClelland's riposte was that work on perceptrons was severely limited by the available computer power and circumstances are now sufficiently different to justify optimism.

The biological relevance of parallel distributed processing remains an open question. Independently of relevance, however, work on network systems may be of interest at a purely theoretical level. The present work is a kind of experimental mathematics, and in that respect is rather similar to that of Mandlebrot on fractals, also made possible and inspired by computers. The hope is that, in future, deductive proofs will give a more rigorous basis to work on networks.

Many interesting problems remain. On what set of functions will a given network topology converge? How can the optimal network for a given task be predicted, and how long will training take? And so on.


/ 010. Hinton, Geoffrey E. "Computation by neural networks," 2000.


Networks of neurons can perform com- putations that have proved very difficult to emulate in conventional computers. In trying to understand how real ner- vous systems achieve their remarkable computational abilities, researchers have been confronted with three major the- oretical issues. How can we characterize the dynamics of neural networks with recurrent  connections?  How  do  the time-varying activities of populations of neurons represent things? How are synapse strengths adjusted to learn these representations? To gain insight into these difficult theoretical issues, it has proved necessary to study grossly ideal- ized models that are as different from real biological neural networks as apples are from planets.

The 1980s saw major progress on all three fronts. In a classic 1982 paper1, Hopfield showed that asynchronous networks with symmetrically connected neurons would settle to locally stable states, known as โpoint attractorsโ, which could be viewed as content-addressable memories. Although these networks were both computationally inefficient and biologically unrealistic, Hopfieldโs work inspired a new generation of recurrent network models; one early example was a learning algorithm that could automatically construct efficient and robust population codes in โhiddenโ neurons whose activities were never explicitly specified by the training environment.

The 1980s also saw the widespread use of the backpropagation algorithm for training the synaptic weights in both feedforward and recurrent neural networks. Backpropagation is simply an efficient method for computing how changing the weight of any given synapse would affect the difference between the way the network actually behaves in response to a particular training input and the way a teacher desires it to behave3. Backpropagation is not a plausible model of how real synapses learn, because it requires a teacher to specify the desired behavior of the network, it uses connections backward, and it is very slow in large networks. However, backpropagation did demonstrate the impressive power of adjusting synapses to optimize a performance measure. It also allowed psychologists to design neural networks that could perform interesting computations in unexpected ways. For example, a recurrent network that is trained to derive the meaning of words from their spelling makes very surprising errors when damaged, and these errors are remarkably similar to those made by adults with dyslexia.

The practical success of backpropagation led researchers to look for an alternative performance measure that did not involve a teacher and that could easily be optimized using information that was locally available at a synapse. A measure with all the right properties emerges from thinking about perception in a peculiar way: the widespread existence of top-down connections in the brain, coupled with our ability to generate mental images, suggests that the perceptual system may literally contain a generative model of sensory data. A generative model stands in the same relationship to perception as do computer graphics to computer vision. It allows the sensory data to be generated from a high-level description of the scene. Perception can be seen as the process of inverting the generative modelโinferring a high-level description from sensory data under the assumption that the data were produced by the generative model. Learning then is the process of updating the parameters of the generative model so as to maximize the likelihood that it would generate the observed sensory data.

Many neuroscientists find this way of thinking unappealing because the obvious function of the perceptual system is to go from the sensory data to a high-level representation, not vice versa. But to understand how we extract the causes from a particular image sequence, or how we learn the classes of things that might be causes, it is very helpful to think in terms of a top-down, stochastic, generative model. This is exactly the approach that statisticians take to modeling data, and recent advances in the complexity of such statistical models5 provide a rich source of ideas for understanding neural computation. All the best speech recognition programs now work by fitting a probabilistic generative model.

If the generative model is linear, the fitting is relatively straightforward but can nevertheless lead to impressive results6,7. There is good empirical evidence that the brain uses generative models with temporal dynamics for motor control8 (see also ref. 9, this issue). If the generative model is nonlinear and allows multiple causes, it can be very difficult to compute the likely causes of a pattern of sensory inputs. When exact inference is unfeasible, it is possible to use bottom-up, feedforward connections to activate approximately the right causes, and this leads to a learning algorithm for fitting hierarchical nonlinear models that requires only information that is locally available at synapses10. So far, theoretical neuroscientists have considered only a few simple types of nonlinear generative model. Although these have produced impressive results, it seems likely that more sophisticated models and better fitting techniques will be required to make detailed contact with neural reality.


/ 011. Rotman, David. "ChatGPT is about to revolutionize the economy. We need to decide what that looks like," 2023.


New large language models will transform many jobs. Whether they will lead to widespread prosperity or not is up to us.

Whether itโs based on hallucinatory beliefs or not, an artificial-intelligence gold rush has started over the last several months to mine the anticipated business opportunities from generative AI models like ChatGPT. App developers, venture-backed startups, and some of the worldโs largest corporations are all scrambling to make sense of the sensational textgenerating bot released by OpenAI last November.

You can practically hear the shrieks from corner offices around the world: โWhat is our ChatGPT play? How do we make money off this?โ

But while companies and executives see a clear chance to cash in, the likely impact of the technology on workers and the economy on the whole is far less obvious. Despite their limitationsโchief among of them their propensity for making stuff upโChatGPT and other recently released generative AI models hold the promise of automating all sorts of tasks that were previously thought to be solely in the realm of human creativity and reasoning, from writing to creating graphics to summarizing and analyzing data. That has left economists unsure how jobs and overall productivity might be affected.

For all the amazing advances in AI and other digital tools over the last decade, their record in improving prosperity and spurring widespread economic growth is discouraging. Although a few investors and entrepreneurs have become very rich, most people havenโt benefited. Some have even been automated out of their jobs.

Productivity growth, which is how countries become richer and more prosperous, has been dismal since around 2005 in the US and in most advanced economies (the UK is a particular basket case). The fact that the economic pie is not growing much has led to stagnant wages for many people.

What productivity growth there has been in that time is largely confined to a few sectors, such as information services, and in the US to a few citiesโ think San Jose, San Francisco, Seattle, and Boston.

Will ChatGPT make the already troubling income and wealth inequality in the US and many other countries even worse? Or could it help? Could it in fact provide a much-needed boost to productivity?

ChatGPT, with its human-like writing abilities, and OpenAIโs other recent release DALL-E 2, which generates images on demand, use large language models trained on huge amounts of data. The same is true of rivals such as Claude from Anthropic and Bard from Google. These so-called foundational models, such as GPT-3.5 from OpenAI, which ChatGPT is based on, or Googleโs competing language model LaMDA, which powers Bard, have evolved rapidly in recent years.

They keep getting more powerful: theyโre trained on ever more data, and the number of parametersโthe variables in the models that get tweakedโis rising dramatically. Earlier this month, OpenAI released its newest version, GPT-4. While OpenAI wonโt say exactly how much bigger it is, one can guess; GPT-3, with some 175 billion parameters, was about 100 times larger than GPT-2.

But it was the release of ChatGPT late last year that changed everything for many users. Itโs incredibly easy to use and compelling in its ability to rapidly create human-like text, including recipes, workout plans, andโ perhaps most surprisingโcomputer code. For many non-experts, including a growing number of entrepreneurs and businesspeople, the user-friendly chat modelโless abstract and more practical than the impressive but often esoteric advances that been brewing in academia and a handful of hightech companies over the last few yearsโis clear evidence that the AI revolution has real potential.

Venture capitalists and other investors are pouring billions into companies based on generative AI, and the list of apps and services driven by large language models is growing longer every day.

Among the big players, Microsoft has invested a reported $10 billion in OpenAI and its ChatGPT, hoping the technology will bring new life to its long-struggling Bing search engine and fresh capabilities to its Office products. In early March, Salesforce said it will introduce a ChatGPT app in its popular Slack product; at the same time, it announced a $250 million fund to invest in generative AI startups. The list goes on, from Coca-Cola to GM. Everyone has a ChatGPT play.

Meanwhile, Google announced it is going to use its new generative AI tools in Gmail, Docs, and some of its other widely used products.

Still, there are no obvious killer apps yet. And as businesses scramble for ways to use the technology, economists say a rare window has opened for rethinking how to get the most benefits from the new generation of AI.

โWeโre talking in such a moment because you can touch this technology. Now you can play with it without needing any coding skills. A lot of people can start imagining how this impacts their workflow, their job prospects,โ says Katya Klinova, the head of research on AI, labor, and the economy at the Partnership on AI in San Francisco.

โThe question is who is going to benefit? And who will be left behind?โ says Klinova, who is working on a report outlining the potential job impacts of generative AI and providing recommendations for using it to increase shared prosperity.

The optimistic view: it will prove to be a powerful tool for many workers, improving their capabilities and expertise, while providing a boost to the overall economy. The pessimistic one: companies will simply use it to destroy what once looked like automation-proof jobs, well-paying ones that require creative skills and logical reasoning; a few high-tech companies and tech elites will get even richer, but it will do little for overall economic growth.

Helping the least skilled

The question of ChatGPTโs impact on the workplace isnโt just a theoretical one.

In the most recent analysis, OpenAIโs Tyna Eloundou, Sam Manning, and Pamela Mishkin, with the University of Pennsylvaniaโs Daniel Rock, found that large language models such as GPT could have some effect on 80% of the US workforce. They further estimated that the AI models, including GPT-4 and other anticipated software tools, would heavily affect 19% of jobs, with at least 50% of the tasks in those jobs โexposed.โ In contrast to what we saw in earlier waves of automation, higher-income jobs would be most affected, they suggest. Some of the people whose jobs are most vulnerable: writers, web and digital designers, financial quantitative analysts, andโjust in case you were thinking of a career changeโ blockchain engineers.

โThere is no question that [generative AI] is going to be usedโitโs not just a novelty,โ says David Autor, an MIT labor economist and a leading expert on the impact of technology on jobs. โLaw firms are already using it, and thatโs just one example. It opens up a range of tasks that can be automated.โ

Autor has spent years documenting how advanced digital technologies have destroyed many manufacturing and routine clerical jobs that once paid well. But he says ChatGPT and other examples of generative AI have changed the calculation.

Previously, AI had automated some office work, but it was those rote stepby- step tasks that could be coded for a machine. Now it can perform tasks that we have viewed as creative, such as writing and producing graphics. โItโs pretty apparent to anyone whoโs paying attention that generative AI opens the door to computerization of a lot of kinds of tasks that we think of as not easily automated,โ he says.

The worry is not so much that ChatGPT will lead to large-scale unemploymentโas Autor points out, there are plenty of jobs in the USโbut that companies will replace relatively well-paying white-collar jobs with this new form of automation, sending those workers off to lower-paying service employment while the few who are best able to exploit the new technology reap all the benefits.

In this scenario, tech-savvy workers and companies could quickly take up the AI tools, becoming so much more productive that they dominate their workplaces and their sectors. Those with fewer skills and little technical acumen to begin with would be left further behind.

But Autor also sees a more positive possible outcome: generative AI could help a wide swath of people gain the skills to compete with those who have more education and expertise.

One of the first rigorous studies done on the productivity impact of ChatGPT suggests that such an outcome might be possible.

Two MIT economics graduate students, Shakked Noy and Whitney Zhang, ran an experiment involving hundreds of college-educated professionals working in areas like marketing and HR; they asked half to use ChatGPT in their daily tasks and the others not to. ChatGPT raised overall productivity (not too surprisingly), but hereโs the really interesting result: the AI tool helped the least skilled and accomplished workers the most, decreasing the performance gap between employees. In other words, the poor writers got much better; the good writers simply got a little faster.

The preliminary findings suggest that ChatGPT and other generative AIs could, in the jargon of economists, โupskillโ people who are having trouble finding work. There are lots of experienced workers โlying fallowโ after being displaced from office and manufacturing jobs over the last few decades, Autor says. If generative AI can be used as a practical tool to broaden their expertise and provide them with the specialized skills required in areas such as health care or teaching, where there are plenty of jobs, it could revitalize our workforce.

Determining which scenario wins out will require a more deliberate effort to think about how we want to exploit the technology.

โI donโt think we should take it as the technology is loose on the world and we must adapt to it. Because itโs in the process of being created, it can be used and developed in a variety of ways,โ says Autor. โItโs hard to overstate the importance of designing what itโs there for.โ

Simply put, we are at a juncture where either less-skilled workers will increasingly be able to take on what is now thought of as knowledge work, or the most talented knowledge workers will radically scale up their existing advantages over everyone else. Which outcome we get depends largely on how employers implement tools like ChatGPT. But the more hopeful option is well within our reach.

Beyond human-like

There are some reasons to be pessimistic, however. Last spring, in โThe Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence,โ the Stanford economist Erik Brynjolfsson warned that AI creators were too obsessed with mimicking human intelligence rather than finding ways to use the technology to allow people to do new tasks and extend their capabilities.

The pursuit of human-like capabilities, Brynjolfsson argued, has led to technologies that simply replace people with machines, driving down wages and exacerbating inequality of wealth and income. It is, he wrote, โthe single biggest explanationโ for the rising concentration of wealth.

A year later, he says ChatGPT, with its human-sounding outputs, โis like the poster child for what I warned aboutโ: it has โturbochargedโ the discussion around how the new technologies can be used to give people new abilities rather than simply replacing them.

Despite his worries that AI developers will continue to blindly outdo each other in mimicking human-like capabilities in their creations, Brynjolfsson, the director of the Stanford Digital Economy Lab, is generally a techno-optimist when it comes to artificial intelligence. Two years ago, he predicted a productivity boom from AI and other digital technologies, and these days heโs bullish on the impact of the new AI models.

Much of Brynjolfssonโs optimism comes from the conviction that businesses could greatly benefit from using generative AI such as ChatGPT to expand their offerings and improve the productivity of their workforce. โItโs a great creativity tool. Itโs great at helping you to do novel things. Itโs not simply doing the same thing cheaper,โ says Brynjolfsson. As long as companies and developers can โstay away from the mentality of thinking that humans arenโt needed,โ he says, โitโs going to be very important.โ

Within a decade, he predicts, generative AI could add trillions of dollars in economic growth in the US. โA majority of our economy is basically knowledge workers and information workers,โ he says. โAnd itโs hard to think of any type of information workers that wonโt be at least partly affected.โ

When that productivity boost will comeโif it doesโis an economic guessing game. Maybe we just need to be patient.

In 1987, Robert Solow, the MIT economist who won the Nobel Prize that year for explaining how innovation drives economic growth, famously said, โYou can see the computer age everywhere except in the productivity statistics.โ It wasnโt until later, in the mid and late 1990s, that the impactsโ particularly from advances in semiconductorsโbegan showing up in the productivity data as businesses found ways to take advantage of ever cheaper computational power and related advances in software.

Could the same thing happen with AI? Avi Goldfarb, an economist at the University of Toronto, says it depends on whether we can figure out how to use the latest technology to transform businesses as we did in the earlier computer age.

So far, he says, companies have just been dropping in AI to do tasks a little bit better: โItโll increase efficiencyโit might incrementally increase productivityโbut ultimately, the net benefits are going to be small. Because all youโre doing is the same thing a little bit better.โ But, he says, โthe technology doesnโt just allow us to do what weโve always done a little bit better or a little bit cheaper. It might allow us to create new processes to create value to customers.โ

The verdict on whenโeven ifโthat will happen with generative AI remains uncertain. โOnce we figure out what good writing at scale allows industries to do differently, orโin the context of Dall-Eโwhat graphic design at scale allows us to do differently, thatโs when weโre going to experience the big productivity boost,โ Goldfarb says. โBut if that is next week or next year or 10 years from now, I have no idea.โ

Power struggle

When Anton Korinek, an economist at the University of Virginia and a fellow at the Brookings Institution, got access to the new generation of large language models such as ChatGPT, he did what a lot of us did: he began playing around with them to see how they might help his work. He carefully documented their performance in a paper in February, noting how well they handled 25 โuse cases,โ from brainstorming and editing text (very useful) to coding (pretty good with some help) to doing math (not great).

ChatGPT did explain one of the most fundamental principles in economics incorrectly, says Korinek: โIt screwed up really badly.โ But the mistake, easily spotted, was quickly forgiven in light of the benefits. โI can tell you that it makes me, as a cognitive worker, more productive,โ he says. โHands down, no question for me that Iโm more productive when I use a language model.โ

When GPT-4 came out, he tested its performance on the same 25 questions that he documented in February, and it performed far better. There were fewer instances of making stuff up; it also did much better on the math assignments, says Korinek.

Since ChatGPT and other AI bots automate cognitive work, as opposed to physical tasks that require investments in equipment and infrastructure, a boost to economic productivity could happen far more quickly than in past technological revolutions, says Korinek. โI think we may see a greater boost to productivity by the end of the yearโcertainly by 2024,โ he says.

Whatโs more, he says, in the longer term, the way the AI models can make researchers like himself more productive has the potential to drive technological progress.

That potential of large language models is already turning up in research in the physical sciences. Berend Smit, who runs a chemical engineering lab at EPFL in Lausanne, Switzerland, is an expert on using machine learning to discover new materials. Last year, after one of his graduate students, Kevin Maik Jablonka, showed some interesting results using GPT-3, Smit asked him to demonstrate that GPT-3 is, in fact, useless for the kinds of sophisticated machine-learning studies his group does to predict the properties of compounds.

โHe failed completely,โ jokes Smit.

It turns out that after being fine-tuned for a few minutes with a few relevant examples, the model performs as well as advanced machine-learning tools specially developed for chemistry in answering basic questions about things like the solubility of a compound or its reactivity. Simply give it the name of a compound, and it can predict various properties based on the structure.

As in other areas of work, large language models could help expand the expertise and capabilities of nonexpertsโ in this case, chemists with little knowledge of complex machine-learning tools. Because itโs as simple as a literature search, Jablonka says, โit could bring machine learning to the masses of chemists.โ

These impressiveโand surprisingโresults are just a tantalizing hint of how powerful the new forms of AI could be across a wide swath of creative work, including scientific discovery, and how shockingly easy they are to use. But this also points to some fundamental questions.

As the potential impact of generative AI on the economy and jobs becomes more imminent, who will define the vision for how these tools should be designed and deployed? Who will control the future of this amazing technology?

Diane Coyle, an economist at Cambridge University in the UK, says one concern is the potential for large language models to be dominated by the same big companies that rule much of the digital world. Google and Meta are offering their own large language models alongside OpenAI, she points out, and the large computational costs required to run the software create a barrier to entry for anyone looking to compete.

The worry is that these companies have similar โadvertising-driven business models,โ Coyle says. โSo obviously you get a certain uniformity of thought, if you donโt have different kinds of people with different kinds of incentives.โ

Coyle acknowledges that there are no easy fixes, but she says one possibility is a publicly funded international research organization for generative AI, modeled after CERN, the Geneva-based intergovernmental European nuclear research body where the World Wide Web was created in 1989. It would be equipped with the huge computing power needed to run the models and the scientific expertise to further develop the technology.

Such an effort outside of Big Tech, says Coyle, would โbring some diversity to the incentives that the creators of the models face when theyโre producing them.โ

While it remains uncertain which public policies would help make sure that large language models best serve the public interest, says Coyle, itโs becoming clear that the choices about how we use the technology canโt be left to a few dominant companies and the market alone.

History provides us with plenty of examples of how important government funded research can be in developing technologies that bring about widespread prosperity. Long before the invention of the web at CERN, another publicly funded effort in the late 1960s gave rise to the internet, when the US Department of Defense supported ARPANET, which pioneered ways for multiple computers to communicate with each other.

In Power and Progress: Our 1000-Year Struggle Over Technology & Prosperity, the MIT economists Daron Acemoglu and Simon Johnson provide a compelling walk through the history of technological progress and its mixed record in creating widespread prosperity. Their point is that itโs critical to deliberately steer technological advances in ways that provide broad benefits and donโt just make the elite richer.

From the decades after World War II until the early 1970s, the US economy was marked by rapid technological changes; wages for most workers rose while income inequality dropped sharply. The reason, Acemoglu and Johnson say, is that technological advances were used to create new tasks and jobs, while social and political pressures helped ensure that workers shared the benefits more equally with their employers than they do now.

In contrast, they write, the more recent rapid adoption of manufacturing robots in โthe industrial heartland of the American economy in the Midwestโ over the last few decades simply destroyed jobs and led to a โprolonged regional decline.โ

The book, which comes out in May, is particularly relevant for understanding what todayโs rapid progress in AI could bring and how decisions about the best way to use the breakthroughs will affect us all going forward. In a recent interview, Acemoglu said they were writing the book when GPT-3 was first released. And, he adds half-jokingly, โwe foresaw ChatGPT.โ

Acemoglu maintains that the creators of AI โare going in the wrong direction.โ The entire architecture behind the AI โis in the automation mode,โ he says. โBut there is nothing inherent about generative AI or AI in general that should push us in this direction. Itโs the business models and the vision of the people in OpenAI and Microsoft and the venture capital community.โ

If you believe we can steer a technologyโs trajectory, then an obvious question is: Who is โweโ? And this is where Acemoglu and Johnson are most provocative. They write: โSociety and its powerful gatekeepers need to stop being mesmerized by tech billionaires and their agenda โฆ One does not need to be an AI expert to have a say about the direction of progress and the future of our society forged by these technologies.โ

The creators of ChatGPT and the businesspeople involved in bringing it to market, notably OpenAIโs CEO, Sam Altman, deserve much credit for offering the new AI sensation to the public. Its potential is vast. But that doesnโt mean we must accept their vision and aspirations for where we want the technology to go and how it should be used.

According to their narrative, the end goal is artificial general intelligence, which, if all goes well, will lead to great economic wealth and abundances. Altman, for one, has promoted the vision at great length recently, providing further justification for his longtime advocacy of a universal basic income (UBI) to feed the non-technocrats among us. For some, it sounds tempting. No work and free money! Sweet!

Itโs the assumptions underlying the narrative that are most troublingโ namely, that AI is headed on an inevitable job-destroying path and most of us are just along for the (free?) ride. This view barely acknowledges the possibility that generative AI could lead to a creativity and productivity boom for workers far beyond the tech-savvy elites by helping to unlock their talents and brains. There is little discussion of the idea of using the technology to produce widespread prosperity by expanding human capabilities and expertise throughout the working population.

As Acemoglu and Johnson write: โWe are heading toward greater inequality not inevitably but because of faulty choices about who has power in society and the direction of technology โฆ In fact, UBI fully buys into the vision of the business and tech elite that they are the enlightened, talented people who should generously finance the rest.โ

Acemoglu and Johnson write of various tools for achieving โa more balanced technology portfolio,โ from tax reforms and other government policies that might encourage the creation of more worker-friendly AI to reforms that might wean academia off Big Techโs funding for computer science research and business schools.

But, the economists acknowledge, such reforms are โa tall order,โ and a social push to redirect technological change is โnot just around the corner.โ

The good news is that, in fact, we can decide how we choose to use ChatGPT and other large language models. As countless apps based on the technology are rushed to market, businesses and individual users will have a chance to choose how they want to exploit it; companies can decide to use ChatGPT to give workers more abilitiesโor to simply cut jobs and trim costs.

Another positive development: there is at least some momentum behind open-source projects in generative AI, which could break Big Techโs grip on the models. Notably, last year more than a thousand international researchers collaborated on a large language model called Bloom that can create text in languages such as French, Spanish, and Arabic. And if Coyle and others are right, increased public funding for AI research could help change the course of future breakthroughs.

Stanford's Brynjolfsson refuses to say heโs optimistic about how it will play out. Still, his enthusiasm for the technology these days is clear. โWe can have one of the best decades ever if we use the technology in the right direction,โ he says. โBut itโs not inevitable.โ


/ 012. Seetharaman, Deepa. "Elon Musk, Other AI Experts Call for Pauwse in Technology's Development," 2023.


Appeal causes tension among artificial-intelligence stakeholders amidconcern over pace of advancement

Several tech executives and top artificial-intelligence researchers, including Tesla Inc. Chief Executive Offi cer Elon Musk and AI pioneer Yoshua Bengio , are calling for a pause in the breakneck development of powerful new AI tools.

A moratorium of six months or more would give the industry time to set safety standards for AI design and head off potential harms of the riskiest AI technologies , the proponents of a pause said.

โWeโve reached the point where these systems are smart enough that they canbe used in ways that are dangerous for society,โ Mr. Bengio, director of theUniversity of Montrealโs Montreal Institute for Learning Algorithms, said in an interview. โAnd we donโt yet understand.โ

These concerns and the recommendation for the pause were laid out in a letter titled โPause Giant AI Experiments: An Open Letterโ coordinated by the non profit Future of Life Institute, which lists Mr. Musk as an external adviser.The letter that was made public Wednesday was also signed by Apple co-founder Steve Wozniak ; Stability AI CEO Emad Mostaque; and co-founders of the Center for Humane Technology, Tristan Harris and Aza Raskin, who have been critical of social media and AI technology, said a spokeswoman for the team authoring the letter.

The letter doesnโt call for all AI development to halt, but urges companies to temporarily stop training systems more powerful than GPT-4, the technology released this month by Microsoft Corp.-backed startup OpenAI. That includes the next generation of OpenAIโs technology, GPT-5.

OpenAI officials say they havenโt started training GPT-5. In an interview, OpenAICEO Sam Altman said the company has long given priority to safety in development and spent more than six months doing safety tests on GPT-4 before its launch.

โIn some sense, this is preaching to the choir,โ Mr. Altman said. โWe have, I think, been talking about these issues the loudest, with the most intensity, for the longest.โ

Calls for a pause clash with a broad desire among tech companies and startups to double down on so-called generative AI, a technology capable of generating original content to human prompts. Buzz around generative AI exploded last fall after OpenAI unveiled a chatbot with its ability to perform functions like providing lengthy answers and producing computer code with humanlike sophistication.

Microsoft has embraced the technology for its Bing search engine and other tools. Alphabet Inc.โs Google has deployed a rival system , and companies such as Adobe Inc., Zoom Video Communications Inc. and Salesforce Inc. have also introduced advanced AI tools.

โA race starts today,โ Microsoft CEO Satya Nadella said last month. โWeโre going to move, and move fast.โ

That approach has spurred renewed concerns that a rapid rollout could have unintended consequences alongside real benefits. Advances in AI have surpassed what many experts believed was possible just a few years ago, said Max Tegmark , one of the organizers of the letter, president of the Future of LifeInstitute and a physics professor at the Massachusetts Institute of Technology.

โIt is unfortunate to frame this as an arms race,โ Mr. Tegmark said. โIt is more of a suicide race. It doesnโt matter who is going to get there first. It just means that humanity as a whole could lose control of its own destiny.โ

The Future of Life Institute started working on the letter last week and initially allowed anybody to sign without identity verification. At one point, Mr. Altmanโs name was added to the letter, but later removed. Mr. Altman said he never signed the letter. He said the company frequently coordinates with other AI companies on safety standards and to discuss broader concerns.

โThere is work that we donโt do because we donโt think we yet know how to make it sufficiently safe,โ he said. โSo yeah, I think there are ways that you can slowdown on multiple axes and thatโs important. And it is part of our strategy.โ Messrs. Musk and Wozniak have both voiced concerns about AI technology . Mr.Musk on Wednesday tweeted that developers of the advanced AI technologyโ will not heed this warning, but at least it was said.โ

Mr. Musk at the same time has embraced some AI tools at Tesla for the companyโs advanced driver-assistance functions. Tesla last month said it was recalling around 362,800 vehicles equipped with its technology marketed as FullSelf-Driving Beta . The U.S. top car-safety agency said the technology could, in rare circumstances, violate local traffic laws, potentially increasing the risk of a collision if a driver fails to intervene.

Yann LeCun, chief AI scientist at Meta Platforms Inc., on Tuesday tweeted that he didnโt sign the letter because he disagreed with its premise .

Mr. Mostaque, Stability AIโs CEO, said in a tweet Wednesday that although he signed the letter, he didnโt agree with a six-month pause. โIt has no force but will kick off an important discussion that will hopefully bring more transparency &governance to an opaque area.โ

Microsoft declined to comment, and Google didnโt immediately respond to a request for comment.

Mr. Tegmark said many companies feel โcrazy commercial pressuresโ to add advanced AI technology into their products. A six-month pause would allow the industry โbreathing room,โ without disadvantaging ones that opt to move carefully.

The letter said a pause should be declared publicly and be verifiable and all key actors in the space should participate. โIf such a pause cannot be enacted quickly, governments should step in and institute a moratorium,โ it said.

AI labs and experts can use this time to develop a set of shared safety rules for advanced AI design that should be audited and overseen by outside experts, the authors wrote.

โI donโt think we can afford to just go forward and break things,โ said Mr. Bengio, who shared a 2018 Turing award for inventing the systems that modern AI is built on. โWe do need to take time to think through this collectively.โ


/ 013. Mims, Christopher. "Artificial Intelligence Is Teaching Us New, Surprising Things About the Human Mind," 2023.


Thought is ever-changing electrical patterns unconnected to individual neurons. Meta is working on a system to read your mind

The world has been learning an awful lot about artificial intelligence lately, thanks to the arrival of eerily human-like chatbots.

Less noticed, but just as important: Researchers are learning a great deal about us โ with the help of AI.

AI is helping scientists decode how neurons in our brains communicate, and explore the nature of cognition. This new research could one day lead to humans connecting with computers merely by thinkingโas opposed to typing or voice commands. But there is a long way to go before such visions become reality.

Celeste Kidd, a psychology professor at the University of California, Berkeley, was surprised by what she discovered when she tried to examine the range of opinions people have about certain politicians, including Barack Obama and Donald Trump.

Her research was intended to explore the widening divergence of how we conceive of subjects to which we attach moral judgements โ such as politicians.Previous work has shown that morally-fraught concepts are the ones peoplep erceive in the most polarized ways.

To establish a baseline for her experiment, she began by asking thousands of study participants about their associations with common nouns, in this case animals.

What she discovered was that even for common animals โ including chickens, whales and salmon โ peopleโs notions of their characteristics are all over the map. Are whales majestic? Youโd be surprised who disagrees. Are penguins heavy? Opinions vary. By quizzing people on many such associations, Dr. Kidd was able to amass a pool of data that clusters people according to which of these associations they agree on. Using this method, she found that people can be grouped into between 10 and 30 different clusters, depending on their perception of an animal.

Dr. Kidd and her team concluded that people tend not to see eye to eye about even the most basic characteristics of common objects. We also overestimate how many people see things as we do. In a world in which it feels like people are increasingly talking past one another, the root of this phenomenon may be the fact that even for citizens of a single country speaking a common language, words simply donโt mean the same thing to different people.

That might not seem like a very profound observation, but what Dr. Kiddโs research suggests is the degree to which thatโs true may be much greater than psychologists previously thought.

Arriving at this insight required the application of a tool of mathematics that makes many kinds of AI possible โ known as a โclustering modelโ.

The most important feature of AI which enables new kinds of research, says Dr.Kidd, is the same that makes possible AI chatbots like OpenAIโs ChatGPT , Googleโs Bard , and Microsoftโs Bing chat : Itโs the capacity of modern computer systems to process a lot more data than in the past. It โopens up a lot of possibilities for new insights, from biology to medicine to cognitive science,โ she adds.

Cracking the brain's neural code

In her research, Tatiana Engel, an assistant professor of neuroscience atPrinceton University, uses the same kinds of networks of artificial neurons that are behind most of what we currently call artificial intelligence. But rather than using these to better-target ads, or to generate fake images, or compose text, shea nd her team use them to interpret the electrical signals of hundreds of neurons at once in the brains of animals.

Dr. Engel and her team then go a step further: they train networks of artificial neurons to perform the same tasks as an animal โ say, a swimming worm. They then find that those artificial networks organize themselves in ways that reasonably approximate the way theyโre organized in real animals. While neural networks in the brain are vastly more complicated, the result of this simulation is a model system that is both close enough to its biological equivalent, and simple enough, to teach us things about how the real brain works, Dr. Engel says.

One key insight this yields is that the actual substance of thought โ the patterns that constitute the mind youโre using to read this sentence โ is dynamic electrical activity in our brains rather than something physically anchored to particular neurons.

In other words, in contrast to what neuroscientists once believed about how we make decisions, there are no โeat the chocolateโ neurons and โdonโt eat the chocolateโ neurons. Thinking, it turns out, is just electrical signals zooming about inside our heads, forming a complex code which is carried by our neurons.

Whatโs more, AI is letting scientists listen in on the things that happen in our brains when weโre not doing anything in particular.

โThis allows us to discover the brainโs internal life,โ says Dr. Engel.

Do androids dream of electric sheep? We donโt know yet, but we may soon beable to determine if humans are thinking about the real thing.

Real-life mind reading

If a research lab owned by Meta Platforms, Facebookโs parent company, figuring out how to read your mind makes you at all uncomfortable, youโre probably not going to be a fan of what the rest of the 21st century has in store.

Historically, itโs been very difficult to measure brain activity inside our heads, because the electrical signals generated by our brains, which are miniscule to begin with, must be measured from outside of our skulls. ( Elon Muskโs aspirations for his Neuralink startup notwithstanding, opening up our heads and putting in brain interfaces hasnโt proved popular.)

AI is helping scientists study EEG readings and explore the nature of cognition. Data gathered from language experiments have been used by Facebook parent, Meta Platforms, to develop an early version of an algorithm that can โreadโ words and phrases from a personโs mind.

But progress in artificial intelligence techniques is yielding a more-powerful amplifier of those weak brain signals. Metaโs AI lab published research on one such mind-reading technology last summer.

Meta scientists didnโt actually stick anyone in a brain scanner. Instead, they used data on brain signals gathered by researchers at universities. This data was captured from human subjects who were listening to words and phrases, while sitting in non-invasive brain scanners. These scanners came in two varieties:

One was the sort of electrodes-embedded-in-a-swim-cap with which many people are familiar, called an EEG (short for โelectroencephalogramโ). The other looks like a supervillainโs attempt to create a world-crushing megabrain, called aMEG (for โmagnetoencephalogramโ).

To analyze this data, researchers used a type of AI called a โself-supervised learning model.โ Without this technique, the latest generation of AI chatbots would be impossible. Such models can extract meaning from giant pools of data without any instruction from humans, and have also been used to try and figure out what animals are communicating with each other.

Researchers evaluate fluctuations in MEG sensors while a subjectโs neural activity is being recorded.Research by Meta suggests that such measurements could be used in the future to allow people to direct computers just by thinking.

A little less than half of the time, Metaโs AI algorithm was able to correctly guess what words a person had heard, based on the activity generated in their brains.That might not sound too impressive, but itโs leaps and bounds better than what such systems have been able to achieve in the past.

Alexandre Dรฉfossez, a scientist at Meta who was part of the team that conducted this research, says that the eventual goal of this work is to create a general-purpose โspeech decoderโ that can directly transform our brain activityโour thoughtsโinto words.

Imagine texting a friend just by thinking about it โ as long as youโre wearing anEEG cap at the moment, at any rate. The technology could have a big impact on the lives of people who are unable to communicate in other ways, adds Dr.Dรฉfossez.

Itโs just one more example of the way that AI might someday give us the tools for improving our individual and collective well-being โ or at least an explanation for why, in the age of social media, both of those things frequently seem so deranged.


/ 014. Zuboff, Shoshana. In the Age of the Smart Machine: The Future of Work and Power, 1988.


INTRODUCTION: DILEMMAS OF TRANSFORMATION IN THE AGE OF THE SMART MACHINE

The history of technology is that of human history in all its diversity. That is why specialist historians of technology hardly ever manage to grasp it entirely in their hands. -- Fernand Braudel, The Structures of Everyday Life

We don't know what will be happening to us in the future. Modern technology is taking over. What will be our place? -- A Piney Wood worker

RNEY WOOD, one of the nation's largest pulp mills, was in the throes of a massive modernization effort that would place every aspect of the production process under computer control. Six workers were crowded around a table in the snack area outside what they called the Star Trek Suite, one of the first control rooms to have been completely converted to microprocessor-based instrumentation. It looked enough like a NASA control room to have earned its name.

It was almost midnight, but despite the late hour and the approach of the shift change, each of the six workers was at once animated and thoughtful. "Knowledge and technology are changing so fast," they said, "what will happen to us?" Their visions of the future foresaw wrenching change. They feared that today's working assumptions could not be relied upon to carry them through, that the future would not resemble the past or the present. More frightening still was the sense of a future moving out of reach so rapidly that there was little opportunity to plan or make choices. The speed of dissolution and renovation seemed to leave no time for assurances that we were not heading toward calamity-and it would be all the more regrettable for having been something of an accident.

The discussion around the table betrayed a grudging admiration for the new technology-its power, its intelligence, and the aura of progress surrounding it. That admiration, however, bore a sense of grief. Each expression of gee-whiz-Buck-Rogers breathless wonder brought with it an aching dread conveyed in images of a future that rendered their authors obsolete. In what ways would computer technology transform their work lives? Did it promise the Big Rock Candy Mountain or a silent graveyard?

In fifteen years there will be nothing for the worker to do. The technology will be so good it will operate itself. You will just sit there behind a desk running two or three areas of the mill yourself and get bored.

The group concluded that the worker of the future would need "an extremely flexible personality" so that he or she would not be "mentally affected" by the velocity of change. They anticipated that workers would need a great deal of education and training in order to "breed flexibility." "We find it all to be a great stress," they said, "but it won't be that way for the new flexible people." Nor did they perceive any real choice, for most agreed that without an investment in the new technology, the company could not remain competitive. They also knew that without their additional flexibility, the technology would not fly right. "We are in a bind," one man groaned, "and there is no way out." The most they could do, it was agreed, was to avoid thinking too hard about the loss of overtime pay, the diminished probability of jobs for their sons and daughters, the fears of seeming incompetent in a strange new milieu, or the possibility that the company might welsh on its promise not to lay off workers.

During the conversation, a woman in stained overalls had remained silent with her head bowed, apparently lost in thought. Suddenly, she raised her face to us. It was lined with decades of hard work, her brow drawn together. Her hands lay quietly on the table. They were calloused and swollen, but her deep brown eyes were luminous, youthful, and kind. She seemed frozen, chilled by her own insight, as she solemnly delivered her conclusion:

I think the country has a problem. The managers want everything to be run by computers. But if no one has a job, no one will know how to do anything anymore. Who will pay the taxes? What kind of society will it be when people have lost their knowledge and depend on computers for everything?

Her voice trailed off as the men stared at her in dazzled silence. They slowly turned their heads to look at one another and nodded in agreement. The forecast seemed true enough. Yes, there was a problem. They looked as though they had just run a hard race, only to stop short at the edge of a cliff. As their heels skidded in the dirt, they could see nothing ahead but a steep drop downward.

Must it be so? Should the advent of the smart machine be taken as an invitation to relax the demands upon human comprehension and critical judgment? Does the massive diffusion of computer technology throughout our workplaces necessarily entail an equally dramatic loss of meaningful employment opportunities? Must the new electronic milieu engender a world in which individuals have lost control over their daily work lives? Do these visions of the future represent the price of economic success or might they signal an industrial legacy that must be overcome if intelligent technology is to yield its full value? Will the new information technology represent an opportunity for the rejuvenation of competitiveness, productive vitality, and organizational ingenuity? Which aspects of the future of working life can we predict, and which will depend upon the choices we make today?

The workers outside the Star Trek Suite knew that the so-called technological choices we face are really much more than that. Their consternation puts us on alert. There is a world to be lost and a world to be gained. Choices that appear to be merely technical will redefine our lives together at work. This means more than simply contemplating the implications or consequences of a new technology. It means that a powerful new technology, such as that represented by the computer, fundamentally reorganizes the infrastructure of our material world. It eliminates former alternatives. It creates new possibilities. It necessitates fresh choices.

The choices that we face concern the conception and distribution of knowledge in the workplace. Imagine the following scenario: Intelligence is lodged in the smart machine at the expense of the human capacity for critical judgment. Organizational members become ever more dependent, docile, and secretly cynical. As more tasks must be accomplished through the medium of information technology (I call this "computer-mediated work"), the sentient body loses its salience as a source of knowledge, resulting in profound disorientation and loss of meaning. People intensify their search for avenues of escape through drugs, apathy, or adversarial conflict, as the majority of jobs in our offices and factories become increasingly isolated, remote, routine, and perfunctory. Alternatively, imagine this scenario: Organizational leaders recognize the new forms of skill and knowledge needed to truly exploit the potential of an intelligent technology. They direct their resources toward creating a work force that can exercise critical judgment as it manages the surrounding machine systems. Work becomes more abstract as it depends upon understanding and manipulating information. This marks the beginning of new forms of mastery and provides an opportunity to imbue jobs with more comprehensive meaning. A new array of work tasks offer unprecedented opportunities for a wide range of employees to add value to products and services.

The choices that we make will shape relations of authority in the workplace. Once more, imagine: Managers struggle to retain their traditional sources of authority, which have depended in an important way upon their exclusive control of the organization's knowledge base. They use the new technology to structure organizational experience in ways that help reproduce the legitimacy of their traditional roles. Managers insist on the prerogatives of command and seek methods that protect the hierarchical distance that distinguishes them from their subordinates. Employees barred from the new forms of mastery relinquish their sense of responsibility for the organization's work and use obedience to authority as a means of expressing their resentment. Imagine an alternative: This technological transformation engenders a new approach to organizational behavior, one in which relationships are more intricate, collaborative, and bound by the mutual responsibilities of colleagues. As the new technology integrates information across time and space, managers and workers each overcome their narrow functional perspectives and create new roles that are better suited to enhancing value-adding activities in a data-rich environment. As the quality of skills at each organizational level becomes similar, hierarchical distinctions begin to blur. Authority comes to depend more upon an appropriate fit between knowledge and responsibility than upon the ranking rules of the traditional organizational pyramid.

The choices that we make will determine the techniques of administration that color the psychological ambience and shape communicative behavior in the emerging workplace. Imagine this scenario: The new technology becomes the source of surveillance techniques that are used to ensnare organizational members or to subtly bully them into conformity. Managers employ the technology to circumvent the demanding work of face-to-face engagement, substituting instead techniques of remote management and automated administration. The new technological infrastructure becomes a battlefield of techniques, with managers inventing novel ways to enhance certainty and control while employees discover new methods of self-protection and even sabotage. Imagine the alternative: The new technological milieu becomes a resource from which are fashioned innovative methods of information sharing and social exchange. These methods in turn produce a deepened sense of collective responsibility and joint ownership, as access to ever-broader domains of information lend new objectivity to data and preempt the dictates of hierarchical authority.

This book is about these alternative futures. Computer-based technologies are not neutral; they embody essential characteristics that are bound to alter the nature of work within our factories and offices, and among workers, professionals, and managers. New choices are laid open by these technologies, and these choices are being confronted in the daily lives of men and women across the landscape of modern organizations. This book is an effort to understand the deep structure of these choices-the historical, psychological, and organizational forces that imbue our conduct and sensibility. It is also a vision of a fruitful future, a call for action that can lead us beyond the stale reproduction of the past into an era that offers a historic opportunity to more fully develop the economic and human potential of our work organizations.

THE TWO FACES OF INTELLIGENT TECHNOLOGY

The past twenty years have seen their share of soothsayers ready to predict with conviction one extreme or another of the alternative futures I have presented. From the unmanned factory to the automated cockpit, visions of the future hail information technology as the final answer to "the labor question," the ultimate opportunity to rid ourselves of the thorny problems associated with training and managing a competent and committed work force. These very same technologies have been applauded as the hallmark of a second industrial revolution, in which the classic conflicts of knowledge and power associated with an earlier age will be synthesized in an array of organizational innovations and new procedures for the production of goods and services, all characterized by an unprecedented degree of labor harmony and widespread participation in management process.' Why the paradox? How can the very same technologies be interpreted in these different ways? Is this evidence that the technology is indeed neutral, a blank screen upon which managers project their biases and encounter only their own limitations? Alternatively, might it tell us something else about the interior structure of information technology?

Throughout history, humans have designed mechanisms to reproduce and extend the capacity of the human body as an instrument of work. The industrial age has carried this principle to a dramatic new level of sophistication with machines that can substitute for and amplify the abilities of the human body. Because machines are mute, and because they are precise and repetitive, they can be controlled according to a set of rational principles in a way that human bodies cannot.

There is no doubt that information technology can provide substitutes for the human body that reach an even greater degree of certainty and precision. When a task is automated by a computer, it must first be broken down to its smallest components. Whether the activity involves spraying paint on an automobile or performing a clerical transaction, it is the information contained in this analysis that translates human agency into a computer program. The resulting software can be used to automatically guide equipment, as in the case of a robot, or to execute an information transaction, as in the case of an automated teller machine.

A computer program makes it possible to rationalize activities more comprehensively than if they had been undertaken by a human being. Programmability means, for example, that a robot will respond with unwavering precision because the instructions that guide it are themselves unvarying, or that office transactions will be uniform because the instructions that guide them have been standardized. Events and processes can be rationalized to the extent that human agency can be analyzed and translated into a computer program.

What is it, then, that distinguishes information technology from earlier generations of machine technology? As information technology is used to reproduce, extend, and improve upon the process of substituting machines for human agency, it simultaneously accomplishes something quite different. The devices that automate by translating information into action also register data about those automated activities, thus generating new streams of information. For example, computer-based, numerically controlled machine tools or microprocessor-based sensing devices not only apply programmed instructions to equipment but also convert the current state of equipment, product, or process into data. Scanner devices in supermarkets automate the checkout process and simultaneously generate data that can be used for inventory control, warehousing, scheduling of deliveries, and market analysis. The same systems that make it possible to automate office transactions also create a vast overview of an organization's operations, with many levels of data coordinated and accessible for a variety of analytical efforts.

Thus, information technology, even when it is applied to automatically reproduce a finite activity, is not mute. It not only imposes information (in the form of programmed instructions) but also produces information. It both accomplishes tasks and translates them into information. The action of a machine is entirely invested in its object, the product. Information technology, on the other hand, introduces an additional dimension of reflexivity: it makes its contribution to the product, but it also reflects back on its activities and on the system of activities to which it is related. Information technology not only produces action but also produces a voice that symbolically renders events, objects, and processes so that they become visible, knowable, and shareable in a new way.

Viewed from this interior perspective, information technology is characterized by a fundamental duality that has not yet been fully appreciated. On the one hand, the technology can be applied to automating operations according to a logic that hardly differs from that of the nineteenth-century machine system-replace the human body with a technology that enables the same processes to be performed with more continuity and control. On the other, the same technology simultaneously generates information about the underlying productive and administrative processes through which an organization accomplishes its work. It provides a deeper level of transparency to activities that had been either partially or completely opaque. In this way information technology supersedes the traditional logic of automation. The word that I have coined to describe this unique capacity is informate. Activities, events, and objects are translated into and made visible by information when a technology informates as well as automates.

The informating power of intelligent technology can be seen in the manufacturing environment when microprocessor-based devices such as robots, programmable logic controllers, or sensors are used to translate the three-dimensional production process into digitized data. These data are then made available within a two-dimensional space, typically on the screen of a video display terminal or on a computer printout, in the form of electronic symbols, numbers, letters, and graphics. These data constitute a quality of information that did not exist before. The programmable controller not only tells the machine what to do-imposing information that guides operating equipment but also tells what the machine has done-translating the production process and making it visible.

In the office environment, the combination of on-line transaction systems, information systems, and communications systems creates a vast information presence that now includes data formerly stored in people's heads, in face-to-face conversations, in metal file drawers, and on widely dispersed pieces of paper. The same technology that processes documents more rapidly, and with less intervention, than a mechanical typewriter or pen and ink can be used to display those documents in a communications network. As more of the underlying transactional and communicative processes of an organization become automated, they too become available as items in a growing organizational data base.

In its capacity as an automating technology, information technology has a vast potential to displace the human presence. Its implications as an informating technology, on the other hand, are not well understood. The distinction between automate and informate provides one way to understand how this technology represents both continuities and discontinuities with the traditions of industrial history. As long as the technology is treated narrowly in its automating function, it perpetuates the logic of the industrial machine that, over the course of this century, has made it possible to rationalize work while decreasing the dependence on human skills. However, when the technology also informates the processes to which it is applied, it increases the explicit information content of tasks and sets into motion a series of dynamics that will ultimately reconfigure the nature of work and the social relationships that organize productive activity.

Because this duality of intelligent technology has not been clearly recognized, the consequences of the technology's informating capacity are often regarded as unintended. Its effects are not planned, and the potential that it lays open remains relatively unexploited. Because the informating process is poorly defined, it often evades the conventional categories of description that are used to gauge the effects of industrial technology.

These dual capacities of information technology are not opposites; they are hierarchically integrated. Informating derives from and builds upon automation. Automation is a necessary but not sufficient condition for informating. It is quite possible to proceed with automation without reference to how it will contribute to the technology's informating potential. When this occurs, informating is experienced as an unintended consequence of automation. This is one point at which choices are laid open. Managers can choose to exploit the emergent informating capacity and explore the organizational innovations required to sustain and develop it. Alternatively, they can choose to ignore or suppress the informating process. In contrast, it is possible to consider informating objectives at the start of an automation process. When this occurs, the choices that are made with respect to how and what to automate are guided by criteria that reflect developmental goals associated with using the technology's unique informating power.

Information technology is frequently hailed as "revolutionary." What are the implications of this term? Revolution means a pervasive, marked, radical change, but revolution also refers to a movement around a fixed course that returns to the starting point. Each sense of the word has relevance for the central problem of this book. The informating capacity of the new computer-based technologies brings about radical change as it alters the intrinsic character of work-the way millions of people experience daily life on the job. It also poses fundamentally new choices for our organizational futures, and the ways in which labor and management respond to these new choices will finally determine whether our era becomes a time for radical change or a return to the familiar patterns and pitfalls of the traditional workplace. An emphasis on the informating capacity of intelligent technology can provide a point of origin for new conceptions of work and power. A more restricted emphasis on its automating capacity can provide the occasion for that second kind of revolution-a return to the familiar grounds of industrial society with divergent interests battling for control, augmented by an array of new material resources with which to attack and defend.

The questions that we face today are finally about leadership. Will there be leaders who are able to recognize the historical moment and the choices it presents? Will they find ways to create the organizational conditions in which new visions, new concepts, and a new language of workplace relations can emerge? Will they be able to create organizational innovations that can exploit the unique capacities of the new technology and thus mobilize their organization's productive potential to meet the heightened rigors of global competition? Will there be leaders who understand the crucial role that human beings from each organizational stratum can play in adding value to the production of goods and services? If not, we will be stranded in a new world with old solutions. We will suffer through the unintended consequences of change, because we have failed to understand this technology and how it differs from what came before. By neglecting the unique informating capacity of advanced computer-based technology and ignoring the need for a new vision of work and organization, we will have forfeited the dramatic business benefits it can provide. Instead, we will find ways to absorb the dysfunctions, putting out brush fires and patching wounds in a slow-burning bewilderment.

THE PLAN OF THIS BOOK

The choices for the future cannot be deduced from economic data or from abstract measures of organizational functioning. They are embedded in the living detail of daily life at work as ordinary people confront the dilemmas raised by the transformational qualities of new information technology. For this reason the research presented here focuses upon the texture of human experience-what people say, feel, and do-in dealing with the technological changes that imbue their immediate environment. I studied eight organizations during the five-year period from 1981 to 1986. Each was well known as a model of technological sophistication within its particular industry. In each, information technology was implemented in ways that fundamentally altered how people were required to accomplish their daily work. In most cases, employees found themselves having to operate through the computer medium in order to perform their tasks, and in almost every instance, this was their first direct experience with information technology.

The most treacherous enemy of such research is what philosophers call "the natural attitude," our capacity to live daily life in a way that takes for granted the objects and activities that surround us. Even when we encounter new objects in our environment, our tendency is to experience them in terms of categories and qualities with which we are already familiar. The natural attitude allows us to assume and predict a great many things about each other's behavior without first establishing premises at the outset of every interaction. The natural attitude can also stand in the way of awareness, for ordinary experience has to be made extraordinary in order to become accessible to reflection. This occurs when we encounter a problem: when our actions do not yield the expected results, we are caught by surprise and so are motivated to reflect upon our initial assumptions. 2 Awareness requires a rupture with the world we take for granted; then old categories of experience are called into question and revised. For example, in the early days of photography, the discrepancies between the camera's eye and the human eye were avidly discussed, but, "once they began to think photographically, people stopped talking about photographic distortion, as it was called."

In the organizations I have studied, the introduction of information technology provided just such a sense of crisis. I found a "window of opportunity" during which people who were working with the technology for the first time were ripe with questions and insights regarding the distinct qualities of their experience. As time passed (usually twelve to eighteen months), they would find ways to accommodate their understanding to the altered conditions of work, making it more difficult to extract fresh insights from beneath a new crust of familiarity. For this reason, I also sought out men and women who had experience accomplishing the same tasks in both the context of an earlier technology- pneumatic controls, paper and pencil, face-to-face interaction, mechanical equipment-and the context of information technology integrated information and control systems, on-line transaction systems, real-time information systems, and computer-conferencing systerns. This provided an experiential frame of reference and heightened their awareness of the continuities and discontinuities in the quality of their work experience.

No sector of the economy is exempt from the technological changes under way or the dilemmas they create. This book seeks to understand the generic themes of this transformation as they cut across a range of organizations engaged in what appear to be wholly distinct kinds of work and to compare and contrast the issues that arise within diverse sectors, such as the offices of a large service organization and an automated manufacturing process. As a result, the following chapters portray a diverse set of organizations-from pulp and paper mills to insurance offices to the elite precincts of an international bank. This attention to similarity and difference also bears upon my observations of the various occupational levels within a given organization-workers, clerks, managers, and professionals were each involved in my research effort. There are generic themes that unify their experiences as well as important sources of difference between them.

The organizations studied include two pulp mills and one pulp and paper mill (located in separate divisions of the American Paper Company); Metro Tel, an operating unit of a telecommunications company; the dental claims operation of Consolidated Underwriters Insurance; the offices for stock and bond transfer of a large corporation known as Universal Technology; the Brazilian offices of Global Bank, a major international financial institution; and a large pharmaceutical company called DrugCorp. (See appendix B for a more complete description of the field methodology.) Each site is not equally represented in the thematic discussions within each chapter because the nature of the technological application in a given organization or the particular kind of work in which people were engaged illuminated certain sets of themes in a particularly important way. This study sought to use the range of sites to build a comprehensive map of the territory in question, rather than to perform a comparative analysis between each organization.

The three mills studied were each in the process of implementing a new control interface based upon microprocessor technology. The level of technological innovation in each case represented the state of the art for process control technology. Two of the mills, Piney Wood and Tiger Creek, had traditional work systems with unionized work forces. They were old mills, and the conversion process represented a radical technological change. The third mill, Cedar Bluff, had been recently constructed and was considered to be one of the most automated pulp mills in the world. Its work force had been newly recruited and thus had no prior experience with other forms of pulping technology. Cedar Bluff's work system had been designed to achieve high levels of employee involvement and commitment; it emphasized worker teams and a pay-for-skills approach to compensation.

Metro Tel had recently implemented a computer-based administrative system that linked managers in a central office to workers in field locations. This was an attempt to create a technologically based administrative infrastructure to support their centralized technical operations, such as switching and repairs. The organizational structure was traditional, hierarchical, and highly centralized. Workers were members of the Communications Workers of America union, and most enjoyed long years of service with the company.

In both Consolidated Underwriter Insurance's dental claims operation and the stock and bond transfer offices of Universal Technology, clerical workers were using information technology for the first time. Each of these offices had installed high-volume transaction systems in which clerks used desktop terminals to receive and enter data.

The Brazilian offices of Global Bank represented 20 percent of the parent corporation's international revenues. Like many other banking institutions, Global Bank Brazil had shifted its strategic thinking from an emphasis on loans to an emphasis on the development of new technology-based products and services. Information, rather than money, was now recognized as the bank's most valuable commodity. As a result of Global Bank Brazil's technological sophistication and financial importance, it was chosen as a site for a pilot program developing a new generation of information technology. This technology, technically referred to as the "data base environment," was regarded as an innovation that would profoundly affect the nature of banking, with consequences for the skills and forms of organization appropriate to each banking function. Though Global Bank Brazil had not fully completed its transition to this new stage of technological deployment, this study documents the organization's efforts to grapple with the likely consequences of the technological change.

Finally, the managerial precincts of DrugCorp provided an opportunity to study one of the world's most extensive computer-conferencing systems. Managers and professionals were linked by a computer network that allowed ongoing dialogue, electronic meetings, and rapid communication. Their experiences with this technology reveal much about the emerging structure of communication within the computerized organization.

This book is structured to reflect the succession of dilemmas that typically accompany an organization's transformation to advanced computer-based technology. 

Part I is directed toward the dilemmas associated with the changing grounds of knowledge as a result of the computer mediation of work. It explores the historical role of the body in both industrial and white-collar work and depicts the emerging demand for intellective skills, that frequently supplant the body as a primary source of know-how. Drawing together data from the mills and the offices, part 1 offers a broad conceptualization of the cognitive and social-psychological requirements for developing and expressing knowledge in the computerized workplace.

Part 2 focuses upon the dilemmas of authority that develop as the new demands for intellective skills blur traditional distinctions between operational and managerial roles. It begins by tracing the historical evolution of managerial authority and proceeds to explore the way in which the managerial hierarchy can subvert the forces of change, using the experiences of the three mills as examples. The mills also illustrate how, despite these attempts to resist change, new roles and relations of authority begin to take shape.

Part 3 concerns the attempts to shore up these threatened authority relations with new techniques of control that draw upon the technology's tendency to heighten the visibility of organizational processes. Managers who doubt the strength of authority-based bonds or who prefer technical certainty to the rigors of managing face-to-face relationships are drawn to the technology as a new source of techniques for shaping the behavior of their subordinates. Their efforts engage a series of organizational responses that, ironically, weaken managerial authority even more profoundly.

The conclusion sets out a portrait of a hypothetical informated workplace. It defines a direction for managerial efforts that would take up the challenge of this historical moment and strike out on a new path. It suggests landmarks as well as pitfalls. It offers a vision.

PART ONE: KNOWLEDGE AND COMPUTER-MEDIATED WORK

CHAPTER ONE: THE LABORING BODY: SUFFERING AND SKILL IN PRODUCTION WORK

We had pleased ourselves with the delectable visions of the spiritualization of labor .... Each stroke of the hoe was to uncover some aromatic root of wisdom .... But ... the clods of earth, which we so constantly belabored and turned over and over, were never etherealized into thought. Our thoughts, on the contrary, were fast becoming cloddish. Our labor symbolized nothing and left us mentally sluggish in the dusk of the evening. - Nathaniel Hawthorne, The Bithedale Romance

THE AUTOMATIC DOORS

The bleach plant is one of the most complex and treacherous areas of a pulp mill. In Piney Wood, a large pulp plant built in the mid-1940s, railroad tank cars filled with chemicals used in the bleaching process pull up alongside the four-story structure in which dirty brown digested pulp is turned gleaming white. Each minute, 4,000 gallons of this brown mash flow through a labyrinth of pipes into a series of cylindrical vats, where they are washed, treated with chlorine-related chemicals, and bleached white. No natural light finds its way into this part of the mill. The fluorescent tubes overhead cast a greenish-yellow pall, and the air is laced with enough chemical flavor that as you breathe it, some involuntary wisdom built deep into the human body registers an assault. The floors are generally wet, particularly in the areas right around the base of one of the large vats that loom like raised craters on a moonscape. Sometimes a washer runs over, spilling soggy cellulose kneedeep across the floor. When this happens, the men put on their high rubber boots and shovel up the mess.

The five stages of the bleaching process include hundreds of operating variables. The bleach operator must monitor and control the flow of stock, chemicals, and water, judge color and viscosity, attend to time, temperature, tank levels, and surge rates-the list goes on. Before computer monitoring and control, an operator in this part of the mill would make continual rounds, checking dials and graph charts located on the equipment, opening and shutting valves, keeping an eye on vat levels, snatching a bit of pulp from a vat to check its color, sniff it, or squeeze it between his fingers ("Is it slick? Is it sticky?") to determine its density or to judge the chemical mix.

In 1981 a central control room was constructed in the bleach plant. A science fiction writer's fantasy, it is a gleaming glass bubble that seems to have erupted like a mushroom in the dark, moist, toxic atmosphere of the plant. The control room reflects a new technological era for continuous-process production, one in which microprocessor-based sensors linked to computers allow remote monitoring and control of the key process variables. In fact, the entire pulp mill was involved in this conversion from the pneumatic control technology of the 1 940s to the microprocessor-based information and control technology of the 1980s.

Inside the control room, the air is filtered and hums with the sound of the air-conditioning unit built into the wall between the control room and a small snack area. Workers sit on orthopedically designed swivel chairs covered with a royal blue fabric, facing video display terminals. The terminals, which display process information for the purposes of monitoring and control, are built into polished oak cabinets. Their screens glow with numbers, letters, and graphics in vivid red, green, and blue. The floor here is covered with slate-gray carpeting; the angled countertops on which the terminals sit are rust brown and edged in black. The walls are covered with a wheat-colored fabric and the molding repeats the polished oak of the cabinetry. The dropped ceiling is of a bronzed metal, and from it is suspended a three dimensional structure into which lights have been recessed and angled to provide the right amount of illumination without creating glare on the screens. The color scheme is repeated on the ceiling-soft tones of beige, rust, brown, and gray in a geometric design.

The terminals each face toward the front of the room-a windowed wall that opens onto the bleach plant. The steel beams, metal tanks, and maze of thick pipes visible through those windows appear to be a world away in a perpetual twilight of steam and fumes, like a city street on a misty night, silent and dimly lit. What is most striking about the juxtaposition of these two worlds, is how a man (and there were only men working in this part of the mill) traverses the boundary between them.

The control room is entered through an automatic sliding-glass door. At the push of a button, the two panels of the door part, and when you step forward, they quickly close behind you. You then find yourself facing two more automatic doors at right angles to one another. The door on the right leads to a narrow snack area with booths, cabinets, a coffee machine, and a refrigerator. The door to the left leads into the control room. It will not open until the first door has shut. This ensures that the filtered air within the control room is protected from the fumes and heat of the bleach plant. The same routine holds in reverse. When a man leaves the control room, he presses a button next to the frame on the inner door, which opens electronically. He then steps through it into the tiny chamber where he must wait for the door to seal behind him so that he can push a second button on the outer door and finally exit into the plant.

This is not what most men do when they move from the control room out into the bleach plant. They step through the inner door, but they do not wait for that door to seal behind them before opening the second door. Instead, they force their fingertips through the rubber seal down the middle of the outer door and, with a mighty heft of their shoulders, pry open the seam and wrench the door apart. Hour after hour, shift after shift, week after week, too many men pit the strength in their arms and shoulders against the electronic mechanism that controls the doors. Three years after the construction of the sleek, glittering glass bubble, the outer door no longer closes tightly. A gap of several inches, running down the center between the two panels of glass, looks like a battle wound. The door is crippled.

The door is broke now because the men pushed it too hard comin' in and out," says one operator. In talking to the men about this occurrence, so mundane as almost to defy reflection, I hear not only a simple impatience and frustration but also something deeper: a forward momentum of their bodies, whose physical power seems trivialized by the new circumstances of their work; a boyish energy that wants to break free; a subtle rebellion against the preprogrammed design that orders their environment and always knows best. Yet these are the men who also complained, "The fumes in the bleach plant will kill you. You can't take that chlorine no matter how big and bad you are. It will bleach your brains and no one (in management) gives a damn."

Technology represents intelligence systematically applied to the problem of the body. It functions to amplify and surpass the organic limits of the body; it compensates for the body's fragility and vulnerability. Industrial technology has substituted for the human body in many of the processes associated with production and so has redefined the limits of production formerly imposed by the body. As a result, society's capacity to produce things has been extended in a way that is unprecedented in human history. This achievement has not been without its costs, however. In diminishing the role of the worker's body in the labor process, industrial technology has also tended to diminish the importance of the worker. In creating jobs that require less human effort, industrial technology has also been used to create jobs that require less human talent. In creating jobs that demand less of the body, industrial production has also tended to create jobs that give less to the body, in terms of opportunities to accrue knowledge in the production process. These two-sided consequences have been fundamental for the growth and development of the industrial bureaucracy, which has depended upon the rationalization and centralization of knowledge as the basis of control.

These consequences also help explain the worker's historical ambivalence toward automation. It is an ambivalence that draws upon the loathing as well as the commitment that human beings can experience toward their work. Throughout most of human history, work has inescapably meant the exertion and often the depletion of the worker's body. Yet only in the context of such exertion was it possible to learn a trade and to master skills. Since the industrial revolution, the accelerated progress of automation has generally meant a reduction in the amount of effort required of the human body in the labor process. It has also tended to reduce the quality of skills that a worker must bring to the activity of making something. Industrial technology has been developed in a manner that increases its capacity to spare the human body, while at the same time it has usurped opportunities for the development and performance of skills that only the body can learn and remember. In their treatment of the automatic doors, the bleach plant workers have created a living metaphor that reflects this ambivalence toward automation. They want to be protected from toxic fumes, but they simultaneously feel a stubborn rebellion against a structure that no longer requires either the strength or the know-how lodged in their bodies.

The progress of automation has been associated with both a general decline in the degree of know-how required of the worker and a decline in the degree of physical punishment to which he or she must be subjected. Information technology, however, does have the potential to redirect the historical trajectory of automation. The intrinsic power of its informating capacity can change the basis upon which knowledge is developed and applied in the industrial production process by lifting knowledge entirely out of the body's domain. The new technology signals the transposition of work activities to the abstract domain of information. Toil no longer implies physical depletion. "Work" becomes the manipulation of symbols, and when this occurs, the nature of skill is redefined. The application of technology that preserves the body may no longer imply the destruction of knowledge; instead, it may imply the reconstruction of knowledge of a different sort.

The significance of this transposition is impossible to grasp without reference to the grounds of knowledge for workers in the past. In the factory, knowledge was intimately bound up with the efforts of the laboring body. The development of industrial technology can be read as a chronicle of attempts to grapple with the body's role in production as a source of both effort and skill and with the specific responses these attempts have evoked from workers and managers. The centrality of the body's historical meaning for production has informed the selfunderstanding of managers and workers and the relationship between them. It has also been a salient force guiding the development and application of manufacturing technology. A better understanding of what the body has meant for industrial work and how it has been linked to the logic of automation will sharpen an appreciation of the character of the current transformation and its capacity to provoke comprehensive change in the relationships that structure the workplace. Before deciphering the present or imagining the future, it is first necessary to take ourselves out of the twentieth century and return, if only briefly, to a time when the nature of work was both simpler and more miserable, a time when work was above all the problem of the laboring body.

THE FRONTIER OF CONTEMPT

The world of production, where primary materials are processed and goods are manufactured, has long been marked by a great divide between those who give of their bodies and those who are exempt from physical depletion. Yet those exempted from bodily alteration may give of themselves in other ways. Their physical presence may be required for purposes of interpersonal influence, communication, and coordination. They may give of their time and attention in both supervisory and analytical activities, or they may give of themselves more abstractly as sources of investment capital or expert knowledge. However, the groups that stand on either side of this divide constitute fundamentally distinct modes of involvement with the production enterprise. The experiential distance between them is one important living source of the divergent interests in terms of which workers and managers have tended to define themselves. Workers facing the physical requirements of labor seek ways to preserve their bodies from exertion, while managers are charged with extracting the maximum feasible effort from the work force.

This divide has been an important characteristic of social hierarchies in virtually every culture known to the historical record. Wealth and power have everywhere meant an escape from toil: the unequal distribution and concentration of wealth and power within a small group is a phenomenon of such universality that the French historian Fernand Braudel has called it a "constant law of societies, a structural law that admits of no exception." 1 In the societies of preindustrial Europe, the decisive challenge of social mobility was to permanently rid oneself of the stigma of physical labor and then to repudiate the commercial activities that made such an escape from work possible. European nobility defined itself by the gulf it created between its members and the hardship of labor. Of the bourgeois families who gained access to the highest ranks of society Braudel writes that "the only feature they had in common with the authentic nobility was their rejection of trade or labor, their taste for idleness or rather leisure, which was for them synonymous with reading and learned discussion with their peers. "

This repugnance toward labor rides a long wave in Western history, a wave that has not, even yet, reached its crest. In the religious zeal of the early Middle Ages, trades that trafficked in money were considered illicit, materialism being an indication of a lack of faith. With the growing urbanization, more detailed division of labor, and accelerated mercantilism of the late Middle Ages, however, this view of economic activity took an important turn: "A new frontier of contempt arose right in the midst of the new classes and even within professions .... Work itself no longer constituted the distinction between respectable and contemptible categories; instead, it was manual labor that had come to be the key factor in the frontier between respect and contempt .... Across from the manouvriers and brassiers who worked with hands and arms, was the patrician world, the new aristocracy, consisting of all those who did no manual labor: employers and rentiers. " 3 Even the guilds were influenced by the contempt toward manual work. Some of them required entrants to have relinquished their trade for twelve months before admission. In 1241 a municipality in Flanders excluded from the urban magistracy all robbers, coiners, and "those who have not given up all manual work for at least one year. " 

The Middle Ages produced a conception of labor infused with a loathing drawn from three traditions: ( 1) the Greco-Roman legacy that associated labor with slavery, (2) the barbarian heritage that disdained those who worked the land and extolled the warrior who gained his livelihood in bloody booty, and (3) Judeo-Christian theology that admired contemplation over action. For centuries European literature and iconography depicted peasants as huge-headed monsters or wild beasts lurking the depths of Europe's dark forests. 5 Labor came to humanity with the fall from grace and was at best a penitential sacrifice enabling purity through humiliation. Labor was toil, distress, trouble, fatigue an exertion both painful and compulsory. Labor was our animal condition, struggling to survive in dirt and darkness.

Freedom from the necessity of labor has been a prominent feature of most utopian thinking. English civil war sects awaited the coming of the Fifth Monarchy because it was said that it would abolish painful labor. Bishop Godwin's Man in the Moone, published in 1638, reported on a society in which "food groweth everywhaer without labour," while all the necessities of life were amply provided. Other utopian writers of the period, Campanella, Winstanley, Bellars, and More, saw the reduction of labor as an important feature of a wholesome moral life. 6 Sir Thomas More's Utopia limited the consumption of commodities to the "necessary" and the "comfortable," in order that any surplus labor could be devoted to learning. The six-hour work day was seen as adequate, and if it turned out to be excessive, the community would further curtail the number of hours assigned to work: "What time may possibly be spared from the necessary occupations and affairs of the Commonwealth, all that the Citizens should withdraw from the bodily service to the free liberty of the mind and garnishing of the same." 

The recent English translation of the historian Norbert Elias's The Civilizin9 Process has helped shed light on a deeper explanation for this repugnance toward work. 8 While a full description of Elias's pathbreaking analysis is beyond the scope of this discussion, it is worth highlighting the skeleton of his discoveries as they contribute to an understanding of the enduring relationship between the universality of social hierarchy and the fact that physical labor is everywhere considered to constitute its lowest echelons.

Elias studied books of etiquette and other documentation of daily life from the early Middle Ages through the eighteenth century. He discovered that norms of daily conduct, particularly those that bear upon bodily functions, have changed radically throughout the course of the centuries. When in I 5 30 Erasmus wrote his treatise On Civility in Children, he provided a portrait, refracted through his admonitions, of contemporary standards of behavior. Our own sensibilities are overcome with repugnance and horror at behavior that was accepted as routine in the sixteenth century. Elias points out the "infinite care and matter-of-factness" with which Erasmus addressed habits concerning bodily functions in his effort to encourage more "civilized" standards: "There should be no snot on the nostrils .... A peasant wipes his nose on his cap and coat, a sausage maker on his arm and elbow. It does not show much more propriety to use one's hand and then wipe it on one's clothing .... It is more decent to take up the snot in a cloth, preferably while turning away. If when blowing the nose with two fingers something falls to the ground, it must be immediately trodden away with the foot. The same applies to spittle. "

Elias found that behaviors related to table manners, bodily functions, nose blowing, spitting, sleeping, sex, and aggression that we have come to consider barbaric and disgusting were once routine. He argues that the process of curbing these behaviors was set into motion by, and in tum helped to promote, stable, centralized forms of social organization. The embryo for these modem forms was evident in the court societies that by the end of the Middle Ages, had begun to spread across Europe.

Here were created the models of more pacified social intercourse which more or less all classes needed, following the transformation of European society at the end of the Middle Ages; here the coarse habits, the wilder, more uninhibited customs of medieval society with its warrior upper class, the corollaries of an uncertain, constantly threatened life, were "softened", "polished" and "civilized". The pressure of court life, the vying for the favour of the prince or the "great"; then, more generally, the necessity to distinguish oneself from others and to fight for opportunities with relatively peaceful means, through intrigue and diplomacy, enforced a constraint on the affects, a self-discipline and self-control, a peculiarly courtly rationality .... This increased restraint and regulation of elementary urges is bound up with increased social constraint, the growing dependence of the nobility on the central lord, the king or prince.

Elias reminds us that violence was inscribed into the very structure of medieval society. Rape and death, the hunting of men and animals, were part of everyday life. "The documents suggest unimaginable emotional outbursts in which-with rare exceptions-everyone who is able abandons himself to extreme pleasures of ferocity, murder, torture, destruction, and sadism." 11 Everyday objects like the fork (at first a source of mockery and still a rare luxury in the seventeenth century), the handkerchief, and the nightgown, or the new sense of repulsion felt at the sight of humans defecating, spitting a piece of food back into the common bowl, or picking their teeth with the communal knife, all symbolized the progress of the civilizing process-an increased control over and distance from the animal life of the human body: "People have begun to construct an affective wall between their bodies and those of others. The fork has been one of the means of drawing distances between other people's bodies and one's own. One repulses the body, isolates it, feels ashamed of it, tries to ignore it .... For many centuries, this wall did not exist."

Elias's discoveries illuminate an unconscious dimension of Western history. The consolidation of stable social hierarchies based upon centralized power and the rule of law demanded a new level of behavioral control. The body had to be reinterpreted as a source of disgust and as an object of discipline. 13 This reinterpretation, and the forms of conduct that developed from it, first took root at the highest levels of the emerging society, where the pressures of interdependence and political opportunism were most acute. These in turn became the models for behavior that successively lower social strata would imitate and finally assimilate.

It is easy to see that gradations in status are related to gradations in power, but Elias's work alerts us to the fact that such differences in status parallel another axis of social comparison whose levels are marked by degrees of distance from the body's own animal life and the animal life of surrounding bodies. The ability to maintain one's distance from the body developed as an important sign of hierarchical position. It has served to intensify the repugnance toward forms of activity that involve the body in sweating, heaving, grunting, hauling, and carting; that expose the body to pain and discomfort from extreme temperature, extended muscular effort, inclement weather, or hurtful substances; and that so immerse human consciousness in the sentient surroundings of effort and fatigue that one fails to notice (or care) whether the nose is dripping, the sweated body is giving off an unbearable stench, or filthy fingers have been used to grab a piece of food-the very proprieties that came to be seen as part of a complex of behaviors distinguishing the barbaric from the civilized.

There is reason enough to want to avoid exhausting work, but the constancy of repugnance was not confined to forms of labor that were extremely punishing. As noted earlier, in the membership practices of some guilds, even the craftsworker was liable to be an object of contempt because of the manual nature of that work. Such repugnance is in itself an act of distancing. It is both a rejection of the animal body and an affirmation of one's ability to translate the impulses of that body into the infinitely more subtle behavioral codes that mediate power in complex organizations. Once this translation occurs, the body is no longer the vehicle for involuntary affective or physical displays. Instead, it becomes the instrument of carefully crafted gestures and behaviors designed to achieve a calculated effect in an environment where interpersonal influence and even a kind of rudimentary psychological insight are critical to success. In the interpersonal world of court society, the body's knowledge involved the ability to be attuned to the psychological needs and demands of others, particularly of superiors, and to produce subtly detailed nonverbal behavior that reflected this awareness.

The court is a kind of stock exchange; as in every "good society", an estimate of the value of each individual is continuously being formed. But here his value has its real foundation ... in the favour he enjoys with the king, the influence he has with other mighty ones .... All this, favour, influence, importance, this whole complex and dangerous game in which physical force and direct affective outbursts are prohibited and a threat to existence, demands of each participant a constant foresight and an exact knowledge of every other, of his position and value in the network of courtly opinions; it exacts precise attunement of his own behavior to this value .... "A man who knows the court is master of his gestures, of his eyes and his expression; he is deep, impenetrable. He dissimulates the bad turns he does, smiles at his enemies, suppresses his ill-temper, disguises his passions, disavows his heart, acts against his feelings."

The tension between hierarchical status and the animal body also may be sustained by the psychologic need to defend oneself from the fact of the body's tragic weakness. The animality of the body is a source of repulsive events that must be controlled, and the most repulsive and least controllable of these events is death itself. Death is the inevitable conclusion ordained by the animality of the body and poses a series of challenges (in addition to the central challenge, which is the cessation of life). Death underscores the commonality of all who share life; death highlights the vulnerability of the body; death reminds the living of the ultimate uncontrollability of the body. Each of these problems is in a way addressed by the act of distantiation. Hierarchical distance rejects the display of animality that is common and deindividualized for civilized conduct, intricately fashioned by personality, wit, and will. Distantiation, in promoting forms of control over the body, can protect its fragility and, in some ways, its health. Finally, distantiation allows us to avoid reminders of animality, thus making it possible to suppress an awareness of the body's inevitable decline.

The close relationship between the rejection of animality and the progress of civilization is at the heart of our modern conception of work. Marx expressed this relationship when he argued that mastery of the material world was the basis upon which man humanized himself and developed culture. This in fact is the civilizing process; humanization means tempering animality with rationality, aesthetic grace, and moral choice. It is a process that has informed much of the impetus toward the extension of material culture powerfully exemplified in the development of industrial technology, which simultaneously frees the production process from the organic limits of the body, frees consumers from having to exercise bodily effort in order to enjoy the panoply of goods produced by the machine system, shapes workers who are capable of exercising considerable control over their own spontaneous impulses (and so can conform to the behavioral demands of mechanized production), and gradually diminishes the most painful forms of exertion associated with the work of making things.

Indeed, the worker's body posed a complicated set of problems for industrial management. Industrial work depended upon the laboring body as much for its raw energy as for its special gifts. In many industries, the worker's body remained central to production well into the early decades of the twentieth century. Only then, in many cases, was labor-saving technology diffused widely enough to substantially alter the role of the body in the production process. If work was to be performed economically and effectively, then the impulsive behavior associated with the body's animality would have to be disciplined. The members of court society were required to turn their bodies into instruments of interpersonal influence, instruments for acting-with. Industrial workers were also required to turn their bodies into instruments, but instruments for acting-on-for producing calculated effects on material and equipment. In the following section, we will see how the first generations of factory owners and their managers were frustrated, confounded, and sometimes ruined as they searched for the methods by which to translate the animal body into a more precise instrument that could be applied to increasingly systematized processes of production.

THE EARLY FACTORY AND THE PROBLEM OF THE BODY

There is ample evidence that throughout the fifteenth and sixteenth centuries, workers were not silent in their degradation. Social cleavage took its toll in thousands of peasant insurrections as well as violent disturbances among urban workers. 15 It was not until the industrial revolution, however, that the focus of the conflict between worker and employer came to rest on the detailed performance of the worker's body and the degree of discipline to which the body might legitimately be subjected.

Consider the case of Britain at the brink of industrialization during the second half of the eighteenth century. For all the bone-crushing labor demanded of the agricultural worker or the cottage weaver, the traditional rhythms of exertion and play were a world removed from the behavioral demands of industrial production. Work patterns were irregular, alternating between intense effort and idleness. Most work activities emanated from the home, and the distractions of the family, the taverns, and the social web of the community limited any undivided commitment to work.

Cottage workers, upon whom most textile production depended, were relatively impervious to the middleman's demand for heightened productivity. Their inclination to physical exertion was guided more by their own immediate needs than by acquisitive ambitions. Throughout the eighteenth century, the British Parliament passed legislation requiring ever-shorter turnaround times for finished goods from domestic workers and imposing increasingly severe sanctions on those who did not comply. Such sanctions were difficult to enforce; finally, only the pressure of immediate supervision was able to induce a greater level and consistency of effort from workers. 16 The need to intensify production was the driving force behind the establishment of the early factories and workshops, even before the widespread diffusion of the steam engine. 17 There is evidence that workers submitted to the physical rigors of factory discipline only when other alternatives had been exhausted. 18 But even those employers who were able to recruit a labor force still faced the haphazard and spasmodic rhythms with which their new employees approached their work.

The employers deplored the fact that the old subsistence mentality had carried over into the new work settings. Of the piece-rate worker they complained, "At the precise inch of cloth he stopped, in the mines, at the necessary pound of coal." 19 Workers in their turn bemoaned the loss of freedom and rebelled at the prospect of long confinement and steady production. The highlander, it was said, "never sits at ease at a loom; it is like putting a deer in the plough." 20 Attendance was irregular; workers would sometimes stay away from the job for days and send for their wages at the end of the week. In South Wales during the 1840s it was estimated that workers were absent 20 percent of the year, a figure that reached as high as 33 percent during the fortnight after their monthly payday.

One study of Birmingham, England, from 1766 to 1876, found that well into the nineteenth century, workers continued to celebrate Saint Monday-a weekly day of leisure spent in the alehouse enjoying drink, bar games, entertainments, "pugilism," and animal fights. 22 The tradition of Saint Monday followed from the bouts of weekend drinking and represented deeply held attitudes toward a potential surplus of wages: "The men ... (are] regulated by the expense of their families, and their necessities; it is very well known that they will not go further than necessity prompts them."

The industrial entrepreneurs tried, usually without success, to prohibit the observance of Monday as a holiday. Boulton and Watt's first enterprise foundered on the continual drunkenness of their work force. 24 The owner of a button-making factory decided that although he would not be able to control his workers, he would make an effort to train his apprentices in more industrious work habits. His diary records his frustration: "This evening Edward Lingard's misconduct in going to the Public House in the afternoon for drink, contrary to my inclination and notwithstanding I had forbidden him from it only yesterday- this I say, and meeting him on his way back, induced me hastily to strike him. With which my middle finger was so stunned as to give me much pain." 25 In addition to the time lost through observing Monday as a holiday, harvest time and other traditional feast days kept workers away. In 1776 the famous Josiah Wedgwood who pioneered new techniques of pottery production and business management, wrote to a colleague: "Our men have been at play 7 days this week, it being Burslem Wakes. I have rough'd and smoothed them over, & promised them a long Xmas, but I know it is all in vain, for Wakes must be observed though the World was to end with them."

Nineteenth-century American industrialists faced a similar set of problems when it came to honing the worker's body as an instrument of production. The owner of a Pennsylvania ironworks complained of frequent "frolicking" that sometimes lasted for days, along with hunting, harvesting, wedding parties, and holiday celebrations. One manufacturer filled his diary with these notes: "All hands drunk; Jacob Vending hunting; molders all agree to quit work and went to the beach. Peter Cox very drunk and gone to bed .... Edward Rutter off a-drinking. It was reported he got drunk on cheese." 27 In 1817 a Medford shipbuilder refused his men grog privileges and they all quit. 28 The ship's carpenter in one New York shipyard describes the typical workday: cakes and pastries in the early morning and again in the late morning, a trip to the grog shop by eleven for whiskey, a big lunch at half past three, a visit from the candyman at five, and supper, ending the workday, at sundown. He recalled one worker who left for grog ten times a day. A cigar manufacturer complained that his men worked no more than two or three hours a day; the rest of the time was spent in the beer saloon, playing pinochle. Coopers were famous for a four-day work week; and the potters in Trenton, New Jersey, immigrants from Staffordshire, were known to work in "great bursts of activity" and then lay off for several days.

We can see that the early apostles of industrialism had to confront the still-rudimentary progress of the civilizing process as it bore upon work behavior. The spontaneous, instinctually gratifying behavior of the new industrial worker had to be suppressed, and that energy channeled into the controlled behavior demanded by the intensification of production. The factory became a pedagogic institution where the new standards of conduct and sensibility, generally referred to as "labor discipline," would be learned. The exhaustive measures that employers took to thwart the animal body are a sign of its very intractability.

The notion of labor discipline signaled a very concrete problem: how to get the human body to remain in one place, pay attention, and perform consistently over a fixed period of time. Elaborate systems of fines were developed, minutely tailored to extinguish particular expressions of the impulsive body. For example, many fines sought to keep the body stationary. One work rule at Haslingden Mill about 1830 read, "Any person found from the usual place of work, except for necessary purposes, or talking with anyone out of their own alley, will be fined." 31 The Hammonds report fines at a textile mill near Manchester for "going further than the roving room door when fetching rovings" and for "any spinner found in another's wheel gate." 32 Ashworth fined his weavers for being found "out of the room. " 33 Other fines addressed the sounds that emitted from the body at work: punishable infractions included singing, whistling, swearing, and yelling. Some fines, intended to enforce a fixity of gaze and attention, punished workers for opening a window. Still other fines concerned the body's smell and appearance: workers were fined for being dirty, for not changing their shirts at least twice a week, and for spitting. Finally, there were fines to discourage aggressiveness, sexuality, and disorderliness-throwing water, seducing females, being drunk, arriving late, or not showing up at all.

American employers also found that only the severest fining policies had an effect on work habits. Workers were routinely fined up to half a day's pay for singing, talking, visiting, or being late. 35 In 18 59 a mill agent in Chicopee, Massachusetts, complained of the general indisposition of factory hands toward steady work, and in the years preceding World War I, it was still not unusual for as much as one-tenth of the work force to be missing on a given day. 36 Quit rates around the turn of the century were high-textile mills, meat-packing plants, automobile plants, steel mills, and machine works often showed annual turnover rates of 100 percent. 37 One survey showed that between 1905 and 1917, the majority of industrial workers changed jobs at least once every three years. Between 1 907 and 1910, turnover in the woolen industry was between 113 percent and 163 percent. It reached 232 percent in New York City garment shops in 1912, 2 52 percent in a sample of Detroit factories in 1916, and 370 percent in the Ford Motor Company in 1913.

These quit rates reflect not only the ambivalence and enduring orneriness of American workers but also the increasingly severe pressure that employers and managers brought to bear on dysfunctional, uncontrolled, and irregular behavior. Many scholars have argued that the introduction of steam power (and, later, other forms of expensive equipment) did more to consolidate the new behavioral norms than the earlier systems of fines alone. 39 This was in part because employers, in an effort to fully utilize their capital investment, became more ruthless in their willingness to dismiss workers who did not comply with the regularity of effort required to efficiently exploit the new machinery and in part because workers had to conform to a pace and quality of production increasingly driven by the machine, rather than by their own traditions and habits of organization.

As traditional working conditions grew scarce, worker resistance to labor discipline itself became more rationalized. The trade union movement set itself to limiting working hours, maintaining employment levels, and protecting wages, but a parallel approach of greater informality and striking continuity with older traditions also emerged. Workers began to act self-consciously to limit their efforts and so preserve their bodies. As early as 17 57, Josiah Tucker wrote in a pamphlet entitled "Instructions for Travelers" a description of domestic weavers transplanted to factory life. He provides one of the earlier descriptions of what was to become a central strategy for the industrial worker-withholding effort: "They think it no crime to get as much wages and to do as little for it as they possibly can, to lie and cheat and do any other bad thing, provided it is only against their master whom they look upon as their common enemy, with whom no faith is to be kept .... Their only happiness is to get drunk and make life pass away with as little thought as possible. "

Though skilled workers were indignant at the idea of deliberate slacking, by the end of the nineteenth century the British unions had come to recognize the characteristics of the trade cycle and to think of their effort as a commodity to be withheld or controlled in the service of free market bargaining. 41 Work banking, goldbricking, soldiering, are the modern terms that convey the legacy of this earlier clash between two wildly different conceptions of the standard of work discipline to which a body should conform.

In the American factory at the turn of the century, the foreman had primary responsibility for implementing management's goals: he was the "undisputed ruler of his department, gang, crew, or ship." 42 When in 1912 a congressional committee investigated the United States Steel Corporation, they attempted to understand just how the foreman functioned. They learned that foremen throughout American industry practiced something known as the "driving method," an approach to supervision that combined authoritarian combativeness with physical intimidation in order to extract the maximum effort from the worker. The driving method was well suited to work that depended upon the consistent exertion of the human body. The foreman's profanity, threats, and punishments were complemented by the workers' methods for limiting output. Methods of withholding labor varied somewhat from industry to industry and might be modified according to economic conditions, but the underlying spirit was everywhere the same-to protect the body by tempering exertion.

There were numerous stories of new employees who were approached by older, presumably wiser, workmen. "See here, young fellow, you're working too fast. You'll spoil our job for us if you don't go slower." If a friendly admonition did not have the desired effect and the man was judged a "rooter" or "rusher," social pressure, threats, and even violence might follow.

The protective response of so many workers to the demands made on their bodies is also the source of many work procedures that have been formalized and institutionalized in labor contracts and work rules. Studies by Lloyd Ulman and Sumner Slichter have shown how glassworkers', textile workers', and metalworkers' unions used agreements on production standards to restrict the driving method of supervision. 44 Work rules codified restrictive labor withholding practices, transforming them from informal methods of self-protection to deliberate contractual agreements.

THE PARADOX OF THE BODY

Until now we have treated the body in one of its aspects-as the scene and source of effort. This was the body that had to be disciplined if effort was to be drawn forth in a way that complemented the demands of an intensified, collectivized, and regulated production process. But the body as the scene of effort, the body to be protected, held a special paradox. For it was also through the body's exertions that learning occurred, and for those who were to become skilled workers, long years of physically demanding experience were an unavoidable requirement. This was the reward of physical involvement, since there was virtually no access to the craftsperson's skills short of an investment in years of effort. The body's meaning for production was not just to be an animal source of motive power or a pair of hands for an endless series of monotonous performances. Where the skilled worker was concerned, the body's sentience was also highly structured by a felt knowledge of materials and procedures. These twin functions of the body as a source of skill and as a source of effort-complicated the way in which employers confronted the problems of labor discipline and the speed with which they invested in labor-saving technology, just as they complicated workers' responses to that technology.

The historian Raphael Samuel has demonstrated that the body continued to play a central role in production, as a source of both effort and skill, throughout the nineteenth century in activities as diverse as mining, and quarrying, agriculture, gardening, and other forms of food production, construction, glass, pottery, and leather trades, woodworking, and metallurgy. 46 His examples are drawn from nineteenth-century Britain, but many of the labor processes he describes were prevalent in America as well.

There were few segments of the mid-Victorian British economy that steam power and machinery had not touched, but fewer still did not depend upon hand technology. Many factors impeded the progress of labor-saving machinery. Many trade unions were successful in resisting the encroachment of machinery on their crafts. From the employers' side, however, there were other compelling reasons. Wages were low, and the possibilities of increasing productivity with hand technology had not yet been exhausted. Employers often found other sources of efficiency in an increasingly minute division of labor and in the use of cheaper labor-saving materials. Market uncertainties made capital investment risky; besides, there was still a plentiful supply of cheap labor. Skill was also an important part of the problem. Ebullient descriptions of the new forms of "self-acting" machinery dominated trade journals of the day, but the new, more automatic machines were often fraught with technical difficulties and could not be relied upon as a substitute for skilled work. Moreover, consumers continued to value the quality of work that only a craftsworker could produce.

Samuel describes the wide variety of jobs that required nothing more than the effort of the laboring body. Coal was excavated by pick and shovel-"tools of the most primitive description, requiring the utmost amount of bodily exertion." 47 Clay-getting required working with a heavy pick. Masses of slime had to be stirred and trampled into the right consistency. Bakeries produced bread almost entirely by manual labor, the hardest operation being that of preparing the dough, "usually carried on in one dark corner of a cellar, by a man, stripped naked down to the waist, and painfully engaged in extricating his fingers from a gluey mass into which he furiously plunges alternately his clenched fists. "

There were many occupations in which the pain and physical alteration that resulted from such exertion were inseparable from the subtle and complex skills from which a craftsperson derived pride and economic power. One account of candy making in an Edinburgh factory ends this way: "The whole process ... requires great skill in the manipulation and it also requires the most severe and continuous muscular labor. We know, indeed, of no other kind of labour that requires more. Not a muscle or joint of the whole body remains inactive. It has quite a marvelous effect in taking down superfluous fat. It is well know that a stout man taken perhaps from lozenge making, and put to work on the hot pan, becomes in six weeks converted into a living skeleton. "

In glassmaking, everything was done by hand, "the gatherers taking the 'metal' from the furnace at the end of an iron rod, the blower shaping the body of the bottle with his breath, while the maker who finished the bottle off ... tooled the neck with a light spring-handled pair of tongs. Each bottle was individually made no matter what household, shop or tavern it was destined for. " 50 There were steady inroads made by machinery in this industry, but despite its labor-saving potential, glassmakers and their union organizations bitterly resisted and successfully impeded its progress. In 1878 one observer summarized the glassmakers' ambivalence: "If in many industries the substitution of mechanical for manual labor offers important advantages because ... it decreases a man's fatigue, we do not think it will have the same effect on the absolutely special work of the glass industry, and we fear that in depriving glassworkers of difficult tasks we will destroy their skill as well as the artistic talents of which glassmakers have the right to be proud." 

Pottery work was very physical and involved personal handling of the clay at every stage of production. Hands, thumbs, fingers, and palms were the sources of a delicately nuanced skill that guided the throwing and shaping of each item: "Dippers prepared the ware for firing by steeping it in a glaze tub. The right hand and arm are plunged nearly up to the elbow as he passes the piece of ware through the liquid .... Then with a rapid circular movement of the hand ... a movement that only years of practice can teach ... he makes the glaze flow thin and even over the surface. "

In the leather trades, the process of currying, which softened the leather for saddlery, coach linings, and shoes, was a highly skilled trade that required a laborer's strength to perform the heavy work of slashing, shaving, and pummeling the hide. lronmaking was another industry that depended upon combinations of skill and strength. New technology utilizing steam power to pound and roll metal together, and new puddling furnaces based on chemical reactions rather than mechanical action, had made the production process cheaper and had increased output. These innovations were not entirely labor-saving, however. In many cases they required new skills, even as they increased the physical demands of work. "The puddler, who had the key role in the new process, was given a task that was simultaneously highly skilled and exhausting, turning a viscous mass of liquid into metal. He worked in conditions of tremendous heat, violently agitating the metal as it boiled, rabbling it from side to side in the furnace, and then gathering it at the end of a rod while the molten liquid thickened .... The men had to relieve each other every few minutes, so great was the exertion and so intense the heat, but even so it was said that every ounce of excess fat was drained from them .... Few puddlers, it was said, lived beyond the age of 50."

These are just a few examples of the body's central presence as the source of both effort and skill in myriad industrial operations throughout the nineteenth century. In many cases, machinery was used to replace humans in supplying the motive power for various subprocesses of production. In most trades, though, labor-saving machinery developed slowly, and many factors inhibited its progress. Sometimes the new machinery, in amplifying the capacity of the human body to perform a given operation and thus increasing output, could also intensify the human participation that was required and thus exacerbate the problems of physical depletion.

Samuel notes that labor-saving machinery spread more rapidly in America, owing to a scarcity of labor and consequent higher wages. However, the historian of technology David Hounshell has recently shown that in some of the largest and most successful American firms, handwork and skilled machine work prevailed during most of the nineteenth century. For example, the Singer Sewing Machine Company was not able to produce perfectly interchangeable parts. As a result, they relied on skilled fitters to assemble each product. The McCormick Reaper Works employed crude manufacturing techniques. Production depended upon skilled machinists, blacksmiths, carpenters, and molders. 54 In 1 911 an observer of the American steel industry tried to convey the notion that skilled work did not imply diminished physical exertion: "New skills, like the puddler's, the catchers, or the machinists included very heavy manual work. As one worker speaking about working in a steel mill declared: 'Hard! I guess it's hard. I lost forty pounds the first three months I came into the business. It sweats the life out of a man!' The differences between the work performed by the skilled workers and the laborers was not of an "intellectual" versus manual activity. The difference lay in the content of a similarly heavy manual work: a content of rationality of participation for skilled workers versus one of total indifference for laborers.

The work of the skilled craftsperson may not have been "intellectual," but it was knowledgeable. These nineteenth-century workers participated in a form of knowledge that had always defined the activity of making things. It was knowledge that accrues to the sentient body in the course of its activity; knowledge inscribed in the laboring bodyin hands, fingertips, wrists, feet, nose, eyes, ears, skin, muscles, shoulders, arms, and legs-as surely as it was inscribed in the brain. It was knowledge filled with intimate detail of materials and ambience-the color and consistency of metal as it was thrust into a blazing fire, the smooth finish of the clay as it gave up its moisture, the supple feel of the leather as it was beaten and stretched, the strength and delicacy of glass as it was filled with human breath. These details were known, though in the practical action of production work, they were rarely made explicit. Few of those who had such knowledge would have been able to explain, rationalize, or articulate it. Such skills were learned through observation, imitation, and action more than they were taught, reflected upon, or verbalized. For example, James J. Davis, later to become Warren Harding's Secretary of Labor, learned the skill of puddling iron by working as his father's helper in a Pennsylvania foundry: "None of us ever went to school and learned the chemistry of it from books .... We learned the trick by doing it, standing with our faces in the scorching heat while our hands puddled the metal in its glaring bath. "

Though this form of knowledge evades explication, it is not fragile; on the contrary, it is extremely robust. The swimmer who has been away from the water for a year, the woman who has not ridden a bicycle since childhood, the grandmother who has not held an infant since she weaned her last, the carpenter who has retired his tools, the guitarist who abandoned her instrument, each of these can confront the object of their skills-the sea, the bicycle, the crying infant, the wood, the guitar-and find that the knowledge they once possessed remains, ready to be activated. Such knowledge is hard won and not easily lost. Within moments, the arms, and legs find their angle and rhythm as they cut through the water, the bicycle moves swiftly, the child is calmed, the chisel, plane, hammer, and saw find the pathways and resonance of the wood, and the fingers rediscover their agility against the strings of the guitar.

Effort may have signaled sacrifice and self-protection, but it was also the occasion and context for the development of this intimate, robust, detailed, and implicit knowledge. Such knowledge formed the basis of the worker's power. Historian David Montgomery has called this the "functional autonomy" of the craftsperson, derived through decades of sustained physical involvement during which the knowledge of each craft was systematized, not in explicit rules, but in the course of practical action. 57 It is appropriate that such knowledge be referred to as "know-how," for it was knowledge that derived from action and displayed itself in action, knowledge that meant knowing how to do, to make, to act-on.

The craftsworker's know-how was also an important source of social integration. The foreman was typically a worker who had turned his wide experience into superior competence. He achieved his position by virtue of his technical skill, and such opportunities were a real source of mobility for an ambitious worker. 58 The fact that workers were required to "use up" their bodies kept them distinct from those who employed them, but the skills mastered in physical activity provided an opportunity for independence, mobility, and identification with superiors.

Such experience-based knowledge had its weaknesses, too. When it came to improving work methods or adapting them to new techniques and business conditions, the practical know-how of the traditional craftsworker could be limiting. 59 With the growing complexity and size of factories, expanding markets that exerted a strong demand for an increase in the volume of production, and a rising engineering profession, there emerged a new and pressing concern to systematize the administration, control, coordination, and planning of factory work.

The man who emerged as the chief symbol of the rational approach to management was Frederick Taylor. Though much has been written on Taylor and the philosophy and methods of scientific management, it is worth highlighting a few central themes for three reasons. 60 First, Taylorism explicitly treats the worker's body in its two dimensions as a source of effort and as a source of skill. Second, workers' responses to Taylorism reveal the ambivalence that this dual role of the body can create, as it did among the bleach plant operators or the nineteenth century glassmakers. Third, the logic that motivated the early purveyors and adapters of scientific management has continued to dominate the course of automation in the twentieth-century workplace. As will be argued later, it is a logic that must undergo a fundamental reevaluation as information technology is widely adapted to productive activity.

THE PURIFICATION OF EFFORT

The agenda for scientific management was to increase productivity by streamlining and rationalizing factory operation from cost accounting and supervision to the dullest job on the shop floor. Efficiency was the mania, and to achieve efficiency, it would be necessary to penetrate the labor process and force it to yield up its secrets. In order that effort be rationalized, the worker's skills had to be made explicit. In many cases, workers' perceptions of their own interests prevented them from articulating their know-how, but there was yet another involuntary barrier. These skills did not easily yield themselves to explication; they were embedded in the ways of the body, in the knacks and know-how of the craftsworker.

Proponents of scientific management believed that observing and explicating workers' activity was nothing less than scientific research. Their goal was to slice to the core of an action, preserving what was necessary and discarding the rest as the sedimentation of tradition or, worse, artifice spawned by laziness. Taylor's disciples were driven by a vision of truth that would place managerial control on a footing of absolute objectivity, impervious to the commotion of class conflict or the stench of sweating bodies. 61 The principal method of acquiring such knowledge was the time study and, later, with the influence of Frank Gilbreth, the time-and-motion study. Here, "expert" observations of worker performance made it possible to translate actions into units of time and reconstruct them more efficiently.

The data from the time-study sheets became the possession of management and helped to fuel an explosion in the ranks of those who would measure, analyze, plan, report, issue orders, and monitor the various aspects of the production process. Armed with such data, planners, time-study experts, and production specialists (frequently organized as a staff group for the plant manager) became responsible for analyzing and organizing work tasks, controlling and monitoring their execution, coordinating functions, managing the flow of materials, and keeping records.

Taylor despised wasted effort at work, whether it resulted from deliberate self-protection or from ignorance. His single-minded devotion to the purification of effort gave rise to a set of practices that, whether adopted in whole or in part, transformed the nineteenth-century factory into the modern mass-production facility. The essential logic of his approach followed three steps. First, the implicit knowledge of the worker was gathered and analyzed through observation and measurement. Second, these data, combined with other systematic information regarding tools and materials, laid the foundation for a new division oflabor within the factory. It became possible to separate planning from task execution, to increase the fragmentation and thus the simplicity of production jobs, and so to minimize the amount of skill and training time associated with efficient operations. Third, the new system required a variety of specific control mechanisms to ensure the regularity and intensity of effort while continuing to supply managers and planners with the data necessary for adjustment and improvement. These mechanisms included the development of incentive payment schemes, monitoring systems, and standard operating procedures.

Taylorism meant that the body as the source of skill was to be the object of inquiry in order that the body as the source of effort could become the object of more exacting control. Once explicated, the worker's know-how was expropriated to the ranks of management, where it became management's prerogative to reorganize that knowledge according to its own interests, needs, and motives. The growth of the management hierarchy depended in part upon this transfer of knowledge from the private sentience of the worker's active body to the systematic lists, flowcharts, and measurements of the planner's office. 63 In 1912 a prominent naval engineer writing in the Journal of the American Society of Naval En9ineers listed the seven laws of scientific management. His first law, from which all the others followed, stated that "it is necessary in any activity to have a complete knowledge of what is to be done and to prepare instructions as to what is to be done before the work is started ... the laborer has only to follow direction. He need not stop to think what his past experience in similar cases has been."

Another contemporary interpreter of scientific management took pains to outline the quality of knowledge upon which this approach was based:

Instead of depending upon judgment, scientific management depends upon knowledge in its task of administration. Judgment is the instinctive and subconscious association of impressions derived from previous experience ... but even the best judgment falls far short of knowledge .... This knowledge is carefully and systematically collected and the data so obtained are classified and digested until the knowledge is instantly available whenever a problem is presented to management. Back of the form of organization is a knowledge of the needs and the work of the plant. Back of the plan of wage payment is a knowledge of psychology and sociology. Back of the instruction sheet is a knowledge of the sciences of cutting metals and of handling work.

Another industrial engineer addressing a conference at Dartmouth University's Amos Tuck School of Management in 1912 stressed the difference between scientific management and the more general movement known as systematic management. The scientific management approach rested on complete knowledge of materials, equipment, routing, job assignments, tools, task organizations, time standards, and performance methods. Each phase of an operation was to be planned completely before anything was done: "By this means the order and assignment of all work, or routing as it is called, should be conducted by the central planning or routing department. This brings the control of all operations in the plant, the progress and order of the work, back to the central point. Information, which even in the systematized plant is supposed to be furnished by the knowledge of the workman or the gang-boss or foreman, is brought back to the planning room and becomes a part of the instruction card. " 66 This transfer of knowledge both necessitated and legitimated a new conception of managerial (line and staff) responsibility in coordinating and controlling the complexities of the factory as it entered the era of mass production.

The complexity of workers' responses to scientific management has much to do with the dilemmas created by the body's dual role in production. As rationalization depleted the worker's skill base, there were reactions of loss and threat. These were not without contradiction. Where rationalization did offer less strenuous ways to accomplish physical tasks while improving tools, working conditions, and wages, there is evidence to suggest that many workers who were suspicious at first later accepted and even welcomed the innovations.

Men like Taylor and Gilbreth, who were firmly committed to raising the total level of worker output by easing the arduousness of physical tasks, looked both to new equipment and to new principles of work organization in order to accomplish their goal. For example, after a meticulous study of bricklaying, Gilbreth introduced an adjustable scaffold for piling up bricks. His invention eliminated the worker's bending over and lifting the weight of his body "a thousand times a day," and increased a worker's output from 1,000 to 2,700 bricks daily. Gilbreth claimed that workers typically responded to his innovations with gratitude, as their jobs were made easier.

Taylor believed it was necessary to share the fruits of such productivity increases and saw the differential piece-rate system, a central tenet of scientific management, as a method of uniting workers and managers in a bond of common interest. But incentive wages are devilishly hard to administer, and all too often, managers attracted to differential wage schemes succumbed to shortcuts that promised fast gains. Managers would frequently change piece rates as workers learned to meet the standards. This lead to the complaints of overwork with which unions relentlessly dogged proponents of Taylorism. Scientific management frequently meant not only that individual effort was simplified (either because of labor-saving equipment or new organizational methods that fragmented tasks into their simplest components), but also that the pace of effort was intensified, thus raising the level of fatigue and stress. Effort was purified-stripped of waste-but not yet eased, and resistance to scientific management harkened back to the age-old issue of the intensity and degree of physical exertion to which the body should be subject. As long as effort was organized by the traditional practices of a craft, it could be experienced as within one's own control and, being inextricably linked to skill, as a source of considerable pride, satisfaction, and independence. Stripped of this context and meaning, demands for greater effort only intensified the desire for self- โข 69 protect10n.

Taylor had believed that the transcendent logic of science, together with easier work and better, more fairly determined wages, could integrate the worker into the organization and inspire a zest for production. Instead, the forms of work organization that emerged with scientific management tended to amplify the divergence of interests between management and workers. Scientific management revised many of the assumptions that had guided the traditional employer-employee relationship in that it allowed a minimal connection between the organization and the individual in terms of skill, training, and the centrality of the worker's contribution. It also permitted a new flexibility in work force management, promoting the maximum interchangeability of personnel and the minimum dependence on their ability, availability, or motivation. 70 The time-study expert became the new focus of workers' antagonisms. 71 Informal production quotas persisted and, in many cases, took on an overtly anti-management spirit. The union contract became the most important means for institutionalizing workers' instincts for self preservation, as rationalization routed out the ordinary graces with which the workday had been laced. Concern over wages and hours, work rules, and working conditions began to replace the craftsworker's culturally embedded practices of effort regulation. 72 Overall, the purification of effort meant a heightened standard of labor discipline, and as such it widened the gulf between those who gave of their bodies in the service of production and those who did not. Yet even into the second decade of the twentieth century, a more traditional sensibility toward work, one that suffused it with the rhythms of a chatty, sensual, and fun-loving humanity, could still be articulated. A machinist gained prominence when he debated Taylor in 1914 and remarked, "we don't want to work as fast as we are able to. We want to work as fast as we think it's comfortable for us to work. We haven't come into existence for the purpose of seeing how great a task we can perform through a lifetime. We are trying to regulate our work so as to make it auxiliary to our lives. "

A GLASS HALF FULL

As the logic of Taylorism took hold, the substitution of machine power for human labor became the obvious method of increasing the speed and volume of production. In an important way, however, the innovations of mass production added a new dimension to work relationships. Beginning with the highly mechanized Ford Highland Park auto assembly plant, technology would be relied upon to complement or supplant human direction: "The instruction cards on which Taylor set so much value, Ford was able to discard. The conveyor belt, the traveling platform, the overhead rails and material conveyors take their place .... Motion analysis has become largely unnecessary, for the task of the assembly line worker is reduced to a few manipulations. Taylor's stop-watch nevertheless remains measuring the time of operations to the fraction of a second. "

The fragmentation of tasks characteristic of the new Ford assembly line achieved dramatic increases in productivity due to the detailed time study of thousands of operations and the invention of the conveyor belt and other equipment that maximized the continuity of assembly. H. L. Arnold, an industrial journalist who wrote enthusiastically about Ford's innovations, summarized some of the key elements of this costreduction strategy. First, all needless motions were eliminated from the worker's actions; second, the task was organized to require the "least expenditure of willpower, and ... brain fatigue." 75 This formula has dominated the design of mass-production techniques throughout the twentieth century. Effort is simplified (though its pace is frequently intensified) while skill demands are reduced by new methods of task organization and new forms of machinery.

A distinction can be made between the new technologies for the organization of production, which allowed management to control the pace of the assembly line (and, thus, the intensity of effort), and the introduction of new forms of machinery that could reduce the requirements for both effort and skill. The continuity of assembly depended upon the production of interchangeable parts for uniform products. A new generation of automatic and semiautomatic machine tools moderated the physical demands on the machinist as they transferred skill from the worker to the machine. The new workers hired to operate these machines "had no skills and simply placed a piece in and removed it from the machine."

In the second decade of the twentieth century, when these developments were being debated, the fast-growing automobile industry faced shortages of skilled workers. Cheap labor was in abundance, particularly as unskilled rural laborers and new immigrants flocked to cities like Detroit in search of employment. Ford's engineers attempted to construct machine tools suited to the skill level of this labor force. The economic rewards of this approach were so great that with the spread of mass production, this pattern of design choices was carried over from the particular historical situation that had engendered them. For the majority of industrial workers in the generations that followed, there would be fewer opportunities to develop or maintain craft skills. Mass production depended upon interchangeability for the standardization of production; this principle required manufacturing operations to free themselves from the particularistic know-how of the craftsworker. Ford was the first to succeed in this endeavor on a massive scale, where others had failed.

Thus, applications of industrial technology have simplified, and generally reduced, physical effort, but because of the bond between effort and skill, they have also tended to reduce or eliminate know-how. 78 This paradox of the body's dual role in production is nowhere better illustrated that in James Bright's 1958 study of automation in American industry and the subsequent reinterpretation of his findings by social critic Harry Braverman in 1974. 79 Bright studied thirteen manufacturing facilities, including automobile engine plants, a refinery, a bakery, an electrical-part manufacturer, plating plants, and others. He observed varieties of automation in production and materials handling (though it should be noted that applications of computers in manufacturing were virtually nonexistent at the time of his study).

Bright concluded that throughout the thirteen plants he studied, workers were receptive and, in many cases, enthusiastic toward the new automated equipment. Why? "Automation takes the heavy labor out of a job .... The gain to management is productivity; the gain to labor is a much easier job .... In one engine plant a grizzled veteran manning a push button signal light control panel governing some hundred feet of machinery was interviewed .... He said, 'Sure I like it better. It's a lot easier. I can tell you one thing-I'll last a lot longer on this job.' " 81 Self-preservation would induce the worker to accept automation. Bright also clearly stated automation's effect on skills: "As the controls become more sensitive and responsive to the requirements of the operation, environment, and the task, the machine assumes responsibility, just as it has already assumed skill, knowledge, and judgment requirements.'' 82 He noted that this was one of labor's biggest "headaches" with automation, 83 but he believed that new wage determination systems, coupled with sensitive implementation processes, could overcome any resistance engendered by skill dilution. As he put it, "the principal shortcoming ... is that very few firms truly do 'sell' the automated equipment to the worker."

In Braverman's influential critique of what he called the "degradation of work" in this century, he used Bright's study to make a very different point. Where Bright saw the glass half full because the physical demands of work were curtailed, Braverman saw the glass being drained, as workers' skills were absorbed by technology. For Braverman, the transfer of skill into machinery represented a triumph of "dead labor over living labor," a necessity of capitalist logic. As machinery is enlarged and perfected, the worker is made puny and insignificant. By substituting capital (in the form of machinery) for labor, Braverman believed that employers merely seized the opportunity to exert greater control over the labor process. As the work force encountered fewer opportunities for skill development, it would become progressively less capable and, thus, less able to exert any senous oppos1t10n.

Although most analyses of industrial organization during the middle decades of the twentieth century have taken up one of these viewpoints, worker ambivalence toward automation has been a persistent theme. 86 Since skilled work is less automated, it also tends to involve more exertion, bodily alteration, dirt, and discomfort. Chinoy's 1955 study of automobile workers found few unskilled men harboring goals of becoming skilled workers. To a certain extent, this finding reflected the real and perceived lack of opportunities to move into skilled positions, but it also reflected a desire to avoid jobs that were physically demanding. "The values which played the greatest part in determining the order of preference among nonskilled jobs were regulation of employment and the relative absence of physical and psychological strain . . . . Men wanted work which was 'not too hard,' 'not too heavy,' 'not too dirty,' and 'not too noisy.' ... They did not like jobs in which 'you got to keep your hands in oil and grease all day.' "

In his account of the skilled auto worker, Bill Goode, a sociologist and former autoworker, reflects on the attitudinal gulf between skilled and unskilled workers in that industry. Most unskilled production workers are not interested in skilled jobs, he contends, because they are not as much fun: "One aspect of skilled work that production workers do not usually experience is dirt ... skilled work is filthy .... The skilled worker carries the marks of his trade under his fingernails and in the creases of his hands .... Assemblers ... have more fun. There is a spontaneity about the play on a line that is unmatched by skilled work. "

Similarly, another worker-turned-sociologist-of-the-workplace, Robert Schrank, reports that workers in a machine shop where he was employed routinely rejected opportunities to become skilled machinists. As these workers expressed themselves to Schrank: "What the hell do you want all the responsibility for? I just stand by this machine, watch it, adjust it once in a while, load it, and I can think about anything I want. I don't have to worry about all those tools and tolerances." 89 Later, Schrank became a set-up man on a turret lathe, a highly skilled position. He began to question the sacrifices he had made to develop and practice his skill: "As I worked in the turret lathe department, my doubts about being a skilled worker increased. The operators seemed to be having more fun than me, playing the races, fooling with the girls, and feeling free ... the pay differential ... did not seem to be worth the added responsibility. "

In the early days of industrialization, skill development was an important source of social integration, as the most skilled workers were often promoted to management ranks. As production jobs offered less opportunity for skill development, they came to have little relevance for the kinds of expertise needed at supervisory levels, and the boundary between these classes of organization members became more rigid. Chinoy's study of the ABC auto plant again speaks to this historical shift: "The structure of opportunity in the ABC plant gives most workers little reason to 'think of the corporation as a pyramid of opportunity.' ... The pattern of working one's way up from the ranks was seen as largely a thing of the past.''

THE SEARH FOR A SOLUTION

By the early I 960s, scholars had begun to ask whether this tendency would continue to grow with increasing levels of automation. As the range and depth of automation increased, further limiting the contribution of physical effort to the production process, would it continue to reduce skill requirements and thus widen the gulf between workers and management?

Robert Blauner's 1964 study Alienation and Freedom contrasted workers' attitudes and experiences in four types of industrial organizations- craft-based (the printing industry), machine-tending (the textile industry), mass production (the auto industry), and continuous-process (the chemical industry). Because of its advanced degree of automation, the continuous-process form of production was considered to "portend the conditions of factory work and employment in a future dominated by automated technology." 92 Would the experiences of continuous process workers simply repeat the disturbing trade-offs in the mass production industries? Or would these highly automated workplaces offer an alternative?

Blauner's report on the social-psychological dynamics associated with the continuous-process form of production revealed a very different relationship between the labor process and the body of the worker (for a description of continuous-process production and how it differs from other manufacturing processes, see appendix A, p. 415). Skill and effort finally seemed to be uncoupled. In the chemical industry, the focus of Blauner's research on continuous-process operations, production depended upon capital investment in highly automated equipment. Workers were called upon to physically exert themselves only when the equipment broke down, thus creating an inverse relationship between manual effort and productivity. The technology had helped to shape a work environment that freed the worker's body from the kinds of disciplinary pressures that had been routine features of industrial life.

For example, operators required to make the rounds of the equipment in a part of the plant in order to read instruments and monitor plant functioning could control the pace of their activities. As one operator told Blauner, "You can eat the soup first and do the work later, or take the readings at 1 :45 and then eat your soup." 93 The very act of making equipment rounds provided a physical variety and freedom of movement that contrasted sharply with the machine-paced work of the assembly line. This mobility typically placed the continuous-process operator out of the range of an immediate supervisor and made it possible to escape scrutiny. As a result, the operators in Blauner's study felt they had "considerable scope to do their jobs in their own way."

In other industrial settings, the close relationship between effort and skill meant that a reduction in the amount of physical involvement required of the worker tended to mean a reduction in the amount of skill he or she had to bring to the job. Did the reduction of effort in continuous-process operations also mean a reduction of skill? Blauner himself had some difficulty answering this question. In general, he argued that the shift from a job-centered to a process-centered form of work organization meant that the individual contribution changed from one of providing skills to one of assuming responsibility. The operator was responsible for the trouble-free operation of the process, the quality of product being produced, and the preservation of expensive automated equipment. Yet the head operator, in whom the greatest responsibility was invested, had achieved his position by virtue of long years of experience in the plant. Presumably, such experience was important because of the opportunities for skill development it afforded. Another fact makes Blauner's logic difficult to accept-the workers themselves felt that the skills demanded of them were considerable: "Virtually all of the workers interviewed said that their tasks required skill, even lots of skill, yet they were unsure of what was involved. "

Research by British investigators during the late I 9 50s and early I 960s provided a better understanding of the kinds of skills that workers had to bring to bear in a continuous-process environment. E. R. F. W. Crossman headed a research team that developed the first comprehensive description of the process operator's skills and activities, based on field visits within a variety of industries. Crossman found that the work of the process operator fell into four main categories. The most important of these was "control," followed by "special procedures and drills," "routine maintenance," and "recording and reporting." Activities related to controlling the process were described this way: "The process-operator ... must monitor the various gauges, attend to the signs coming from the plant itself, such as noises, smells and vibration, and occasionally carry out special tests on the product .... Apart from the 'official' controls, there are often special ways of influencing the process, such as propping doors open to give greater cooling, or tapping pipes to loosen deposits. "

Crossman went on to analyze what he called the "five components" of process-control skill: sensing ("the smells and appearance which indicate how the plant is running"), perceiving, prediction, familiarity with controls, and decision making. It was this final component that Crossman considered to be the most crucial, and he identified three principal ways in which an operator might approach decision making: by rule of thumb, by having an intuitive feel of what is best, or by pursuing an explicit and logical analysis. Crossman concluded that "on the whole, discussions with operators have suggested that the first, or 'rule-of-thumb,' method is common among the less good operators, and the second, or 'intuitive,' method is often characteristic of the better ones. But few operators seem to use a fully rational or conceptual approach .... The intuitive understanding which enables him to deal with subtle changes and unusual situations seems to come with experience alone. "

Blauner's discussion of operator skills lacks the depth and nuance of Crossman's observations, but his conclusions are similar. Blauner noted that although the worker in such an operation may have a wide range of knowledge about the factory, that knowledge is concrete rather than conceptual: "Whereas continuous-process production increases the worker's breadth of knowledge and awareness of the factory as a totality, it does not necessarily enhance the worker's depth of understanding of the technical and scientific processes ... the very complexity of the scientifically-based reaction means that there is more that is unknown to the average worker in the continuous-process industries than to the all-competent printing craftsperson or the automobile assembler."

What are the implications of this skill profile? To most observers, the continuous-process operation appears to present a significant discontinuity with other forms of production work. It would seem that this sense of uniqueness derives in large measure from the way in which these environments have achieved a de facto resolution of the paradox of the body in the labor process. Skill and effort are no longer inextricably linked. The operator must put forward a minimum of strenuous physical exertion, but the most critical skills he or she brings to the process still depend on the subtle and inarticulate know-how that a body accumulates with years of experience. In this respect, the skills of continuous-process work are in a direct line of descent from earlier generations of craft knowledge. The implicit action-embedded quality of such skills (remember James Davis on learning to puddle iron) would help to explain why the operators Blauner interviewed had a difficult time describing what precisely their skills were.

Based on his comparative studies, Blauner formulated the nowclassic U-curve hypothesis. Comparing levels of social integration across industries at distinct stages of automation, he found the highest levels within those industries that had the least (printing) and the most (chemical) automation. He found that workers in these craft-based and continuous-process industries were considerably less socially alienated and more socially integrated than their counterparts in the massproduction industries of textiles and automobiles. He further argued that rather than exacerbating industrial tensions according to some presumed linear function, higher levels of automation actually begin to reverse some of the most prominent negative tendencies associated with the rationalization of manufacturing work. Continuous-process operators experienced a greater degree of identification with their managers and more loyalty toward their companies, resulting in a heightened sense of mutuality and collaboration.

In the twenty years since Blauner published his study, other researchers have replicated his findings. 99 They observed higher levels of social integration among workers in the most automated industries. Yet some other scholars, notably Mallet, Braverman, and Gallie, have argued to the contrary. ' 00 Duncan Gallie's empirical work in particular shows that the line distinguishing the interests of the continuous-process operator and the manager can remain very clearly drawn.

To understand why and to what degree the continuous-process organization is likely to reveal a strong tendency toward social integration (particularly when compared to the mass-production industries), consider again the body's dual meanings for the production process-as a source of effort and skill.

Continuous-process operators are spared much of the exertion, exhaustion, and depletion that typifies factory work in less automated industries. Because automation has reduced the amount of physical sacrifice required of such workers, the antagonism between those who give of their bodies and those who do not has been muted. Continuousprocess operators are driven by neither the foreman nor the machine system. They are not forced to conform to a narrow behavioral standard. Like their managers, they feel that they are in control of their bodies as they move through time and space.

However, the nature of the operators' skills both contributes to and ultimately limits the potential for social integration in a continuousprocess operation. Blauner saw the level of responsibility that characterized the operator's role as a crucial factor accounting for the high degree of social integration. If the technology necessitates that an operator assumes responsibility for monitoring and controlling broad sections of the plant, how can management regard its workers as adversaries? Hence, "the alienation curve begins to decline from its previous height as employees in automated industries gain a new dignity from responsibility and a sense of individual function.

The level of responsibility to which Blauner and others have pointed remains firmly linked to an implicit judgment of the operator's skill. It is the operator with the most experience who becomes the head shift operator or crew leader. This is because the skills he or she must have, the "intuitive feel" described by Crossman, are developed only through many years of working at a variety of jobs in the plant and of observing a wide range of equipment operating under diverse conditions. The fact that this competence is experience-based helps orient the operator toward the stratified occupational structure within the work force. Because skill is cumulative, know-how gathered at the lower status levels is relevant to performance at higher levels, and the prospect of advancement serves as an important integrating mechanism. The extended temporal horizon necessary for skill development provides the opportunity for an individual to develop an identification with and loyalty to the organization.

Responsibility not only is a measure of the skill level of the operator but also denotes the dependence of management on the worker. The worker's bodily involvement with the production process over many years provides exclusive access to the subtle and specific knowledge required for competent operations. Blauner's observation of small work teams imposing their own standards of work discipline is reminiscent of the work groups in the early factory, whose unique craft knowledge empowered them to exert their own vision of appropriate work organization and behavior. 102 Like those early craftspeople, the exclusive knowledge of the workers in a continuous-process operation lashes them to their managers with bonds of reciprocity.

That the worker's body, through the sensual information it accumulates with physical involvement in the production process, remains a crucial source of skill both defines and limits the extent to which the worker is likely to be a fully integrated member of this kind of organization. The operator's knowledge continues to depend upon sentience, and it is the personal, specific bodily character of this knowledge that persistently differentiates the operator from the management superstructure. As long as their knowledge is concrete and specific rather than conceptual and technical, workers will tend to be confined to a certain set of roles. Without a conceptual understanding of the process in which they work, indeed, of their own actions, workers will find it difficult to make a contribution to that domain of more comprehensive functions typically labeled "managerial."

Thus, despite the high level of social integration in many continuousprocess organizations, a fundamental aspect of Tayloristic logic has been preserved. The much-touted collaborative atmosphere of continuous-process industries derives in large measure from the minimal emphasis placed upon the need for physical sacrifice from the worker. However, the body as the operator's source of skill remains a strong link to the industrial past and continues to demarcate the boundary between those who give of their bodies in the service of production and others who do not. Should operators perceive any devaluation of their skills, it is likely that this boundary would deepen and the sense of mutuality would diminish.

The history of work has been, in part, the history of the worker's body. Production depended on what the body could accomplish with strength and skill. Techniques that improve output have been driven by a general desire to decrease the pain of labor as well as by employers' intentions to escape dependency upon that knowledge which only the sentient laboring body could provide. Skilled workers historically have been ambivalent toward automation, knowing that the bodies it would augment or replace were the occasion for both their pain and their power. In Blauner's model, the problem seemed to have been solved; automation was able to minimize exertion without depleting skill. Yet even within this optimistic rendering, the body's role in the labor process continues to describe the distance between managers and the managed.

From this vantage point it is possible to see that the progress of automation has been a result of a transfer of knowledge and has, in turn, further enabled that transfer. However, the term transfer must be doubly laden if it is to adequately describe this process. Knowledge was first transferred from one quality of knowing to another-from knowing that was sentient, embedded, and experience-based to knowing that was explicit and thus subject to rational analysis and perpetual reformulation. The mechanisms used to accomplish this transfer were themselves labor intensive (that is, they depended upon first-hand observation of time-study experts) and were designed solely in the context of, and with the express purpose of, enabling a second transfer - one that entailed the migration of knowledge from labor to management with its pointed implications for the distribution of authority and the division of labor in the industrial organization. In the case of Blauner's chemical plant, operators' sentient knowledge could not be explicated. Their skills were left intact but in a way that continued to circumscribe their role in the organization.

As information technology is applied to the production process, what effect will it have upon the grounds of knowledge? What will it take "to know"? What kind of knowledge will enable competent participation in the processes associated with making things? History alerts us to the fact that answers to these questions will require some understanding of the relationship between the new technology and the sentient body. Will information technology continue to diminish physical effort but allow the retention of experience-based skills as in Blauner's scenario? Will effort and skill, indeed the very presence of the worker, be wiped out altogether as Braverman (or Bright) would lead us to predict?

The following chapter presents a third, relatively uncharted, alternative, suggesting that this latest technological transformation can indeed alter the terms of the centuries-old paradox concerning effort and skill. By redefining the grounds of knowledge from which competent behavior is derived, new information technology lifts skill from its historical dependence upon a laboring sentient body. While it is true that computer-based automation continues to displace the human body and its know-how (a process that has come to be known as deskilling), the informating power of the technology simultaneously creates pressure for a profound reskilling. How are these new skills to be understood? What might be their implications for the differences that have separated workers from their managers, differences that have, in an important way, depended upon the necessity of the body's labor and the body's learning.

CONCLUSION

MANAGING THE INFORMATED ORGANIZATION

What defines humanity is not the capacity to create a second nature-economic, social, or cultural-beyond biological nature; it is rather the capacity of going beyond created structures in order to create others. -MAURICE MERLEAU-PONTY The Structure of Behavior

TECHNOLOGY IS A PLACE

Put your eye to the kaleidoscope and hold it toward the light. There is a burst of color, tiny fragments in an intricate composition. Imagine a hand nudging the kaleidoscope's rim until hundreds of angles collapse, merge, and separate to form a new design. A fundamental change in an organization's technological infrastructure wields the power of the hand at the turning rim. Technological change defines the horizon of our material world as it shapes the limiting conditions of what is possible and what is barely imaginable. It erodes taken-for-granted assumptions about the nature of our reality, the "pattern" in which we dwell, and lays open new choices. When the telephone makes it possible to pursue intimate conversations without bodies that touch or eyes that meet, or when the electric light rescues the night from darkness, the experience is more than simply an element within the pattern. Such innovations give form and definition to our worldly place and provoke a new vision of the potential for relatedness within it. It is in this sense that technology cannot be considered neutral. Technology is brimming with valence and specificity in that it both creates and forecloses avenues of experience.

History reveals the power of certain technological innovations to transform the mental life of an era-the feelings, sensibilities, perceptions, expectations, assumptions, and, above all, possibilities that define a community. From the social influence of the medieval castle,I to the coming of the printed book, 2 to the social and physical upheaval associated with the rise of the automobile 3 -each specific example serves to drive home a similar message. An important technological innovation is not usefully thought of as a unitary cause eliciting a series of discrete effects. Instead, it can be seen as an alteration of the material horizon of our world, with transformative implications for both the contours and the interior texture of our lives. Technology makes the world a new place-a conception expressed by Fernand Braudel when he wrote:

It was only when things went wrong, when society came up against the ceiling of the possible, that people turned of necessity to technology, and interest was aroused for the thousand potential inventions, out of which one would be recognized as the best, the one that would break through the obstacle and open the door to a different future .... In this sense, technology is indeed a queen: it does change the world.

Some theorists have attributed systematic and purposeful agency to the managerial use of technology. They argue that managers are interested exclusively in technology as a means of controlling, limiting, and ultimately weakening their work force. 5 The data I have presented suggest a more complicated reality. Even where control or de skilling has been the intent of managerial choices with respect to new information technology, managers themselves are also captive to a wide range of impulses and pressures. Only rarely is there a grand design concocted by an elite group ruthlessly grinding its way toward the fulfillment of some special and secret plan. Instead, there is a concentration of forces and consequences, which in turn develop their own momentum. Sometimes these lines of force run in predictably straight paths. At other times, they twist and spiral, turn corners, and flow to their opposite. Activities that seem to represent choices are often inert reproductions of accepted practice. In many cases, they are convenient responses to the press of local exigencies. In some instances, they may actually reflect a plan.

To fully grasp the way in which a major new technology can change the world, as described by Braudel, it is necessary to consider both the manner in which it creates intrinsically new qualities of experience and the way in which new possibilities are engaged by the often-conflicting demands of social, political, and economic interests in order to produce a "choice." To concentrate only on intrinsic change and the texture of an emergent mentality is to ignore the real weight of history and the diversity of interests that pervade collective behavior. However, to narrow all discussion of technological change to the play of these interests overlooks the essential power of technology to reorder the rules of the game and thus our experience as players. Moreover, these two dimensions of technological change, the intrinsic and the contingent, need to be understood, not separately, but in relation to one another. The same innovation that abstracts work and increases its intellectual content, thus enhancing the learning of lower level employees (remember the operators at Tiger Creek and their experience with the new expensetracking system described in chapter 7), can also, within the context of the choices by which it is adapted, be experienced as a new source of divisiveness and control (as Tiger Creek's managers perceived a threat to their roles and resisted the potential for change).

The dilemmas of transformation that have been described are embedded in the living detail of everyday life in the workplace as it undergoes computerization. They are dilemmas precisely because of the way they reveal the subtle interplay between essence and choice. Information technology essentially alters the contours of reality-work becomes more abstract, intelligence may be programmed, organizational memory and visibility are increased by an order of magnitude beyond any historical capability. Individuals caught up in this newly configured reality face questions that did not need to be asked before. New possibilities arise and require new deliberations. The duality of information technology-its capacity to automate and to informate- provides a vantage point from which to consider these choices. The relative emphasis that organizations give to these capacities will become the foundation for a strategic conception of technology deployment and so will shape the way the dilemmas are confronted and resolved.

The organizations described in this book have illustrated how the need to defend and reproduce the legitimacy of managerial authority can channel potential innovation toward a conventional emphasis on automation. In this context, managers emphasize machine intelligence and managerial control over the knowledge base at the expense of developing knowledge in the operating work force. They use the technology as a fail-safe system to increase their sense of certainty and control over both production and organizational functions. Their experiences suggest that the traditional environment of imperative control is fatally flawed in its ability to adequately exploit the informating capacity of the new technology.

In these organizations, the promise of automation seemed to exert a magnetic force, a seduction that promised to fulfill a dream of perfect control and heal egos wounded by their needs for certainty. The dream contains the image of "people serving a smart machine," but in the shadow of the dream, human beings have lost the experience of critical judgment that would allow them to no longer simply respond but to know better than, to question, to say no. This dream brings us closer to fulfilling Hannah Arendt's dreadful forecast of a world in which behaviorism comes true:

The last stage of the laboring society, the society of jobholders, demands of its members a sheer automatic functioning, as though individual life had actually been submerged in the over-all life process of the species and the only active decision still required of the individual were to let go, so to speak, to abandon his individuality, the still individually sensed pain and trouble of living, and acquiesce in a dazed, "tranquilized," functional type of behavior. The trouble with modern theories of behaviorism is not that they are wrong but that they could become true, that they actually are the best possible conceptualization of certain obvious ttends in modern society. It is quite conceivable that the modern age-which began with such an unprecedented and promising outburst of human activity-may end in the deadliest, most sterile passivity history has ever known.

That managers may give themselves over to this dream because of inertia and convenience rather than cogent analysis is all the more disturbing. Organizations that take steps toward an exclusively automating strategy can set a course that is not easily reversed. They are likely to find themselves crippled by antagonism from the work force and the depletion of knowledge that would be needed in value-adding activities. The absence of a self-conscious strategy to exploit the informating capacity of the new technology has tended to mean that managerial action flows along the path of least resistance-a path that, at least superficially, appears to serve only the interests of managerial hegemony.

Yet what would seem to be a maddeningly predictable story line has its share of surprises, false starts, dead ends, trap doors, tarnished hopes, and real failures. The seeds of an informating strategy were apparent in each of the organizations described here, especially in Cedar Bluff, Global Bank Brazil, and DrugCorp. In the absence of a comprehensive strategy, no single organization fully succeeded in exploiting the opportunity to informate.

The interdependence of the three dilemmas of transformation I have described-knowledge, authority, and technique-indicates the necessary comprehensiveness of an informating strategy. The shifting grounds of knowledge invite managers to recognize the emergent demands for intellective skills and develop a learning environment in which such skills can develop. That very recognition contains a threat to managerial authority, which depends in part upon control over the organization's knowledge base. A commitment to intellective skill development is likely to be hampered when an organization's division of labor continuously replenishes the felt necessity of imperative control. Managers who must prove and defend their own legitimacy do not easily share knowledge or engage in inquiry. Workers who feel the requirements of subordination are not enthusiastic learners. New roles cannot emerge without the structures to support them. If managers are to alter their behavior, then methods of evaluation and reward that encourage them to do so must be in place. If employees are to learn to operate in new ways and to broaden their contribution to the life of the business, then career ladders and reward systems reflecting that change must be designed. In this context, access to information is critically important; the structure of access to information expresses the organization's underlying conception of authority. Employees and managers can hardly be partners in learning if there is a one-way mirror between them. Techniques of control that are meant to safeguard authority create suspicion and animosity, which is particularly dysfunctional when an organization needs to apply its human energies to inventing an alternative form of work organization better suited to the new technological context.

The interdependence among these dilemmas means that technology alone, no matter how well designed or implemented, cannot be relied upon to carry the full weight of an informating strategy. Managers must have an awareness of the choices they face, a desire to exploit the informating capacity of the new technology, and a commitment to fundamental change in the landscape of authority if a comprehensive informating strategy is to succeed. Without this strategic commitment, the hierarchy will use technology to reproduce itself. Technological developments, in the absence of organizational innovation, will be assimilated into the status quo.

THE DIVISION OF LABOR AND THE DIVISION OF LEARNING

Organizational theorists frequently have promoted a conception of organizations as "interpretation systems." 7 The computer mediation of an organization's productive and administrative infrastructure places an even greater premium upon an organization's interpretive capabilities, as each organizational level experiences a relatively greater preponderance of abstract cues requiring interpretation. This is as true for the plant manager as for the pulp mill worker, for the banker as well as for the clerk. In each case, oral culture and the action-centered skills upon which that culture depends are gradually eroded, and perhaps finally displaced, by the incursions of explicit information and intellective skill.

As bureaucratic coordination and communication become more dependent upon mastering the electronic text, the acting-with skills of the white-collar body are subordinated to the demands associated with dominating increasing quantities of abstracted information. In many cases, traditional functional distinctions no longer reflect the requirements of the business. When managers increase their engagement with the electronic text, they also risk a new kind of hyperrationalisn and impersonalization, as they operate at a greater distance from employees and customers.

When the textualizing consequences of an informating technology become more comprehensive, the body's traditional role in the production process (as a source of effort and/or skill in the service of actingon) is also transformed. The rigid separation of mental and material work characteristic of the industrial division of labor and vital to the preservation of a distinct managerial group (in the office as well as in the factory) becomes, not merely outmoded, but perilously dysfunctional. Earlier distinctions between white and blue "collars" collapse. Even more significant is the increased intellectual content of work tasks across organizational levels that attenuates the conventional designations of manager and managed. This does not mean that there are no longer useful distinctions to be made among organizational members, but whatever these distinctions may be, they will no longer convey fundamentally different modes of involvement with the life of the organization represented by the division of abstract and physical labor. Instead, the total organizational skill base becomes more homogeneous.

In the highly informated organization, the data base takes on a life of its own. As organizations like Cedar Bluff develop mechanisms that allow data to be automatically generated, captured, and stored, they begin to create their own image in the form of dynamic, detailed, realtime, integrated electronic texts. These texts can provide access to internal operations as well as external business and customer data; they can be designed with enough reflexivity to be able to organize, summarize, and analyze aspects of their own content. The electronic text becomes a vast symbolic surrogate for the vital detail of an organization's daily life. Such data constitute an autonomous domain. They are a public symbolization of organizational experience, much of which was previously private, fragmented, and implicit-lodged in people's heads, in their sensual know-how, in discussions at meetings or over lunch, in file drawers, or on desktops.

The textualization process moves away from a conception of information as something that individuals collect, process, and disseminate; instead, it invites us to imagine an organization as a group of people gathered around a central core that is the electronic text. Individuals take up their relationship toward that text according to their responsibilities and their information needs. In such a scenario, work is, in large measure, the creation of meaning, and the methods of work involve the application of intellective skill to data.

Under these circumstances, work organization requires a new division of learning to support a new division of labor. The traditional system of imperative control, which was designed to maximize the relationship between commands and obedience, depended upon restricted hierarchical access to knowledge and nurtured the belief that those who were excluded from the organization's explicit knowledge base were intrinsically less capable of learning what it had to offer. In contrast, an informated organization is structured to promote the possibility of useful learning among all members and thus presupposes relations of equality. However, this does not mean that all members are assumed to be identical in their orientations, proclivities, and capacities; rather, the organization legitimates each member's right to learn as much as his or her temperament and talent will allow. In the traditional organization, the division of learning lent credibility to the legitimacy of imperative control. In an informated organization, the new division of learning produces experiences that encourage a synthesis of members' interests, and the flow of value-adding knowledge helps legitimate the organization as a learning community.

The contemporary language of work is inadequate to express these new realities. We remain, in the final years of the twentieth century, prisoners of a vocabulary in which managers require employees; superiors have subordinates; jobs are defined to be specific, detailed, narrow, and task-related; and organizations have levels that in turn make possible chains of command and spans of control. The guiding metaphors are military; relationships are thought of as contractual and often adversarial. The foundational image of work is still one of a manufacturing enterprise where raw materials are transformed by physical labor and machine power into finished goods. However, the images associated with physical labor can no longer guide our conception of work.

The informated workplace, which may no longer be a "place" at all, is an arena through which information circulates, information to which intellective effort is applied. The quality, rather than the quantity, of effort will be the source from which added value is derived. Economists may continue to measure labor productivity as if the entire world of work could be represented adequately by the assembly line, but their measures will be systematically indifferent to what is most valuable in the informated organization. A new division of learning requires another vocabulary-one of colleagues and co-learners, of exploration, experimentation, and innovation. Jobs are comprehensive, tasks are abstractions that depend upon insight and synthesis, and power is a roving force that comes to rest as dictated by function and need. A new vocabulary cannot be invented all at once-it will emerge from the practical action of people struggling to make sense in a new "place" and driven to sever their ties with an industrial logic that has ruled the imaginative life of our century.

The informated organization is a learning institution, and one of its principal purposes is the expansion of knowledge-not knowledge for its own sake (as in academic pursuit), but knowledge that comes to reside at the core of what it means to be productive. Learning is no longer a separate activity that occurs either before one enters the workplace or in remote classroom settings. Nor is it an activity preserved for a managerial group. The behaviors that define learning and the behaviors that define being productive are one and the same. Learning is not something that requires time out from being engaged in productive activity; learning is the heart of productive activity. To put it simply, learning is the new form of labor.

The precise contours of a new division of learning will depend upon the business, products, services, and markets that people are engaged in learning about. The empowerment, commitment, and involvement of a wide range of organizational members in self-managing activities means that organizational structures are likely to be both emergent and flexible, changing as members continually learn more about how to organize themselves for learning about their business. However, some significant conceptual issues are raised by the prospect of a new division of learning in the informated organization. The following discussion of these issues does not offer a rigid prescription for practice but suggests the kinds of concrete choices that define an informating strategy.

MANAGERIAL ACTIVITIES IN THE INFORMATED ORGANIZATION

As the intellective skill base becomes the organization's most precious resource, managerial roles must function to enhance its quality. Members can be thought of as being arrayed in concentric circles around a central core, which is the electronic data base. The skills required by those at the core do not differ in kind from those required at a greater distance from the core. Instead of striking phenomenological differences in the work that people do, the distance of any given role from the center denotes the range and comprehensiveness of responsibilities, the time frame spanned by those responsibilities, and the degree of accountability for cross-functional integration attached to the role. The data base may be accessed from any ring in the circle, though data can be formatted and analyzed in ways that are most appropriate to the information needs of any particular ring of responsibility.

On the innermost ring, nearest to the core, are those who interact with information on a real-time basis. They have responsibility for daily operations, and the level of data they utilize is the most detailed and immediate. Because intellective skill is relevant to the work of each ring of responsibility, the skills of those who manage daily operations form an appropriate basis for their progression into roles with more comprehensive responsibilities.

The jobs at the data interface become increasingly similar to one another as the informating process evolves. In the advanced stages of informating, these become "meta jobs," because the general characteristics of intellective skill become more central to performance than the particular expertise associated with specific production-related functions. Expertise either is available from on-site specialists or is built into information systems. For example, at Cedar Bluff, top managers believed that they would solve the problem of vanishing artistry by building that expertise into the information system capability. The know-how of managers with years of experience could be systematized and made available to operators who would never have the same degree of involvement in the action contexts that had developed the personal and specific knowledge associated with action-centered skill.

This relationship between general intellective skills and expertise in specific areas was also illustrated in the case of the calculator models at Cedar Bluff, discussed in chapter 7. Operators needed the kind of understanding that would allow them to know when and how to use a model, and when to be critical of its assumptions or outputs. That quality of understanding did not depend upon being able to match the expertise that went into the models' calculations. The operator with a conceptual approach to the process, skilled in data-based reasoning, and familiar with the theory that links elements in the production process, may not be able to reproduce the knowledge of an individual with years of hands-on experience or expert training. Nevertheless, he or she should be able to understand the conceptual underpinning of a problem well enough to select among potential analytic strategies and to access the expert knowledge that is required. Intellective skill is brought to bear in the definition of the problem for analysis, the determination of the data that is required for analysis, the consideration of the appropriateness of an analytical approach, and the application of the analysis to improved performance.

The activities arrayed on the responsibility rings at a greater distance from the core incorporate at least four domains of managerial activity: intellective skill development, technology development, strategy formulation, and social system development. For example, the crucial importance of the intellective skill base requires that a significant level of organizational resources be devoted to its expansion and refinement. This means that some organizational members will be involved in both higher-order analysis and conceptualization, as well as in promoting learning and skill development among those with operational responsibility. Their aim is to expand the knowledge base and to improve the effectiveness with which data is assimilated, interpreted, and responded to. They have a central role in creating an organizational environment that invites learning and in supporting those in other managerial domains to develop their talents as educators and as learners. In this domain, managers are responsible for task-related learning, for learning about learning, and for educating others in each of the other three domains.

A new division of learning depends upon the continued progress of informating applications. This managerial domain of technologyrelated activity comprises a hierarchy of responsibilities in addition to those tasks normally associated with systems engineering, development, and maintenance. It includes maintaining the reliability of the data base while improving its breadth and quality, developing approaches to system design that support an informating strategy, and scanning for technical innovations that can lead to new informating opportunities. Members with responsibility for the development of technology must be as concerned with the use of technology (Do people understand the information? Do they know how to use it?) as they are with other aspects of design and implementation. This kind of technological development can occur only in the closest possible alignment with organizational efforts to promote learning and social integration. Technology develops as a reflection of the informating strategy and provides the material infrastructure of the learning environment.

Learning increases the pace of change. For an organization to pursue an informating strategy, it must maximize its own ability to learn and explore the implications of that learning for its long-range plans with respect to markets, product development, new sources of comparative advantage, et cetera. A division of learning that supports an informating strategy results in a distribution of knowledge and authority that enables a wide range of members to contribute to these activities. Still, some members will need to guide and coordinate learning efforts in order to lead an assessment of strategic alternatives and to focus organizational intelligence in areas of strategic value. These managers lead the organization in a way that allows members to participate in defining purpose and in supporting the direction of long-term planning. The increased time horizon of their responsibilities provides the reflective distance with which they can gauge the quality of the learning environment and can guide change that would improve collective learning.

There is considerable interdependence among these four domains of managerial activity (intellective skill development, technology development, strategy formulation, and social system development). For example, activities related to intellective skill development cannot proceed without the social system management that helps to foster roles and relationships appropriate to a new division of learning. Activities in either of these domains will be frustrated without technological development that supports an informating strategy. Integration and learning are responsibilities that fall within each domain, because without a shared commitment to interdependence and the production of value-adding knowledge, the legitimacy of the learning community will suffer. Business outcomes such as cost, efficiency, quality, product development, customer service, productivity, et cetera, would result from coordinated initiatives across domains. Managerial work would thus be team-oriented and interdisciplinary, and would promote the fluid movement of members across these four domains of managerial activity.

The concentric structure depends upon and promotes both vertical and horizontal organizational integration. There are no predetermined boundaries between any rings within the organizational sphere or between the domains of managerial authority. The skills that are required at the data interface nearest to the core of daily operating responsibilities provide a coherent basis for the kind of continual learning that would prepare people for increasingly comprehensive responsibilities. The relative homogeneity of the total organizational skill base suggests a vision of organizational membership that resembles the trajectory of a professional career, rather than the two-class system marked by an insurmountable gulf between workers and managers. The interpenetration between rings provides a key source of organizational integration.

Some observers of the emerging technological environment have predicted an increasingly bifurcated distribution of skills in the workplace. 8 On the other extreme, the operators at Piney Wood, whose discussion of the future opened our Introduction, believed that in the future all factory workers would be college graduates. The concentric organizational structure suggests that the solution to future skill requirements need not be as drastic as either of these scenarios implies. While it is probable that entry-level requirements will become more demanding, the increased homogeneity of skills and the continuity between organizational rings entails an ongoing commitment to training and education in order to facilitate career progression. The shape of skill distribution thus is more likely to represent a curve than the discontinuous step function that characterizes the traditional hierarchy, with its more rigid distinction between managers and the managed.

THE BODY'S NEW WORK: MANAGING THE INTRICACY OF POSTHIERARCHICAL RELATIONSHIPS

The vision of a concentric organization is one that seems to rely upon metaphors of wholeness-interdependency, fluidity, and homogeneity each contribute to organizational integration. What is required of managers in such a workplace, where learning and integration constitute the two most vital organizational priorities? How is the social system of such an organization to be managed?

As we have seen, the abstract precincts of the data interface heighten the need for communication. Interpretive processes depend upon creating and sharing meaning through inquiry and dialogue. New sources of personal influence are associated with the ability to learn and to engender learning in others, in contrast to an earlier emphasis upon contractual relationships or the authority derived from function and position.

What new patterns of relationships will characterize this kind of learning environment? What will replace the familiar map that the model of imperative control has provided? The answer to this question derives from one of the most significant dialetics of an informating strategy. In a conventional organization, managers' action-centered skills are geared toward the politics of interpersonal influence, principally as they pertain to maintaining reciprocities, managing superiors, and gathering or disseminating information. These skills are shaped by the demands of achieving smooth operations and personal success under conditions of hierarchical authority. People develop their expectations about how to treat one another largely in reference to rank and function. An informating strategy does place severe demands upon managers' action-centered skills in the service of acting-with but in a very different context. The relationships to be managed are both more dynamic and more intricate than earlier patterns. The shape and quality of relationships will vary in relation to what people know, what they feel, and what the task at hand requires. Relationships will need to be fashioned and refashioned as part of the dynamism of the social processes, like inquiry and dialogue, that mediate learning. Such relationships are more intricate because their character derives from the specifics of the situation that are always both pragmatic-what it takes to get the work done best-and psychological-what people need to sustain motivation and commitment.

In the information panopticon, managers (like those at Metro Tel) frequently tried to simplify their managerial tasks by displacing faceto- face engagement with techniques of surveillance and control. As a consequence, they became isolated from the realities of their organizations as they were increasingly insulated by an electronic text that in turn was even more vulnerable to workers' antagonisms. The demands of managing intricate relationships reintroduce the importance of the sentient body and so provide a counterpoint to the threat of hyperrationalism and impersonalization that is posed by computer mediation. The body now functions as the scene of human feeling rather than as the source of physical energy or as an instrument of political influence. Human feeling operates here in two ways. First, as members engage in their work together, their feelings are an important source of data from which intricate relations are structured. Second, a manager's felt sense of the group and its learning needs is a vital source of personal knowledge that informs the development of new action-centered skills in the service of acting-with.

The demands of a learning environment can reduce the psychological distance between the self and the organization because active engagement in the social processes associated with interpretation requires more extensive participation of the human personality. In a traditional approach to work organization, employees could be treated as objectively measurable bodies, and in return, they could give of their labor without giving of their selves. The human being as wage earner and the human being as subjective actor could remain separate. In an environment of imperative control, managers can remain indifferent to what their subordinates feel, as long as they perform adequately. This "is" was eventually translated to "ought," as incursions of private feeling into the workday came to be seen as squandering the organization's time. It was this view that ultimately triumphed over the professionals at DrugCorp who had unwittingly textualized their own playfulness. As they struggled with their notions of legitimate work behavior, DrugCorp's managers tried to define the self out of the workday. But when work involves a collective effort to create and communicate meaning, the dynamics of human feeling cannot be relegated to the periphery of an organization's concerns. How people feel about themselves, each other, and the organization's purposes is closely linked to their capacity to sustain the high levels of internal commitment and motivation that are demanded by the abstraction of work and the new division of learning.

The relationships that characterize a learning environment thus can be thought of as posthierarchical. This does not imply that differentials of knowledge, responsibility, and power no longer exist; rather, they can no longer be assumed. Instead, they shift and flow and develop their character in relation to the situation, the task, and the actors at hand. Managing intricacy calls for a new level of action-centered skill, as dialogue and inquiry place a high premium on the intuitive and imaginative sources of understanding that temper and hone the talents related to actin9-with. The dictates of a learning environment, rather than those of imperative control, now shape the development of such interpersonal know-how.

The seeds of this new intricacy are already evident in several of the organizations I have described. The managers at Cedar Bluff who learned to join their workers in asking questions and searching for answers were already engaged in forging patterns far more intricate than the simpler prescriptions derived from faith in managerial authority. The professionals of DrugCorp, normally divided by function, professional discipline, and organizational rank, had invented new modes of relationships based upon a valued exchange of information, shared inquiry, and play. At Global Bank Brazil, bankers and operations managers were groping for new relationships that would reach beyond turf and hierarchy in order to better serve their customers. At Tiger Creek, managers and workers reached and stumbled as they attempted to shift the logic of their relationship from one based on hierarchy to one shaped by the pragmatic opportunities offered by a redistribution of knowledge. In each of these cases, organizational members were confronted with rich new possibilities engendered by an informating technology. In each case, they discovered that exploiting these new opportunities required new forms of relationships governed by the necessities of learning and performance rather than by the rules of an older faith rules that sort, rank, and separate.

DISSENT FROM WHOLENESS

Is there a dark side to this vision of a wholistic organization with its emphasis on relationships that are intricate, dynamic, and constructed ad hoc? What new psychological costs might it imply? What new mechanisms might be required to ensure justice and equity? To answer these questions, we need to return to the voices of the mill workers as they lived through the transition to an informated environment.

One source of insight into the potential pitfalls of the wholistic organization comes from those workers at Piney Wood who drew strength from the institutional arrangements of a traditional work system. When workers at that plant anticipated the future, they saw computer-based technology as a catalyst for new work systems that would be more flexible, collaborative, and socially integrative than anything they had known. Many were curious and even enthusiastic about such a prospect, but there were others who dissented from this vision of wholeness. Their perspective reminds us of how a certain breed of American worker has found psychological sustenance in the norms of the industrial workplace. It can also alert us to what might be lost in the transformation to a new form of organization and asks that we think carefully about what was best in the old arrangement and should be preserved.

Though the corrosive effects of an adversarial environment cannot be denied, there are those who found an important source of psychological freedom in the rigorously defined contractual relationships that characterized Piney Wood. In such a workplace, the union contract defines binding arrangements-jobs are meticulously defined, and seniority rules guide pay and promotion decisions. Individual workers know exactly what is required of them and, in return, what rights they possess. The worker's first obligation is above all to the job-to perform it competently and completely. As long as individuals uphold their end of the employment contract, their rights are protected. These protections provide them a status under the law that is equal to that of their employer. If the contract is violated, then there are institutionalized mechanisms of due process, invested ultimately with the legitimate authority of the state, through which a person can redress a grievance or seek to influence the policies that inform the labor management relationship.

The organizing principle of such a workplace is based on the individual. People are held accountable for their particular jobs, and these jobs are treated as distinct elements that must be assembled in order to accomplish the work of the organization. That one's obligation is first of all to the job, rather than to the enterprise, creates a certain psychological distance between the self and the organization. Living up to the terms of the employment contract leaves a wide range of behavior that is unspecified and noncontractual. A worker need not buy into the purposes or values of the organization in order to perform competently and enjoy the rewards that he or she has earned. There is no pressing need to be liked by those around you, either superiors or peers, when one's primary obligation is to fulfill the demands of a narrow job description.

These arrangements can provide the worker with a measure of independence and autonomy. They make it possible to feel that one is, as the idiom goes, one's "own man." U.S. labor relations reflect a conception of equality in which parties each seek advantage through negotiation. Neither party must buy into the worldview of the other. A pluralism of values and interpretations may coexist, so long as the job gets done.

When the workers in such an organization consider their managers, they see a very different world. Those who most value the psychological distance and autonomy provided by the contractual relationship, view managers with a certain pity. They believe that managers are at the mercy of a system that can make unlimited demands, because the boundaries that define the manager's job are vague and permeable. Managerial jobs themselves are abstract enough to be subject to diverse interpretations. Managers might well ask themselves, have I done my job? If the job is that abstract, then it is also easy for peers and superiors to question what a manager has done and to formulate their own evaluations of his or her performance. Without a collective contract, the manager is vulnerable and dependent; he or she must surrender to the organization's purposes and values. Instead of the feistiness and pluralism that characterize the labor-management relationship, many workers see in the managers' world overbearing demands for ideological unity, loyalty, and the submergence of the self. In other words, managers may seek to control their subordinates, but they are not in control of their own work lives.

There is a breed of American worker who cherishes the autonomy and sense of self-control afforded by his or her skills and protected by the union contract. When these workers contemplate the prospect of the socially integrated high-technology workplace, they feel despair. They anticipate a loss of their unique identities, of freedom and autonomy, and of well-defined rights and responsibilities. They fear that without the traditional sources of protection provided by their job descriptions and their contract, they will become prey to every capricious whim of their superiors. They understand that the managers' world requires the body as a political instrument for self-presentation and influence, but they know that these are talents they have not developed and toward which they feel more than a little distaste.

They say that with this new technology we need a more flexible system, one that will make us competitive. They figure it works for management, so why not for the blue-collar worker. If a manager hasn't saved money, he won't get his extra one hundred dollars a week. If he doesn't produce, he's out the gate. But right now, I don't have anything to worry about except doing my job and doing it well. I don't have to be friends with people in order to move up. I don't have to use anybody. In the management world, you have got to be a salesman to a certain extent. You have go to know how to manipulate the human system. If I am like management, it means I will have to be doubly nice to you whether I like you or not. You have to see everybody as a stepping-stone.

Other operators believe that in a fluid, socially integrated workplace, without clear job descriptions and contracts, they would lose the clarity of rights and obligations that currently offer an important sense of personal control. Without such definition, how will one know what to expect each day and how will it be possible to manage the extent of one's own exertion? They fear flexible arrangements that would change according to the needs of the total organization, in place of discrete task assignments on an individual basis. Their "have-skills-will-travel" image is a kind of emotional insurance policy, but an enterprise centered approach to task distribution would make each individual more dependent upon and integrated with the organization.

They say the new technology will require a flexible system. But under a flexible system, you have no choice but to go where they send you, when they send you. You can get to earn high pay, but you have no choice about what your job is, and you can't have your own job. You never know what to wear to work-do you wear your good Levi's or do you wear your greasy ones?

This statement represents the operators' worst fears-that the loss of control over one's work would invade the most intimate and ordinary details of everyday life. Stumbling around the bedroom on a dark morning, trying to get ready in time to have a cup of coffee before leaving for the plant, the worker must ask, What do I wear? What kind of day should I look forward to? What is in store for me today? Will I feel good about the things I am asked to do? Without the capacity to set one's expectations, it is difficult to locate oneself emotionally. It is easy to feel helpless, as if one is at the mercy of others.

The disquiet these workers feel culminates in a frank concern over power. In a workplace in which divisions among workers and between workers and managers are minimized, where all members are supposed to pull together in the service of the organization, what rights will individuals have? How will these rights be specified? What mechanisms will ensure that individuals have a voice in and influence over the policies and practices that shape their work life? Who will have the power to define the circle of legitimate behavior by which all members are evaluated?

They say that in a new flexible system the criteria for advancement are fitness and ability instead of seniority. But who gets to say you are fit? Who decides what is fit? It will turn out to be that if you are nice to your supervisor, if you do what your supervisor wants you to do, that is what makes you fit.

When these workers consider an organizational system that puts a premium on wholeness, they also see a system that will require manipulation, ingratiation, and conformity. They envision a new approach to the work system without also having considered the changes in the distribution of authority that would be necessary for an informating strategy to fully succeed. However, their concerns do warn of the potential for tyranny in a flexible and socially integrated organization. There are two immediate implications of this warning. First, there presumably will continue to be individuals for whom psychological distance and contractualized responsibilities are very important. To acertain extent, such persons might simply choose to avoid membership in a wholistic informated organization. Viewed in another way, the perspective offered by such individuals probably would be healthy for any learning community. They provide a sensitive barometer for organizational processes that violate respect for the individual or somehow endanger the balance that must exist between individual and organizational interests. A second crucial issue involves the need for mechanisms that can ensure equity and due process within the informated organization. Labor-management contracts grew out of a need for workers to protect themselves from the unilateral authority of their employers. I have already suggested that a new division of labor will not thrive in nor easily tolerate unilateral authority. Nonetheless, informated organizations will have to pay careful attention to developing a constitutional infrastructure that legitimates public debate and mutual influence. The clarity of individual rights within the enterprise is likely to become extremely important to the extent that the learning community requires the participation of the "total person" in its endeavors. Because such a system will exert considerable pressure on an individual's psychological boundaries, it will require mechanisms that can arbitrate competing interpretations of rights and obligations.

The notion of a wholistic informated organization must be qualified in still another way. I have offered a vision of organization in which there are no a priori designations of managers and managed. Instead, people move from the operating core to increasing responsibility and comprehensive influence based upon the degree to which they excel in the skills required by an informated task environment. The intellective skills developed at the data interface provide an important part of the basis for later movement to further rings in the concentric structure. These intellective skills are unlikely to be equally distributed, and the variation in their distribution can become a new source of hierarchical distinctions within even the most fluid organization. For example, at Cedar Bluff, new shades of meaning that began to appear in everyday language foretold the implicit criteria according to which some members would be more highly valued and so implied new sources of conflict as well. These ranking rules were expressed in the metaphorical devices most commonly used in discussions of the new technological environment. Typically, spatial designations were used to convey the worth of people and activities. 11 Consider these interrelated and most often repeated uses of the spatial metaphor:

I. Formal intelligence is up; experiential know-how is down.

A low-IQ operator will not accept a computer on the job. He will ignore the data in favor of his own way of doing things.

At Piney Wood, digital instruments seem like black magic, but at Cedar Bluff, operators are more educated. They average one to two years of college instead of just tenth grade. It's a higher, more advanced type of person.

2. Young is up; old is down.

Younger people find it easier to grasp the computer stuff because they are a higher caliber. Installing new technology is a message to the older operators. It says you must have computer and electronic skills. Some want to retire when they hear this.

We will be displacing older people who cannot qualify for the new jobs we are creating. They have to lift themselves up to standard.

3. Abstract work is up; manual work is down.

With this technology, we are bringing people to a higher level intellectually. It is more demanding, so you have to treat them differently. The physical demands are gone. There are no hammers and wrenches. To deal with problems now is more complex. The computer operators are going to be higher than the man on the floor, because the physical is easier to train that the brain.

We need a higher caliber of people now. They have to use their heads now, not just what comes below the neck. Not just do things.

Cedar Bluff's plant manager recounted their efforts to recruit a "high" caliber work force suited to the demands of "high" technology:

We believed that having a number of people with higher levels of understanding would create group dynamics effects that would result in better problem solving. A high level of knowledge, a high level of intelligence, will result in a higher individual contribution to the business. We need people to be able to optimize our use of the technology.

Workers had been promised a completely computerized operation and believed their jobs would be "push-button easy" with no "nasty work" to leave them soiled and exhausted at the end of the day. The reality was somewhat different, requiring a fair amount of interaction with the operating equipment, particularly in the first years as operators learned their way around the mill. Though their direct involvement with equipment was substantially less than it would have been in a conventional mill, their only frame of reference was that initial promise of "workless" work. Subtle hierarchical pressures began to take shape within this work force that had been so painstakingly recruited and primed for teamwork. People began to believe that the best and the brightest were those who excelled at the data interface, while others gravitated (or were nudged) toward maintenance work. A manager described this subtle sifting process:

We need our most-qualified people manning the computer screens. The people who really know what the equipment feels like and sounds like are becoming second-class citizens. In a traditional system, you spent your whole life developing that feel, and your livelihood depended on it. But the new generation must trust the computer screen. A new pecking order emerges. The computer screen takes more mental skills, and we reward those skills. Maybe the work in the field is intrinsically as valuable, but people around here don't see it that way.

There was a great deal of resentment when a crisis such as a pulp spill pulled the control room operators "out into the process" with tall rubber boots and shovels. Operators called these kinds of tasks "nasty work," a designation that clearly demarcated such activities from their "real" work at the data interface and provided a sense of psychological distance from the role of the laboring body. The special label signaled their preferred self-conception as people who could keep their collars clean:

If there is nasty work here, it means someone made a mistake. We don't like to get nasty. They told us all we would do is run computers.

The resentments that surround physical exertion once defined the boundary between managers and workers but now were felt within the operating work force. Subtle conflicts erupted among operators as they struggled to balance the satisfactions of intellective virtuosity against the comforts of group solidarity. Several operators at Cedar Bluff told of incidents in which they had made a special effort to break new ground in their understanding of the theoretical and practical aspects of the relationships among the process variables under their control. In each case, their discoveries had made it possible to solve a chronic problem and to generate significant savings. These operators became the target for derisive chiding from their peers. One man told of spending several months studying a problem that concerned the operation of the boiler. He conducted experiments, ran tests, monitored data, and read theories. He finally concluded what was wrong with the way they had been controlling the boilers. He wrote up the material in the format of a proficiency exam, supplying all the answers to the questions with the material he had amassed.

I wanted to share it with the team, to learn it better by teaching them. They said, "I don't want to hear that shit!" They didn't even want to see the book.

Another told of a similar experience. His discovery was of such economic value that he was sent to divisional headquarters to present his findings to the engineering staff.

They had me talk to a VP about it, so I got recognized. In fact, it embarrassed me that I got so much recognition. I would have liked less because of the peer pressure I got. None of my peers would go along with it from the beginning. So I had to go to all the shift coordinators instead. My peers treated me like someone who thinks they know it all, a company man. They said I was getting too close to management. That was tough to work with on a day-to-day basis. I learned how to work it now-just play it low key.

One explanation for this behavior is that it repeats the game of self protection so well known to generations of industrial workers. Gloria Schuck has labeled it "intellective rate busting": any worker who excels at the data interface increases the demands on the whole group. 13 Another interpretation is that the barriers that separate managerial from operating jobs have been maintained but simply pushed down to a lower level of the organization. A gulf continues to exist; what has changed is the distribution of members on each side.

The reactions of these disgruntled operators does not take into account the possibility of a change in the logic of imperative control-a logic that sustains the necessity of managerial authority and perpetuates adversarial feeling. In the context of that logic, workers learn that they cannot learn, and managers learn that they must already know. Thus, the interests of each group remain divergent, even as they are locked in the fitful interdependence of imperative control.

In contrast, an informating strategy implies a discontinuity in the logic of imperative control. The new division of learning should organize experience in ways that help perpetuate belief in a synthesis of interests and thus legitimate a learning environment that presupposes relationships among equals. Only under such conditions can knowledge be shared in a way that strengthens the collective effort. Barriers between information-intensive and non-information-intensive responsibilities limit equality and inhibit the spirit of inquiry that animates the informated organization, thus inviting adversarial games of self protection and domination.

The problem appears to be that not every job in even the most informated organization is likely to require intellective skills. Certainly in the near term, most organizations will continue to have some tasks that either are physically demanding or involve highly routinized versions of information handling. How such jobs are distributed becomes a crucial question for any organization that would commit itself to a new division of labor. Does the fact of these persistent differences between tasks lead inevitably to a reproduction of the labor-management caste system?

It is important to reiterate here that the presupposition of relations of equality does not imply a correlative assumption that all organizational members are exactly alike. In any organization, there will be some individuals who reject the demands of intellective work. Not everyone will contain in equal measure the internal commitment, motivation, or cognitive style associated with the responsibilities of life at the data interface. Some will be repelled by the mental stress of such work and find it too perplexing or anxiety-inducing. Others will prefer to conduct their work lives in motion, drawing sustenance from using their bodies to accomplish their tasks. There were many such operators in the Piney Wood Mill. One expressed his anxiety about his place in the emerging organization when he asked:

It is important to reiterate here that the presupposition of relations of equality does not imply a correlative assumption that all organizational members are exactly alike. In any organization, there will be some individuals who reject the demands of intellective work. Not everyone will contain in equal measure the internal commitment, motivation, or cognitive style associated with the responsibilities of life at the data interface. Some will be repelled by the mental stress of such work and find it too perplexing or anxiety-inducing. Others will prefer to conduct their work lives in motion, drawing sustenance from using their bodies to accomplish their tasks. There were many such operators in the Piney Wood Mill. One expressed his anxiety about his place in the emerging organization when he asked:

There was a smaller but still significant subset of workers at Cedar Bluff who felt deeply uncomfortable with the work available to them in the control rooms, despite the importance and prestige attached to it

The work in the control room is mental. It isn't as hard as out in the process, but I would rather be out on the floor working. I am moving out there. I don't like to be cramped up with no room to breathe. If they padded this room, I would have my body indented in the walls from bouncing off them all the time.

While some degree of hierarchy is inevitable in any social group, the values and beliefs that animate these distinctions can operate very differently from the traditional assumptions of imperative control. In the informated organization, there is no reason why these individuals could not elect to align themselves with the jobs best suited to their sensibilities or talents. The difference here lies in the voluntary, nonarbitrary, and reversible nature of their decisions. Instead of facing rigid and practically irreversible designations that are reinforced by virtually every aspect of organizational experience, these individuals could be free to self-select into or out of the more abstract forms of work.

The freedom of self-selection can be maintained as long as institutional arrangements ensure the full participation of these members in the political life of the learning community. There are several mechanisms through which this could be accomplished. First, these individuals, like other members, would have broad access to the streams of circulating information within the organization. Were they to so choose, they could be rotated into positions that provided a greater opportunity for the development of their intellective potential. Second, the close coordination and integration of the various aspects of the production process would require these members to keep abreast of critical information from daily operational issues to changes in the business context, or a new direction in the strategic plan. Finally, the rewards available for their work would be commensurate with their value to the production process and not undervalued as a matter of 14 course.

THE INFORMATED ORGANIZATION AND RECENT TRENDS IN WORK ORGANIZATION

As the years passed at Cedar Bluff, managers began to confront more openly and honestly the challenges to their skills that had been unleashed by the informating process. It was not untypical for organizations pursuing high commitment strategies to feel the strains of participative management and to experience a good bit of conflict concerning the limits of managerial prerogatives. At Cedar Bluff, however, the demands on managers had become relentlessly insistent. The knowledge requirements of the data interface, the vulnerability of plant performance to variations in operator skill and motivation, and the broad accessibility of data had lent new urgency to questions about the skills, roles, and structures that should define the organization. This was vividly illustrated by the turmoil that surrounded the development of a new pay and promotion system for hourly workers. The operators had become increasingly proficient in operating the plant, and after several years of producing pulp at far below the equipment's true capacity, production levels began to climb. As operators' skills improved, so did their dissatisfaction with the pay and promotion system. They believed that it arbitrarily limited the amount of learning for which they could be rewarded. Amid mounting dissension, a committee consisting of operators and managers from the various areas of the plant was appointed to gather data on the problem and to recommend new policies. The plant had reached production levels of more than nine hundred tons a day, but as the committee went into session, operators were heard to say, "That's the last nine-hundred-ton day this plant will see until the pay and promotion problems are resolved."

In the months that followed, their predictions came true; the plant returned to earlier production levels. Cedar Bluff's managers were especially frustrated because they could not identify the causes of the downturn. Only rarely could a manager point to something that operators were doing incorrectly that might be contributing to poor production. They concluded that the disappointing performance could be attributed, not to what operators were doing, but to what they were not doing. The operators' errors were sins of omission-an underutilization of the data interface resulting from their refusal to notice, to think, to explore, to experiment, or to improve. In other words, the power of the new technology was going to waste. Managers felt helpless to alter the situation. It was only when the new pay and promotion system was finally developed and accepted by a majority vote that production levels began to climb once again.

There are several lessons to be learned here. First, the requirements of an informating strategy support existing work-improvement efforts, such as the high commitment approach to work force management, with its emphasis on self-managing teams, participation, and decentralization. Organizations that are already pursuing this approach are more likely to have developed both the ideological context and the social skills necessary to plan and implement an informating strategy. In this regard, Cedar Bluff provides an important contrast not only to Piney Wood but also to organizations, like Global Bank Brazil, that have minimal experience with work-system innovation. Second, the demands for a redistribution of knowledge and the consequent challenge to the managerial role that can be unleashed by the informating process are likely to exacerbate the growing pains associated with participative management and to accelerate the need for positive change. Third, organizational innovations designed to create high commitment work systems typically have focused upon the hourly work force. In most cases, and Cedar Bluff is one example, the managerial hierarchy has remained relatively intact, while team organization and pay-for-skill systems have been designed for the operational work force. In contrast, an informating strategy suggests the need for a more wholistic reconceptualization of the skills, roles, and structures that define the total organization. Partial change efforts, as at Tiger Creek, or technology-driven initiatives, as at Global Bank Brazil, are unlikely to result in the kind of learning environment necessary for an ongoing and robust approach to the informating process. Finally, managing in an informated environment is a delicate human process. The ability to use information for real business benefit is as much a function of the quality of commitment and relationships as it is a function of the quality of intellective skills.

The words of the clerk at Global Bank Brazil continue to echo: "Will things be any different now?" In response, we can say that the opportunity is there, and we now know more about what it will take. An informating strategy requires a comprehensive vision based upon an understanding of the unique capacities of intelligent technology and the opportunity to use the organization to liberate those capacities. It means forging a new logic of technological deployment based upon that vision. A coherent rationale will be necessary, particularly when the tide of conventional thinking and familiar assumptions on this subject can submerge many important choices regarding basic technological design and management. Cedar Bluff's plant manager foresaw this danger:

The technology is going in the direction that says one person operates the master controls. Is the technology right? We don't believe it is, and we are working hard to convince our vendors to leave the design flexible enough so that it does not preclude the uses we want to make of it.

The informated organization does move in another direction. It relies on the human capacities for teaching and learning, criticism and insight. It implies an approach to business improvement that rests upon the improvement and innovation made possible by the enhanced comprehensibility of core processes. It reflects a fertile interdependence between the human mind and some of its most sophisticated productions. As one worker from Tiger Creek mused:

If you don't let people grow and develop and make more decisions, it's a waste of human life-a waste of human potential. If you don't use your knowledge and skill, it's a waste of life. Using the technology to its full potential means using the man to his full potential.

APPENDIX A : THE SCOPE OF INFORMATION TECHNOLOGY IN THE MODERN WORKPLACE

Information technology is a label that reflects the convergence of several streams of technical developments, including microelectronics, computer science, telecommunications, software engineering, and system analysis. It is a technology that dramatically increases the ability to record, store, analyze, and transmit information in ways that permit flexibility, accuracy, immediacy, geographic independence, volume, and complexity. Information technology has a unique capability to restructure operations that depend upon information for the purposes of transaction, record keeping, analysis, control, or communication.

There is hardly a segment of the U.S. economy that has not been penetrated by some form of computer-based technology. The core of this technology is the silicon-integrated circuit, or "chip." The equivalent of hundreds of thousands of transistors can be built on a silicon chip measuring no more than a fraction of an inch. The astonishing reductions in the cost of these microprocessors, coupled with their equally impressive performance levels, have been exhaustively documented. During the past thirty years, the price per second of instruction has decreased dramatically: a computation that now costs one dollar would have cost about $30,000 in 1950. 1 Porter and Millar calculate that the cost of computer power relative to the cost of manual information processing is at least eight thousand times less than the cost thirty years ago. Between 1958 and 1980, the amount of time needed for one electronic operation fell by a factor of 80 million. They also cite Department of Defense studies that show that the error rate in recording data through bar coding is one in 3 million, compared to one error in three hundred manual data entries. 2 During the past fifteen years, the memory capacity of an integrated circuit has increased by a factor of one thousand, as has its reliability. As another writer remarked, "If the automotive industry had paralleled the advances that the computer industry has experienced in the last 25 years, a Rolls Royce would cost 50 cents and would deliver 1 5 million miles to the gallon.

Numerous studies by economists and industry analysts have concluded that computer-based information technologies will profoundly affect the structure of the U.S. economy. 4 One analyst estimates that in 1980, approximately 10 million Americans interacted daily with a video display terminal and that this number would increase to 2 5 million by 1 990. 5 Another estimates that by the year 1 990, 50 million American office workers will spend a significant portion of their workday interacting with a computer terminal of some sort. 6 Another expert predicts that in 1990, 65 percent of all professional, managerial, technical, and administrative workers (a group that now constitutes almost half the labor force) will depend upon computer-based workstations. 7 The Congressional Office of Technology Assessment has predicted that there will be at least one computer terminal for every two or three office workers by 1990. 8 One recent survey of 5 30 employees representing every organizational level in twenty-six companies offers some accurate, if more limited, findings. Respondents were divided according to occupational groups: executives, managers, professionals, technicians, secretaries, and clerks. Averaging across all categories, researchers found that 67 percent of all the respondents interacted directly with a computer during the regular course of their work and that 26 percent expected to do so in the near future. When executives (for whom the figures were 36 percent and 46 percent, respectively) are eliminated from this average, the figure for current usage jumps to 74 percent. Of these same respondents, about half reported using the computer during 30 percent or less of their working time, another quarter of the group reported spending up to 70 percent of their time at the terminal, while the final quarter spent up to 100 percent of their workday at the computer terminal. Such figures suggest the degree to which information technology is affecting everyday life across a broad spectrum of the work force.

THE SERVICE SECTOR

Information technology has different applications in the service and manufacturing sectors of the economy. In the service sector, the technology has been used primarily to meet the mushrooming demands of handling information. Businesses now are faced with the task of controlling 400 billion documents, a number that is expected to increase at the rate of 72 billion per year. 10 As the technology develops, clerks working with documents in the back office can enter data directly into terminals linked to a computer mainframe, which does the actual processing. In many cases, even the documents have disappeared; clerks are able to perform all their transactions through the electronic medium. Such on-line applications began to be widespread in the early 1980s and have become a prominent trend in most large service organizations.

The early 1980s also saw new applications of information technology for professionals, managers, and technical workers. The introduction of small, stand-alone word processors, microcomputers, and personal computers made it easier for nonspecialists to use databases, to manipulate text and quantitative data, to generate tables and graphic displays, to utilize analytical software, and to communicate with one another through a computer network. Today, microcomputers are being linked to one another as well as to central computers so that they can be used independently or in conjunction with corporate data bases. These emerging systems also can be linked to external data bases and communications networks; further, they can cross organizational and national boundaries.

Most large information-processing organizations, such as banks and insurance companies, still require a sizeable clerical work force to enter data and perform routine transactions on a computer system. Many such organizations are searching for ways to circumvent this massive clerical effort and at the same time increase the continuity of their operations. An increasingly popular solution is to incorporate the data-entry function in operations that are external to the organization. For example, when a customer uses an automated teller machine, the data-entry function is accomplished automatically without clerical input. A hospital computer may send bills directly to the health insurer's computer, which in turn instructs the bank's computer to transfer funds. An interorganizational computer system thus can eliminate routine clerical work in several organizations.

Technologies that were once relatively distinct now have begun to converge. The functions once accomplished by typewriters, printing presses, copying machines, telephones, files, calculators, and mail sorting systems, are becoming either subsumed within or linked to the functioning of the comprehensive computer-based network. Software is more powerful and easier to use. New procedures allow people to interact with the computer in ways that encourage more familiarity and immediacy, such as touch-sensitive screens and voice-activated II programs.

THE MANUFACTURING SECTOR

Applications of computer technology in the manufacturing sector have developed along distinct lines in discrete parts manufacturing (for example, automobiles, farm equipment, electronics) and in the continuous-process industries (for example, oil refineries, chemical processors, food and beverages, paper and pulp), which each face different problems in manufacturing process. Continuous-process production typically involves a flow of material that can pass through several stages before emerging in its final form. The production process manipulates the composition of materials by chemical reaction, purification, and blending of component materials. At each stage, different operations are performed, such as heating, cooling, mixing, chemical reaction, distillation, drying, or pressurization. This requires continual measurement and control of variables like time, temperature, raw material characteristics, steam pressure, chemical levels, densities, viscosities, liquid levels, flow rates, et cetera. Twenty percent of all industrial computers, and 40 percent or more of all minicomputers and microcomputers, are accounted for by continuous-process applications designed for monitoring, analysis, control, and optimization. Process computers evolved from simple data recording devices: they collected real-time operational data and set off alarms under critical conditions. The next phase of development produced "open loop" systems, in which operators rely on either first-hand observations of the process or data generated by microprocessor-based sensors and programmable logic controllers built into operating equipment. With this information, they can use the computer system for mathematical analysis to help them adjust process conditions. The computer also becomes the medium through which operators can manipulate process variables and parameters to meet the desired conditions. Most recent developments include supervisory process-control systems that can be programmed to receive information directly from instruments monitoring the process, to set control points, to perform computations, and to adjust control variables to continually approximate optimum levels of functioning. Theoretically, such systems mimimize the need for operator involvement, except in upset conditions.

As plants apply these control systems to their operations, the amount of physical interaction with the production process is reduced and workers typically operate from remote control rooms, where they monitor video terminals that display data reflecting the state of the production process. In some process industries, such as oil refining or steel production, where the various steps of the conversion from raw material to output are well understood, there has been considerable progress in developing supervisory control systems. These were among the first industries to comprehensively apply microprocessor-based control technology. In other process industries, such as paper and pulp or food and beverages, the manufacturing process has not yet been entirely explicated, and there are no adequate sensing devices to measure all key variables. These industries have been slower to adopt the technology and face more difficulty in programming supervisory control.

In discrete parts manufacturing, the problems are more geometric in nature, involving the placement of parts and equipment in relation to one another, as well as their movement from one stage of assembly to another. Typical applications of information technology in discrete parts manufacturing include computer-aided design (drafting and engineering), computer-aided manufacturing (robots and numerically controlled machine tools), flexible manufacturing systems, automated material handling, and automated storage-and-retrieval systems. Computer-aided design systems facilitate the use of previous designs and allow more rapid design changes. They can improve the design process by allowing engineers to try out a dozen or a hundred different variations, when they previously might have been limited to building three or four prototype models. It is estimated that in 1983, there were thirty-two thousand computer-aided design workstations being used in the United States.

Robots are mechanical manipulators that can be programmed to move workpieces or tools along a prescribed path. While most robots today can perform only relatively well-defined and repetitive tasks, efforts are under way to incorporate more intelligence and sensory capacities within these machines. There are many differing estimates as to the extent of robotization in manufacturing. The Robot Institute of America indicates that in 1983, 66 percent (31,900) of the world's robots were operating in Japan and 1 3 percent ( 6,301) were in the United States. 14 Nobel-prize-winning economist Wassily Leontief has predicted a 30 to 40 percent annual growth rate in the market for industrial robots between 198 5 and 1990.

Computer-numerically-controlled machine tools fashion metal according to programmed instructions that indicate the desired dimensions of a part and the sequence to be followed in the machining process. Since the late 1 970s, these machines have been equipped with microprocessors or dedicated minicomputers and frequently include a screen and keyboard for writing or editing the programs that guide the equipment. These devices are the basis for "direct numerical control machines," in which a larger mini-computer or mainframe computer is used to program and run more than one numerically controlled tool simultaneously. The Congressional Office of Technology Assessment reports that as the price of small computers has declined, these machine tools are being equipped with microcomputers that can be linked to one another and to a central controlling computer, thus creating a hierarchy of computer control. 16 Many observers have remarked upon the relatively slow diffusion of these applications due to their high capital cost and the bottlenecks in developing and maintaining software programs. In 1983, numerically controlled machine tools represented only about 5 percent of the machine tools in U.S. metalworking, but this may change rapidly. 17 For example, General Motors Corporation has stated that by the end of the 1980s, 90 percent of all new capital investments will be in computer numerically controlled machines.

Flexible manufacturing systems integrate these more discrete applications of technology. They consist of computer-controlled machining centers that sculpt complicated metal parts at high speed and with great reliability, robots that handle the parts, and remotely guided carts that deliver materials. These components are linked by computer-based controls that dictate what will happen at each stage of the manufacturing sequence. Many consider the great advantage of these systems to be their ability to achieve low-cost production in small volumes, without having to rely on the economics of scale associated with mass production. "A flexible automation system can turn out a small batch or even a single copy of a product as efficiently as a production line designed to turn out a million identical items." 19 These systems are complex and costly, and thus still relatively rare. Reliable statistics are difficult to obtain because of conflicting definitions of precisely what level of integration and control constitutes "flexibility." However, Fortune magazine recently counted thirty such systems operating in the United States and considerably more in Japan. According to their report, one Japanese firm alone, Toyoda Machine Tool Co., has thirty such systems in operation. 20 Despite the still-modest number of such systems in the United States, Fortune estimates that by 1990, the sales of equipment to support flexible manufacturing-robots, computer controls, material-handling devices, et cetera-will rise to $30 billion annually from 1982 levels of $4 billion.

While information technologies in these two manufacturing domains- discrete parts and continuous-process-have developed separately in order to address distinct types of problems, their differences are increasingly diminished by new developments that use computer systems for comprehensive production management. This approach, known as computer-integrated manufacturing, increases the continuity of the production process in discrete parts manufacturing, thus bringing it closer to the "optimal model" of manufacturing in which continuity and controllability are maximized. 21 Under these conditions, workers' tasks begin to look very similar to those of operators in the continuous process environment, as they come to emphasize monitoring and control (though the types of variables and procedures continue to differ). A recent study published by the National Academy of Sciences points to the integration of computer-aided design and manufacturing with manufacturing resource planning (software that translates demand for products into the parts needed to produce them and then orders the parts from inventory or from suppliers) and computer-aided process planning (software that routes parts through the factory to maximize operating time and to eliminate bottlenecks). "The four technologies are increasingly 'speaking' to each other through local-area networks, and formerly isolated applications are being linked as computer integrated manufacturing. 

Experts continue to disagree on the ultimate consequences of these developments for employment in the manufacturing sector. Some believe that computer-integrated manufacturing will provide the basis for nearly unmanned factories; others insist that automation rarely can be complete and that people will be needed to monitor, control, maintain, manage, and plan these processes, although at lower levels of employment than have characterized older technologies. Whether the unmanned factory is a likely scenario in some cases or not, the coming decade will continue to see manufacturing operations that depend upon people, not just computers. These organizations will become more alike as they are able to increase the continuity and controllability of production through computer integration.

APPENDIX B: NOTES ON FIELD-RESEARCH METHODOLOGY

Behind every method lies a belief. Researchers must have a theory of reality and of how that reality might surrender itself to their knowledge-seeking efforts. These epistemological fundamentals are subject to debate but not to ultimate proof. Each epistemology implies a set of methods uniquely suited to it, and these methods will render the qualities of data that reflect a researcher's assessment of what is vital. I believe that researchers ought to indicate something about their beliefs, so that readers can have access to the intellectual choices that are embedded in the research effort.






/ 015. Bostrom, Nick. Superintelligence: Paths, Dangers, Strategies, 2014.

/ 016. Daughtery, Paul R. et al. "Reimagining Work in the Age of AI." 2023.

/ 017. Crawford, Kate. Atlas of AI, 2021.

/ 018. Russell, Stuart. Human Compatible Artificial Intelligence and the Problem of Control, 2019.

/ 019. Srineck, Nick et al. Inventing the Future Postcapitalism and a World Without Work, 2015.

/ 020. Brynjolfsson, Erik et al. The Second Machine Age, 2014.

/ 021. Weil, Elizabeth. You Are Not a a Parrot And a chatbot is not a human. And a linguist named Emily M. Bender is very worried what will happen when we forget this, 2023.

/ 022. Kurzweil, Ray. "The Age of Intelligent Machines: Can Machines Think?" 1990.

/ 023. O'Malley JP. "On Consciousness in Silicon Systems," 2023.

/ 024. Bridle, James. "Rise of the machines: has technology evolved beyond our control?" 2018.

/ 025. "Policymaking in the Pause," 2023.

/ 026. Jackson, Lauren. "What if A.I. Sentience Is a Question of Degree?" 2023.

/ 027. Lee, Kai-Fu. AI Superpowers: China, Silicon Valley, and the New World Order, 2018.

/ 028. Reese, Hope. "A Human-Centered Approach to the AI Revolution," 2022.

/ 029. Sanders, Nathan E. et al. "How AI could write our laws," 2023.

/ 030. HAI Stanford University, Artificial Intelligence Index Report, 2023.

/ 031. Bubeck, Sebastien et al. "Sparks of Artificial General Intelligence: Early experiments with GPT-4," 2023.

