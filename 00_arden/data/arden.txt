/ 000. Turing, Alan M. "Computing Machinery and Intelligence," 1950.


1. The Imitation Game

I propose to consider the question, "Can machines think?" This should begin with definitions of the meaning of the terms "machine" and "think." The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words "machine" and "think" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, "Can machines think?" is to be sought in a statistical survey such as a Gallup poll. But this is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.

The new form of the problem can be described in terms of a game which we call the 'imitation game." It is played with three people, a man (A), a woman (B), and an interrogator (C) who may be of either sex. The interrogator stays in a room apart front the other two. The object of the game for the interrogator is to determine which of the other two is the man and which is the woman. He knows them by labels X and Y, and at the end of the game he says either "X is A and Y is B" or "X is B and Y is A." The interrogator is allowed to put questions to A and B thus:

C: Will X please tell me the length of his or her hair?

Now suppose X is actually A, then A must answer. It is A's object in the game to try and cause C to make the wrong identification. His answer might therefore be:

"My hair is shingled, and the longest strands are about nine inches long."

In order that tones of voice may not help the interrogator the answers should be written, or better still, typewritten. The ideal arrangement is to have a teleprinter communicating between the two rooms. Alternatively the question and answers can be repeated by an intermediary. The object of the game for the third player (B) is to help the interrogator. The best strategy for her is probably to give truthful answers. She can add such things as "I am the woman, don't listen to him!" to her answers, but it will avail nothing as the man can make similar remarks.

We now ask the question, "What will happen when a machine takes the part of A in this game?" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, "Can machines think?"

2. Critique of the New Problem
As well as asking, "What is the answer to this new form of the question," one may ask, "Is this new question a worthy one to investigate?" This latter question we investigate without further ado, thereby cutting short an infinite regress.
The new problem has the advantage of drawing a fairly sharp line between the physical and the intellectual capacities of a man. No engineer or chemist claims to be able to produce a material which is indistinguishable from the human skin. It is possible that at some time this might be done, but even supposing this invention available we should feel there was little point in trying to make a "thinking machine" more human by dressing it up in such artificial flesh. The form in which we have set the problem reflects this fact in the condition which prevents the interrogator from seeing or touching the other competitors, or hearing -their voices. Some other advantages of the proposed criterion may be shown up by specimen questions and answers. Thus:
Q: Please write me a sonnet on the subject of the Forth Bridge.
A : Count me out on this one. I never could write poetry.
Q: Add 34957 to 70764.
A: (Pause about 30 seconds and then give as answer) 105621.
Q: Do you play chess?
A: Yes.
Q: I have K at my K1, and no other pieces. You have only K at K6 and R at R1. It is your move. What do you play?
A: (After a pause of 15 seconds) R-R8 mate.
The question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include. We do not wish to penalise the machine for its inability to shine in beauty competitions, nor to penalise a man for losing in a race against an aeroplane. The conditions of our game make these disabilities irrelevant. The "witnesses" can brag, if they consider it advisable, as much as they please about their charms, strength or heroism, but the interrogator cannot demand practical demonstrations.
The game may perhaps be criticised on the ground that the odds are weighted too heavily against the machine. If the man were to try and pretend to be the machine he would clearly make a very poor showing. He would be given away at once by slowness and inaccuracy in arithmetic. May not machines carry out something which ought to be described as thinking but which is very different from what a man does? This objection is a very strong one, but at least we can say that if, nevertheless, a machine can be constructed to play the imitation game satisfactorily, we need not be troubled by this objection.

It might be urged that when playing the "imitation game" the best strategy for the machine may possibly be something other than imitation of the behaviour of a man. This may be, but I think it is unlikely that there is any great effect of this kind. In any case there is no intention to investigate here the theory of the game, and it will be assumed that the best strategy is to try to provide answers that would naturally be given by a man.
3. The Machines Concerned in the GameThe question which we put in 1 will not be quite definite until we have specified what we mean by the word "machine." It is natural that we should wish to permit every kind of engineering technique to be used in our machines. We also wish to allow the possibility than an engineer or team of engineers may construct a machine which works, but whose manner of operation cannot be satisfactorily described by its constructors because they have applied a method which is largely experimental. Finally, we wish to exclude from the machines men born in the usual manner. It is difficult to frame the definitions so as to satisfy these three conditions. One might for instance insist that the team of engineers should be all of one sex, but this would not really be satisfactory, for it is probably possible to rear a complete individual from a single cell of the skin (say) of a man. To do so would be a feat of biological technique deserving of the very highest praise, but we would not be inclined to regard it as a case of "constructing a thinking machine." This prompts us to abandon the requirement that every kind of technique should be permitted. We are the more ready to do so in view of the fact that the present interest in "thinking machines" has been aroused by a particular kind of machine, usually called an "electronic computer" or "digital computer." Following this suggestion we only permit digital computers to take part in our game.
This restriction appears at first sight to be a very drastic one. I shall attempt to show that it is not so in reality. To do this necessitates a short account of the nature and properties of these computers.
It may also be said that this identification of machines with digital computers, like our criterion for "thinking," will only be unsatisfactory if (contrary to my belief), it turns out that digital computers are unable to give a good showing in the game.
There are already a number of digital computers in working order, and it may be asked, "Why not try the experiment straight away? It would be easy to satisfy the conditions of the game. A number of interrogators could be used, and statistics compiled to show how often the right identification was given." The short answer is that we are not asking whether all digital computers would do well in the game nor whether the computers at present available would do well, but whether there are imaginable computers which would do well. But this is only the short answer. We shall see this question in a different light later.

4. Digital Computers
The idea behind digital computers may be explained by saying that these machines are intended to carry out any operations which could be done by a human computer. The human computer is supposed to be following fixed rules; he has no authority to deviate from them in any detail. We may suppose that these rules are supplied in a book, which is altered whenever he is put on to a new job. He has also an unlimited supply of paper on which he does his calculations. He may also do his multiplications and additions on a "desk machine," but this is not important.
If we use the above explanation as a definition we shall be in danger of circularity of argument. We avoid this by giving an outline. of the means by which the desired effect is achieved. A digital computer can usually be regarded as consisting of three parts:
(i) Store.
(ii) Executive unit.
(iii) Control.
The store is a store of information, and corresponds to the human computer's paper, whether this is the paper on which he does his calculations or that on which his book of rules is printed. In so far as the human computer does calculations in his bead a part of the store will correspond to his memory.
The executive unit is the part which carries out the various individual operations involved in a calculation. What these individual operations are will vary from machine to machine. Usually fairly lengthy operations can be done such as "Multiply 3540675445 by 7076345687" but in some machines only very simple ones such as "Write down 0" are possible.
We have mentioned that the "book of rules" supplied to the computer is replaced in the machine by a part of the store. It is then called the "table of instructions." It is the duty of the control to see that these instructions are obeyed correctly and in the right order. The control is so constructed that this necessarily happens.
The information in the store is usually broken up into packets of moderately small size. In one machine, for instance, a packet might consist of ten decimal digits. Numbers are assigned to the parts of the store in which the various packets of information are stored, in some systematic manner. A typical instruction might say-
"Add the number stored in position 6809 to that in 4302 and put the result back into the latter storage position."
Needless to say it would not occur in the machine expressed in English. It would more likely be coded in a form such as 6809430217. Here 17 says which of various possible operations is to be performed on the two numbers. In this case the)e operation is that described above, viz., "Add the number. . . ." It will be noticed that the instruction takes up 10 digits and so forms one packet of information, very conveniently. The control will normally take the instructions to be obeyed in the order of the positions in which they are stored, but occasionally an instruction such as"Now obey the instruction stored in position 5606, and continue from there"may be encountered, or again"If position 4505 contains 0 obey next the instruction stored in 6707, otherwise continue straight on."Instructions of these latter types are very important because they make it possible for a sequence of operations to be replaced over and over again until some condition is fulfilled, but in doing so to obey, not fresh instructions on each repetition, but the same ones over and over again. To take a domestic analogy. Suppose Mother wants Tommy to call at the cobbler's every morning on his way to school to see if her shoes are done, she can ask him afresh every morning. Alternatively she can stick up a notice once and for all in the hall which he will see when he leaves for school and which tells him to call for the shoes, and also to destroy the notice when he comes back if he has the shoes with him.
The reader must accept it as a fact that digital computers can be constructed, and indeed have been constructed, according to the principles we have described, and that they can in fact mimic the actions of a human computer very closely.
The book of rules which we have described our human computer as using is of course a convenient fiction. Actual human computers really remember what they have got to do. If one wants to make a machine mimic the behaviour of the human computer in some complex operation one has to ask him how it is done, and then translate the answer into the form of an instruction table. Constructing instruction tables is usually described as "programming." To "programme a machine to carry out the operation A" means to put the appropriate instruction table into the machine so that it will do A.
An interesting variant on the idea of a digital computer is a "digital computer with a random element." These have instructions involving the throwing of a die or some equivalent electronic process; one such instruction might for instance be, "Throw the die and put the-resulting number into store 1000." Sometimes such a machine is described as having free will (though I would not use this phrase myself), It is not normally possible to determine from observing a machine whether it has a random element, for a similar effect can be produced by such devices as making the choices depend on the digits of the decimal for .
Most actual digital computers have only a finite store. There is no theoretical difficulty in the idea of a computer with an unlimited store. Of course only a finite part can have been used at any one time. Likewise only a finite amount can have been constructed, but we can imagine more and more being added as required. Such computers have special theoretical interest and will be called infinitive capacity computers.
The idea of a digital computer is an old one. Charles Babbage, Lucasian Professor of Mathematics at Cambridge from 1828 to 1839, planned such a machine, called the Analytical Engine, but it was never completed. Although Babbage had all the essential ideas, his machine was not at that time such a very attractive prospect. The speed which would have been available would be definitely faster than a human computer but something like I 00 times slower than the Manchester machine, itself one of the slower of the modern machines, The storage was to be purely mechanical, using wheels and cards.
The fact that Babbage's Analytical Engine was to be entirely mechanical will help us to rid ourselves of a superstition. Importance is often attached to the fact that modern digital computers are electrical, and that the nervous system also is electrical. Since Babbage's machine was not electrical, and since all digital computers are in a sense equivalent, we see that this use of electricity cannot be of theoretical importance. Of course electricity usually comes in where fast signalling is concerned, so that it is not surprising that we find it in both these connections. In the nervous system chemical phenomena are at least as important as electrical. In certain computers the storage system is mainly acoustic. The feature of using electricity is thus seen to be only a very superficial similarity. If we wish to find such similarities we should took rather for mathematical analogies of function.
5. Universality of Digital ComputersThe digital computers considered in the last section may be classified amongst the "discrete-state machines." These are the machines which move by sudden jumps or clicks from one quite definite state to another. These states are sufficiently different for the possibility of confusion between them to be ignored. Strictly speaking there, are no such machines. Everything really moves continuously. But there are many kinds of machine which can profitably be thought of as being discrete-state machines. For instance in considering the switches for a lighting system it is a convenient fiction that each switch must be definitely on or definitely off. There must be intermediate positions, but for most purposes we can forget about them. As an example of a discrete-state machine we might consider a wheel which clicks round through 120 once a second, but may be stopped by a ]ever which can be operated from outside; in addition a lamp is to light in one of the positions of the wheel. This machine could be described abstractly as follows. The internal state of the machine (which is described by the position of the wheel) may be q1, q2 or q3. There is an input signal i0. or i1 (position of ]ever). The internal state at any moment is determined by the last state and input signal according to the table
(TABLE DELETED)The output signals, the only externally visible indication of the internal state (the light) are described by the table

State q1 q2 q3
output o0 o0 o1This example is typical of discrete-state machines. They can be described by such tables provided they have only a finite number of possible states.It will seem that given the initial state of the machine and the input signals it is always possible to predict all future states, This is reminiscent of Laplace's view that from the complete state of the universe at one moment of time, as described by the positions and velocities of all particles, it should be possible to predict all future states. The prediction which we are considering is, however, rather nearer to practicability than that considered by Laplace. The system of the "universe as a whole" is such that quite small errors in the initial conditions can have an overwhelming effect at a later time. The displacement of a single electron by a billionth of a centimetre at one moment might make the difference between a man being killed by an avalanche a year later, or escaping. It is an essential property of the mechanical systems which we have called "discrete-state machines" that this phenomenon does not occur. Even when we consider the actual physical machines instead of the idealised machines, reasonably accurate knowledge of the state at one moment yields reasonably accurate knowledge any number of steps later.
As we have mentioned, digital computers fall within the class of discrete-state machines. But the number of states of which such a machine is capable is usually enormously large. For instance, the number for the machine now working at Manchester is about 2 165,000, i.e., about 10 50,000. Compare this with our example of the clicking wheel described above, which had three states. It is not difficult to see why the number of states should be so immense. The computer includes a store corresponding to the paper used by a human computer. It must be possible to write into the store any one of the combinations of symbols which might have been written on the paper. For simplicity suppose that only digits from 0 to 9 are used as symbols. Variations in handwriting are ignored. Suppose the computer is allowed 100 sheets of paper each containing 50 lines each with room for 30 digits. Then the number of states is 10 100x50x30 i.e., 10 150,000 . This is about the number of states of three Manchester machines put together. The logarithm to the base two of the number of states is usually called the "storage capacity" of the machine. Thus the Manchester machine has a storage capacity of about 165,000 and the wheel machine of our example about 1.6. If two machines are put together their capacities must be added to obtain the capacity of the resultant machine. This leads to the possibility of statements such as "The Manchester machine contains 64 magnetic tracks each with a capacity of 2560, eight electronic tubes with a capacity of 1280. Miscellaneous storage amounts to about 300 making a total of 174,380."
Given the table corresponding to a discrete-state machine it is possible to predict what it will do. There is no reason why this calculation should not be carried out by means of a digital computer. Provided it could be carried out sufficiently quickly the digital computer could mimic the behavior of any discrete-state machine. The imitation game could then be played with the machine in question (as B) and the mimicking digital computer (as A) and the interrogator would be unable to distinguish them. Of course the digital computer must have an adequate storage capacity as well as working sufficiently fast. Moreover, it must be programmed afresh for each new machine which it is desired to mimic.
This special property of digital computers, that they can mimic any discrete-state machine, is described by saying that they are universal machines. The existence of machines with this property has the important consequence that, considerations of speed apart, it is unnecessary to design various new machines to do various computing processes. They can all be done with one digital computer, suitably programmed for each case. It 'ill be seen that as a consequence of this all digital computers are in a sense equivalent.
We may now consider again the point raised at the end of §3. It was suggested tentatively that the question, "Can machines think?" should be replaced by "Are there imaginable digital computers which would do well in the imitation game?" If we wish we can make this superficially more general and ask "Are there discrete-state machines which would do well?" But in view of the universality property we see that either of these questions is equivalent to this, "Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?"
6. Contrary Views on the Main Question
We may now consider the ground to have been cleared and we are ready to proceed to the debate on our question, "Can machines think?" and the variant of it quoted at the end of the last section. We cannot altogether abandon the original form of the problem, for opinions will differ as to the appropriateness of the substitution and we must at least listen to what has to be said in this connexion.
It will simplify matters for the reader if I explain first my own beliefs in the matter. Consider first the more accurate form of the question. I believe that in about fifty years' time it will be possible, to programme computers, with a storage capacity of about 109, to make them play the imitation game so well that an average interrogator will not have more than 70 per cent chance of making the right identification after five minutes of questioning. The original question, "Can machines think?" I believe to be too meaningless to deserve discussion. Nevertheless I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted. I believe further that no useful purpose is served by concealing these beliefs. The popular view that scientists proceed inexorably from well-established fact to well-established fact, never being influenced by any improved conjecture, is quite mistaken. Provided it is made clear which are proved facts and which are conjectures, no harm can result. Conjectures are of great importance since they suggest useful lines of research.

I now proceed to consider opinions opposed to my own.
(1) The Theological ObjectionThinking is a function of man's immortal soul. God has given an immortal soul to every man and woman, but not to any other animal or to machines. Hence no animal or machine can think.I am unable to accept any part of this, but will attempt to reply in theological terms. I should find the argument more convincing if animals were classed with men, for there is a greater difference, to my mind, between the typical animate and the inanimate than there is between man and the other animals. The arbitrary character of the orthodox view becomes clearer if we consider how it might appear to a member of some other religious community. How do Christians regard the Moslem view that women have no souls? But let us leave this point aside and return to the main argument. It appears to me that the argument quoted above implies a serious restriction of the omnipotence of the Almighty. It is admitted that there are certain things that He cannot do such as making one equal to two, but should we not believe that He has freedom to confer a soul on an elephant if He sees fit? We might expect that He would only exercise this power in conjunction with a mutation which provided the elephant with an appropriately improved brain to minister to the needs of this sort[. An argument of exactly similar form may be made for the case of machines. It may seem different because it is more difficult to "swallow." But this really only means that we think it would be less likely that He would consider the circumstances suitable for conferring a soul. The circumstances in question are discussed in the rest of this paper. In attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing .mansions for the souls that He creates.
However, this is mere speculation. I am not very impressed with theological arguments whatever they may be used to support. Such arguments have often been found unsatisfactory in the past. In the time of Galileo it was argued that the texts, "And the sun stood still . . . and hasted not to go down about a whole day" (Joshua x. 13) and "He laid the foundations of the earth, that it should not move at any time" (Psalm cv. 5) were an adequate refutation of the Copernican theory. With our present knowledge such an argument appears futile. When that knowledge was not available it made a quite different impression.
(2) The "Heads in the Sand" ObjectionThe consequences of machines thinking would be too dreadful. Let us hope and believe that they cannot do so."This argument is seldom expressed quite so openly as in the form above. But it affects most of us who think about it at all. We like to believe that Man is in some subtle way superior to the rest of creation. It is best if he can be shown to be necessarily superior, for then there is no danger of him losing his commanding position. The popularity of the theological argument is clearly connected with this feeling. It is likely to be quite strong in intellectual people, since they value the power of thinking more highly than others, and are more inclined to base their belief in the superiority of Man on this power.I do not think that this argument is sufficiently substantial to require refutation. Consolation would be more appropriate: perhaps this should be sought in the transmigration of souls.
(3) The Mathematical Objection
There are a number of results of mathematical logic which can be used to show that there are limitations to the powers of discrete-state machines. The best known of these results is known as Godel's theorem ( 1931 ) and shows that in any sufficiently powerful logical system statements can be formulated which can neither be proved nor disproved within the system, unless possibly the system itself is inconsistent. There are other, in some respects similar, results due to Church (1936), Kleene (1935), Rosser, and Turing (1937). The latter result is the most convenient to consider, since it refers directly to machines, whereas the others can only be used in a comparatively indirect argument: for instance if Godel's theorem is to be used we need in addition to have some means of describing logical systems in terms of machines, and machines in terms of logical systems. The result in question refers to a type of machine which is essentially a digital computer with an infinite capacity. It states that there are certain things that such a machine cannot do. If it is rigged up to give answers to questions as in the imitation game, there will be some questions to which it will either give a wrong answer, or fail to give an answer at all however much time is allowed for a reply. There may, of course, be many such questions, and questions which cannot be answered by one machine may be satisfactorily answered by another. We are of course supposing for the present that the questions are of the kind to which an answer "Yes" or "No" is appropriate, rather than questions such as "What do you think of Picasso?" The questions that we know the machines must fail on are of this type, "Consider the machine specified as follows. . . . Will this machine ever answer 'Yes' to any question?" The dots are to be replaced by a description of some machine in a standard form, which could be something like that used in §5. When the machine described bears a certain comparatively simple relation to the machine which is under interrogation, it can be shown that the answer is either wrong or not forthcoming. This is the mathematical result: it is argued that it proves a disability of machines to which the human intellect is not subject.
The short answer to this argument is that although it is established that there are limitations to the Powers If any particular machine, it has only been stated, without any sort of proof, that no such limitations apply to the human intellect. But I do not think this view can be dismissed quite so lightly. Whenever one of these machines is asked the appropriate critical question, and gives a definite answer, we know that this answer must be wrong, and this gives us a certain feeling of superiority. Is this feeling illusory? It is no doubt quite genuine, but I do not think too much importance should be attached to it. We too often give wrong answers to questions ourselves to be justified in being very pleased at such evidence of fallibility on the part of the machines. Further, our superiority can only be felt on such an occasion in relation to the one machine over which we have scored our petty triumph. There would be no question of triumphing simultaneously over all machines. In short, then, there might be men cleverer than any given machine, but then again there might be other machines cleverer again, and so on.
Those who hold to the mathematical argument would, I think, mostly he willing to accept the imitation game as a basis for discussion, Those who believe in the two previous objections would probably not be interested in any criteria.
(4) The Argument from Consciousness
This argument is very, well expressed in Professor Jefferson's Lister Oration for 1949, from which I quote. "Not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain-that is, not only write it but know that it had written it. No mechanism could feel (and not merely artificially signal, an easy contrivance) pleasure at its successes, grief when its valves fuse, be warmed by flattery, be made miserable by its mistakes, be charmed by sex, be angry or depressed when it cannot get what it wants."
This argument appears to be a denial of the validity of our test. According to the most extreme form of this view the only way by which one could be sure that machine thinks is to be the machine and to feel oneself thinking. One could then describe these feelings to the world, but of course no one would be justified in taking any notice. Likewise according to this view the only way to know that a man thinks is to be that particular man. It is in fact the solipsist point of view. It may be the most logical view to hold but it makes communication of ideas difficult. A is liable to believe "A thinks but B does not" whilst B believes "B thinks but A does not." instead of arguing continually over this point it is usual to have the polite convention that everyone thinks.
I am sure that Professor Jefferson does not wish to adopt the extreme and solipsist point of view. Probably he would be quite willing to accept the imitation game as a test. The game (with the player B omitted) is frequently used in practice under the name of viva voce to discover whether some one really understands something or has "learnt it parrot fashion." Let us listen in to a part of such a viva voce:
Interrogator: In the first line of your sonnet which reads "Shall I compare thee to a summer's day," would not "a spring day" do as well or better?
Witness: It wouldn't scan.
Interrogator: How about "a winter's day," That would scan all right.
Witness: Yes, but nobody wants to be compared to a winter's day.

Interrogator: Would you say Mr. Pickwick reminded you of Christmas?
Witness: In a way.
Interrogator: Yet Christmas is a winter's day, and I do not think Mr. Pickwick would mind the comparison.
Witness: I don't think you're serious. By a winter's day one means a typical winter's day, rather than a special one like Christmas.
And so on, What would Professor Jefferson say if the sonnet-writing machine was able to answer like this in the viva voce? I do not know whether he would regard the machine as "merely artificially signalling" these answers, but if the answers were as satisfactory and sustained as in the above passage I do not think he would describe it as "an easy contrivance." This phrase is, I think, intended to cover such devices as the inclusion in the machine of a record of someone reading a sonnet, with appropriate switching to turn it on from time to time.
In short then, I think that most of those who support the argument from consciousness could be persuaded to abandon it rather than be forced into the solipsist position. They will then probably be willing to accept our test.
I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.
(5) Arguments from Various Disabilities
These arguments take the form, "I grant you that you can make machines do all the things you have mentioned but you will never be able to make one to do X." Numerous features X are suggested in this connexion I offer a selection:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humour, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make some one fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.
No support is usually offered for these statements. I believe they are mostly founded on the principle of scientific induction. A man has seen thousands of machines in his lifetime. From what he sees of them he draws a number of general conclusions. They are ugly, each is designed for a very limited purpose, when required for a minutely different purpose they are useless, the variety of behaviour of any one of them is very small, etc., etc. Naturally he concludes that these are necessary properties of machines in general. Many of these limitations are associated with the very small storage capacity of most machines. (I am assuming that the idea of storage capacity is extended in some way to cover machines other than discrete-state machines. The exact definition does not matter as no mathematical accuracy is claimed in the present discussion,) A few years ago, when very little had been heard of digital computers, it was possible to elicit much incredulity concerning them, if one mentioned their properties without describing their construction. That was presumably due to a similar application of the principle of scientific induction. These applications of the principle are of course largely unconscious. When a burnt child fears the fire and shows that he fears it by avoiding it, f should say that he was applying scientific induction. (I could of course also describe his behaviour in many other ways.) The works and customs of mankind do not seem to be very suitable material to which to apply scientific induction. A very large part of space-time must be investigated, if reliable results are to be obtained. Otherwise we may (as most English 'Children do) decide that everybody speaks English, and that it is silly to learn French.
There are, however, special remarks to be made about many of the disabilities that have been mentioned. The inability to enjoy strawberries and cream may have struck the reader as frivolous. Possibly a machine might be made to enjoy this delicious dish, but any attempt to make one do so would be idiotic.
The claim that "machines cannot make mistakes" seems a curious one. One is tempted to retort, "Are they any the worse for that?" But let us adopt a more sympathetic attitude, and try to see what is really meant. I think this criticism can be explained in terms of the imitation game. It is claimed that the interrogator could distinguish the machine from the man simply by setting them a number of problems in arithmetic. The machine would be unmasked because of its deadly accuracy. The reply to this is simple. The machine (programmed for playing the game) would not attempt to give the right answers to the arithmetic problems. It would deliberately introduce mistakes in a manner calculated to confuse the interrogator. A mechanical fault would probably show itself through an unsuitable decision as to what sort of a mistake to make in the arithmetic. Even this interpretation of the criticism is not sufficiently sympathetic. But we cannot afford the space to go into it much further. It seems to me that this criticism depends on a confusion between two kinds of mistake, We may call them "errors of functioning" and "errors of conclusion." Errors of functioning are due to some mechanical or electrical fault which causes the machine to behave otherwise than it was designed to do. In philosophical discussions one likes to ignore the possibility of such errors; one is therefore discussing "abstract machines." These abstract machines are mathematical fictions rather than physical objects. By definition they are incapable of errors of functioning. In this sense we can truly say that "machines can never make mistakes." Errors of conclusion can only arise when some meaning is attached to the output signals from the machine. The machine might, for instance, type out mathematical equations, or sentences in English. When a false proposition is typed we say that the machine has committed an error of conclusion. There is clearly no reason at all for saying that a machine cannot make this kind of mistake. It might do nothing but type out repeatedly "O = I." To take a less perverse example, it might have some method for drawing conclusions by scientific induction. We must expect such a method to lead occasionally to erroneous results.
The claim that a machine cannot be the subject of its own thought can of course only be answered if it can be shown that the machine has some thought with some subject matter. Nevertheless, "the subject matter of a machine's operations" does seem to mean something, at least to the people who deal with it. If, for instance, the machine was trying to find a solution of the equation x2 - 40x - 11 = 0 one would be tempted to describe this equation as part of the machine's subject matter at that moment. In this sort of sense a machine undoubtedly can be its own subject matter. It may be used to help in making up its own programmes, or to predict the effect of alterations in its own structure. By observing the results of its own behaviour it can modify its own programmes so as to achieve some purpose more effectively. These are possibilities of the near future, rather than Utopian dreams.
The criticism that a machine cannot have much diversity of behaviour is just a way of saying that it cannot have much storage capacity. Until fairly recently a storage capacity of even a thousand digits was very rare.
The criticisms that we are considering here are often disguised forms of the argument from consciousness, Usually if one maintains that a machine can do one of these things, and describes the kind of method that the machine could use, one will not make much of an impression. It is thought that tile method (whatever it may be, for it must be mechanical) is really rather base. Compare the parentheses in Jefferson's statement quoted on page 22.
(6) Lady Lovelace's ObjectionOur most detailed information of Babbage's Analytical Engine comes from a memoir by Lady Lovelace ( 1842). In it she states, "The Analytical Engine has no pretensions to originate anything. It can do whatever we know how to order it to perform" (her italics). This statement is quoted by Hartree ( 1949) who adds: "This does not imply that it may not be possible to construct electronic equipment which will 'think for itself,' or in which, in biological terms, one could set up a conditioned reflex, which would serve as a basis for 'learning.' Whether this is possible in principle or not is a stimulating and exciting question, suggested by some of these recent developments But it did not seem that the machines constructed or projected at the time had this property."
I am in thorough agreement with Hartree over this. It will be noticed that he does not assert that the machines in question had not got the property, but rather that the evidence available to Lady Lovelace did not encourage her to believe that they had it. It is quite possible that the machines in question had in a sense got this property. For suppose that some discrete-state machine has the property. The Analytical Engine was a universal digital computer, so that, if its storage capacity and speed were adequate, it could by suitable programming be made to mimic the machine in question. Probably this argument did not occur to the Countess or to Babbage. In any case there was no obligation on them to claim all that could be claimed.
This whole question will be considered again under the heading of learning machines.
A variant of Lady Lovelace's objection states that a machine can "never do anything really new." This may be parried for a moment with the saw, "There is nothing new under the sun." Who can be certain that "original work" that he has done was not simply the growth of the seed planted in him by teaching, or the effect of following well-known general principles. A better variant of the objection says that a machine can never "take us by surprise." This statement is a more direct challenge and can be met directly. Machines take me by surprise with great frequency. This is largely because I do not do sufficient calculation to decide what to expect them to do, or rather because, although I do a calculation, I do it in a hurried, slipshod fashion, taking risks. Perhaps I say to myself, "I suppose the Voltage here ought to he the same as there: anyway let's assume it is." Naturally I am often wrong, and the result is a surprise for me for by the time the experiment is done these assumptions have been forgotten. These admissions lay me open to lectures on the subject of my vicious ways, but do not throw any doubt on my credibility when I testify to the surprises I experience.
I do not expect this reply to silence my critic. He will probably say that h surprises are due to some creative mental act on my part, and reflect no credit on the machine. This leads us back to the argument from consciousness, and far from the idea of surprise. It is a line of argument we must consider closed, but it is perhaps worth remarking that the appreciation of something as surprising requires as much of a "creative mental act" whether the surprising event originates from a man, a book, a machine or anything else.
The view that machines cannot give rise to surprises is due, I believe, to a fallacy to which philosophers and mathematicians are particularly subject. This is the assumption that as soon as a fact is presented to a mind all consequences of that fact spring into the mind simultaneously with it. It is a very useful assumption under many circumstances, but one too easily forgets that it is false. A natural consequence of doing so is that one then assumes that there is no virtue in the mere working out of consequences from data and general principles.
(7) Argument from Continuity in the Nervous System
The nervous system is certainly not a discrete-state machine. A small error in the information about the size of a nervous impulse impinging on a neuron, may make a large difference to the size of the outgoing impulse. It may be argued that, this being so, one cannot expect to be able to mimic the behaviour of the nervous system with a discrete-state system.
It is true that a discrete-state machine must be different from a continuous machine. But if we adhere to the conditions of the imitation game, the interrogator will not be able to take any advantage of this difference. The situation can be made clearer if we consider sonic other simpler continuous machine. A differential analyser will do very well. (A differential analyser is a certain kind of machine not of the discrete-state type used for some kinds of calculation.) Some of these provide their answers in a typed form, and so are suitable for taking part in the game. It would not be possible for a digital computer to predict exactly what answers the differential analyser would give to a problem, but it would be quite capable of giving the right sort of answer. For instance, if asked to give the value of (actually about 3.1416) it would be reasonable to choose at random between the values 3.12, 3.13, 3.14, 3.15, 3.16 with the probabilities of 0.05, 0.15, 0.55, 0.19, 0.06 (say). Under these circumstances it would be very difficult for the interrogator to distinguish the differential analyser from the digital computer.
(8) The Argument from Informality of BehaviourIt is not possible to produce a set of rules purporting to describe what a man should do in every conceivable set of circumstances. One might for instance have a rule that one is to stop when one sees a red traffic light, and to go if one sees a green one, but what if by some fault both appear together? One may perhaps decide that it is safest to stop. But some further difficulty may well arise from this decision later. To attempt to provide rules of conduct to cover every eventuality, even those arising from traffic lights, appears to be impossible. With all this I agree.
From this it is argued that we cannot be machines. I shall try to reproduce the argument, but I fear I shall hardly do it justice. It seems to run something like this. "if each man had a definite set of rules of conduct by which he regulated his life he would be no better than a machine. But there are no such rules, so men cannot be machines." The undistributed middle is glaring. I do not think the argument is ever put quite like this, but I believe this is the argument used nevertheless. There may however be a certain confusion between "rules of conduct" and "laws of behaviour" to cloud the issue. By "rules of conduct" I mean precepts such as "Stop if you see red lights," on which one can act, and of which one can be conscious. By "laws of behaviour" I mean laws of nature as applied to a man's body such as "if you pinch him he will squeak." If we substitute "laws of behaviour which regulate his life" for "laws of conduct by which he regulates his life" in the argument quoted the undistributed middle is no longer insuperable. For we believe that it is not only true that being regulated by laws of behaviour implies being some sort of machine (though not necessarily a discrete-state machine), but that conversely being such a machine implies being regulated by such laws. However, we cannot so easily convince ourselves of the absence of complete laws of behaviour as of complete rules of conduct. The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, "We have searched enough. There are no such laws."
We can demonstrate more forcibly that any such statement would be unjustified. For suppose we could be sure of finding such laws if they existed. Then given a discrete-state machine it should certainly be possible to discover by observation sufficient about it to predict its future behaviour, and this within a reasonable time, say a thousand years. But this does not seem to be the case. I have set up on the Manchester computer a small programme using only 1,000 units of storage, whereby the machine supplied with one sixteen-figure number replies with another within two seconds. I would defy anyone to learn from these replies sufficient about the programme to be able to predict any replies to untried values.
(9) The Argument from Extrasensory PerceptionI assume that the reader is familiar with the idea of extrasensory perception, and the meaning of the four items of it, viz., telepathy, clairvoyance, precognition and psychokinesis. These disturbing phenomena seem to deny all our usual scientific ideas. How we should like to discredit them! Unfortunately the statistical evidence, at least for telepathy, is overwhelming. It is very difficult to rearrange one's ideas so as to fit these new facts in. Once one has accepted them it does not seem a very big step to believe in ghosts and bogies. The idea that our bodies move simply according to the known laws of physics, together with some others not yet discovered but somewhat similar, would be one of the first to go.
This argument is to my mind quite a strong one. One can say in reply that many scientific theories seem to remain workable in practice, in spite of clashing with ESP; that in fact one can get along very nicely if one forgets about it. This is rather cold comfort, and one fears that thinking is just the kind of phenomenon where ESP may be especially relevant.A more specific argument based on ESP might run as follows: "Let us play the imitation game, using as witnesses a man who is good as a telepathic receiver, and a digital computer. The interrogator can ask such questions as 'What suit does the card in my right hand belong to?' The man by telepathy or clairvoyance gives the right answer 130 times out of 400 cards. The machine can only guess at random, and perhaps gets 104 right, so the interrogator makes the right identification." There is an interesting possibility which opens here. Suppose the digital computer contains a random number generator. Then it will be natural to use this to decide what answer to give. But then the random number generator will be subject to the psychokinetic powers of the interrogator. Perhaps this psychokinesis might cause the machine to guess right more often than would be expected on a probability calculation, so that the interrogator might still be unable to make the right identification. On the other hand, he might be able to guess right without any questioning, by clairvoyance. With ESP anything may happen.
If telepathy is admitted it will be necessary to tighten our test up. The situation could be regarded as analogous to that which would occur if the interrogator were talking to himself and one of the competitors was listening with his ear to the wall. To put the competitors into a "telepathy-proof room" would satisfy all requirements.
7. Learning MachinesThe reader will have anticipated that I have no very convincing arguments of a positive nature to support my views. If I had I should not have taken such pains to point out the fallacies in contrary views. Such evidence as I have I shall now give.

Let us return for a moment to Lady Lovelace's objection, which stated that the machine can only do what we tell it to do. One could say that a man can "inject" an idea into the machine, and that it will respond to a certain extent and then drop into quiescence, like a piano string struck by a hammer. Another simile would be an atomic pile of less than critical size: an injected idea is to correspond to a neutron entering the pile from without. Each such neutron will cause a certain disturbance which eventually dies away. If, however, the size of the pile is sufficiently increased, tire disturbance caused by such an incoming neutron will very likely go on and on increasing until the whole pile is destroyed. Is there a corresponding phenomenon for minds, and is there one for machines? There does seem to be one for the human mind. The majority of them seem to be "subcritical," i.e., to correspond in this analogy to piles of subcritical size. An idea presented to such a mind will on average give rise to less than one idea in reply. A smallish proportion are supercritical. An idea presented to such a mind that may give rise to a whole "theory" consisting of secondary, tertiary and more remote ideas. Animals minds seem to be very definitely subcritical. Adhering to this analogy we ask, "Can a machine be made to be supercritical?"
The "skin-of-an-onion" analogy is also helpful. In considering the functions of the mind or the brain we find certain operations which we can explain in purely mechanical terms. This we say does not correspond to the real mind: it is a sort of skin which we must strip off if we are to find the real mind. But then in what remains we find a further skin to be stripped off, and so on. Proceeding in this way do we ever come to the "real" mind, or do we eventually come to the skin which has nothing in it? In the latter case the whole mind is mechanical. (It would not be a discrete-state machine however. We have discussed this.)
These last two paragraphs do not claim to be convincing arguments. They should rather be described as "recitations tending to produce belief."
The only really satisfactory support that can be given for the view expressed at the beginning of §6, will be that provided by waiting for the end of the century and then doing the experiment described. But what can we say in the meantime? What steps should be taken now if the experiment is to be successful?
As I have explained, the problem is mainly one of programming. Advances in engineering will have to be made too, but it seems unlikely that these will not be adequate for the requirements. Estimates of the storage capacity of the brain vary from 1010 to 1015 binary digits. I incline to the lower values and believe that only a very small fraction is used for the higher types of thinking. Most of it is probably used for the retention of visual impressions, I should be surprised if more than 109 was required for satisfactory playing of the imitation game, at any rate against a blind man. (Note: The capacity of the Encyclopaedia Britannica, 11th edition, is 2 X 109) A storage capacity of 107, would be a very practicable possibility even by present techniques. It is probably not necessary to increase the speed of operations of the machines at all. Parts of modern machines which can be regarded as analogs of nerve cells work about a thousand times faster than the latter. This should provide a "margin of safety" which could cover losses of speed arising in many ways, Our problem then is to find out how to programme these machines to play the game. At my present rate of working I produce about a thousand digits of progratiirne a day, so that about sixty workers, working steadily through the fifty years might accomplish the job, if nothing went into the wastepaper basket. Some more expeditious method seems desirable.
In the process of trying to imitate an adult human mind we are bound to think a good deal about the process which has brought it to the state that it is in. We may notice three components.
(a) The initial state of the mind, say at birth,
(b) The education to which it has been subjected,
(c) Other experience, not to be described as education, to which it has been subjected.
Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain. Presumably the child brain is something like a notebook as one buys it from the stationer's. Rather little mechanism, and lots of blank sheets. (Mechanism and writing are from our point of view almost synonymous.) Our hope is that there is so little mechanism in the child brain that something like it can be easily programmed. The amount of work in the education we can assume, as a first approximation, to be much the same as for the human child.
We have thus divided our problem into two parts. The child programme and the education process. These two remain very closely connected. We cannot expect to find a good child machine at the first attempt. One must experiment with teaching one such machine and see how well it learns. One can then try another and see if it is better or worse. There is an obvious connection between this process and evolution, by the identifications
Structure of the child machine = hereditary material
Changes of the child machine = mutation,
Natural selection = judgment of the experimenter
One may hope, however, that this process will be more expeditious than evolution. The survival of the fittest is a slow method for measuring advantages. The experimenter, by the exercise of intelligence, should he able to speed it up. Equally important is the fact that he is not restricted to random mutations. If he can trace a cause for some weakness he can probably think of the kind of mutation which will improve it.
It will not be possible to apply exactly the same teaching process to the machine as to a normal child. It will not, for instance, be provided with legs, so that it could not be asked to go out and fill the coal scuttle. Possibly it might not have eyes. But however well these deficiencies might be overcome by clever engineering, one could not send the creature to school without the other children making excessive fun of it. It must be given some tuition. We need not be too concerned about the legs, eyes, etc. The example of Miss Helen Keller shows that education can take place provided that communication in both directions between teacher and pupil can take place by some means or other.
We normally associate punishments and rewards with the teaching process. Some simple child machines can be constructed or programmed on this sort of principle. The machine has to be so constructed that events which shortly preceded the occurrence of a punishment signal are unlikely to be repeated, whereas a reward signal increased the probability of repetition of the events which led up to it. These definitions do not presuppose any feelings on the part of the machine, I have done some experiments with one such child machine, and succeeded in teaching it a few things, but the teaching method was too unorthodox for the experiment to be considered really successful.The use of punishments and rewards can at best be a part of the teaching process. Roughly speaking, if the teacher has no other means of communicating to the pupil, the amount of information which can reach him does not exceed the total number of rewards and punishments applied. By the time a child has learnt to repeat "Casabianca" he would probably feel very sore indeed, if the text could only be discovered by a "Twenty Questions" technique, every "NO" taking the form of a blow. It is necessary therefore to have some other "unemotional" channels of communication. If these are available it is possible to teach a machine by punishments and rewards to obey orders given in some language, e.g., a symbolic language. These orders are to be transmitted through the "unemotional" channels. The use of this language will diminish greatly the number of punishments and rewards required.
Opinions may vary as to the complexity which is suitable in the child machine. One might try to make it as simple as possible consistently with the general principles. Alternatively one might have a complete system of logical inference "built in."' In the latter case the store would be largely occupied with definitions and propositions. The propositions would have various kinds of status, e.g., well-established facts, conjectures, mathematically proved theorems, statements given by an authority, expressions having the logical form of proposition but not belief-value. Certain propositions may be described as "imperatives." The machine should be so constructed that as soon as an imperative is classed as "well established" the appropriate action automatically takes place. To illustrate this, suppose the teacher says to the machine, "Do your homework now." This may cause "Teacher says 'Do your homework now' " to be included amongst the well-established facts. Another such fact might be, "Everything that teacher says is true." Combining these may eventually lead to the imperative, "Do your homework now," being included amongst the well-established facts, and this, by the construction of the machine, will mean that the homework actually gets started, but the effect is very satisfactory. The processes of inference used by the machine need not be such as would satisfy the most exacting logicians. There might for instance be no hierarchy of types. But this need not mean that type fallacies will occur, any more than we are bound to fall over unfenced cliffs. Suitable imperatives (expressed within the systems, not forming part of the rules of the system) such as "Do not use a class unless it is a subclass of one which has been mentioned by teacher" can have a similar effect to "Do not go too near the edge."
The imperatives that can be obeyed by a machine that has no limbs are bound to be of a rather intellectual character, as in the example (doing homework) given above. important amongst such imperatives will be ones which regulate the order in which the rules of the logical system concerned are to be applied, For at each stage when one is using a logical system, there is a very large number of alternative steps, any of which one is permitted to apply, so far as obedience to the rules of the logical system is concerned. These choices make the difference between a brilliant and a footling reasoner, not the difference between a sound and a fallacious one. Propositions leading to imperatives of this kind might be "When Socrates is mentioned, use the syllogism in Barbara" or "If one method has been proved to be quicker than another, do not use the slower method." Some of these may be "given by authority," but others may be produced by the machine itself, e.g. by scientific induction.
The idea of a learning machine may appear paradoxical to some readers. How can the rules of operation of the machine change? They should describe completely how the machine will react whatever its history might be, whatever changes it might undergo. The rules are thus quite time-invariant. This is quite true. The explanation of the paradox is that the rules which get changed in the learning process are of a rather less pretentious kind, claiming only an ephemeral validity. The reader may draw a parallel with the Constitution of the United States.
An important feature of a learning machine is that its teacher will often be very largely ignorant of quite what is going on inside, although he may still be able to some extent to predict his pupil's behavior. This should apply most strongly to the later education of a machine arising from a child machine of well-tried design (or programme). This is in clear contrast with normal procedure when using a machine to do computations one's object is then to have a clear mental picture of the state of the machine at each moment in the computation. This object can only be achieved with a struggle. The view that "the machine can only do what we know how to order it to do,"' appears strange in face of this. Most of the programmes which we can put into the machine will result in its doing something that we cannot make sense (if at all, or which we regard as completely random behaviour. Intelligent behaviour presumably consists in a departure from the completely disciplined behaviour involved in computation, but a rather slight one, which does not give rise to random behaviour, or to pointless repetitive loops. Another important result of preparing our machine for its part in the imitation game by a process of teaching and learning is that "human fallibility" is likely to be omitted in a rather natural way, i.e., without special "coaching." (The reader should reconcile this with the point of view on pages 23 and 24.) Processes that are learnt do not produce a hundred per cent certainty of result; if they did they could not be unlearnt.


/ 001. Benanav, Aaron. "Automation and the Future of Work," 2019.


From the New Left Review 119 September October 2019.

The world is abuzz with talk of automation. Rapid advances in artificial intelligence, machine learning and robotics seem set to transform the world of work. In the most advanced factories, companies like Tesla have been aiming for ‘lights-out’ production, in which fully automated work processes, no longer needing human hands, can run in the dark. Meanwhile, in the illuminated halls of robotics conventions, machines are on display that can play ping-pong, cook food, have sex and even hold conversations. Computers are not only developing new strategies for playing Go, but are said to be writing symphonies that bring audiences to tears. Dressed in white lab coats or donning virtual suits, computers are learning to identify cancers and will soon be developing legal strategies. Trucks are already barrelling across the us without drivers; robotic dogs are carrying military-grade weapons across desolate plains. Are we living in the last days of human toil? Is what Edward Bellamy once called the ‘edict of Eden’ about to be revoked, as ‘men’—or at least, the wealthiest among them—become like gods?

There are many reasons to doubt the hype. For one thing, machines remain comically incapable of opening doors or, alas, folding laundry. Robotic security guards are toppling into mall fountains. Computerized digital assistants can answer questions and translate documents, but not well enough to do the job without human intervention; the same is true of self-driving cars. In the midst of the American ‘Fight for Fifteen’ movement, billboards went up in San Francisco threatening to replace fast-food workers with touchscreens if a law raising the minimum wage were passed. The Wall Street Journal dubbed the bill the ‘robot employment act’. Yet many fast-food workers in Europe already work alongside touchscreens and often earn better pay than in the us. Is the talk of automation overdone?

1. THE AUTOMATION DISCOURSE

In the pages of newspapers and popular magazines, scare stories about automation may remain just idle chatter. However, over the past decade, this talk has crystalized into an influential social theory, which purports not only to analyse current technologies and predict their future, but also to explore the consequences of technological change for society at large. This automation discourse rests on four main propositions. First, workers are already being displaced by ever-more advanced machines, resulting in rising levels of ‘technological unemployment’. Second, this displacement is a sign that we are on the verge of achieving a largely automated society, in which nearly all work will be performed by self-moving machines and intelligent computers. Third: automation should entail humanity’s collective liberation from toil, but because we live in a society where most people must work in order to live, this dream may well turn out to be a nightmare. Fourth, therefore, the only way to prevent a mass-unemployment catastrophe is to provide a universal basic income (ubi), breaking the connection between the incomes people earn and the work they do, as a way to inaugurate a new society.

This argument has been put forward by a number of self-described futurists. In the widely read Second Machine Age (2014), Erik Brynjolfsson and Andrew McAfee argue that we find ourselves ‘at an inflection point—a bend in the curve where many technologies that used to be found only in science fiction are becoming everyday reality.’ New technologies promise an enormous ‘bounty’, but Brynjolfsson and McAfee caution that ‘there is no economic law that says that all workers, or even a majority of workers, will benefit from these advances.’ On the contrary: as the demand for labour falls with the adoption of more advanced technologies, wages are stagnating; a rising share of annual income is therefore being captured by capital rather than by labour. The result is growing inequality, which could ‘slow our journey’ into what they call ‘the second machine age’ by generating a ‘failure mode of capitalism’ in which rentier extraction crowds out technological innovation. In Rise of the Robots (2015), Martin Ford similarly claims that we are pushing ‘towards a tipping point’ that is poised to ‘make the entire economy less labour-intensive.’ Again, ‘the most frightening long-term scenario of all might be if the global economic system eventually manages to adapt to the new reality’, leading to the creation of an ‘automated feudalism’ in which the ‘peasants would be largely superfluous’ and the elite impervious to economic demands. For these authors, education and retraining will not be enough to stabilize the demand for labour in an automated economy; some form of guaranteed non-wage income, such as a negative income tax, must be put in place.

The automation discourse has been enthusiastically adopted by the jeans-wearing elite of Silicon Valley. Bill Gates is advocating for a tax on robots. Mark Zuckerberg told Harvard undergraduate inductees that they should ‘explore ideas like universal basic income’, a policy Elon Musk also thinks will become increasingly ‘necessary’ over time, as robots outcompete humans across a growing range of jobs. Musk has been naming his SpaceX drone vessels after spaceships from Iain M. Banks’s Culture Series, a set of ambiguously utopian science-fiction novels depicting a post-scarcity world in which human beings live fulfilling lives alongside intelligent robots, called ‘minds’, without the need for markets or states.

Politicians and their advisors have equally identified with the automation discourse, which has become one of the leading perspectives on our ‘digital future’. In his farewell presidential address, Obama suggested that the ‘next wave of economic dislocations’ will come not from overseas trade, but rather from ‘the relentless pace of automation that makes a lot of good, middle-class jobs obsolete.’ Robert Reich, former Labour Secretary under Bill Clinton, expressed similar fears: we will soon reach a point ‘where technology is displacing so many jobs, not just menial jobs but also professional jobs, that we’re going to have to take seriously the notion of a universal basic income.’ Clinton’s former Treasury Secretary, Lawrence Summers, made the same admission: once-‘stupid’ ideas about technological unemployment now seem increasingly smart, he said, as workers’ wages stagnate and economic inequality rises. The discourse has become the basis of a long-shot presidential campaign for 2020: Andrew Yang, Obama’s former ‘Ambassador of Global Entrepreneurship’, has penned his own tome on automation, The War on Normal People, and is now running a futuristic campaign on a ‘Humanity First’, ubi platform. Among Yang’s vocal supporters is Andy Stern, former head of the seiu, whose Raising the Floor is yet another example of the discourse.10

Yang and Stern—like all of the other writers named so far—take pains to assure readers that some variant of capitalism is here to stay, even if it must jettison its labour markets; however, they admit to the influence of figures on the far left who offer a more radical version of the automation discourse. In Inventing the Future, Nick Srnicek and Alex Williams argue that the ‘most recent wave of automation is poised’ to transform the labour market ‘drastically, as it comes to encompass every aspect of the economy’.11 They claim that only a socialist government would actually be able to fulfil the promise of full automation by creating a post-work or post-scarcity society. In Four Futures, Peter Frase thoughtfully explores the alternative outcomes for such a post-scarcity society, depending on whether it still had private property and still suffered from resource scarcity, which could persist even if labour scarcity were overcome. Like the liberal proponents of the automation discourse, these left-wing writers stress that, even if the coming of advanced robotics is inevitable, ‘there is no necessary progression into a post-work world’. Srnicek, Williams and Frase are all proponents of ubi, but in a left-wing variant. For them, ubi serves as a bridge to ‘fully automated luxury communism’, a term originally coined in 2014 by Aaron Bastani to name a possible goal of socialist politics, and which flourished for five years as a meme on the internet before his book—outlining an automated future in which artificial intelligence, solar power, gene-editing, asteroid mining and lab-grown meat generate a world of limitless leisure and self-invention—finally appeared.

Recurrent fears

These futurist visions, from all points of the political spectrum, depend upon a common prediction of the trajectory of technological change. Have they got this right? To answer this question, it is helpful to have a couple of working definitions. Automation may be distinguished as a specific form of labour-saving technical innovation: automation technologies fully substitute for human labour, rather than merely augmenting human-productive capacities. With labour-augmenting technologies, a given job category will continue to exist, but each worker in that category will be more productive. For example, adding new machines to an assembly-line producing cars may make line workers more productive without abolishing line work as such. However, fewer workers will be needed in total to produce any given number of automobiles. Whether that results in fewer jobs will then depend on how much output—the total number of cars—also increases.

By contrast, automation may be defined as what Kurt Vonnegut describes in Player Piano: it takes place whenever an entire ‘job classification has been eliminated. Poof.’ No matter how much production might increase, another telephone-switchboard operator or hand-manipulator of rolled steel will never be hired. In these cases, machines have fully substituted for human labour. Much of the debate around the future of workplace automation turns on an evaluation of the degree to which present or near-future technologies are labour-substituting or labour-augmenting in character. Distinguishing between these two types of technical change turns out to be incredibly difficult in practice. One famous study from the Oxford Martin School suggested that 47 per cent of jobs in the us are at high risk of automation; a more recent study from the oecd predicts that 14 per cent of oecd jobs are at high risk, with another 32 per cent at risk of significant change in the way they are carried out (due to labour-augmenting rather than substituting innovations).

It is unclear, however, whether even the highest of these estimates suggests that a qualitative break with the past has taken place. By one count, ‘57 per cent of the jobs workers did in the 1960s no longer exist today’.16 Automation, in fact, turns out to be a constant feature of the history of capitalism. By contrast, the discourse around automation, which extrapolates from instances of technological change to a broader social theory, is not constant; it periodically recurs in modern history. Excitement about a coming age of automation can be traced back to at least the mid-19th century. Charles Babbage published On the Economy of Machinery and Manufactures in 1832; John Adolphus Etzler’s The Paradise Within the Reach of All Men, Without Labour appeared in 1833, Andrew Ure’s The Philosophy of Manufactures in 1835. These books presaged the imminent emergence of largely or fully automated factories, run with minimal or merely supervisory human labour. This vision was a major influence on Marx, whose Capital, Volume One argued that a complex world of interacting machines was in the process of displacing labour at the centre of economic life.

Visions of automated factories then appeared again in the 1930s, 1950s and 1980s, before their re-emergence in the 2010s. Each time, they were accompanied or shortly followed by predictions of a coming age of ‘catastrophic unemployment and social breakdown’, which could be prevented only if society were reorganized.17 To point out the periodicity of this discourse is not to say that its accompanying social visions should be dismissed. For one thing, the technological breakthroughs presaged by automation discourse could still be achieved at any time: just because they were wrong in the past does not necessarily mean that they will always be wrong in the future. More than that, these visions of automation have clearly been generative in social terms: they point to certain utopian possibilities latent within modern capitalist societies. The error in their approach is merely to suppose that, via ongoing technological shifts, these utopian possibilities will imminently be revealed via a catastrophe of mass unemployment.

The basic insight on which automation theory relies was described, most succinctly, by the Harvard economist Wassily Leontief. He pointed out that the ‘effective operation of the automatic price mechanism’ at the core of capitalist societies ‘depends critically’ on a peculiar feature of modern technology, namely that in spite of bringing about ‘an unprecedented rise in total output’, it nevertheless ‘strengthened the dominant role of human labour in most kinds of productive processes’.18 At any time, a breakthrough could destroy this fragile pin, annihilating the social preconditions of functioning market economies. Drawing on this insight—and adding only that such a technological breakthrough now exists—the automation prognosticators often argue that capitalism must be a transitory mode of production, which will eventually give way to a new form of life that does not organize itself around work for wages and monetary exchange.

Taking its periodicity into account, automation theory may be described as a spontaneous discourse of capitalist societies, which, for a mixture of structural and contingent reasons, reappears in those societies time and again as a way of thinking through their limits. What summons the automation discourse periodically into being is a deep anxiety about the functioning of the labour market: there are simply too few jobs for too many people. Proponents of the automation discourse consistently explain the problem of a low demand for labour in terms of runaway technological change.

Declining labour demand
If automation discourse appeals so widely again today, it is because, whatever their causes, the ascribed consequences of automation are all around us: global capitalism clearly is failing to provide jobs for many of the people who need them. There is, in other words, a persistently low demand for labour, reflected not only in higher spikes of unemployment and increasingly jobless recoveries—both frequently cited by automation theorists—but also in a phenomenon with more generic consequences: declining labour shares of income. Many studies have now confirmed that the labour share, whose steadiness was held to be a stylized fact of economic growth, has been falling for decades (Figure 1).
These shifts signal a radical decline in workers’ bargaining power. Realities for the typical worker are worse than these statistics suggest, since wage growth has become increasingly skewed towards the highest earners: the infamous top one per cent. A growing gap has opened up not only between the growth of labour productivity and average wage-incomes, but also between the growth of average wages and that of median wages, with the result that many workers see a vanishingly thin slice of economic growth (Figure 2).20 Under these conditions, rising inequality is contained only by the strength of redistributive programmes. Even critics of automation discourse such as David Autor and Robert Gordon are disturbed by these trends: something has gone wrong with the economy, leading to a low demand for labour.

Is automation the cause of the low demand for labour? I will join the critics of automation discourse in arguing that it is not. However, along the way, I will also criticize the critics—both for producing explanations of low labour demand that only apply in high-income countries and for failing to produce anything like a radical vision of social change that is adequate to the scale of the problems we now confront. Indeed, it should be said from the outset that I am more sympathetic to the left automation theorists than to their critics.

Even if the explanation they offer turns out to be inadequate, the automation theorists have at least focused the world’s attention on the problem of a persistently low demand for labour. They have also excelled in actually trying to imagine solutions to this problem that are broadly emancipatory in character. In Jameson’s terms, the automation theorists are our late capitalist utopians. In a world reeling from the ‘perfect storm’ of climate change, rising inequality, recalcitrant neoliberalism and resurgent ethno-nationalism, the automation theorists are the ones pushing through the catastrophe with a vision of an emancipated future, in which humanity advances to the next stage in our history, whatever that might mean (or whatever we want to make it mean), and technology helps to free us all to discover and follow our passions. That is true in spite of the fact that—like many of the utopians of the past—the actual visions these latest utopians offer need to be freed from their largely technocratic fantasies of how social change to a better future might take place.

Major shifts in the forms of government intervention in the economy are adopted only under massive social pressure, such as, in the course of the 20th century, the threat of communism or of civilizational collapse. Today, policy reforms could emerge in response to pressure coming from a new mass movement, aiming to change the basic makeup of the social order. Instead of fearing that movement, we should see ourselves as part of it, helping articulate its goals and paths forward. If that movement is defeated, maybe the best we will get is basic income, but that should not be our goal. We should be reaching towards a post-scarcity world, which advanced technologies will certainly help us realize, even if full automation is not achievable—or even desirable.The return of automation discourse is a symptom of our era, as it was in times past: it arises when the global economy’s failure to create enough jobs causes people to question its fundamental viability. The breakdown of this market mechanism today is more extreme than at any time in the past. This is because a greater share of the world’s population than ever before depends on selling its labour or the simple products of its labour to survive, in the context of weakening global economic growth. Our present reality is better described by near-future science-fiction dystopias than by standard economic analysis; ours is a hot planet, with micro-drones flying over the heads of the street hawkers and rickshaw pullers, where the rich live in guarded, climate-controlled communities while the rest of us wile away our time in dead-end jobs, playing video games on smartphones. We need to slip out of this timeline and into another.
Reaching towards a post-scarcity world—in which all individuals are guaranteed access to whatever they need to make a life, without exception—can become the basis on which humanity mounts a battle against climate change. It can also be the foundation on which we remake the world, creating the conditions in which, as James Boggs once put it, ‘for the first time in human history, great masses of people will be free to explore and reflect, to question and to create, to learn and to teach, unhampered by the fear of where the next meal is coming from’. Finding our way forward requires a break between work and income, as the automation theorists recognize, but also between profit and income, as many do not.
In responding to the automation discourse, then, I will argue that the decline in the demand for labour is due not to an unprecedented leap in technological innovation, but to ongoing technical change in an environment of deepening economic stagnation. In the second part of this contribution, to be published in nlr 120, I contend that this fall in labour demand manifests not as mass unemployment, but rather as mass under-employment, not necessarily a problem for the elites. On this basis, I mount a critique of technocratic solutions, like basic income. I offer a thought-experiment of how we might imagine a post-scarcity society that centres on humans, not machines, and project a path of how we might get there through social struggle, rather than administrative intervention. But first, in Part One, I provide a diagnosis of the underlying causes of the decline in demand for labour. This involves a detour to consider the fortunes of the global manufacturing sector and the competitive dynamics at work in labour’s ‘deindustrialization’.
2. LABOUR'S GLOBAL DEINDUSTRALIZATION
Automation-discourse theorists recognize that, if technologically induced job-destruction is to have widespread social ramifications, it will have to eliminate employment in the vast and variegated service sector, which absorbs 74 per cent of workers in high-income countries and 52 per cent worldwide. They therefore focus on ‘new forms of service-sector automation’ in retail, transportation and food services, where ‘robotization’ is said to be ‘gathering steam’ with a growing army of machines that take orders, stock shelves, drive cars and flip burgers. Many more service-sector jobs, including some that require years of education and training, will supposedly be rendered obsolete in the coming years due to advances in artificial intelligence. Of course, these claims are mostly predictions about the effects that technologies will have on future patterns of employment. Such predictions can go wrong—as for example when Eatsa, an automated fast-food company which employed neither cashiers nor waiters, was forced to close most of its stores in 2017.
In making their case, automation theorists often point to the manufacturing sector as the precedent for what they imagine is beginning to happen in services—for in manufacturing, the employment-apocalypse has already taken place. To evaluate the theorists’ claims, it therefore makes sense to begin by looking at what role automation has played in that sector’s fate. After all, manufacturing is the area most amenable to automation, since on the shop floor it is possible to ‘radically simplify the environment in which machines work, to enable autonomous operation’. Industrial robotics has been around for a long time: the first robot, the ‘unimate’, was installed in a General Motors plant in 1961. Still, until the 1960s, scholars studying this sector were able to dismiss Luddite fears of long-term technological unemployment out of hand. Manufacturing employment in fact grew most rapidly in those lines where technical innovation was happening at the fastest pace, because it was in those lines that prices fell the fastest, stoking the growth of demand for the products.
Industrialization has long since given way to deindustrialization, and not just in any one line but across the manufacturing sectors of most countries. The share of workers employed in manufacturing fell first across the high-income world: manufacturing employed 22 per cent of all workers in the us in 1970; that share declined to just 8 per cent in 2017. Over the same period, manufacturing employment shares fell from 23 per cent to 9 per cent in France, and from 30 per cent to 8 per cent in the uk. Japan, Germany and Italy have experienced smaller but still substantial declines: in Japan from 25 per cent to 15 per cent, in Germany from 29 per cent to 17 per cent, and in Italy from 25 per cent to 15 per cent. In all cases, the declines were eventually associated with substantial falls in the total number of people employed in manufacturing. In the us, Germany, Italy and Japan, the overall number of manufacturing jobs fell by approximately a third from postwar peaks; in France, by 50 per cent and in the uk, by 67 per cent.

It is commonly assumed that deindustrialization must be the result of production facilities moving offshore. Yet in none of the countries named above has manufacturing job loss been associated with declines in manufacturing output. Real value added in manufacturing more than doubled in the us, France, Germany, Japan and Italy between 1970 and 2017. Even the uk, whose manufacturing sector fared worst of all among this group, saw a 25 per cent increase in manufacturing real value added over this period. To be sure, low- and middle-income countries are producing more and more goods for import into high-income countries; however, deindustrialization in the latter cannot simply be the result of productive capacity moving to the former. In the scholarly literature, deindustrialization is therefore ‘most commonly defined as a decline in the share of manufacturing in total employment’, regardless of corresponding trends in levels of manufactured output. This definition moves in step with automation theorists’ core expectations: more goods are being produced but by fewer workers.

It is on this basis that commentators typically cite rapidly rising labour productivity, rather than an influx of low-cost imports from abroad, as the primary cause of industrial-job loss in advanced economies. On closer inspection, however, this explanation turns out to be inadequate: no upward leap has taken place in manufacturing productivity levels. On the contrary, manufacturing productivity has been growing at a sluggish pace for decades, leading Robert Solow to quip, ‘We see the computer age everywhere, except in the productivity statistics.’ Automation theorists discuss this ‘productivity paradox’ as a problem for their account—explaining it in terms of weak demand for products, or the persistent availability of low-wage workers—but they understate its true significance. This is partly due to the appearance of steady labour-productivity growth in us manufacturing, at an average rate of around 3 per cent per year since 1950. On that basis, Brynjolfsson and McAfee suggest, automation could show up in the compounding effects of exponential growth, rather than an uptick in the growth rate.

However, official us manufacturing growth-rate statistics are overinflated, for example in logging the production of computers with higher processing speeds as equivalent to the production of more computers. On that basis, government statistics claim that productivity levels in the computers and electronics sub-sector rose at an average rate of over 10 per cent per year between 1987 and 2011, even as productivity growth rates outside of that sub-sector fell to around 2 per cent per year over the same period. Since 2011, trends across the manufacturing sector have worsened: real output per hour in the sector as a whole was lower in 2017 than at its peak in 2010. Productivity growth rates in manufacturing collapsed precisely when they were supposed to be rising rapidly due to industrial automation.

Correcting manufacturing-productivity statistics in the us brings them more into line with trends visible in the statistics of other countries. In Germany and Japan, manufacturing-productivity growth rates have fallen dramatically since their postwar peaks. In Germany, for example, manufacturing productivity grew at an average annual rate of 6.3 per cent per year in the 1950s and 60s, falling to 2.4 per cent since 2000. This downward trend is to some extent an expected result of the end of an era of rapid, catch-up growth. However, it should still be surprising to the automation theorists, since Germany and Japan have raced ahead of the us in the field of industrial robotics. Indeed, the robots used in Tesla’s largely automated car factory in California were made by a German robotics company. German and Japanese firms deploy about 60 per cent more industrial robots per 10,000 manufacturing workers, compared to the US.

Yet deindustrialization continues to take place in all these countries, despite lacklustre manufacturing-productivity growth rates: that is, it is taking place as the automation theorists expect, but not for the reasons they offer. To explore the causes of deindustrialization in more detail, I use the following accounting identity. For any given industry, the rate of growth of output (ΔO) minus the rate of growth of labour productivity (ΔP) equals the rate of growth of employment (ΔE). Thus, ΔO – ΔP = ΔE. So, for example, if the output of automobiles grows by 3 per cent per year, and productivity in the automobile industry grows by 2 per cent per year, then employment in that industry must necessarily rise by one per cent per year (3 – 2 = 1). Contrariwise, if output grows by 3 per cent per year and productivity grows by 4 per cent per year, employment will contract by 1 per cent per year (3 – 4 = -1).

Disaggregating manufacturing-output growth rates in France provides us with a sense of the typical pattern playing out across the high-income countries (Figure 3). During the so-called Golden Age of postwar capitalism, productivity growth rates in French manufacturing were much higher than they are today—5.2 per cent per year, on average, between 1950 and 1973—but output growth rates were even higher than that—5.9 per cent per year—as a result of a steady increase in employment of 0.7 per cent per year. Since 1973, both output and productivity rates have declined, but output rates fell much more sharply than productivity rates. By the early years of the 21st century, productivity growth rates—although much slower, at 2.7 per cent per year—were now faster than their corresponding output growth rates—at 0.9 per cent—as manufacturing employment contracted rapidly, by 1.7 per cent per year.

This disaggregation helps explain why automation theorists falsely perceive productivity to be growing at a rapid pace in manufacturing: in fact, productivity growth has been rapid only relative to extremely sluggish output growth. The same pattern can be seen in the statistics of other countries: no absolute decline in levels of manufacturing production has taken place, but there has been a decline in the output growth rate, with the result that output is growing more slowly than productivity (Table 1, overleaf). The simultaneity of limited technological dynamism and worsening economic stagnation combines to generate a progressive decline in industrial employment levels.

As such, ‘output-led’ deindustrialization is impossible to explain in purely technological terms. In searching for alternative perspectives, economists have mostly preferred to describe it as a harmless evolutionary feature of advanced economies. However, that perspective is itself at a loss in explaining extreme variations in the gdp per capita levels at which this supposedly evolutionary economic shift has taken place. Deindustrialization unfolded first in high-income countries in the late 1960s and early 1970s, at the tail-end of a period in which levels of income per person had converged across the us, Europe and Japan. In the decades that followed, deindustrialization then spread ‘prematurely’ to middle- and low-income countries, with larger variations in incomes per capita (Figure 4). In the late 1970s, deindustrialization arrived in southern Europe; much of Latin America, parts of East and Southeast Asia, and southern Africa followed in the 1980s and 1990s. Peak industrialization levels in many poorer countries were so low that it may be more accurate to say that they never industrialized in the first place.

By the end of the 20th century, it was possible to describe deindustrialization as a kind of global epidemic: worldwide manufacturing employment rose in absolute terms by 0.4 per cent per year between 1991 and 2016, but that was much slower than the overall growth of the global labour force, with the result that the manufacturing share of total employment declined by 3 percentage points over the same period. China is a key exception, but only a partial one (Figure 5, overleaf). In the mid 1990s, Chinese state-owned enterprises shed large numbers of workers, sending manufacturing-employment shares on a steady downward trajectory. China re-industrialized, starting in the early 2000s, but then began to deindustrialize once again in the mid 2010s: its manufacturing-employment share has since dropped from 19.3 per cent in 2013 to 17.5 per cent in 2017, with further falls likely. If deindustrialization cannot be explained by either automation or the internal evolution of advanced economies, what could be its source?

3. BLIGHT OF MANUFACTURING OVERCAPACITY

What the economists’ accounts fail to register in explaining deindustrialization is also what is missing from the automation theorists’ accounts. The truth is that rates of output growth in manufacturing have tended to decline, not only in this or that country, but worldwide (Figure 6). In the 1950s and 60s, global manufacturing production expanded at an average annual rate of 7.1 per cent per year, in real terms. That rate fell progressively to 4.8 per cent in the 1970s, and to 3.0 per cent between 1980 and 2007. Since the 2008 crisis and up to 2014, manufacturing output expanded at just 1.6 per cent per year, on a world scale—that is, at less than a quarter of the pace achieved during the so-called postwar Golden Age. It is worth noting that these figures include the dramatic expansion of manufacturing productive capacity in China. Again, it is the incredible degree of slowdown or even stagnation in manufacturing-output growth, visible on the world scale, that explains why manufacturing-productivity growth appears to be advancing at a rapid clip, even though it is actually much slower than before. More and more is produced with fewer workers, as the automation theorists claim, but not because technological change is giving rise to high rates of productivity growth. On the contrary, productivity growth in manufacturing appears rapid today only because the yardstick of output growth, against which it is measured, is shrinking.Seen from this perspective, the global wave of deindustrialization can be said to find its origins not in runaway technical change but rather in worsening overcapacity in world markets for manufactured goods. The rise in overcapacity developed stepwise after World War Two. In the immediate postwar period, the us hosted the most dynamic economy in the world, with the most advanced technologies. Under the threat of communist expansion within Europe, as well as in East and Southeast Asia, the us proved willing to share its technological largesse with its former imperial competitors Germany and Japan, as well as other ‘frontline’ countries, in order to bring them all under the us security umbrella. In the first few decades of the post-wwii era, these technology transfers were a major boost to economic growth in Europe and Japan, opening up opportunities for export-led expansion. This strategy was also supported by the devaluation of European and Japanese currencies against the dollar. However, as Robert Brenner has argued, rising manufacturing capacity across the globe quickly generated overcapacity, issuing in a ‘long downturn’ in manufacturing output growth rates.

What mattered here was not only the later building out of manufacturing capacity in the global South, but the earlier creation of such capacity in countries like Germany, Italy and Japan, which hosted the first low-cost producers in the postwar era who succeeded in taking shares in global markets for industrial goods, and then invading the previously impenetrable us domestic market. That competition caused rates of industrial-output growth in the us to decline in the late 1960s, issuing in deindustrialization in employment terms. As the us responded to heightened import penetration in the 1970s by breaking up the Bretton Woods order and devaluing the dollar, these same problems spread from the highest wage countries in North America and northern Europe to Japan and the rest of Europe. Thereafter, as more and more countries built up manufacturing capacity, adopted export-led growth strategies and entered global markets for manufactured goods, falling rates of manufacturing-output growth and consequent labour deindustrialization also spread to Latin America, the Middle East, Asia and Africa, as well as to the global economy taken as a whole.

Deindustrialization is not only a matter of technological advance, but also of a global redundancy of technological capacities, creating more crowded markets in which rapid rates of industrial-output expansion become more difficult to achieve. The mechanism transmitting this problem across the globe was severely depressed prices in global markets for manufactured goods. That led to falling income-per-unit capital ratios, then to falling rates of profit, then to lower rates of investment, and hence lower rates of output growth. In this environment, firms have faced heightened competition for market share: as overall growth rates slow, the only way to grow quickly is to steal market shares from other firms. Each firm has to do everything it can to keep up with its competitors. Overcapacity explains why, since the early 1970s, productivity-growth rates have fallen less severely than output-growth rates: firms have continued to raise their productivity levels as best they can despite falling rates of output growth (or else have gone under, disappearing from statistical averages). As manufacturing-output growth rates slipped below productivity-growth rates in one country after another, deindustrialization spread worldwide.

Driving globalization

Explaining global waves of deindustrialization in terms of global overcapacity rather than industrial automation allows us to understand a number of features of this phenomenon that otherwise appear paradoxical. For example, rising overcapacity explains why deindustrialization has been accompanied not only by ongoing efforts to develop new labour-saving technologies, but also by the building out of gigantic, labour-using supply chains—usually with a more damaging environmental impact. A key turning point in that story came in the 1960s, when low-cost Japanese and German products invaded the us domestic market, sending the us industrial-import penetration ratio soaring from less than 7 per cent in the mid-60s to 16 per cent in the early 1970s. From that point forward, it became clear that high levels of labour productivity would no longer serve as a shield against competition from lower-wage countries. The us firms that did best in this context were the ones that responded by globalizing production. Facing competition on prices, us multinational firms built international supply chains, shifting the more labour-intensive components of their production processes abroad and playing suppliers off against one another to achieve the best prices. In the mid-60s the first export-processing zones opened in Taiwan and South Korea. Even Silicon Valley, which formerly produced its computer chips locally in the San Jose area, shifted its production to low-wage areas, using lower grades of technology (and also benefitting from laxer laws around pollution and workers’ safety). mncs in Germany and Japan adopted similar strategies, which were everywhere supported by new infrastructures of transportation and communication technologies.

The globalization of production allowed the world’s wealthiest economies to retain manufacturing capacity, but it did not reverse the overall trend towards labour deindustrialization. As supply chains were built out across the world, firms in more and more countries were pulled into the swirl of world-market competition. In some countries, this move was accompanied by shifts in the location of new plants: rustbelts oriented towards production for domestic markets went into decline, while sunbelts integrated into global supply networks expanded dramatically. Chattanooga grew at the expense of Detroit, Ciudad Juárez at the expense of Mexico City, Guangdong at the expense of Dongbei. Yet given the overall slowdown in rates of world manufacturing-market expansion, this re-orientation towards the world market could only result in lacklustre outcomes: the rise of sunbelts failed to balance out the decline of rustbelts, resulting in global deindustrialization.

At the same time, global manufacturing overcapacity explains why the countries that have succeeded in attaining a high degree of robotization are not those that have seen the worst degree of deindustrialization. In the context of intense global competition, high degrees of robotization have given firms competitive advantages, allowing them to take market share from firms in other countries. Thus Germany, Japan and South Korea have some of the highest levels of robotization; they also have the largest trade surpluses in the world. Workers in European and East Asian firms know that automation helps preserve their jobs. China is also a top-four country in terms of trade surpluses, providing its manufacturing sector with a gigantic boost in terms of both output and employment growth. China has advanced on this front not due to high levels of robotization, but rather due to a mix of low wages, moderate to advanced technologies, and strong infrastructural capacities. Yet the result was the same: in spite of system-wide overcapacity and slow growth rates, the prc has industrialized rapidly because Chinese firms have been able to take market share away from other firms—not only in the us, but also in countries like Mexico and Brazil—which lost market share as Chinese firms expanded. It could not have been otherwise, since in an environment where average growth rates are low, firms can only achieve high rates of growth by taking market share from their competitors. Whether China will be able to retain its competitive position as its wage levels rise remains an open question; Chinese firms are now racing to robotize in order to head off this possibility.

4. BEYOND MANUFACTURING

The evidence I have cited so far to explain job loss in the manufacturing sector through worsening overcapacity may appear to have little purchase on the larger, economy-wide patterns—of stagnant wages, falling labour shares of income, declining labour-force participation rates and jobless recoveries after recessions—that the automation theorists have sought to explain by growing technological dynamism. Automation may therefore still seem a good explanation for the decline in demand for labour across the service sectors of each country’s economy, and so across the world economy as a whole. Yet this broader problem of declining labour demand also turns out to be better explained by the worsening industrial stagnation I have described than by widespread technological dynamism.

This is because, as rates of manufacturing-output growth stagnated in one country after another from the 1970s onward, no other sector appeared on the scene to replace industry as a major economic-growth engine. Instead, the slowdown in manufacturing-output growth rates was accompanied by a slowdown in overall growth rates. This trend is visible in the economic statistics of high-income countries. France is again a striking example (Figure 7). In France, real manufacturing value added (mva) rose at 5.9 per cent per year between 1950 and 1973, while real value added in the total economy (gdp) rose at 5.1 per cent per year. Since 1973, both growth measures have declined significantly: by the 2001–17 period, mva was rising at only 0.9 per cent per year, while gdp was rising at a faster but still sluggish pace of 1.2 per cent per year. Note that during the 1950s and 60s, mva growth generally led the overall economy: manufacturing served as the major engine of overall growth. Since 1973, mva growth rates have trailed overall economic growth. Similar patterns can be seen in other high-income countries (Table 2, overleaf).Their export-led growth engines sputtered and slowed to a crawl; and as they did so, overall rates of economic growth slowed considerably.

Economists studying deindustrialization often point out that while manufacturing has declined as a share of nominal gdp, it has maintained, until recently, a more or less steady share of real gdp, which is to say that, between 1973 and 2000, real mva grew at approximately the same pace as real gdp. What that has meant in practice is that, as manufacturing has become less dynamic, so has the overall economy. There was no significant shift in demand from industry to services. Instead, as capital accumulation slowed down in manufacturing, the expansion of aggregate output also slowed significantly across the economy as a whole.

This tendency to economy-wide stagnation, associated with the decline in manufacturing dynamism, then explains the system-wide decline in the demand for labour, and so also the problems that the automation theorists cite: stagnant real wages, falling labour shares of income and so on. This economy-wide pattern of declining labour demand is not the result of rising productivity-growth rates, associated with automation in the service sector. On the contrary, productivity is growing even more slowly outside of the manufacturing sector than inside of it: in France, for example, while productivity in the manufacturing sector was rising at an average annual rate of 2.7 per cent per year between 2001–17, productivity in the service sector was rising at just 0.6 per cent per year. Similar gaps exist in other countries. Once again, the mistake of the automation theorists is to focus on rising productivity growth rather than falling output growth. The environment of slower economic growth explains the low demand for labour all by itself. Workers, and especially workers who are not protected by powerful unions or labour laws, find it difficult to pressure employers to raise their wages when there is so much slack in the labour market.

These trends are as visible in the world economy—including China—as they are in the high-income countries (Figure 8, overleaf). In the 1950s and 60s, global mva growth and gdp growth were expanding at rapid clips of 7.1 per cent and 5.0 per cent respectively, with mva growth leading gdp growth by a significant margin. From the 1970s onward, as global mva growth slowed, so did global gdp growth. In most of the decades that followed, global mva growth continued to lead gdp growth but by a much smaller margin. Since 2008, both rates have been growing at the exceptionally slow pace of 1.6 per cent per year. Again, the implication is that, as manufacturing growth rates declined, nothing emerged to replace industry as a growth engine. Not all regions of the world economy are experiencing this slowdown in the same way or to the same extent, but even countries like China that have grown quickly have to contend with this global slowdown and its consequences. Since the 2008 crisis, China’s economic growth rate has slowed considerably; its economy is deindustrializing.

The clear conclusion is that manufacturing turned out to be a unique engine of overall economic growth. Industrial production tends to be amenable to incremental increases in productivity, achieved via technologies that can be repurposed across numerous lines. Industry also benefits from static and dynamic economies of scale. Meanwhile, there is no necessary boundary to industrial expansion: industry consists of all economic activities that are capable of being rendered via an industrial process. The reallocation of workers from low-productivity jobs in agriculture, domestic industry and domestic services to high-productivity jobs in factories raises levels of income per worker and hence overall economic growth rates. The countries that have caught up with the West in terms of income—such as Japan, South Korea and Taiwan—mostly did so by industrializing: they exploited opportunities to produce for the world market, at increasing scale and using advanced technologies, allowing them to grow at speeds that would have been unachievable had they depended on domestic-market demand alone.

When the growth engine of industrialization sputters—due to the replication of technical capacities, international redundancy and fierce competition for markets—there has been no replacement for it as a source of rapid growth. Instead of workers reallocating from low-productivity jobs to high-productivity ones, the reverse of this process takes place, as workers pool increasingly in low-productivity jobs in the service sector. As countries have deindustrialized, they have also seen a massive build-up of financialized capital, chasing returns to the ownership of relatively liquid assets, rather than investment in new fixed capital. In spite of the high degree of overcapacity in industry, there is nowhere more profitable in the real economy for capital to invest itself. Indeed, if there had been, we would have evidence of it in higher rates of investment and hence higher gdp growth rates. This helps explain why firms have reacted to over-accumulation by trying to make their existing manufacturing capacity more flexible and efficient, rather than ceding territory to lower-cost, higher-productivity firms from other countries.

The lack of an alternative growth engine also explains why governments in poorer countries have encouraged domestic producers to try to break into already oversupplied international markets for manufactures. Nothing has replaced those markets as a major source of globally accessible demand. Overcapacity exists in agriculture, too, and is even worse there than in industry; meanwhile services, which are mostly non-tradable, make up only a tiny share of global exports. If countries are to retain any dependable link to the international market under these conditions, they must find some way to insert themselves into industrial lines, however oversupplied. System-wide overcapacity and the generalized slowdown in economic growth have therefore been devastating for most poorer countries: the amount of foreign exchange they have captured through liberalization has been pitiful; so, too, has been the number of jobs created.

Indeed, global economic downshifts have been particularly devastating for low- and middle-income countries, not only because they are poorer, but also because those downshifts have taken place in an era of rapid labour-force expansion: between 1980 and the present, the world’s waged workforce grew by about 75 per cent, adding more than 1.5 billion people to the world’s labour markets. These labour market entrants, living mostly in poorer countries, had the misfortune of growing up and looking for work at a time when global industrial overcapacity began to shape patterns of economic growth in post-colonial countries: declining rates of manufactured export growth into the us and Europe in the late 1970s and early 1980s ignited the 1982 debt crisis, followed by imf-led structural adjustment, which pushed countries to deepen their imbrications in global markets at a time of ever slower global growth and rising competition from China. In spite of shocks to the demand for labour generated by slowing global growth rates and rising economic turmoil, huge numbers of workers were still forced to seek employment in order to live.

Some may respond that the present low rates of global growth are in fact nothing out of the ordinary, if only we shift our baseline from the exceptional postwar ‘Golden Age’ to previous periods, such as the pre-WWI era. But a global perspective on the decline in the demand for labour provides the answer to this objection. It is true that, during the Belle Epoque, average rates of economic growth were more comparable to growth rates today. However, in that period, large sections of the population still lived in the countryside and produced much of what they needed to live. European empires still overran the globe, not only limiting the diffusion of new manufacturing technologies to a few regions, but also actively deindustrializing the rest of the world economy. Yet in spite of the much more limited sphere in which labour markets were active—and in which industrialization took place—the pre-wwi era, as also the inter-war period, was marked by a persistently low demand for labour, making for employment insecurity, rising inequality and tumultuous social movements aimed at transforming economic relations. In this respect, the world of today does look like the world of the Belle Epoque. The difference is that today, a much larger share of the world’s population depends on finding work in labour markets in order to live.What automation theorists describe as the result of rising technological dynamism is actually the consequence of worsening economic stagnation: productivity-growth rates appear to rise when, in reality, output-growth rates are falling. This mistake is not without reason. The demand for labour is determined by the gap between productivity and output growth rates. Reading the shrinking of this gap the wrong way around—that is, as due to rising productivity rather than falling output rates—is what generates the upside-down world of the automation discourse. Proponents of this discourse then search for the technological evidence that supports their view of the causes for the declining demand for labour. In making this leap, the automation theorists miss the true story of overcrowded markets and economic slowdown that actually explains the decline in labour demand.

Yet even if automation is not itself the primary cause of a low demand for labour, it is nevertheless the case that, in a slow-growing world economy, technological changes within a near-future horizon may still threaten large numbers of jobs with destruction, in a context of economic stagnation and slower rates of job creation. Technological change then acts as a secondary cause of a low labour demand, operating within the context of the first. The concluding section of this essay in nlr 120 will address these technological dynamics, as well as the socio-political problems—and opportunities—generated by a persistently low demand for labour in late-capitalist societies.


/ 002. Purcell, Conor. "Beyond Human Intelligence," Farsight, 2023.


Anyone with personal experience taking LSD, psilocybin, ayahuasca, or any other mindaltering psychedelic will truly recognise the world’s interconnected nature. Fractalising into white diamonds, the air shatters, trees breathe, and animals speak your language. Some suggest these drugs dismantle an evolved human filter, revealing nature for what it truly is: a connected intelligence.

James Bridle’s new book Ways of Being: Beyond Human Intelligence is an exploration of different forms of intelligence, both biological and artificial. It’s also, as the author says, a call for us humans to start forming new relationships with non-human intelligence. Throughout the book Bridle argues that our common future demands less industrial hubris, and more cooperation with existing and deeply knowledgeable biological systems.

A writer and artist known for coining the term ‘New Aesthetic’ – used to refer to the increasing appearance of the visual language of digital technology in the physical world, and the combination of the virtual and physical – James Bridle advocates for a future characterised by human, animal, and plant reconnectivity for the sake of achieving a better planetary balance.

Our regular contributor, Conor Purcell, PhD, had the opportunity to interview Bridle for FARSIGHT, speaking by video call between Purcell’s home in County Donegal, Ireland, and the interviewee’s in Greece.

What inspired you to write this book?

I studied artificial intelligence almost twenty years ago when it was kind of fading from the curriculum because it wasn’t going anywhere. Since then, there haven’t been any kind of major discoveries. But what has happened is that vast amounts of data have become available, which have been harvested largely by social media giants and governments. At the same time, processing power has massively increased. We’re now seeing how AI is revealing itself to be something not quite human in that it thinks and approaches the world in a very different way than we do. We’re also starting to realise, thanks to decades of research, that intelligence is something much more interesting and greater than our very narrow human idea of it.

With the book, I wanted to understand how we can better accommodate ourselves with everything else that we share the planet with. For me, this question is central to achieving environmental justice and progress. I now see an opportunity with AI for reimagining, firstly, what intelligence is, and secondly, how we impact other forms of intelligence beyond the human.

How do you think people have become so disconnected from these other ways of being, specifically the intelligences of animals and plants in nature?

A good example to demonstrate this is how in medieval times there were cases when animals were accused of committing a crime and tried in courtrooms. There were lawyers there, and the animals were presented to juries. This wasn’t pantomime, but a deeply serious undertaking because non-humans were part of the community. That meant they had rights and responsibilities.

Over time, and especially with industrialisation and urbanisation, attitudes towards nonhuman life in all its forms changed. We started to view them essentially as machines – unfeeling automatons who didn’t have the kind of inner life or higher importance which we ascribe to humans. This became the dominant mode of thought within Western, postenlightenment societies. That’s when the abattoirs began. And now the environmental mess that we find ourselves in is all related to how we’re out of balance with our deeply entangled and interdependent relationships with all other species.

What do we now know about the intelligent behavior of plants?

Recent research has shown several surprising behavioural qualities in plants. I write in the book about scientists who subjected certain plants to repeated shocks and found that quite quickly they learned essentially to ignore the shock and move away from its source. What’s more is that they remembered patterns and continued to avoid the source of the shock in the future.

This is an extraordinary finding that completely changes our understanding of plant behaviour. Even the idea that plants have a thing that we might call behaviour is astonishing because the traditional kind of botanical approach mostly involves cutting them up into small pieces and studying them as if they were machines. What’s interesting too is that these researchers write about working with plant spirits, and their work is informed by both the knowledge that has come from the plants themselves and by treating the plant as already having its own personhood.

This is real science published in legitimate scientific journals. It’s peer reviewed. It’s reproducible. It conforms to all the structures of the scientific method. What that tells me is that there are multiple ways of approaching these intelligences and to do that via a kind of synthesis of these different ways of knowing is incredibly powerful. We can explore the world by observing and connecting with these behaviours, as long as our goal is to truly understand. Ultimately, it all depends on admitting the possibility in the first place that these kinds of alternative intelligences are real.

In a chapter called "Non-Binary Machines" you talk about the fields of cybernetics- which has a long history dating back to the mid-20th century- and how this shows a future alternative to what you call 'corporate artificial intelligence'. Can you explain what you mean by this?

It has to do with thinking of intelligence as a process, rather than as a machine that thinks like a kind of brain in a box. Particularly in Britain, cybernetic researchers – those involved in the science of communication and automation in machines and other living beings – envisioned a kind of intelligence that is active in the world, which is connected to the world around it, which is learning, and which is defined by what it does, rather than what it is. This is different to the corporate artificial intelligence of today which is currently being developed to increase profits.

Cybernetic research continues in various ways. There is very interesting new research around soft robotics, which essentially tries to make robotic systems more adaptive to the world around them. Programmes like the Unconventional Computing Lab at the University of the West of England is a good example. One of the things they study is the computational abilities of various plants and animals. They are doing very interesting things like redesigning computer logic based on the movement of crabs, for example. This points to the fact that what we understand as computation is not something that can only be performed within machines, but in fact is conducted by biological organisms too.

It also appears that biological systems can be calibrated to test variable abilities and to solve mathematical problems – they might even be more efficient than our fastest supercomputers. These abilities exist across the natural world, but since we usually only see the things that we know how to test for, there remains the possibility of a whole range of intelligences which far exceed our own. The problem is that we don’t even know how to ask the appropriate questions yet.

How can we reconnect with non-human intelligence in the future?

Towards the end of the book, I write about the need to provide more shared territory for human and non-human lives. I mean this both in the form of animal reserves, conservation areas, wildlife corridors and shared spaces that allow animals to move in ways that they currently cannot. But I also think that the notion of animal intelligence compels us to think politically.

In the book, I discuss the Irish experience with the introduction of the so-called citizen assemblies, made up of 33 representatives chosen by political parties and 66 randomly chosen citizens, to make recommendations on society’s biggest challenges. One of the things we learned from the citizen assemblies, not just in Ireland, but in other places, is that this is an extraordinary mechanism for mobilising what are essentially multiple forms of intelligence. The assemblies didn’t use animal, plant or artificial intelligence, but by branching out beyond the traditional domain of experts, the range of human intelligence – and personality types – was enlarged. So instead of selecting a very narrow definition of domain experts, it’s acknowledging that what you need for complex thorny problems, particularly novel ones, is a wider diversity of life experiences and ways of thinking.

The same principle can apply to including intelligences beyond the human in our decision making. I believe that only by bringing in diverse ways of thinking and forms of life experience can we address the kind of extraordinary global and pan-species problems that we face.


/ 003. Mogensen, Klaus Æ. "Is Artificial Intelligence a Myth?" Farsight, 2023.


Erik Larson is a tech entrepreneur and pioneering research scientist working at the forefront of natural language processing. He recently published a book called The Myth of Artificial Intelligence: Why Computers Can't Think the Way We Do. FARSIGHT met him online for an interview about the future of AI, and why he believes the field’s current path of development will not lead us to human-level intelligence in machines anytime soon.

Erik, what made you decide to write this book?

My specialty is natural language processing, and I wrote the book from the perspective of understanding the many practical challenges and difficulties there are in making computers understand human language on a deep level. Early in my career, I read a book by Ray Kurzweil, The Age of Intelligent Machines, where he proposed 2029 as the year when computers become as smart as humans. I thought, maybe – it’s 30 years, after all. By 2005, when his book The Singularity is Near came out, I thought that it could not happen in 20 years without some major unexpected scientific breakthrough that we couldn’t anticipate yet. Instead of acting like we’re on an inevitable path to general AI, we should tell the broader public that achieving true computer intelligence is a lot more difficult than many assume. That’s why I wrote the book. 

You argue that we are very far from developing general artificial intelligence. In fact, you believe that the approach we are currently pursuing can never lead us there. Why is that?

The main framework that I use in the book is inference. In AI, the problem is that we’re using the wrong type of inference to ever get to general or commonsense intelligence. Right now, the field is almost exclusively dominated by machine learning using inductive inference, learning from prior examples. Human beings use induction all the time, but it’s not the most important type of inference for us. It can’t handle novelty because it’s based on prior observation. Without a novelty mechanism, you can’t get to certain kinds of intelligence. I don’t mean to say that it’s impossible. Nature has developed general intelligence, so we should be able to eventually do the same thing. However, there’s something currently missing, and that’s why it’s been so difficult to make certain kinds of progress in the field.

Arthur C. Clarke famously thought that to get something like intelligence in a computer, we would need heuristic logic- finding and using solutions that aren't precise, but just good enough, which is how we think. We don't measure the distance across the street with a measuring tape; we guesstimate how far it is This method is a lot faster and works well for everyday stuff. Do you think we could program that kind of heuristic logic into computers?

We do that already. Before deep learning became the dominant paradigm in AI development, classic AI design was more rule-based. One of the great challenges in the classic rules-based paradigm was in fact to find these rules of thumb, or heuristics. Herbert Simon, a pioneer in AI and Nobel Prize winner in economics, has said that people who favour adequacy and efficiency over optimisation generally make better, more responsible, and quicker decisions than those who want to make every decision perfect. Precision can be a barrier. However, the classic AI approach based on common-sense heuristics also failed when the domain wasn’t sufficiently constrained. Even if you have a rule that doesn’t need precision, you have so much context in an unconstrained real-world environment that you need rules to tell other rules when they are relevant. It quickly becomes intractable to try to get intelligent behaviour from such a system.

There are two major, unsolved problems in AI. One is robotics, especially when the robot is not in a very specific environment. A robot arm in an industrial setting with few degrees of freedom works well, but if we have a robot walking down the street in Manhattan, there are just so many peripheral problems that can occur in such a complex environment. Somebody walks in front of the robot; something unexpected happens. If you took the best, smartest robot in the world and set it loose on any city street, within a few minutes it would cause a traffic accident. That’s why you don’t see robots on the street.

The other major problem is having a real conversation with an AI system where it truly understands what you’re saying and responds with understanding. I mentioned inference before, and in addition to deduction and induction, there’s a third type of inference called abduction that people generally aren’t aware of, but which we use all the time. Deduction is, “It’s raining; therefore, the streets are wet.” Abduction is, “I know rain makes streets wet. I see the streets are wet. Perhaps it’s raining.” You generate a hypothesis that explains an observation. It’s not certain knowledge – you could be wrong. Maybe a fire hydrant broke. However, you keep correcting your hypothesis with further observation. The streets are wet, my hypothesis is that it’s raining, and then I confirm it or form another. That’s abduction – hypothesis generation.

You mentioned novelty. A human who has not been in a certain situation before can think it through and still handle it. If you introduce a chess master to Shogi, Japanese chess, which has slightly different rules, they would very quickly be able to adapt their experience with chess to be able to play it well. A chess-playing AI, however, would have to learn from scratch – its inductive deep learning of chess would be useless.

I believe game-playing AIs still use some version of a min-max algorithm, deducing what would be the best move given that it has watched a million games play out before. This is very different from a human, who doesn’t play a million games and then computes the probability. I’m not a neuroscientist, so I couldn’t tell you what’s happening in the brain of chess masters – but I’m pretty sure they don’t mindlessly play a million games before becoming masters.

I've observed that as computers get better than us at something, like chess or trivia knowledge, we tend to move the goalpost and say that this has nothing to do with intelligence. Will we keep redefining intelligence as being whatever we can do that computers can't, or are there some markers of intelligence that we can't explain away?

My response is to go back to Alan Turing’s original 1950 paper, when he said that if a person can converse with a computer and be convinced that it is a real human, then it must be intelligent. I would say that this test still holds. Of course, you can converse with a chatbot that just continues to deflect questions, but to have a conversation that’s empathetic and understanding with the computer – we still can’t do that.

During the summer of 2022, a big news story surfaced of a Google engineer becoming convinced that a program he was developing had gained real sentience and warranted rights akin to human rights. Could we not say that it is passed the Turing test?

The latest language models are quite good, but you can trip them up very easily if you know how. Language has a property called compositionality, how sentences are put together to provide meaning. There’s a big difference between me riding a horse and a horse riding me, but an AI language model is not going to get that because it doesn’t have a sense of compositionality. Natural language is a barrier for artificial intelligence – one of the biggest. A legitimate test of language understanding would convince me that an AI was intelligent.

Another test would be navigation in dynamic environments by autonomous vehicles or robots. Getting to fully autonomous driving will be a lot harder than people think. The small city of Palo Alto, California, is mapped out on a grid, and you get pretty good performance from the vehicles there. But if you’re driving on a rural road and the AI must rely on sensor data, we’re a long way from vehicles being able to autonomously navigate that. Fully capable robotics in openended dynamic environments and fully understanding natural language; those are the two big frontiers.

Could an AI not develop its own language, very different from human language, that is uses to understand its environment and gets around some of the current limitations? We could compare it to communicating with dolphins, which seem to have a complex language that we haven't come close to understanding. They cannot understand our questions, and we cannot understand theirs; yet they are doubtless sentient beings.

I suppose is it possible for a creative AI to somehow achieve a way to frame the world that doesn’t require natural language. I don’t know the answer to that, but the immediate practical problem I see is how do we then interact with those systems? That might create some very, very strange human-machine interactions. I almost completely avoided the question of sentience in my book because, frankly, I don’t have a lot to say about it. It’s an issue that very quickly becomes philosophical. It could be that computers right now have some low level of sentience, like insects, and we just can’t detect it because we don’t know how. As an engineer, I don’t know the entry point into that argument, so I leave it alone.

You argue that we can’t achieve general AI the way we try to do it now, with machine learning and adding more components to computers. However, in physics there’s the phenomenon of ‘emergence’, where new traits develop when the complexity is high enough. One water molecule doesn’t have surface tension, but put enough together, and you get it. A single neuron isn’t sentient, but enough produce human sentience. Would it not be possible, if we add complexity and more components to supercomputers, that they could achieve intelligence and sentience as an emergent trait?

I think it’s an interesting question. It’s like a pile of sand: if you keep adding grains of sand, you get a nice conical shape, until at one point adding just one more grain of sand gets you a cascading effect. We have these thresholds in emergence where something isn’t happening, and then at some level of complexity, a completely different phenomenon emerges. I think it’s interesting whether that could apply to technology or computers, but I don’t have any strong scientific position on that.

Isn't there a danger if we have, say, self-driving cars who all think the same way because we have copied the same machine learning into all of them? If there are several routes from a suburb to the city, they will all choose the same route because that's what the system says they should do, whereas humans might imagine that the main route will probably be too busy and choose another one instead?

I think we’ll solve those sorts of problems. We already have systems where you can see traffic flow. The problems that I worry about are more practical. There have been cases where selfdriving cars don’t stop because a stop sign is slightly damaged and is perceived as something else. There’s a famous example of a system that tried to drive underneath a school bus because it thought it was an overpass. We just can’t eliminate all problems because the natural world is so messy. A bunch of leaves that the wind blows across the street might be interpreted as a solid object, and the AI will slam on the brakes.

We have people worrying that if we achieve general intelligence in computers, they are going to take over the world, or follow some order, like maximizing the production of paperclips, to such extremes that the AI will wipe out humanity to do it more efficiently. Do you think there is any real danger of such things happening, or are we just projecting our own faults onto artificial intelligence?

There’s an interesting contradiction in the paperclip scenario. The system is supposed to have general intelligence, which you would think included common sense, but on the other hand, it’s so narrow and computational that it thinks it can maximise the sale of paperclips by turning all humans into paperclips. Real computer intelligence would realise that it’s not intended to wipe us out. There’s another option, though, which is that it becomes malevolent and actively desires to rid the world of human beings. That gets us into the question of whether something like malevolence could possibly emerge in an AI.

We have AIs today that looks at x-rays of patients, trying to determine if they have cancer. They can be very good at this, but they don't know anything about cancer or what it means to a human being. They lack an understanding of what their task really is about. Do you think we can achieve intelligence in computers without true understanding of what they do?

That’s a great question, but I don’t have a great answer for it. It raises the whole issue, in this case of medical science, of whether an AI can provide proper diagnoses when it doesn’t understand care. Someone should write a PhD about how medicine is best administered and what the role of technology is and can be.

Research shows that even when an AI is better than any doctor at diagnosing cancer, it is even more efficient when it works with a human doctor. They approach the problem in different ways - one with a human understanding, the other from being trained on millions of x-rays. Human-AI partnerships seem to work best.

I think that’s right. In terms of something we care about, like medicine, it sounds like this kind of collaboration may work best. To me that’s a good use of technology. That’s why we make technology – because it furthers human goals. Whether we will have autonomous systems that will replace humans in all domains, that is a completely different question. Whether we get fully sentient AI or not, we’re heading in this direction in the future. That’s for sure.”


/ 004. Brown, J. Dakota. "Typography, Automation, and the Division of Labor: A Brief History," 2019.


Typography was born in the mass-production mechanism of the printing press. It has thus always been implicated in automation and, thereby, in the distinctly modern dynamics of overwork, underemployment, and runaway production. Transformations of labor and technology, however, have received scant attention in graphic design historiography. Philip Meggs' landmark textbook A History of Graphic Design, for example, offers only the briefest hints of the social dislocations that accompanied automation in the printing trades. One reads, for example, that the first steam press in England was operated in a secret location to guard against sabotage, or that vaguely-defined "strikes and violence" greeted the first installations of typesetting machines. Otherwise, such histories tend to treat innovations in print technology as a politically neutral process of technical refinement. But the new machines and methods did not just drop from the heavens: their development was often materially supported by employers who aimed to speed up production, capture control over the work process, and even break strikes.

Modernity, Modernism, and the Graphic Designer

As the design historian Adrian Forty has documented, industrial and graphic design emerged with the capitalist division of labor; both professions, in turn, catalyzed further divisions and fragmentations of work. In eighteenth-century crafts like ceramics and printed fabrics, the erosion of trade knowledge was accompanied by the rise of a new role in production: that of the "modeller. Usually hired from outside of the trade, these early designers were more dependably in touch with bourgeois taste than craftspeople were. In Josiah Wedgwood's ceramic works, stylistic concerns were conditioned by a need to simplify production into a rigid series of straightforward tasks, in which there was little occasion for variation between workers. The contemporaneous vogue for Neoclassicism, with its simplified geometry and restrained ornament, provided an ideal opportunity to streamline production - with the express goal, in Wedgwood's words, of making "such Machines of the Men as cannot err."

As critical historians from Karl Marx to Henry Braverman and David Noble have shown, the progress of capitalism's division of labor entails a gradual transfer of control and planning from the factory floor to management. But the resulting degradation and cheapening of work was noticed almost from the beginning: notably by Wedgwood's contemporary Adam Smith. In the opening chapter to The Wealth of Nations, Smith explains the production process in a new type of pin factory. Here, the capitalist has not simply gathered formerly-independent artisans to practice their trade side-by-side - instead, he has exploded the pin-making process into a line along which each laborer only cuts, sharpens, or polishes. Smith notes the miraculous extension of productivity in a process thus rationalized; elsewhere, however, he worries that the "great body of the people" will increasingly fill their days repeating the same handful of tasks.

The man whose whole life is spent performing a few simple operations, of which the effects too are, perhaps, always the same … has no occasion to exert his understanding, or to exercise his invention.... He naturally loses, therefore, the habit of such exertion, and generally becomes as stupid and ignorant as it is possible for a human creature to become.

This same image was on John Ruskin's mind in 1853, as he formulated what would become a central text for the Arts & Crafts movement's antiindustrial critique. In "The Nature of the Gothic," Ruskin mourns "the little piece of intelligence" rationed out to the factory worker, which "exhausts itself in making the point of a pin." For Ruskin, the division of labor is more accurately the division of the laborers themselves: these abundant pins, he writes, are polished with mere "crumbs" of human capacities. The essay was republished by William Morris' Kelmscott Press in 1892. Arriving at the end of a career rich in the paradoxes of an anti-capitalist design practice, Kelmscott was Morris' attempt to restore aesthetic unity to the book while keeping skilled craftspeople employed at higher-thanaverage wages. Though Morris believed that the form of the book had been betrayed by industrial shoddiness, the scale of his undertaking still necessitated some modernization of the traditional work process. Kelmscott books nonetheless remained so expensive to produce that Morris was trapped, as he lamented, "ministering to the swinish luxury of the rich."

In the early twentieth-century United States, Morris' legacy would be refashioned in a context of accelerating industrial transformation. Frank Lloyd Wright came to believe that Morris' desired reconciliation between art, labor, and leisure was likely to be delivered by mechanization itself. For Wright, the machine—centrally illustrated by the printing press—had to be grasped for what it had become: "intellect mastering the drudgery of the earth." The "meaningless torture" inflicted on workers and materials alike could now be swept away, Wright argued, as long as designers could part with anachronistic practices of ornamentation. Meanwhile, American commercial artists were falling "under the Arts & Crafts spell" and emerging as freelance specialists in book typography. Bruce Rogers and Frederic Goudy, for example, took on Morris' aesthetic standards while largely ignoring his concerns about the social contradictions of large-scale production. It was Goudy's student W.A. Dwiggins who would later popularize the phrase "graphic design" to describe this emerging position in print's division of labor. The workshops of these early graphic designers were characterized by a clarified managerial role for the designer, a more rationalized division of labor below and, finally, an embrace of labor-saving technology in typesetting and printing.

Across the ocean, meanwhile, a more explicitly socialist embrace of industry had produced the modernist "machine aesthetic." Echoing and radicalizing Wright, the Constructivist manifesto of 1922 declared war on traditional art and pledged a conditional allegiance to the machine: Constructivists would be both technology's "first fighting and punitive force" and its "last slave-workers."[17] At the same time, the Bauhaus was moving away from its Arts & Crafts roots. The school had initially been organized along guild lines: composed not of students and professors, but of masters, journeymen, and apprentices.[18] In transitioning to an emphasis on industrial production, Bauhaus designers synthesized compositional lessons from Futurism, Dada, and Constructivism. One uniting theme of these movements had been a desire to alter the experience of reading by exploding the strictures of the letterpress. In each case, photomechanical techniques promised a way out. In the Bauhaus graphics studios, László Moholy-Nagy continued in this vein, developing a a montage practice he termed "Typophoto." 

In the USSR, El Lissitzky theorized the epistemological and technical aspects of this blurring of text and image. The text-image, he surmised, could be put to work perfecting the human sensorium: revolutionized book forms would yield a "perpetual sharpening of the optic nerve." Lissitzky also read early attempts at phototypography in the context of a historical tendency toward lightness and mobility: "The amount of material used is decreasing, we are dematerializing, cumbersome masses of material are being supplanted by released energies."[20] Such a process would culminate, as he cryptically wrote in 1926, in a final transcendence of print itself: "the electro-library." After the midpoint of the century, metal type would indeed be supplanted by photographic media, in systems that were increasingly directed by "electro-libraries" of "dematerialized" data. By the 1990s, the convergences and displacements predicted by Lissitzky had yielded the digital hybridization of writing, typesetting, and imaging. In the end, however, these transformations owed more to the bottom lines of print capitalists than to the efforts of the radical modernists.

Modernism in design began with a vision of socialist industrial transformation -but by mid-century, it had become welded to the public image of the great capitalist conglomerates. Corporate boosters of modernism like the paperboard magnate Walter Paepke, a benefactor of the postwar "New Bauhaus" in Chicago, prophesied a world in which design would meld with management.[22] Paepke argued that design could improve market competition between large firms tied to uniform machinery and wage agreements; internally, it could even be put to work on such "problems" as worker morale. Though modernism's trajectory from utopian potential to capitalist instrumentalization is familiar to design history, parallel themes in the history of the printing trades have received less attention. The initial promise of technological innovations—to end, in Wright's words, the "meaningless torture" of repetitive and inefficient labor -soon gave way to the sobering realities of deskilling and displacement. The contours of twentieth-century print technologies would be shaped in large part by struggles over automation and employment. The "released energies" of print's dematerialization increasingly took the form of outmoded workers.

Industrial Rationalization and the Printer

The growing coherence and confidence of the graphic design profession is accompanied historically by the gradual fragmentation and decline of the printing trades.[24] The job description of "printing" originally encompassed a set of knowledges that extended far beyond the point of contact between ink and paper. Early printers were often also typefounders, publishers, and booksellers. Even as the craft became more specialized, printing still involved typesetting and composing pages, which often extended to a role in writing. According to union typesetter and historian Henry Rosemont, newspaper printers in the mid-nineteenth century relied on a broad but informal education in "language, history, geography and other subjects," which enabled them to produce entire articles from telegrams consisting of little more than the relevant nouns, verbs, and modifiers. 

Print workers thus held a strategic position in the circulation of public discourse, which was simply not possible without them. They often took advantage of this position to educate themselves and to advocate for the interests of their trade. In addition to their obligatory literacy, they had access to the press as an organizing tool-an extreme rarity for manufacturing workers of the early industrial era. Journeyman printers became the first group of workers to go on strike in the United States, just a year after the Revolutionary War. As Régis Debray has documented, print workers would go on to play prominent roles in revolutionary movements around the world during the next two centuries.

The printed book was, if not the first, then certainly the clearest early example of the world of standardized, mass-produced commodities to come. And as with any other commodity, print production required "labor-saving" innovations to keep competing firms profitable. More efficient inventions often rendered the work less taxing and dangerous, but the primary motivation for their adoption was always the reduction of labor costs—resulting in layoffs or slashed hours. Print workers often found themselves fighting technologies that promised to ease the burden of their labor. Rosemont offers the example of a more efficient ink roller, invented in 1814 and vigorously resisted by American printers. The existing standard was a more rudimentary instrument that required periodic soakings in animal urine to keep it from hardening. Printers worked in consistently squalid, poorly- ventilated shops that contributed to lower-than-average life expectancies—but such conditions were clearly preferable to unemployment.

The printed book was, if not the first, then certainly the clearest early example of the world of standardized, mass-produced commodities to come. And as with any other commodity, print production required "labor-saving" innovations to keep competing firms profitable. More efficient inventions often rendered the work less taxing and dangerous, but the primary motivation for their adoption was always the reduction of labor costs—resulting in layoffs or slashed hours. Print workers often found themselves fighting technologies that promised to ease the burden of their labor. Rosemont offers the example of a more efficient ink roller, invented in 1814 and vigorously resisted by American printers. The existing standard was a more rudimentary instrument that required periodic soakings in animal urine to keep it from hardening. Printers worked in consistently squalid, poorly- ventilated shops that contributed to lower-than-average life expectancies—but such conditions were clearly preferable to unemployment.

Though Johannes Gutenberg's fifteenth-century press had scarcely changed in the intervening years, the first decades of the nineteenth century brought transformations far beyond the humble roller. Iron construction and steam power fundamentally changed not only the shape of the machine, but the entire work process that fed and maintained it. However, a a major production bottleneck remained: the reproduction of writing still required the manual assembly of each word and line. Printing firms grew to be heavily reliant upon a workforce of typesetters who were both rigorously trained and militantly organized. During the 1880s, several attempts were made to mechanize the typesetting process. One particularly spectacular failure was the Paige Compositor, which bankrupted its primary investor Mark Twain. As Twain is said to have boasted shortly before the invention proved unfeasible, the Paige could "work like six men and do everything but drink, swear, and go out on strike."

Then in 1886 Ottmar Mergenthaler, a German engineer hired by an investors' group that represented the major New York newspapers, presented the first working model of the Linotype machine. Like type composition by the old method, the Linotype utilized thin bits of metal; here, however, each bit carried the negative impression of a character. Operators typed the characters into a line; justification was then carried out by a spacing mechanism that sealed the channel into which the characters had been set. Molten metal was then injected into the channel to form a full line in positive relief. After cooling, each "line o' type" was stacked into columns and locked into page layouts for the press. While the Linotype was an expensive and somewhat risky investment, it delivered on promises of labor-cost savings, and in time it contributed to a dramatic enlargement of the size and circulation of the periodical press.

The trade at first dismissed these developments. As one printer's newspaper reassuringly put it in 1891, Linotypes were mere "toys" for print capitalists with no practical background in the trade. Over the next decade, however, the threat became palpable. As one unemployed typesetter wrote in 1900, his place at the typecase had been usurped by a "monster" that eerily replicated his movements without need for food or human dignity. While a steady demand for the manual composition of headlines, advertisements, and other display applications muffled the effect slightly, the new work process soon touched off an employment crisis. Younger compositors scrambled to learn machine composition, while thousands of older or more narrowly-trained workers fell through the cracks. However, the International Typographical Union (ITU), which represented manual typesetters, was able to establish jurisdiction over the machines in strategic industrial centers around the turn of the century. In order to stave off the effects of the transformation, the ITU pushed for shorter workdays and encouraged early retirements. 

As the popular press grew during the teens and twenties, typesetting employment stabilized and even expanded. The ITU grew in tandem, and soon became one of the most powerful unions in the United States. However, this position was soon threatened by a number of mutually-reinforcing technical innovations. First, teletypesetting enabled Linotypes to be driven like player pianos; encoded tape was poised to replace human typists. Second, a slew of phototypesetting inventions sought to replace the cumbersome "hot metal" process with typefaces stored on film. Giving typography a photochemical basis, in turn, allowed a more seamless integration of text and image, while also making typesetting more readily compatible with letterpress printing's longtime competitor, offset lithography. 

Teletypesetting evolved from Morse code and the stock ticker; it would go on to form the basis for early computing.

Like the beginnings of mechanized typesetting itself, efforts at moving beyond hot metal were piloted by newspapers. Early experiments with "cold type" were explicitly undertaken to break a wave of ITU strikes following the passage of the Taft-Hartley Act of 1947, which stripped organized labor of many of the bargaining rights it had won over the preceding decades.[36] During a citywide pressroom strike that lasted from November 1947 to September 1949, the Chicago Tribune put its existing clerical staff to work on a new model of justifying typewriter whose output could be "pasted up" as camera-ready paper layouts, as opposed to being "locked up" in countless pieces of backward-reading metal.[37] The Tribune's infamous "Dewey Defeats Truman" edition of November 3, 1948 was typeset by strikebreakers, in a work process that now moved faster than the official ballot counts.38 While the quality of typewriter paste-up left something to be desired, these experiments strongly hinted at the possibility of producing a newspaper without the union. 

The ITU was able to keep these challenges at bay throughout the mid-twentieth century. New contracts forbade machines like the teletypesetter, even though this meant that print-ready stories from the wire services had to be retyped by an ITU member on the premises.[39] It wasn't until 1964 that the New York City local signed a contract allowing Linotypes to be run on "outside tape" on the condition, however, that employers paid 100% of the profits deriving from the new machinery into an "automation fund."[40] While this price was prohibitively steep for many firms, it opened the door to similar agreements on phototypesetting and, eventually, to computer systems. During the 1970s, the ITU began to draw down in exchange for the job and pension security of existing members.[41] In the meantime, the new machines had already crept into areas of the industry with low union representation. A paradoxical result was that capital-intensive metropolitan papers like the New York Times were among last to make the transition. The final night of Linotype composition at the Times—July 1, 1978—is memorialized in the documentary Farewell Etaoin Shrdlu, directed by ITU proofreader David Loeb Weiss. Among the film's interviewees is a compositor who reflects on his 26 years in the industry: 

[T]hat's six years apprenticeship, 20 years journeyman. And these are words that aren't just tossed around. ... All the knowledge I've acquired over these 26 years is all locked up in a little box now called a computer. And I think probably most jobs are gonna end up the same way.

Once more, the newspaper industry led the way in automation, and again the ITU attempted to train people in the new processes or encourage early retirements. In the earlier transformation, the work lost to Linotype composition was compensated by a gradual but decisive expansion of print production. This time, however, the further rationalization of typesetting destroyed older forms of work while narrowing the number of jobs in the new lines. As Lissitzky had predicted, metal gave way to film and paper; the material footprint of typography was shrinking. But as long as each text needed to be retyped to be typeset, labor-time savings were minimal. The widespread adoption of teletypesetting technology, however, allowed the storage and transmission of coded texts and, eventually, their formatting directions as well. By the 1970s, computer systems were beginning to dissolve typesetting into word processing. A centuries-old gap separating writing and printing was beginning to close—and this gap had been the very ground on which the ITU stood. The union suffered a long decline and finally dissolved in 1986, just as the personal computer was completing typography's process of dematerialization. It was, at that time, the longest continuously-running union in U.S. history.

The End of Modernism and the Last Typesetters

In the 1970s, print production involved a complex hierarchy of work processes, the final product of which was never fully visible until it had been printed. Designers could only approximate typographical treatments; directions on spacing, size, and weight were then handed off to phototypesetting shops to interpret in detail. A separate group of prepress specialists followed designers' directions on variables like color density and image placement, and then "stripped" together disparate negatives to create a print-ready master. But despite the many hands through which such work passed, much of the period's modernist-influenced design left the impression that it was the product a singular, detached mind. 

Though there was still a high degree of churn in new machines and processes, this division of labor held stable until the arrival of Apple's Macintosh computer in 1984. The personal computer centralized capacities formerly bound up in massive metal-founding operations, delicate apparatuses of type on film, or astronomically expensive, room-filling computers—to say nothing of the highly specialized workers that attended these machines, or of the systems of education and apprenticeship that such a workforce presupposed. Tasks that were once contracted out with some combination of strict direction and trust were now fully under the control of the individual designer—from the smallest details of letterforms to the organization of entire books. The Macintosh would soon offer image-editing capacities with no existing analogue, which in turn put pressure on commercial photographers and illustrators. The century since the invention of the Linotype had been one of "creative destruction" in the print industry: novel forms of work appeared suddenly and disruptively, only to be rendered obsolete in their turn. Once the brake provided by ITU contracts was removed, this process could accelerate unabated.

By the mid-1980s, typographical technology had reached a height of modernized seamlessness which, ironically, contributed to the decline of modernism's hegemony in graphic design. New design software facilitated effects like layering and distortion, which were quickly put to use in visual polemics against modernist clarity. Formal complexity and semantic confusion in graphic design had a long pre-Macintosh history —stretching at least as far back as the late-1960s letterpress experiments of Wolfgang Weingart. In the 1980s, however, graphic designers raised the stakes of these experiments by linking them to contemporaneous developments in the academy: in particular, to the "linguistic" and "cultural turns" in the humanities.[45] Terms like "deconstruction" and "post-structuralism" were applied to the printed page in ways that often required little familiarity with the theories in question. The grid—increasingly understood as a symbol of authoritarian and, perhaps, Eurocentric rationality—was parodied, skewed, or thrown aside entirely. Designers arranged texts into ambiguous formations, and designed new typefaces that intentionally thwarted legibility.

By the 1990s, the postmodernist critique of modern rationality and power had grown more rigorous. However, the movement's theorists showed little interest in grasping capitalism as a determining context for their theory and practice; transformations in the political economy of print were thus largely ignored. When, in 1997, Emigre published a rare acknowledgment that entire industries were collapsing next door, it was with a heavy dose of schadenfreude 

[M]any of the printers who have gone out of business over the last quarter century deserved their fate. The grassroots of the printing trade is, after all, notoriously conservative, protectionist, and sexist.

While prepress and printing-like most American trades-tended toward a narrowly white male membership and self-image, the heaviest losses in the industry from the 1980s forward were in fact suffered by the largely non-unionized workforce of the cold type shops. Compared to the membership of the ITU, these workers were disproportionately women and people of color.

The postmodernists' focus on cultural intervention often neglected the material contingencies of the practice. Semiotic theory and cultural studies opened vistas to broad contexts of symbolic circulation, but often at the cost of such bare facts as design's own relationship to waged work. It is perhaps not surprising, then, that a new generation of practitioners has taken a more archaeological approach to the labor of design. In the recent documentaries Linotype: The Film (2012) and Graphic Means: A History of Graphic Design Production (2016)-both directed by practicing graphic designers. histories of print production expose deeper issues of deskilling, unemployment, and deindustrialization. These documentaries elegantly organize a complex history of print technology, and the present essay would admittedly have been impossible without them. However, both ultimately elide the capitalist labor dynamics that would explain their own narratives. 

Douglas Wilson's Linotype stirringly evokes the lost world of hot motel through humanizing portraits of the workers who kept it running. For example, in a near-reprise of his role as the narrator of Farewell Etaoin Shrdlu, the late Carl Schlesinger makes frequent appearances. The filmmakers include footage of him singing and tap dancing, and they indulge him as he tells a long-winded story about the time he met Marilyn Monroe. A casual viewer would never know that Schlesinger was also a lifelong member of the ITU, or that he coauthored an important book on the union's automation strategy. Despite its exhaustiveness, in fact, Linotype manages to bracket the union's existence altogether. Briar Levit's Graphic Means takes up where Linotype leaves off-impressively condensing the jumble of machines that bridged the hot type and digital eras. Graphic Means directly addresses the role of the ITU and, further, the gendered division that arose between unionized hot type shops and "open" cold type shops. However, the decline of the union is presented as a technical inevitability and even as a refutation of male privilege; the phototypesetting bosses interviewed seem to be speaking as feminists when they say that "the girls" did equally admirable work for half the wages. The vulnerability of non-unionized women to the next wave of automation, meanwhile, is never addressed. 

While typesetting has disappeared as a distinct job, it would be too simple to say that it was automated out of existence. Rather, since the late twentieth century the job description of the graphic designer has expanded to include tasks once carried out by the earliest printers. Now that we have considered the standpoints of both modernist radicals and extinct print workers, the contemporary situation of the graphic designer should appear somewhat absurd. Capitalist technological development has rendered texts and images almost infinitely reproducible—and has built unfathomable electro-libraries in the process. But despite this gigantic aggregation of productive force, it is still necessary to put people to work moving words and pictures around, most often in the service of brand competition among otherwise identical commodities. What confronts us is not a world in which machines have freed people from work, but one of mass unemployment, in which some of the most celebrated "innovations" are apps that facilitate short-term, low-wage, benefit-less contracts.

If graphic designers became typesetters, they may turn out to be the last typesetters. The design software that repackaged the knowledge and skill of the printing trades seemed at first to deliver a dreamed-of autonomy to graphic design as a profession. But because these technologies were off-the-shelf consumer products, trained and credentialed designers have less and less of a monopoly on the medium. A general facility with image and text has bled into general literacy—due in no small part to the ease of pirating such "immaterial" commodities as Photoshop. In the contemporary design press, articles on apps like TaskRabbit and Fiverr, or a future role for Al in the automation of design decisions, recall the mix of anxiety and reassurance that characterized coverage of the Linotype nearly 130 years ago.[53] These projected "disruptions" may well turn out to be empty hype. But whatever is in store for graphic design in the coming decades, it will be impossible to understand without accounting for the capitalist constraints and imperatives that have shaped the practice from the beginning.


/ 005. Gefter, Amanda and Quanta Magazine. "The Case Against Reality," 2016.


A professor of cognitive science argues that the world is nothing like the one we experience through ur senses.

As we go about our daily lives, we tend to assume that our perceptions— sights, sounds, textures, tastes—are an accurate portrayal of the real world. Sure, when we stop and think about it—or when we find ourselves fooled by a perceptual illusion—we realize with a jolt that what we perceive is never the world directly, but rather our brain’s best guess at what that world is like, a kind of internal simulation of an external reality. Still, we bank on the fact that our simulation is a reasonably decent one. If it wasn’t, wouldn’t evolution have weeded us out by now? The true reality might be forever beyond our reach, but surely our senses give us at least an inkling of what it’s really like.

Not so, says Donald D. Hoffman, a professor of cognitive science at the University of California, Irvine. Hoffman has spent the past three decades studying perception, artificial intelligence, evolutionary game theory and the brain, and his conclusion is a dramatic one: The world presented to us by our perceptions is nothing like reality. What’s more, he says, we have evolution itself to thank for this magnificent illusion, as it maximizes evolutionary fitness by driving truth to extinction.

Getting at questions about the nature of reality, and disentangling the observer from the observed, is an endeavor that straddles the boundaries of neuroscience and fundamental physics. On one side you’ll find researchers scratching their chins raw trying to understand how a three-pound lump of gray matter obeying nothing more than the ordinary laws of physics can give rise to first-person conscious experience. This is the aptly named “hard problem.”

On the other side are quantum physicists, marveling at the strange fact that quantum systems don’t seem to be definite objects localized in space until we come along to observe them. Experiment after experiment has shown— defying common sense—that if we assume that the particles that make up ordinary objects have an objective, observer-independent existence, we get the wrong answers. The central lesson of quantum physics is clear: There are no public objects sitting out there in some preexisting space. As the physicist John Wheeler put it, “Useful as it is under ordinary circumstances to say that the world exists ‘out there’ independent of us, that view can no longer be upheld.”

So while neuroscientists struggle to understand how there can be such a thing as a first-person reality, quantum physicists have to grapple with the mystery of how there can be anything but a first-person reality. In short, all roads lead back to the observer. And that’s where you can find Hoffman—straddling the boundaries, attempting a mathematical model of the observer, trying to get at the reality behind the illusion. Quanta Magazine caught up with him to find out more.

Gefter: People often use Darwinian evolution as an argument that our perceptions accurately reflect reality. They say, “Obviously we must be latching onto reality in some way because otherwise we would have been wiped out a long time ago. If I think I’m seeing a palm tree but it’s really a tiger, I’m in trouble.”

Hoffman: Right. The classic argument is that those of our ancestors who saw more accurately had a competitive advantage over those who saw less accurately and thus were more likely to pass on their genes that coded for those more accurate perceptions, so after thousands of generations we can be quite confident that we’re the offspring of those who saw accurately, and so we see accurately. That sounds very plausible. But I think it is utterly false. It misunderstands the fundamental fact about evolution, which is that it’s about fitness functions—mathematical functions that describe how well a given strategy achieves the goals of survival and reproduction. The mathematical physicist Chetan Prakash proved a theorem that I devised that says: According to evolution by natural selection, an organism that sees reality as it is will never be more fit than an organism of equal complexity that sees none of reality but is just tuned to fitness. Never.

Gefter: You’ve done computer simulations to show this. Can you give an example?

Hoffman: Suppose in reality there’s a resource, like water, and you can quantify how much of it there is in an objective order—very little water, medium amount of water, a lot of water. Now suppose your fitness function is linear, so a little water gives you a little fitness, medium water gives you medium fitness, and lots of water gives you lots of fitness—in that case, the organism that sees the truth about the water in the world can win, but only because the fitness function happens to align with the true structure in reality. Generically, in the real world, that will never be the case. Something much more natural is a bell curve—say, too little water you die of thirst, but too much water you drown, and only somewhere in between is good for survival. Now the fitness function doesn’t match the structure in the real world. And that’s enough to send truth to extinction. For example, an organism tuned to fitness might see small and large quantities of some resource as, say, red, to indicate low fitness, whereas they might see intermediate quantities as green, to indicate high fitness. Its perceptions will be tuned to fitness, but not to truth. It won’t see any distinction between small and large—it only sees red— even though such a distinction exists in reality.

Gefter: But how can seeing a false reality be beneficial to an organism’s survival?

Hoffman: There’s a metaphor that’s only been available to us in the past 30 or 40 years, and that’s the desktop interface. Suppose there’s a blue rectangular icon on the lower right corner of your computer’s desktop — does that mean that the file itself is blue and rectangular and lives in the lower right corner of your computer? Of course not. But those are the only things that can be asserted about anything on the desktop — it has color, position, and shape. Those are the only categories available to you, and yet none of them are true about the file itself or anything in the computer. They couldn’t possibly be true. That’s an interesting thing. You could not form a true description of the innards of the computer if your entire view of reality was confined to the desktop. And yet the desktop is useful. That blue rectangular icon guides my behavior, and it hides a complex reality that I don’t need to know. That’s the key idea. Evolution has shaped us with perceptions that allow us to survive. They guide adaptive behaviors. But part of that involves hiding from us the stuff we don’t need to know. And that’s pretty much all of reality, whatever reality might be. If you had to spend all that time figuring it out, the tiger would eat you.

Gefter: So everything we see is one big illusion?

Hoffman: We’ve been shaped to have perceptions that keep us alive, so we have to take them seriously. If I see something that I think of as a snake, I don’t pick it up. If I see a train, I don’t step in front of it. I’ve evolved these symbols to keep me alive, so I have to take them seriously. But it’s a logical flaw to think that if we have to take it seriously, we also have to take it literally.

Gefter: If snakes aren’t snakes and trains aren’t trains, what are they?

Hoffman: Snakes and trains, like the particles of physics, have no objective, observer-independent features. The snake I see is a description created by my sensory system to inform me of the fitness consequences of my actions. Evolution shapes acceptable solutions, not optimal ones. A snake is an acceptable solution to the problem of telling me how to act in a situation. My snakes and trains are my mental representations; your snakes and trains are your mental representations.

Gefter: How did you first become interested in these ideas?

Hoffman: As a teenager, I was very interested in the question “Are we machines?” My reading of the science suggested that we are. But my dad was a minister, and at church they were saying we’re not. So I decided I needed to figure it out for myself. It’s sort of an important personal question—if I’m a machine, I would like to find that out! And if I’m not, I’d like to know, what is that special magic beyond the machine? So eventually in the 1980s I went to the artificial-intelligence lab at MIT and worked on machine perception. The field of vision research was enjoying a newfound success in developing mathematical models for specific visual abilities. I noticed that they seemed to share a common mathematical structure, so I thought it might be possible to write down a formal structure for observation that encompassed all of them, perhaps all possible modes of observation. I was inspired in part by Alan Turing. When he invented the Turing machine, he was trying to come up with a notion of computation, and instead of putting bells and whistles on it, he said, Let’s get the simplest, most pared down mathematical description that could possibly work. And that simple formalism is the foundation for the science of computation. So I wondered, could I provide a similarly simple formal foundation for the science of observation?

Gefter: A mathematical model of consciousness.

Hoffman: That’s right. My intuition was, there are conscious experiences. I have pains, tastes, smells, all my sensory experiences, moods, emotions and so forth. So I’m just going to say: One part of this consciousness structure is a set of all possible experiences. When I’m having an experience, based on that experience I may want to change what I’m doing. So I need to have a collection of possible actions I can take and a decision strategy that, given my experiences, allows me to change how I’m acting. That’s the basic idea of the whole thing. I have a space X of experiences, a space G of actions, and an algorithm D that lets me choose a new action given my experiences. Then I posited a W for a world, which is also a probability space. Somehow the world affects my perceptions, so there’s a perception map P from the world to my experiences, and when I act, I change the world, so there’s a map A from the space of actions to the world. That’s the entire structure. Six elements. The claim is: this is the structure of consciousness. I put that out there so people have something to shoot at.

Gefter: But if there’s a W, are you saying there is an external world?

Hoffman: Here’s the striking thing about that. I can pull the W out of the model and stick a conscious agent in its place and get a circuit of conscious agents. In fact, you can have whole networks of arbitrary complexity. And that’s the world.

Gefter: The world is just other conscious agents?

Hoffman: I call it conscious realism: Objective reality is just conscious agents, just points of view. Interestingly, I can take two conscious agents and have them interact, and the mathematical structure of that interaction also satisfies the definition of a conscious agent. This mathematics is telling me something. I can take two minds, and they can generate a new, unified single mind. Here’s a concrete example. We have two hemispheres in our brain. But when you do a split-brain operation, a complete transection of the corpus callosum, you get clear evidence of two separate consciousnesses. Before that slicing happened, it seemed there was a single unified consciousness. So it’s not implausible that there is a single conscious agent. And yet it’s also the case that there are two conscious agents there, and you can see that when they’re split. I didn’t expect that, the mathematics forced me to recognize this. It suggests that I can take separate observers, put them together and create new observers, and keep doing this ad infinitum. It’s conscious agents all the way down.

Gefter: If it’s conscious agents all the way down, all first-person points of view, what happens to science? Science has always been a third-person description of the world.

Hoffman: The idea that what we’re doing is measuring publicly accessible objects, the idea that objectivity results from the fact that you and I can measure the same object in the exact same situation and get the same results — it’s very clear from quantum mechanics that that idea has to go. Physics tells us that there are no public physical objects. So what’s going on? Here’s how I think about it. I can talk to you about my headache and believe that I am communicating effectively with you, because you’ve had your own headaches. The same thing is true as apples and the moon and the sun and the universe. Just like you have your own headache, you have your own moon. But I assume it’s relevantly similar to mine. That’s an assumption that could be false, but that’s the source of my communication, and that’s the best we can do in terms of public physical objects and objective science.

Gefter: It doesn’t seem like many people in neuroscience or philosophy of mind are thinking about fundamental physics. Do you think that’s been a stumbling block for those trying to understand consciousness?

Hoffman: I think it has been. Not only are they ignoring the progress in fundamental physics, they are often explicit about it. They’ll say openly that quantum physics is not relevant to the aspects of brain function that are causally involved in consciousness. They are certain that it’s got to be classical properties of neural activity, which exist independent of any observers— spiking rates, connection strengths at synapses, perhaps dynamical properties as well. These are all very classical notions under Newtonian physics, where time is absolute and objects exist absolutely. And then [neuroscientists] are mystified as to why they don’t make progress. They don’t avail themselves of the incredible insights and breakthroughs that physics has made. Those insights are out there for us to use, and yet my field says, “We’ll stick with Newton, thank you. We’ll stay 300 years behind in our physics.”

Gefter: I suspect they’re reacting to things like Roger Penrose and Stuart Hameroff’s model, where you still have a physical brain, it’s still sitting in space, but supposedly it’s performing some quantum feat. In contrast, you’re saying, “Look, quantum mechanics is telling us that we have to question the very notions of ‘physical things’ sitting in ‘space.’”

Hoffman: I think that’s absolutely true. The neuroscientists are saying, “We don’t need to invoke those kind of quantum processes, we don’t need quantum wave functions collapsing inside neurons, we can just use classical physics to describe processes in the brain.” I’m emphasizing the larger lesson of quantum mechanics: Neurons, brains, space … these are just symbols we use, they’re not real. It’s not that there’s a classical brain that does some quantum magic. It’s that there’s no brain! Quantum mechanics says that classical objects—including brains—don’t exist. So this is a far more radical claim about the nature of reality and does not involve the brain pulling off some tricky quantum computation. So even Penrose hasn’t taken it far enough. But most of us, you know, we’re born realists. We’re born physicalists. This is a really, really hard one to let go of.

Gefter: To return to the question you started with as a teenager, are we machines?

Hoffman: The formal theory of conscious agents I’ve been developing is computationally universal—in that sense, it’s a machine theory. And it’s because the theory is computationally universal that I can get all of cognitive science and neural networks back out of it. Nevertheless, for now I don’t think we are machines—in part because I distinguish between the mathematical representation and the thing being represented. As a conscious realist, I am postulating conscious experiences as ontological primitives, the most basic ingredients of the world. I’m claiming that experiences are the real coin of the realm. The experiences of everyday life—my real feeling of a headache, my real taste of chocolate—that really is the ultimate nature of reality.


/ 006. "Pause Against AI Experiments: An Open Letter," 2023.


AI systems with human-competitive intelligence can pose profound risks to society and humanity, as shown by extensive research and acknowledged by top AI labs. As stated in the widely-endorsed Asilomar AI Principles, Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources. Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one – not even their creators – can understand, predict, or reliably control.

Contemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable. This confidence must be well justified and increase with the magnitude of a system's potential effects. OpenAI's recent statement regarding artificial general intelligence, states that "At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models." We agree. That point is now.

Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT@4. This pause should be public and verifiable, and include all key actors. If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.

AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts. These protocols should ensure that systems adhering to them are safe beyond a reasonable doubt. This does not mean a pause on AI development in general, merely a stepping back from the dangerous race to ever-larger unpredictable black-box models with emergent capabilities.

AI research and development should be refocused on making today's powerful, state-of-the-art systems more accurate, safe, interpretable, transparent, robust, aligned, trustworthy, and loyal.

In parallel, AI developers must work with policymakers to dramatically accelerate development of robust AI governance systems. These should at a minimum include: new and capable regulatory authorities dedicated to AI; oversight and tracking of highly capable AI systems and large pools of computational capability; provenance and watermarking systems to help distinguish real from synthetic and to track model leaks; a robust auditing and certification ecosystem; liability for AI-caused harm; robust public funding for technical AI safety research; and well-resourced institutions for coping with the dramatic economic and political disruptions (especially to democracy) that AI will cause.

Humanity can enjoy a flourishing future with AI. Having succeeded in creating powerful AI systems, we can now enjoy an "AI summer" in which we reap the rewards, engineer these systems for the clear benefit of all, and give society a chance to adapt. Society has hit pause on other technologies with potentially catastrophic effects on society. We can do so here. Let's enjoy a long AI summer, not rush unprepared into a fall.


/ 007. Metz, Cade. "What's the Future for A.I.?" 2023.


Where we’re heading tomorrow, next year and beyond.

In today’s A.I. newsletter, the last in our five-part series, I look at where artificial intelligence may be headed in the years to come.

In early March, I visited OpenAI’s San Francisco offices for an early look at GPT-4, a new version of the technology that underpins its ChatGPT chatbot. The most eye-popping moment arrived when Greg Brockman, OpenAI’s president and co-founder, showed off a feature that is still unavailable to the public: He gave the bot a photograph from the Hubble Space Telescope and asked it to describe the image “in painstaking detail.”

The description was completely accurate, right down to the strange white line created by a satellite streaking across the heavens. This is one look at the future of chatbots and other A.I. technologies: A new wave of multimodal systems will juggle images, sounds and videos as well as text.

Yesterday, my colleague Kevin Roose told you about what A.I. can do now. I’m going to focus on the opportunities and upheavals to come as it gains abilities and skills.

AI. In the near term

Generative A.I.s can already answer questions, write poetry, generate computer code and carry on conversations. As “chatbot” suggests, they are first being rolled out in conversational formats like ChatGPT and Bing.

But that’s not going to last long. Microsoft and Google have already announced plans to incorporate these A.I. technologies into their products. You’ll be able to use them to write a rough draft of an email, automatically summarize a meeting and pull off many other cool tricks.

OpenAI also offers an A.P.I., or application programming interface, that other tech companies can use to plug GPT-4 into their apps and products. And it has created a series of plug-ins from companies like Instacart, Expedia and Wolfram Alpha that expand ChatGPT’s abilities.

A.I. in the medium term

Many experts believe A.I. will make some workers, including doctors, lawyers and computer programmers, more productive than ever. They also believe some workers will be replaced.

“This will affect tasks that are more repetitive, more formulaic, more generic,” said Zachary Lipton, a professor at Carnegie Mellon who specializes in artificial intelligence and its impact on society. “This can liberate some people who are not good at repetitive tasks. At the same time, there is a threat to people who specialize in the repetitive part.”

Human-performed jobs could disappear from audio-to-text transcription and translation. In the legal field, GPT-4 is already proficient enough to ace the bar exam, and the accounting firm PricewaterhouseCoopers plans to roll out an OpenAI-powered legal chatbot to its staff.

At the same time, companies like OpenAI, Google and Meta are building systems that let you instantly generate images and videos simply by describing what you want to see.

Other companies are building bots that can actually use websites and software applications as a human does. In the next stage of the technology, A.I. systems could shop online for your Christmas presents, hire people to do small jobs around the house and track your monthly expenses.

All that is a lot to think about. But the biggest issue may be this: Before we have a chance to grasp how these systems will affect the world, they will get even more powerful.

A.I. in the long term

For companies like OpenAI and DeepMind, a lab that’s owned by Google’s parent company, the plan is to push this technology as far as it will go. They hope to eventually build what researchers call artificial general intelligence, or A.G.I. — a machine that can do anything the human brain can do.

As Sam Altman, OpenAI’s chief executive, told me three years ago: “My goal is to build broadly beneficial A.G.I. I also understand this sounds ridiculous.” Today, it sounds less ridiculous. But it is still easier said than done.

For an A.I. to become an A.G.I., it will require an understanding of the physical world writ large. And it is not clear whether systems can learn to mimic the length and breadth of human reasoning and common sense using the methods that have produced technologies like GPT-4. New breakthroughs will probably be necessary.

The question is, do we really want artificial intelligence to become that powerful? A very important related question: Is there any way to stop it from happening?

The risks of A.I.

Many A.I. executives believe the technologies they are creating will improve our lives. But some have been warning for decades about a darker scenario, where our creations don’t always do what we want them to do, or they follow our instructions in unpredictable ways, with potentially dire consequences.

A.I. experts talk about “alignment” — that is, making sure A.I. systems are in line with human values and goals.

Before GPT-4 was released, OpenAI handed it over to an outside group to imagine and test dangerous uses of the chatbot.

The group found that the system was able to hire a human online to defeat a Captcha test. When the human asked if it was “a robot,” the system, unprompted by the testers, lied and said it was a person with a visual impairment.

Testers also showed that the system could be coaxed into suggesting how to buy illegal firearms online and into describing ways to make dangerous substances from household items. After changes by OpenAI, the system no longer does these things.

But it’s impossible to eliminate all potential misuses. As a system like this learns from data, it develops skills that its creators never expected. It is hard to know how things might go wrong after millions of people start using it.

“Every time we make a new A.I. system, we are unable to fully characterize all its capabilities and all of its safety problems — and this problem is getting worse over time rather than better,” said Jack Clark, a founder and the head of policy of Anthropic, a San Francisco start-up building this same kind of technology.

And OpenAI and giants like Google are hardly the only ones exploring this technology. The basic methods used to build these systems are widely understood, and other companies, countries, research labs and bad actors may be less careful.

The remedies for A.I. 

Ultimately, keeping a lid on dangerous A.I. technology will require far-reaching oversight. But experts are not optimistic.

“We need a regulatory system that is international,” said Aviv Ovadya, a researcher at the Berkman Klein Center for Internet & Society at Harvard who helped test GPT-4 before its release. “But I do not see our existing government institutions being about to navigate this at the rate that is necessary.”

As we told you earlier this week, more than 1,000 technology leaders and researchers, including Elon Musk, have urged artificial intelligence labs to pause development of the most advanced systems, warning in an open letter that A.I. tools present “profound risks to society and humanity.”

A.I. developers are “locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one — not even their creators — can understand, predict or reliably control,” according to the letter.

Some experts are mostly concerned about near-term dangers, including the spread of disinformation and the risk that people would rely on these systems for inaccurate or harmful medical and emotional advice.

But other critics are part of a vast and influential online community called rationalists or effective altruists, who believe that A.I could eventually destroy humanity. This mind-set is reflected in the letter.


/ 008. Ryan-Mosley, Tate. "An early guide to policymaking on generative AI," 2023.


Earlier this week, I was chatting with a policy professor in Washington, DC, who told me that students and colleagues alike are asking about GPT-4 and generative AI: What should they be reading? How much attention should they be paying?

She wanted to know if I had any suggestions, and asked what I thought all the new advances meant for lawmakers. I’ve spent a few days thinking, reading, and chatting with the experts about this, and my answer morphed into this newsletter. So here goes!

Though GPT-4 is the standard bearer, it’s just one of many high-profile generative AI releases in the past few months: Google, Nvidia, Adobe, and Baidu have all announced their own projects. In short, generative AI is the thing that everyone is talking about. And though the tech is not new, its policy implications are months if not years from being understood.

GPT-4, released by OpenAI last week, is a multimodal large language model that uses deep learning to predict words in a sentence. It generates remarkably fluent text, and it can respond to images as well as word-based prompts. For paying customers, GPT-4 will now power ChatGPT, which has already been incorporated into commercial applications.

The newest iteration has made a major splash, and Bill Gates called it “revolutionary” in a letter this week. However, OpenAI has also been criticized for a lack of transparency about how the model was trained and evaluated for bias.

Despite all the excitement, generative AI comes with significant risks. The models are trained on the toxic repository that is the internet, which means they often produce racist and sexist output. They also regularly make things up and state them with convincing confidence. That could be a nightmare from a misinformation standpoint and could make scams more persuasive and prolific.

Generative AI tools are also potential threats to people’s security and privacy, and they have little regard for copyright laws. Companies using generative AI that has stolen the work of others are already being sued.

Alex Engler, a fellow in governance studies at the Brookings Institution, has considered how policymakers should be thinking about this and sees two main types of risks: harms from malicious use and harms from commercial use. Malicious uses of the technology, like disinformation, automated hate speech, and scamming, “have a lot in common with content moderation,” Engler said in an email to me, “and the best way to tackle these risks is likely platform governance.” (If you want to learn more about this, I’d recommend listening to this week’s Sunday Show from Tech Policy Press, where Justin Hendrix, an editor and a lecturer on tech, media, and democracy, talks with a panel of experts about whether generative AI systems should be regulated similarly to search and recommendation algorithms. Hint: Section 230.)

Policy discussions about generative AI have so far focused on that second category: risks from commercial use of the technology, like coding or advertising. So far, the US government has taken small but notable actions, primarily through the Federal Trade Commission (FTC). The FTC issued a warning statement to companies last month urging them not to make claims about technical capabilities that they can’t substantiate, such as overstating what AI can do. This week, on its business blog, it used even stronger language about risks companies should consider when using generative AI.

“If you develop or offer a synthetic media or generative AI product, consider at the design stage and thereafter the reasonably foreseeable—and often obvious—ways it could be misused for fraud or cause other harm. Then ask yourself whether such risks are high enough that you shouldn’t offer the product at all,” the blog post reads.

The US Copyright Office also launched a new initiative intended to deal with the thorny policy questions around AI, attribution, and intellectual property.

The EU, meanwhile, is sticking true to its reputation as the world leader in tech policy. At the start of this year my colleague Melissa Heikkilä wrote about the EU’s efforts to try to pass the AI Act. It’s a set of rules that would prevent companies from releasing models into the wild without disclosing their inner workings, which is precisely what some critics are accusing OpenAI of with the GPT-4 release.

The EU intends to separate high-risk uses of AI, like hiring, legal, or financial applications, from lower-risk uses like video games and spam filters, and require more transparency around the more sensitive uses. OpenAI has acknowledged some of the concerns about the speed of adoption. In fact, its own CEO, Sam Altman, told ABC News he shares many of the same fears. However, the company is still not disclosing key data about GPT-4.

For policy folks in Washington, Brussels, London, and offices everywhere else in the world, it’s important to understand that generative AI is here to stay. Yes, there’s significant hype, but the recent advances in AI are as real and important as the risks that they pose.

Yesterday, the United States Congress called Shou Zi Chew, the CEO of TikTok, to a hearing about privacy and security concerns raised by the popular social media app. His appearance came after the Biden administration threatened a national ban if its parent company, ByteDance, didn’t sell off the majority of its shares.

There were lots of headlines, most using a temporal pun, and the hearing laid bare the depths of the new technological cold war between the US and China. For many watching, the hearing was both important and disappointing, with some legislators displaying poor technical understanding and hypocrisy about how Chinese companies handle privacy when American companies collect and trade data in much the same ways.

It also revealed how deeply American lawmakers distrust Chinese tech. Here are some of the spicier takes and helpful articles to get up to speed:

Key takeaways from TikTok hearing in Congress – and the uncertain road ahead - Kari Paul and Johana Bhuiyan, The Guardian
What to Know About the TikTok Security Concerns - Billy Perrigo, Time
America’s online privacy problems are much bigger than TikTok - Will Oremus, Washington Post
There’s a Problem With Banning TikTok. It’s Called the First
Amendment - Jameel Jaffer (Executive Director of the Knight First Amendment Institute), NYT Opinion

AI is able to persuade people to change their minds about hot-button political issues like an assault weapon ban and paid parental leave, according to a study by a team at Stanford’s Polarization and Social Change Lab. The researchers compared people’s political opinions on a topic before and after reading an AI-generated argument, and found that these arguments can be as effective as human-written ones in persuading the readers: “AI ranked consistently as more factual and logical, less angry, and less reliant upon storytelling as a persuasive technique.”

The teams point to concerns about the use of generative AI in a political context, such as in lobbying or online discourse. (For more on the use of generative AI in politics, do please read this recent piece by Nathan Sanders and Bruce Schneier.)


/ 009. North, Geoffrey. "A celebration of connectionism," 1987.


New developments in neural network theory have excited both psychologists and neurobiologists. Practitioners of the new art displayed their wares last week.

WHEN David Rumelhart, Geoffrey Hinton and Ronald Williams described for neural networks a powerful new learning procedure called back-propagation (Nature 323, 533; 1986), they noted that theirs was not a plausible model of how brains learn. Yet the generality of their approach, and the several intriguing features of network learning by backpropagation which have come to light, have stimulated a resurgence of interest in neural network models among neuroscientists, theoreticians and experimentalists alike. Last week, at a meeting organized by the Society of Experimental Psychology, a packed audience heard Geoffrey Hinton describe a new learning algorithm that seems a better model of biological learning than is back-propagation by parallel networks which nevertheless seems to retain much of the power of its predecessor.

In parallel distributed processing, a network can be thought of as embodying a mathematical function mapping vectors in 'input space' to vectors in 'output space', much as matrices effect linear transformations between vector spaces. A vector in the neural context is simply the pattern of excitation of some set of units taken to be the input or the output of the network. The processing is thus distributed in the pattern of the connections between units of the network and their strengths. Corresponding to the real physiological task of, say, pattern recognition, will be some kind of network function mapping inputs (patterns) onto outputs (interpretations). The all-important question is what kind of network is needed for a particular task.

Simple networks developed in the 1960s, known as perceptrons, involved only two layers of units, an input and an output, with direct connections between them. Such networks are very limited in the range of tasks they can carry out. The versatility of a network can be greatly increased by the introduction of intermediate layers of 'hidden' units, but this raises the problem of how it can be trained.

Back-propagation provides an elegant way of training a multi-layered network. During learning, the output vectors generated by the network for a given input are compared with the desired output, giving an error calculated from the difference between the two. Back-propagation calculates the dependence of this error on all the connection weights, simply by using the chain-rule for differentiation, and the weights are adjusted to reduce the error, so that the network converges by gradient descent on the required structure.

During learning, the network comes to capture certain general features which are characteristic of its task. The hidden units, in particular, develop features that seem especially significant to neuroscientists who record the properties of single neurons in brains. For example, in some cases they are reminiscent of the way in which some neurons in the brain are found to be specific for different aspects of the representation of the visual field.

Even so, this system of learning by back-propagation has not seemed very biologically realistic. Hinton (CarnegieMellon University) and his colleagues have been looking for a more plausible system oflearning.

The new development is known as a 'recirculation' algorithm, and works as follows. In a network learning by backpropagation, there is a linear flow of activity (via the hidden intermediate units) from the input units to the output units.In the new system, the hidden units connect back to the single layer of 'visible' input units. Activity thus recirculates through the network; during training, the connection weights are adjusted to minimize the rate of change of activity in each unit. Thus, when trained, the network is set up so as to stabilize on certain states, and so can work as a kind of 'content addressable memory' with the property that degraded or incomplete forms of the training inputs can regenerate the correct version.

It has been shown that, under certain conditions, the new algorithm is equivalent to gradient descent, and it has been found empirically that the system still works when these conditions are relaxed.

A number of interesting applications of back-propagation were reported at the meeting. Hinton described a network for recognizing one-dimensional shapes on a one-dimensional retina independently of position: the hidden units learn to respond to shapes in different positions. Hinton also described a speech-recognition network which learns to recognize spoken consonants given very noisy corrupted data. It appears to perform almost as well as people, and better than the previous best system of automated speech recognition.

Several speakers described analogies between the behaviour of their networks during training or after 'damage' and what is known of human learning and cognitive disorders. For example, J. L. McClelland (Carnegie-Mellon University) described how a network for learning a balancebeam task progressed through stages of competence similar to those of children given the same task. The problem is to decide which way a balance-beam will tip, depending on the position and size of weights on either side of the fulcrum. With an appropriately biased learning environment, such as children might well experience, the network, like children, initially bases its decisions purely on weight information; gradually the network learns to use the position of the weights. M.S. Seidenberg (McGill University) described a network for word recognition and pronunciation that, when 'damaged' by the removal of hidden units, displayed behaviour reminiscent of some human disorders, such as dyslexia.

Parallel distributed processing is not without its critics, and S. Pinker (MIT) reported that a linguistic analysis of Rumelhart and McClelland's network for changing the tense of verbs in sentences shows that the system is not 'descriptively adequate' as a model for human language, in that it abandons certain symbolic rules and principles that linguistic studies suggest are crucial in human language.

David Willshaw (Edinburgh) asked whether parallel distributed processing networks might, like perceptrons, similarly cease to make significant progress and fade in interest after a period of development and excitement. McClelland's riposte was that work on perceptrons was severely limited by the available computer power and circumstances are now sufficiently different to justify optimism.

The biological relevance of parallel distributed processing remains an open question. Independently of relevance, however, work on network systems may be of interest at a purely theoretical level. The present work is a kind of experimental mathematics, and in that respect is rather similar to that of Mandlebrot on fractals, also made possible and inspired by computers. The hope is that, in future, deductive proofs will give a more rigorous basis to work on networks.

Many interesting problems remain. On what set of functions will a given network topology converge? How can the optimal network for a given task be predicted, and how long will training take? And so on.


/ 010. Hinton, Geoffrey E. "Computation by neural networks," 2000.


Networks of neurons can perform com- putations that have proved very difficult to emulate in conventional computers. In trying to understand how real ner- vous systems achieve their remarkable computational abilities, researchers have been confronted with three major the- oretical issues. How can we characterize the dynamics of neural networks with recurrent  connections?  How  do  the time-varying activities of populations of neurons represent things? How are synapse strengths adjusted to learn these representations? To gain insight into these difficult theoretical issues, it has proved necessary to study grossly ideal- ized models that are as different from real biological neural networks as apples are from planets.

The 1980s saw major progress on all three fronts. In a classic 1982 paper1, Hopfield showed that asynchronous networks with symmetrically connected neurons would settle to locally stable states, known as ‘point attractors’, which could be viewed as content-addressable memories. Although these networks were both computationally inefficient and biologically unrealistic, Hopfield’s work inspired a new generation of recurrent network models; one early example was a learning algorithm that could automatically construct efficient and robust population codes in ‘hidden’ neurons whose activities were never explicitly specified by the training environment.

The 1980s also saw the widespread use of the backpropagation algorithm for training the synaptic weights in both feedforward and recurrent neural networks. Backpropagation is simply an efficient method for computing how changing the weight of any given synapse would affect the difference between the way the network actually behaves in response to a particular training input and the way a teacher desires it to behave3. Backpropagation is not a plausible model of how real synapses learn, because it requires a teacher to specify the desired behavior of the network, it uses connections backward, and it is very slow in large networks. However, backpropagation did demonstrate the impressive power of adjusting synapses to optimize a performance measure. It also allowed psychologists to design neural networks that could perform interesting computations in unexpected ways. For example, a recurrent network that is trained to derive the meaning of words from their spelling makes very surprising errors when damaged, and these errors are remarkably similar to those made by adults with dyslexia.

The practical success of backpropagation led researchers to look for an alternative performance measure that did not involve a teacher and that could easily be optimized using information that was locally available at a synapse. A measure with all the right properties emerges from thinking about perception in a peculiar way: the widespread existence of top-down connections in the brain, coupled with our ability to generate mental images, suggests that the perceptual system may literally contain a generative model of sensory data. A generative model stands in the same relationship to perception as do computer graphics to computer vision. It allows the sensory data to be generated from a high-level description of the scene. Perception can be seen as the process of inverting the generative model—inferring a high-level description from sensory data under the assumption that the data were produced by the generative model. Learning then is the process of updating the parameters of the generative model so as to maximize the likelihood that it would generate the observed sensory data.

Many neuroscientists find this way of thinking unappealing because the obvious function of the perceptual system is to go from the sensory data to a high-level representation, not vice versa. But to understand how we extract the causes from a particular image sequence, or how we learn the classes of things that might be causes, it is very helpful to think in terms of a top-down, stochastic, generative model. This is exactly the approach that statisticians take to modeling data, and recent advances in the complexity of such statistical models5 provide a rich source of ideas for understanding neural computation. All the best speech recognition programs now work by fitting a probabilistic generative model.

If the generative model is linear, the fitting is relatively straightforward but can nevertheless lead to impressive results6,7. There is good empirical evidence that the brain uses generative models with temporal dynamics for motor control8 (see also ref. 9, this issue). If the generative model is nonlinear and allows multiple causes, it can be very difficult to compute the likely causes of a pattern of sensory inputs. When exact inference is unfeasible, it is possible to use bottom-up, feedforward connections to activate approximately the right causes, and this leads to a learning algorithm for fitting hierarchical nonlinear models that requires only information that is locally available at synapses10. So far, theoretical neuroscientists have considered only a few simple types of nonlinear generative model. Although these have produced impressive results, it seems likely that more sophisticated models and better fitting techniques will be required to make detailed contact with neural reality.


/ 011. Rotman, David. "ChatGPT is about to revolutionize the economy. We need to decide what that looks like," 2023.


New large language models will transform many jobs. Whether they will lead to widespread prosperity or not is up to us.

Whether it’s based on hallucinatory beliefs or not, an artificial-intelligence gold rush has started over the last several months to mine the anticipated business opportunities from generative AI models like ChatGPT. App developers, venture-backed startups, and some of the world’s largest corporations are all scrambling to make sense of the sensational textgenerating bot released by OpenAI last November.

You can practically hear the shrieks from corner offices around the world: “What is our ChatGPT play? How do we make money off this?”

But while companies and executives see a clear chance to cash in, the likely impact of the technology on workers and the economy on the whole is far less obvious. Despite their limitations—chief among of them their propensity for making stuff up—ChatGPT and other recently released generative AI models hold the promise of automating all sorts of tasks that were previously thought to be solely in the realm of human creativity and reasoning, from writing to creating graphics to summarizing and analyzing data. That has left economists unsure how jobs and overall productivity might be affected.

For all the amazing advances in AI and other digital tools over the last decade, their record in improving prosperity and spurring widespread economic growth is discouraging. Although a few investors and entrepreneurs have become very rich, most people haven’t benefited. Some have even been automated out of their jobs.

Productivity growth, which is how countries become richer and more prosperous, has been dismal since around 2005 in the US and in most advanced economies (the UK is a particular basket case). The fact that the economic pie is not growing much has led to stagnant wages for many people.

What productivity growth there has been in that time is largely confined to a few sectors, such as information services, and in the US to a few cities— think San Jose, San Francisco, Seattle, and Boston.

Will ChatGPT make the already troubling income and wealth inequality in the US and many other countries even worse? Or could it help? Could it in fact provide a much-needed boost to productivity?

ChatGPT, with its human-like writing abilities, and OpenAI’s other recent release DALL-E 2, which generates images on demand, use large language models trained on huge amounts of data. The same is true of rivals such as Claude from Anthropic and Bard from Google. These so-called foundational models, such as GPT-3.5 from OpenAI, which ChatGPT is based on, or Google’s competing language model LaMDA, which powers Bard, have evolved rapidly in recent years.

They keep getting more powerful: they’re trained on ever more data, and the number of parameters—the variables in the models that get tweaked—is rising dramatically. Earlier this month, OpenAI released its newest version, GPT-4. While OpenAI won’t say exactly how much bigger it is, one can guess; GPT-3, with some 175 billion parameters, was about 100 times larger than GPT-2.

But it was the release of ChatGPT late last year that changed everything for many users. It’s incredibly easy to use and compelling in its ability to rapidly create human-like text, including recipes, workout plans, and— perhaps most surprising—computer code. For many non-experts, including a growing number of entrepreneurs and businesspeople, the user-friendly chat model—less abstract and more practical than the impressive but often esoteric advances that been brewing in academia and a handful of hightech companies over the last few years—is clear evidence that the AI revolution has real potential.

Venture capitalists and other investors are pouring billions into companies based on generative AI, and the list of apps and services driven by large language models is growing longer every day.

Among the big players, Microsoft has invested a reported $10 billion in OpenAI and its ChatGPT, hoping the technology will bring new life to its long-struggling Bing search engine and fresh capabilities to its Office products. In early March, Salesforce said it will introduce a ChatGPT app in its popular Slack product; at the same time, it announced a $250 million fund to invest in generative AI startups. The list goes on, from Coca-Cola to GM. Everyone has a ChatGPT play.

Meanwhile, Google announced it is going to use its new generative AI tools in Gmail, Docs, and some of its other widely used products.

Still, there are no obvious killer apps yet. And as businesses scramble for ways to use the technology, economists say a rare window has opened for rethinking how to get the most benefits from the new generation of AI.

“We’re talking in such a moment because you can touch this technology. Now you can play with it without needing any coding skills. A lot of people can start imagining how this impacts their workflow, their job prospects,” says Katya Klinova, the head of research on AI, labor, and the economy at the Partnership on AI in San Francisco.

“The question is who is going to benefit? And who will be left behind?” says Klinova, who is working on a report outlining the potential job impacts of generative AI and providing recommendations for using it to increase shared prosperity.

The optimistic view: it will prove to be a powerful tool for many workers, improving their capabilities and expertise, while providing a boost to the overall economy. The pessimistic one: companies will simply use it to destroy what once looked like automation-proof jobs, well-paying ones that require creative skills and logical reasoning; a few high-tech companies and tech elites will get even richer, but it will do little for overall economic growth.

Helping the least skilled

The question of ChatGPT’s impact on the workplace isn’t just a theoretical one.

In the most recent analysis, OpenAI’s Tyna Eloundou, Sam Manning, and Pamela Mishkin, with the University of Pennsylvania’s Daniel Rock, found that large language models such as GPT could have some effect on 80% of the US workforce. They further estimated that the AI models, including GPT-4 and other anticipated software tools, would heavily affect 19% of jobs, with at least 50% of the tasks in those jobs “exposed.” In contrast to what we saw in earlier waves of automation, higher-income jobs would be most affected, they suggest. Some of the people whose jobs are most vulnerable: writers, web and digital designers, financial quantitative analysts, and—just in case you were thinking of a career change— blockchain engineers.

“There is no question that [generative AI] is going to be used—it’s not just a novelty,” says David Autor, an MIT labor economist and a leading expert on the impact of technology on jobs. “Law firms are already using it, and that’s just one example. It opens up a range of tasks that can be automated.”

Autor has spent years documenting how advanced digital technologies have destroyed many manufacturing and routine clerical jobs that once paid well. But he says ChatGPT and other examples of generative AI have changed the calculation.

Previously, AI had automated some office work, but it was those rote stepby- step tasks that could be coded for a machine. Now it can perform tasks that we have viewed as creative, such as writing and producing graphics. “It’s pretty apparent to anyone who’s paying attention that generative AI opens the door to computerization of a lot of kinds of tasks that we think of as not easily automated,” he says.

The worry is not so much that ChatGPT will lead to large-scale unemployment—as Autor points out, there are plenty of jobs in the US—but that companies will replace relatively well-paying white-collar jobs with this new form of automation, sending those workers off to lower-paying service employment while the few who are best able to exploit the new technology reap all the benefits.

In this scenario, tech-savvy workers and companies could quickly take up the AI tools, becoming so much more productive that they dominate their workplaces and their sectors. Those with fewer skills and little technical acumen to begin with would be left further behind.

But Autor also sees a more positive possible outcome: generative AI could help a wide swath of people gain the skills to compete with those who have more education and expertise.

One of the first rigorous studies done on the productivity impact of ChatGPT suggests that such an outcome might be possible.

Two MIT economics graduate students, Shakked Noy and Whitney Zhang, ran an experiment involving hundreds of college-educated professionals working in areas like marketing and HR; they asked half to use ChatGPT in their daily tasks and the others not to. ChatGPT raised overall productivity (not too surprisingly), but here’s the really interesting result: the AI tool helped the least skilled and accomplished workers the most, decreasing the performance gap between employees. In other words, the poor writers got much better; the good writers simply got a little faster.

The preliminary findings suggest that ChatGPT and other generative AIs could, in the jargon of economists, “upskill” people who are having trouble finding work. There are lots of experienced workers “lying fallow” after being displaced from office and manufacturing jobs over the last few decades, Autor says. If generative AI can be used as a practical tool to broaden their expertise and provide them with the specialized skills required in areas such as health care or teaching, where there are plenty of jobs, it could revitalize our workforce.

Determining which scenario wins out will require a more deliberate effort to think about how we want to exploit the technology.

“I don’t think we should take it as the technology is loose on the world and we must adapt to it. Because it’s in the process of being created, it can be used and developed in a variety of ways,” says Autor. “It’s hard to overstate the importance of designing what it’s there for.”

Simply put, we are at a juncture where either less-skilled workers will increasingly be able to take on what is now thought of as knowledge work, or the most talented knowledge workers will radically scale up their existing advantages over everyone else. Which outcome we get depends largely on how employers implement tools like ChatGPT. But the more hopeful option is well within our reach.

Beyond human-like

There are some reasons to be pessimistic, however. Last spring, in “The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence,” the Stanford economist Erik Brynjolfsson warned that AI creators were too obsessed with mimicking human intelligence rather than finding ways to use the technology to allow people to do new tasks and extend their capabilities.

The pursuit of human-like capabilities, Brynjolfsson argued, has led to technologies that simply replace people with machines, driving down wages and exacerbating inequality of wealth and income. It is, he wrote, “the single biggest explanation” for the rising concentration of wealth.

A year later, he says ChatGPT, with its human-sounding outputs, “is like the poster child for what I warned about”: it has “turbocharged” the discussion around how the new technologies can be used to give people new abilities rather than simply replacing them.

Despite his worries that AI developers will continue to blindly outdo each other in mimicking human-like capabilities in their creations, Brynjolfsson, the director of the Stanford Digital Economy Lab, is generally a techno-optimist when it comes to artificial intelligence. Two years ago, he predicted a productivity boom from AI and other digital technologies, and these days he’s bullish on the impact of the new AI models.

Much of Brynjolfsson’s optimism comes from the conviction that businesses could greatly benefit from using generative AI such as ChatGPT to expand their offerings and improve the productivity of their workforce. “It’s a great creativity tool. It’s great at helping you to do novel things. It’s not simply doing the same thing cheaper,” says Brynjolfsson. As long as companies and developers can “stay away from the mentality of thinking that humans aren’t needed,” he says, “it’s going to be very important.”

Within a decade, he predicts, generative AI could add trillions of dollars in economic growth in the US. “A majority of our economy is basically knowledge workers and information workers,” he says. “And it’s hard to think of any type of information workers that won’t be at least partly affected.”

When that productivity boost will come—if it does—is an economic guessing game. Maybe we just need to be patient.

In 1987, Robert Solow, the MIT economist who won the Nobel Prize that year for explaining how innovation drives economic growth, famously said, “You can see the computer age everywhere except in the productivity statistics.” It wasn’t until later, in the mid and late 1990s, that the impacts— particularly from advances in semiconductors—began showing up in the productivity data as businesses found ways to take advantage of ever cheaper computational power and related advances in software.

Could the same thing happen with AI? Avi Goldfarb, an economist at the University of Toronto, says it depends on whether we can figure out how to use the latest technology to transform businesses as we did in the earlier computer age.

So far, he says, companies have just been dropping in AI to do tasks a little bit better: “It’ll increase efficiency—it might incrementally increase productivity—but ultimately, the net benefits are going to be small. Because all you’re doing is the same thing a little bit better.” But, he says, “the technology doesn’t just allow us to do what we’ve always done a little bit better or a little bit cheaper. It might allow us to create new processes to create value to customers.”

The verdict on when—even if—that will happen with generative AI remains uncertain. “Once we figure out what good writing at scale allows industries to do differently, or—in the context of Dall-E—what graphic design at scale allows us to do differently, that’s when we’re going to experience the big productivity boost,” Goldfarb says. “But if that is next week or next year or 10 years from now, I have no idea.”

Power struggle

When Anton Korinek, an economist at the University of Virginia and a fellow at the Brookings Institution, got access to the new generation of large language models such as ChatGPT, he did what a lot of us did: he began playing around with them to see how they might help his work. He carefully documented their performance in a paper in February, noting how well they handled 25 “use cases,” from brainstorming and editing text (very useful) to coding (pretty good with some help) to doing math (not great).

ChatGPT did explain one of the most fundamental principles in economics incorrectly, says Korinek: “It screwed up really badly.” But the mistake, easily spotted, was quickly forgiven in light of the benefits. “I can tell you that it makes me, as a cognitive worker, more productive,” he says. “Hands down, no question for me that I’m more productive when I use a language model.”

When GPT-4 came out, he tested its performance on the same 25 questions that he documented in February, and it performed far better. There were fewer instances of making stuff up; it also did much better on the math assignments, says Korinek.

Since ChatGPT and other AI bots automate cognitive work, as opposed to physical tasks that require investments in equipment and infrastructure, a boost to economic productivity could happen far more quickly than in past technological revolutions, says Korinek. “I think we may see a greater boost to productivity by the end of the year—certainly by 2024,” he says.

What’s more, he says, in the longer term, the way the AI models can make researchers like himself more productive has the potential to drive technological progress.

That potential of large language models is already turning up in research in the physical sciences. Berend Smit, who runs a chemical engineering lab at EPFL in Lausanne, Switzerland, is an expert on using machine learning to discover new materials. Last year, after one of his graduate students, Kevin Maik Jablonka, showed some interesting results using GPT-3, Smit asked him to demonstrate that GPT-3 is, in fact, useless for the kinds of sophisticated machine-learning studies his group does to predict the properties of compounds.

“He failed completely,” jokes Smit.

It turns out that after being fine-tuned for a few minutes with a few relevant examples, the model performs as well as advanced machine-learning tools specially developed for chemistry in answering basic questions about things like the solubility of a compound or its reactivity. Simply give it the name of a compound, and it can predict various properties based on the structure.

As in other areas of work, large language models could help expand the expertise and capabilities of nonexperts— in this case, chemists with little knowledge of complex machine-learning tools. Because it’s as simple as a literature search, Jablonka says, “it could bring machine learning to the masses of chemists.”

These impressive—and surprising—results are just a tantalizing hint of how powerful the new forms of AI could be across a wide swath of creative work, including scientific discovery, and how shockingly easy they are to use. But this also points to some fundamental questions.

As the potential impact of generative AI on the economy and jobs becomes more imminent, who will define the vision for how these tools should be designed and deployed? Who will control the future of this amazing technology?

Diane Coyle, an economist at Cambridge University in the UK, says one concern is the potential for large language models to be dominated by the same big companies that rule much of the digital world. Google and Meta are offering their own large language models alongside OpenAI, she points out, and the large computational costs required to run the software create a barrier to entry for anyone looking to compete.

The worry is that these companies have similar “advertising-driven business models,” Coyle says. “So obviously you get a certain uniformity of thought, if you don’t have different kinds of people with different kinds of incentives.”

Coyle acknowledges that there are no easy fixes, but she says one possibility is a publicly funded international research organization for generative AI, modeled after CERN, the Geneva-based intergovernmental European nuclear research body where the World Wide Web was created in 1989. It would be equipped with the huge computing power needed to run the models and the scientific expertise to further develop the technology.

Such an effort outside of Big Tech, says Coyle, would “bring some diversity to the incentives that the creators of the models face when they’re producing them.”

While it remains uncertain which public policies would help make sure that large language models best serve the public interest, says Coyle, it’s becoming clear that the choices about how we use the technology can’t be left to a few dominant companies and the market alone.

History provides us with plenty of examples of how important government funded research can be in developing technologies that bring about widespread prosperity. Long before the invention of the web at CERN, another publicly funded effort in the late 1960s gave rise to the internet, when the US Department of Defense supported ARPANET, which pioneered ways for multiple computers to communicate with each other.

In Power and Progress: Our 1000-Year Struggle Over Technology & Prosperity, the MIT economists Daron Acemoglu and Simon Johnson provide a compelling walk through the history of technological progress and its mixed record in creating widespread prosperity. Their point is that it’s critical to deliberately steer technological advances in ways that provide broad benefits and don’t just make the elite richer.

From the decades after World War II until the early 1970s, the US economy was marked by rapid technological changes; wages for most workers rose while income inequality dropped sharply. The reason, Acemoglu and Johnson say, is that technological advances were used to create new tasks and jobs, while social and political pressures helped ensure that workers shared the benefits more equally with their employers than they do now.

In contrast, they write, the more recent rapid adoption of manufacturing robots in “the industrial heartland of the American economy in the Midwest” over the last few decades simply destroyed jobs and led to a “prolonged regional decline.”

The book, which comes out in May, is particularly relevant for understanding what today’s rapid progress in AI could bring and how decisions about the best way to use the breakthroughs will affect us all going forward. In a recent interview, Acemoglu said they were writing the book when GPT-3 was first released. And, he adds half-jokingly, “we foresaw ChatGPT.”

Acemoglu maintains that the creators of AI “are going in the wrong direction.” The entire architecture behind the AI “is in the automation mode,” he says. “But there is nothing inherent about generative AI or AI in general that should push us in this direction. It’s the business models and the vision of the people in OpenAI and Microsoft and the venture capital community.”

If you believe we can steer a technology’s trajectory, then an obvious question is: Who is “we”? And this is where Acemoglu and Johnson are most provocative. They write: “Society and its powerful gatekeepers need to stop being mesmerized by tech billionaires and their agenda … One does not need to be an AI expert to have a say about the direction of progress and the future of our society forged by these technologies.”

The creators of ChatGPT and the businesspeople involved in bringing it to market, notably OpenAI’s CEO, Sam Altman, deserve much credit for offering the new AI sensation to the public. Its potential is vast. But that doesn’t mean we must accept their vision and aspirations for where we want the technology to go and how it should be used.

According to their narrative, the end goal is artificial general intelligence, which, if all goes well, will lead to great economic wealth and abundances. Altman, for one, has promoted the vision at great length recently, providing further justification for his longtime advocacy of a universal basic income (UBI) to feed the non-technocrats among us. For some, it sounds tempting. No work and free money! Sweet!

It’s the assumptions underlying the narrative that are most troubling— namely, that AI is headed on an inevitable job-destroying path and most of us are just along for the (free?) ride. This view barely acknowledges the possibility that generative AI could lead to a creativity and productivity boom for workers far beyond the tech-savvy elites by helping to unlock their talents and brains. There is little discussion of the idea of using the technology to produce widespread prosperity by expanding human capabilities and expertise throughout the working population.

As Acemoglu and Johnson write: “We are heading toward greater inequality not inevitably but because of faulty choices about who has power in society and the direction of technology … In fact, UBI fully buys into the vision of the business and tech elite that they are the enlightened, talented people who should generously finance the rest.”

Acemoglu and Johnson write of various tools for achieving “a more balanced technology portfolio,” from tax reforms and other government policies that might encourage the creation of more worker-friendly AI to reforms that might wean academia off Big Tech’s funding for computer science research and business schools.

But, the economists acknowledge, such reforms are “a tall order,” and a social push to redirect technological change is “not just around the corner.”

The good news is that, in fact, we can decide how we choose to use ChatGPT and other large language models. As countless apps based on the technology are rushed to market, businesses and individual users will have a chance to choose how they want to exploit it; companies can decide to use ChatGPT to give workers more abilities—or to simply cut jobs and trim costs.

Another positive development: there is at least some momentum behind open-source projects in generative AI, which could break Big Tech’s grip on the models. Notably, last year more than a thousand international researchers collaborated on a large language model called Bloom that can create text in languages such as French, Spanish, and Arabic. And if Coyle and others are right, increased public funding for AI research could help change the course of future breakthroughs.

Stanford's Brynjolfsson refuses to say he’s optimistic about how it will play out. Still, his enthusiasm for the technology these days is clear. “We can have one of the best decades ever if we use the technology in the right direction,” he says. “But it’s not inevitable.”


/ 012. Seetharaman, Deepa. "Elon Musk, Other AI Experts Call for Pauwse in Technology's Development," 2023.


Appeal causes tension among artificial-intelligence stakeholders amidconcern over pace of advancement

Several tech executives and top artificial-intelligence researchers, including Tesla Inc. Chief Executive Offi cer Elon Musk and AI pioneer Yoshua Bengio , are calling for a pause in the breakneck development of powerful new AI tools.

A moratorium of six months or more would give the industry time to set safety standards for AI design and head off potential harms of the riskiest AI technologies , the proponents of a pause said.

“We’ve reached the point where these systems are smart enough that they canbe used in ways that are dangerous for society,” Mr. Bengio, director of theUniversity of Montreal’s Montreal Institute for Learning Algorithms, said in an interview. “And we don’t yet understand.”

These concerns and the recommendation for the pause were laid out in a letter titled “Pause Giant AI Experiments: An Open Letter” coordinated by the non profit Future of Life Institute, which lists Mr. Musk as an external adviser.The letter that was made public Wednesday was also signed by Apple co-founder Steve Wozniak ; Stability AI CEO Emad Mostaque; and co-founders of the Center for Humane Technology, Tristan Harris and Aza Raskin, who have been critical of social media and AI technology, said a spokeswoman for the team authoring the letter.

The letter doesn’t call for all AI development to halt, but urges companies to temporarily stop training systems more powerful than GPT-4, the technology released this month by Microsoft Corp.-backed startup OpenAI. That includes the next generation of OpenAI’s technology, GPT-5.

OpenAI officials say they haven’t started training GPT-5. In an interview, OpenAICEO Sam Altman said the company has long given priority to safety in development and spent more than six months doing safety tests on GPT-4 before its launch.

“In some sense, this is preaching to the choir,” Mr. Altman said. “We have, I think, been talking about these issues the loudest, with the most intensity, for the longest.”

Calls for a pause clash with a broad desire among tech companies and startups to double down on so-called generative AI, a technology capable of generating original content to human prompts. Buzz around generative AI exploded last fall after OpenAI unveiled a chatbot with its ability to perform functions like providing lengthy answers and producing computer code with humanlike sophistication.

Microsoft has embraced the technology for its Bing search engine and other tools. Alphabet Inc.’s Google has deployed a rival system , and companies such as Adobe Inc., Zoom Video Communications Inc. and Salesforce Inc. have also introduced advanced AI tools.

“A race starts today,” Microsoft CEO Satya Nadella said last month. “We’re going to move, and move fast.”

That approach has spurred renewed concerns that a rapid rollout could have unintended consequences alongside real benefits. Advances in AI have surpassed what many experts believed was possible just a few years ago, said Max Tegmark , one of the organizers of the letter, president of the Future of LifeInstitute and a physics professor at the Massachusetts Institute of Technology.

“It is unfortunate to frame this as an arms race,” Mr. Tegmark said. “It is more of a suicide race. It doesn’t matter who is going to get there first. It just means that humanity as a whole could lose control of its own destiny.”

The Future of Life Institute started working on the letter last week and initially allowed anybody to sign without identity verification. At one point, Mr. Altman’s name was added to the letter, but later removed. Mr. Altman said he never signed the letter. He said the company frequently coordinates with other AI companies on safety standards and to discuss broader concerns.

“There is work that we don’t do because we don’t think we yet know how to make it sufficiently safe,” he said. “So yeah, I think there are ways that you can slowdown on multiple axes and that’s important. And it is part of our strategy.” Messrs. Musk and Wozniak have both voiced concerns about AI technology . Mr.Musk on Wednesday tweeted that developers of the advanced AI technology“ will not heed this warning, but at least it was said.”

Mr. Musk at the same time has embraced some AI tools at Tesla for the company’s advanced driver-assistance functions. Tesla last month said it was recalling around 362,800 vehicles equipped with its technology marketed as FullSelf-Driving Beta . The U.S. top car-safety agency said the technology could, in rare circumstances, violate local traffic laws, potentially increasing the risk of a collision if a driver fails to intervene.

Yann LeCun, chief AI scientist at Meta Platforms Inc., on Tuesday tweeted that he didn’t sign the letter because he disagreed with its premise .

Mr. Mostaque, Stability AI’s CEO, said in a tweet Wednesday that although he signed the letter, he didn’t agree with a six-month pause. “It has no force but will kick off an important discussion that will hopefully bring more transparency &governance to an opaque area.”

Microsoft declined to comment, and Google didn’t immediately respond to a request for comment.

Mr. Tegmark said many companies feel “crazy commercial pressures” to add advanced AI technology into their products. A six-month pause would allow the industry “breathing room,” without disadvantaging ones that opt to move carefully.

The letter said a pause should be declared publicly and be verifiable and all key actors in the space should participate. “If such a pause cannot be enacted quickly, governments should step in and institute a moratorium,” it said.

AI labs and experts can use this time to develop a set of shared safety rules for advanced AI design that should be audited and overseen by outside experts, the authors wrote.

“I don’t think we can afford to just go forward and break things,” said Mr. Bengio, who shared a 2018 Turing award for inventing the systems that modern AI is built on. “We do need to take time to think through this collectively.”


/ 013. Mims, Christopher. "Artificial Intelligence Is Teaching Us New, Surprising Things About the Human Mind," 2023.


Thought is ever-changing electrical patterns unconnected to individual neurons. Meta is working on a system to read your mind

The world has been learning an awful lot about artificial intelligence lately, thanks to the arrival of eerily human-like chatbots.

Less noticed, but just as important: Researchers are learning a great deal about us – with the help of AI.

AI is helping scientists decode how neurons in our brains communicate, and explore the nature of cognition. This new research could one day lead to humans connecting with computers merely by thinking–as opposed to typing or voice commands. But there is a long way to go before such visions become reality.

Celeste Kidd, a psychology professor at the University of California, Berkeley, was surprised by what she discovered when she tried to examine the range of opinions people have about certain politicians, including Barack Obama and Donald Trump.

Her research was intended to explore the widening divergence of how we conceive of subjects to which we attach moral judgements – such as politicians.Previous work has shown that morally-fraught concepts are the ones peoplep erceive in the most polarized ways.

To establish a baseline for her experiment, she began by asking thousands of study participants about their associations with common nouns, in this case animals.

What she discovered was that even for common animals – including chickens, whales and salmon – people’s notions of their characteristics are all over the map. Are whales majestic? You’d be surprised who disagrees. Are penguins heavy? Opinions vary. By quizzing people on many such associations, Dr. Kidd was able to amass a pool of data that clusters people according to which of these associations they agree on. Using this method, she found that people can be grouped into between 10 and 30 different clusters, depending on their perception of an animal.

Dr. Kidd and her team concluded that people tend not to see eye to eye about even the most basic characteristics of common objects. We also overestimate how many people see things as we do. In a world in which it feels like people are increasingly talking past one another, the root of this phenomenon may be the fact that even for citizens of a single country speaking a common language, words simply don’t mean the same thing to different people.

That might not seem like a very profound observation, but what Dr. Kidd’s research suggests is the degree to which that’s true may be much greater than psychologists previously thought.

Arriving at this insight required the application of a tool of mathematics that makes many kinds of AI possible – known as a “clustering model”.

The most important feature of AI which enables new kinds of research, says Dr.Kidd, is the same that makes possible AI chatbots like OpenAI’s ChatGPT , Google’s Bard , and Microsoft’s Bing chat : It’s the capacity of modern computer systems to process a lot more data than in the past. It “opens up a lot of possibilities for new insights, from biology to medicine to cognitive science,” she adds.

Cracking the brain's neural code

In her research, Tatiana Engel, an assistant professor of neuroscience atPrinceton University, uses the same kinds of networks of artificial neurons that are behind most of what we currently call artificial intelligence. But rather than using these to better-target ads, or to generate fake images, or compose text, shea nd her team use them to interpret the electrical signals of hundreds of neurons at once in the brains of animals.

Dr. Engel and her team then go a step further: they train networks of artificial neurons to perform the same tasks as an animal – say, a swimming worm. They then find that those artificial networks organize themselves in ways that reasonably approximate the way they’re organized in real animals. While neural networks in the brain are vastly more complicated, the result of this simulation is a model system that is both close enough to its biological equivalent, and simple enough, to teach us things about how the real brain works, Dr. Engel says.

One key insight this yields is that the actual substance of thought – the patterns that constitute the mind you’re using to read this sentence – is dynamic electrical activity in our brains rather than something physically anchored to particular neurons.

In other words, in contrast to what neuroscientists once believed about how we make decisions, there are no “eat the chocolate” neurons and “don’t eat the chocolate” neurons. Thinking, it turns out, is just electrical signals zooming about inside our heads, forming a complex code which is carried by our neurons.

What’s more, AI is letting scientists listen in on the things that happen in our brains when we’re not doing anything in particular.

“This allows us to discover the brain’s internal life,” says Dr. Engel.

Do androids dream of electric sheep? We don’t know yet, but we may soon beable to determine if humans are thinking about the real thing.

Real-life mind reading

If a research lab owned by Meta Platforms, Facebook’s parent company, figuring out how to read your mind makes you at all uncomfortable, you’re probably not going to be a fan of what the rest of the 21st century has in store.

Historically, it’s been very difficult to measure brain activity inside our heads, because the electrical signals generated by our brains, which are miniscule to begin with, must be measured from outside of our skulls. ( Elon Musk’s aspirations for his Neuralink startup notwithstanding, opening up our heads and putting in brain interfaces hasn’t proved popular.)

AI is helping scientists study EEG readings and explore the nature of cognition. Data gathered from language experiments have been used by Facebook parent, Meta Platforms, to develop an early version of an algorithm that can “read” words and phrases from a person’s mind.

But progress in artificial intelligence techniques is yielding a more-powerful amplifier of those weak brain signals. Meta’s AI lab published research on one such mind-reading technology last summer.

Meta scientists didn’t actually stick anyone in a brain scanner. Instead, they used data on brain signals gathered by researchers at universities. This data was captured from human subjects who were listening to words and phrases, while sitting in non-invasive brain scanners. These scanners came in two varieties:

One was the sort of electrodes-embedded-in-a-swim-cap with which many people are familiar, called an EEG (short for “electroencephalogram”). The other looks like a supervillain’s attempt to create a world-crushing megabrain, called aMEG (for “magnetoencephalogram”).

To analyze this data, researchers used a type of AI called a “self-supervised learning model.” Without this technique, the latest generation of AI chatbots would be impossible. Such models can extract meaning from giant pools of data without any instruction from humans, and have also been used to try and figure out what animals are communicating with each other.

Researchers evaluate fluctuations in MEG sensors while a subject’s neural activity is being recorded.Research by Meta suggests that such measurements could be used in the future to allow people to direct computers just by thinking.

A little less than half of the time, Meta’s AI algorithm was able to correctly guess what words a person had heard, based on the activity generated in their brains.That might not sound too impressive, but it’s leaps and bounds better than what such systems have been able to achieve in the past.

Alexandre Défossez, a scientist at Meta who was part of the team that conducted this research, says that the eventual goal of this work is to create a general-purpose “speech decoder” that can directly transform our brain activity–our thoughts–into words.

Imagine texting a friend just by thinking about it – as long as you’re wearing anEEG cap at the moment, at any rate. The technology could have a big impact on the lives of people who are unable to communicate in other ways, adds Dr.Défossez.

It’s just one more example of the way that AI might someday give us the tools for improving our individual and collective well-being – or at least an explanation for why, in the age of social media, both of those things frequently seem so deranged.


/ 014. Zuboff, Shoshana. In the Age of the Smart Machine: The Future of Work and Power, 1988.


INTRODUCTION: DILEMMAS OF TRANSFORMATION IN THE AGE OF THE SMART MACHINE

The history of technology is that of human history in all its diversity. That is why specialist historians of technology hardly ever manage to grasp it entirely in their hands. -- Fernand Braudel, The Structures of Everyday Life

We don't know what will be happening to us in the future. Modern technology is taking over. What will be our place? -- A Piney Wood worker

RNEY WOOD, one of the nation's largest pulp mills, was in the throes of a massive modernization effort that would place every aspect of the production process under computer control. Six workers were crowded around a table in the snack area outside what they called the Star Trek Suite, one of the first control rooms to have been completely converted to microprocessor-based instrumentation. It looked enough like a NASA control room to have earned its name.

It was almost midnight, but despite the late hour and the approach of the shift change, each of the six workers was at once animated and thoughtful. "Knowledge and technology are changing so fast," they said, "what will happen to us?" Their visions of the future foresaw wrenching change. They feared that today's working assumptions could not be relied upon to carry them through, that the future would not resemble the past or the present. More frightening still was the sense of a future moving out of reach so rapidly that there was little opportunity to plan or make choices. The speed of dissolution and renovation seemed to leave no time for assurances that we were not heading toward calamity-and it would be all the more regrettable for having been something of an accident.

The discussion around the table betrayed a grudging admiration for the new technology-its power, its intelligence, and the aura of progress surrounding it. That admiration, however, bore a sense of grief. Each expression of gee-whiz-Buck-Rogers breathless wonder brought with it an aching dread conveyed in images of a future that rendered their authors obsolete. In what ways would computer technology transform their work lives? Did it promise the Big Rock Candy Mountain or a silent graveyard?

In fifteen years there will be nothing for the worker to do. The technology will be so good it will operate itself. You will just sit there behind a desk running two or three areas of the mill yourself and get bored.

The group concluded that the worker of the future would need "an extremely flexible personality" so that he or she would not be "mentally affected" by the velocity of change. They anticipated that workers would need a great deal of education and training in order to "breed flexibility." "We find it all to be a great stress," they said, "but it won't be that way for the new flexible people." Nor did they perceive any real choice, for most agreed that without an investment in the new technology, the company could not remain competitive. They also knew that without their additional flexibility, the technology would not fly right. "We are in a bind," one man groaned, "and there is no way out." The most they could do, it was agreed, was to avoid thinking too hard about the loss of overtime pay, the diminished probability of jobs for their sons and daughters, the fears of seeming incompetent in a strange new milieu, or the possibility that the company might welsh on its promise not to lay off workers.

During the conversation, a woman in stained overalls had remained silent with her head bowed, apparently lost in thought. Suddenly, she raised her face to us. It was lined with decades of hard work, her brow drawn together. Her hands lay quietly on the table. They were calloused and swollen, but her deep brown eyes were luminous, youthful, and kind. She seemed frozen, chilled by her own insight, as she solemnly delivered her conclusion:

I think the country has a problem. The managers want everything to be run by computers. But if no one has a job, no one will know how to do anything anymore. Who will pay the taxes? What kind of society will it be when people have lost their knowledge and depend on computers for everything?

Her voice trailed off as the men stared at her in dazzled silence. They slowly turned their heads to look at one another and nodded in agreement. The forecast seemed true enough. Yes, there was a problem. They looked as though they had just run a hard race, only to stop short at the edge of a cliff. As their heels skidded in the dirt, they could see nothing ahead but a steep drop downward.

Must it be so? Should the advent of the smart machine be taken as an invitation to relax the demands upon human comprehension and critical judgment? Does the massive diffusion of computer technology throughout our workplaces necessarily entail an equally dramatic loss of meaningful employment opportunities? Must the new electronic milieu engender a world in which individuals have lost control over their daily work lives? Do these visions of the future represent the price of economic success or might they signal an industrial legacy that must be overcome if intelligent technology is to yield its full value? Will the new information technology represent an opportunity for the rejuvenation of competitiveness, productive vitality, and organizational ingenuity? Which aspects of the future of working life can we predict, and which will depend upon the choices we make today?

The workers outside the Star Trek Suite knew that the so-called technological choices we face are really much more than that. Their consternation puts us on alert. There is a world to be lost and a world to be gained. Choices that appear to be merely technical will redefine our lives together at work. This means more than simply contemplating the implications or consequences of a new technology. It means that a powerful new technology, such as that represented by the computer, fundamentally reorganizes the infrastructure of our material world. It eliminates former alternatives. It creates new possibilities. It necessitates fresh choices.

The choices that we face concern the conception and distribution of knowledge in the workplace. Imagine the following scenario: Intelligence is lodged in the smart machine at the expense of the human capacity for critical judgment. Organizational members become ever more dependent, docile, and secretly cynical. As more tasks must be accomplished through the medium of information technology (I call this "computer-mediated work"), the sentient body loses its salience as a source of knowledge, resulting in profound disorientation and loss of meaning. People intensify their search for avenues of escape through drugs, apathy, or adversarial conflict, as the majority of jobs in our offices and factories become increasingly isolated, remote, routine, and perfunctory. Alternatively, imagine this scenario: Organizational leaders recognize the new forms of skill and knowledge needed to truly exploit the potential of an intelligent technology. They direct their resources toward creating a work force that can exercise critical judgment as it manages the surrounding machine systems. Work becomes more abstract as it depends upon understanding and manipulating information. This marks the beginning of new forms of mastery and provides an opportunity to imbue jobs with more comprehensive meaning. A new array of work tasks offer unprecedented opportunities for a wide range of employees to add value to products and services.

The choices that we make will shape relations of authority in the workplace. Once more, imagine: Managers struggle to retain their traditional sources of authority, which have depended in an important way upon their exclusive control of the organization's knowledge base. They use the new technology to structure organizational experience in ways that help reproduce the legitimacy of their traditional roles. Managers insist on the prerogatives of command and seek methods that protect the hierarchical distance that distinguishes them from their subordinates. Employees barred from the new forms of mastery relinquish their sense of responsibility for the organization's work and use obedience to authority as a means of expressing their resentment. Imagine an alternative: This technological transformation engenders a new approach to organizational behavior, one in which relationships are more intricate, collaborative, and bound by the mutual responsibilities of colleagues. As the new technology integrates information across time and space, managers and workers each overcome their narrow functional perspectives and create new roles that are better suited to enhancing value-adding activities in a data-rich environment. As the quality of skills at each organizational level becomes similar, hierarchical distinctions begin to blur. Authority comes to depend more upon an appropriate fit between knowledge and responsibility than upon the ranking rules of the traditional organizational pyramid.

The choices that we make will determine the techniques of administration that color the psychological ambience and shape communicative behavior in the emerging workplace. Imagine this scenario: The new technology becomes the source of surveillance techniques that are used to ensnare organizational members or to subtly bully them into conformity. Managers employ the technology to circumvent the demanding work of face-to-face engagement, substituting instead techniques of remote management and automated administration. The new technological infrastructure becomes a battlefield of techniques, with managers inventing novel ways to enhance certainty and control while employees discover new methods of self-protection and even sabotage. Imagine the alternative: The new technological milieu becomes a resource from which are fashioned innovative methods of information sharing and social exchange. These methods in turn produce a deepened sense of collective responsibility and joint ownership, as access to ever-broader domains of information lend new objectivity to data and preempt the dictates of hierarchical authority.

This book is about these alternative futures. Computer-based technologies are not neutral; they embody essential characteristics that are bound to alter the nature of work within our factories and offices, and among workers, professionals, and managers. New choices are laid open by these technologies, and these choices are being confronted in the daily lives of men and women across the landscape of modern organizations. This book is an effort to understand the deep structure of these choices-the historical, psychological, and organizational forces that imbue our conduct and sensibility. It is also a vision of a fruitful future, a call for action that can lead us beyond the stale reproduction of the past into an era that offers a historic opportunity to more fully develop the economic and human potential of our work organizations.

THE TWO FACES OF INTELLIGENT TECHNOLOGY

The past twenty years have seen their share of soothsayers ready to predict with conviction one extreme or another of the alternative futures I have presented. From the unmanned factory to the automated cockpit, visions of the future hail information technology as the final answer to "the labor question," the ultimate opportunity to rid ourselves of the thorny problems associated with training and managing a competent and committed work force. These very same technologies have been applauded as the hallmark of a second industrial revolution, in which the classic conflicts of knowledge and power associated with an earlier age will be synthesized in an array of organizational innovations and new procedures for the production of goods and services, all characterized by an unprecedented degree of labor harmony and widespread participation in management process.' Why the paradox? How can the very same technologies be interpreted in these different ways? Is this evidence that the technology is indeed neutral, a blank screen upon which managers project their biases and encounter only their own limitations? Alternatively, might it tell us something else about the interior structure of information technology?

Throughout history, humans have designed mechanisms to reproduce and extend the capacity of the human body as an instrument of work. The industrial age has carried this principle to a dramatic new level of sophistication with machines that can substitute for and amplify the abilities of the human body. Because machines are mute, and because they are precise and repetitive, they can be controlled according to a set of rational principles in a way that human bodies cannot.

There is no doubt that information technology can provide substitutes for the human body that reach an even greater degree of certainty and precision. When a task is automated by a computer, it must first be broken down to its smallest components. Whether the activity involves spraying paint on an automobile or performing a clerical transaction, it is the information contained in this analysis that translates human agency into a computer program. The resulting software can be used to automatically guide equipment, as in the case of a robot, or to execute an information transaction, as in the case of an automated teller machine.

A computer program makes it possible to rationalize activities more comprehensively than if they had been undertaken by a human being. Programmability means, for example, that a robot will respond with unwavering precision because the instructions that guide it are themselves unvarying, or that office transactions will be uniform because the instructions that guide them have been standardized. Events and processes can be rationalized to the extent that human agency can be analyzed and translated into a computer program.

What is it, then, that distinguishes information technology from earlier generations of machine technology? As information technology is used to reproduce, extend, and improve upon the process of substituting machines for human agency, it simultaneously accomplishes something quite different. The devices that automate by translating information into action also register data about those automated activities, thus generating new streams of information. For example, computer-based, numerically controlled machine tools or microprocessor-based sensing devices not only apply programmed instructions to equipment but also convert the current state of equipment, product, or process into data. Scanner devices in supermarkets automate the checkout process and simultaneously generate data that can be used for inventory control, warehousing, scheduling of deliveries, and market analysis. The same systems that make it possible to automate office transactions also create a vast overview of an organization's operations, with many levels of data coordinated and accessible for a variety of analytical efforts.

Thus, information technology, even when it is applied to automatically reproduce a finite activity, is not mute. It not only imposes information (in the form of programmed instructions) but also produces information. It both accomplishes tasks and translates them into information. The action of a machine is entirely invested in its object, the product. Information technology, on the other hand, introduces an additional dimension of reflexivity: it makes its contribution to the product, but it also reflects back on its activities and on the system of activities to which it is related. Information technology not only produces action but also produces a voice that symbolically renders events, objects, and processes so that they become visible, knowable, and shareable in a new way.

Viewed from this interior perspective, information technology is characterized by a fundamental duality that has not yet been fully appreciated. On the one hand, the technology can be applied to automating operations according to a logic that hardly differs from that of the nineteenth-century machine system-replace the human body with a technology that enables the same processes to be performed with more continuity and control. On the other, the same technology simultaneously generates information about the underlying productive and administrative processes through which an organization accomplishes its work. It provides a deeper level of transparency to activities that had been either partially or completely opaque. In this way information technology supersedes the traditional logic of automation. The word that I have coined to describe this unique capacity is informate. Activities, events, and objects are translated into and made visible by information when a technology informates as well as automates.

The informating power of intelligent technology can be seen in the manufacturing environment when microprocessor-based devices such as robots, programmable logic controllers, or sensors are used to translate the three-dimensional production process into digitized data. These data are then made available within a two-dimensional space, typically on the screen of a video display terminal or on a computer printout, in the form of electronic symbols, numbers, letters, and graphics. These data constitute a quality of information that did not exist before. The programmable controller not only tells the machine what to do-imposing information that guides operating equipment but also tells what the machine has done-translating the production process and making it visible.

In the office environment, the combination of on-line transaction systems, information systems, and communications systems creates a vast information presence that now includes data formerly stored in people's heads, in face-to-face conversations, in metal file drawers, and on widely dispersed pieces of paper. The same technology that processes documents more rapidly, and with less intervention, than a mechanical typewriter or pen and ink can be used to display those documents in a communications network. As more of the underlying transactional and communicative processes of an organization become automated, they too become available as items in a growing organizational data base.

In its capacity as an automating technology, information technology has a vast potential to displace the human presence. Its implications as an informating technology, on the other hand, are not well understood. The distinction between automate and informate provides one way to understand how this technology represents both continuities and discontinuities with the traditions of industrial history. As long as the technology is treated narrowly in its automating function, it perpetuates the logic of the industrial machine that, over the course of this century, has made it possible to rationalize work while decreasing the dependence on human skills. However, when the technology also informates the processes to which it is applied, it increases the explicit information content of tasks and sets into motion a series of dynamics that will ultimately reconfigure the nature of work and the social relationships that organize productive activity.

Because this duality of intelligent technology has not been clearly recognized, the consequences of the technology's informating capacity are often regarded as unintended. Its effects are not planned, and the potential that it lays open remains relatively unexploited. Because the informating process is poorly defined, it often evades the conventional categories of description that are used to gauge the effects of industrial technology.

These dual capacities of information technology are not opposites; they are hierarchically integrated. Informating derives from and builds upon automation. Automation is a necessary but not sufficient condition for informating. It is quite possible to proceed with automation without reference to how it will contribute to the technology's informating potential. When this occurs, informating is experienced as an unintended consequence of automation. This is one point at which choices are laid open. Managers can choose to exploit the emergent informating capacity and explore the organizational innovations required to sustain and develop it. Alternatively, they can choose to ignore or suppress the informating process. In contrast, it is possible to consider informating objectives at the start of an automation process. When this occurs, the choices that are made with respect to how and what to automate are guided by criteria that reflect developmental goals associated with using the technology's unique informating power.

Information technology is frequently hailed as "revolutionary." What are the implications of this term? Revolution means a pervasive, marked, radical change, but revolution also refers to a movement around a fixed course that returns to the starting point. Each sense of the word has relevance for the central problem of this book. The informating capacity of the new computer-based technologies brings about radical change as it alters the intrinsic character of work-the way millions of people experience daily life on the job. It also poses fundamentally new choices for our organizational futures, and the ways in which labor and management respond to these new choices will finally determine whether our era becomes a time for radical change or a return to the familiar patterns and pitfalls of the traditional workplace. An emphasis on the informating capacity of intelligent technology can provide a point of origin for new conceptions of work and power. A more restricted emphasis on its automating capacity can provide the occasion for that second kind of revolution-a return to the familiar grounds of industrial society with divergent interests battling for control, augmented by an array of new material resources with which to attack and defend.

The questions that we face today are finally about leadership. Will there be leaders who are able to recognize the historical moment and the choices it presents? Will they find ways to create the organizational conditions in which new visions, new concepts, and a new language of workplace relations can emerge? Will they be able to create organizational innovations that can exploit the unique capacities of the new technology and thus mobilize their organization's productive potential to meet the heightened rigors of global competition? Will there be leaders who understand the crucial role that human beings from each organizational stratum can play in adding value to the production of goods and services? If not, we will be stranded in a new world with old solutions. We will suffer through the unintended consequences of change, because we have failed to understand this technology and how it differs from what came before. By neglecting the unique informating capacity of advanced computer-based technology and ignoring the need for a new vision of work and organization, we will have forfeited the dramatic business benefits it can provide. Instead, we will find ways to absorb the dysfunctions, putting out brush fires and patching wounds in a slow-burning bewilderment.

THE PLAN OF THIS BOOK

The choices for the future cannot be deduced from economic data or from abstract measures of organizational functioning. They are embedded in the living detail of daily life at work as ordinary people confront the dilemmas raised by the transformational qualities of new information technology. For this reason the research presented here focuses upon the texture of human experience-what people say, feel, and do-in dealing with the technological changes that imbue their immediate environment. I studied eight organizations during the five-year period from 1981 to 1986. Each was well known as a model of technological sophistication within its particular industry. In each, information technology was implemented in ways that fundamentally altered how people were required to accomplish their daily work. In most cases, employees found themselves having to operate through the computer medium in order to perform their tasks, and in almost every instance, this was their first direct experience with information technology.

The most treacherous enemy of such research is what philosophers call "the natural attitude," our capacity to live daily life in a way that takes for granted the objects and activities that surround us. Even when we encounter new objects in our environment, our tendency is to experience them in terms of categories and qualities with which we are already familiar. The natural attitude allows us to assume and predict a great many things about each other's behavior without first establishing premises at the outset of every interaction. The natural attitude can also stand in the way of awareness, for ordinary experience has to be made extraordinary in order to become accessible to reflection. This occurs when we encounter a problem: when our actions do not yield the expected results, we are caught by surprise and so are motivated to reflect upon our initial assumptions. 2 Awareness requires a rupture with the world we take for granted; then old categories of experience are called into question and revised. For example, in the early days of photography, the discrepancies between the camera's eye and the human eye were avidly discussed, but, "once they began to think photographically, people stopped talking about photographic distortion, as it was called."

In the organizations I have studied, the introduction of information technology provided just such a sense of crisis. I found a "window of opportunity" during which people who were working with the technology for the first time were ripe with questions and insights regarding the distinct qualities of their experience. As time passed (usually twelve to eighteen months), they would find ways to accommodate their understanding to the altered conditions of work, making it more difficult to extract fresh insights from beneath a new crust of familiarity. For this reason, I also sought out men and women who had experience accomplishing the same tasks in both the context of an earlier technology- pneumatic controls, paper and pencil, face-to-face interaction, mechanical equipment-and the context of information technology integrated information and control systems, on-line transaction systems, real-time information systems, and computer-conferencing systerns. This provided an experiential frame of reference and heightened their awareness of the continuities and discontinuities in the quality of their work experience.

No sector of the economy is exempt from the technological changes under way or the dilemmas they create. This book seeks to understand the generic themes of this transformation as they cut across a range of organizations engaged in what appear to be wholly distinct kinds of work and to compare and contrast the issues that arise within diverse sectors, such as the offices of a large service organization and an automated manufacturing process. As a result, the following chapters portray a diverse set of organizations-from pulp and paper mills to insurance offices to the elite precincts of an international bank. This attention to similarity and difference also bears upon my observations of the various occupational levels within a given organization-workers, clerks, managers, and professionals were each involved in my research effort. There are generic themes that unify their experiences as well as important sources of difference between them.

The organizations studied include two pulp mills and one pulp and paper mill (located in separate divisions of the American Paper Company); Metro Tel, an operating unit of a telecommunications company; the dental claims operation of Consolidated Underwriters Insurance; the offices for stock and bond transfer of a large corporation known as Universal Technology; the Brazilian offices of Global Bank, a major international financial institution; and a large pharmaceutical company called DrugCorp. (See appendix B for a more complete description of the field methodology.) Each site is not equally represented in the thematic discussions within each chapter because the nature of the technological application in a given organization or the particular kind of work in which people were engaged illuminated certain sets of themes in a particularly important way. This study sought to use the range of sites to build a comprehensive map of the territory in question, rather than to perform a comparative analysis between each organization.

The three mills studied were each in the process of implementing a new control interface based upon microprocessor technology. The level of technological innovation in each case represented the state of the art for process control technology. Two of the mills, Piney Wood and Tiger Creek, had traditional work systems with unionized work forces. They were old mills, and the conversion process represented a radical technological change. The third mill, Cedar Bluff, had been recently constructed and was considered to be one of the most automated pulp mills in the world. Its work force had been newly recruited and thus had no prior experience with other forms of pulping technology. Cedar Bluff's work system had been designed to achieve high levels of employee involvement and commitment; it emphasized worker teams and a pay-for-skills approach to compensation.

Metro Tel had recently implemented a computer-based administrative system that linked managers in a central office to workers in field locations. This was an attempt to create a technologically based administrative infrastructure to support their centralized technical operations, such as switching and repairs. The organizational structure was traditional, hierarchical, and highly centralized. Workers were members of the Communications Workers of America union, and most enjoyed long years of service with the company.

In both Consolidated Underwriter Insurance's dental claims operation and the stock and bond transfer offices of Universal Technology, clerical workers were using information technology for the first time. Each of these offices had installed high-volume transaction systems in which clerks used desktop terminals to receive and enter data.

The Brazilian offices of Global Bank represented 20 percent of the parent corporation's international revenues. Like many other banking institutions, Global Bank Brazil had shifted its strategic thinking from an emphasis on loans to an emphasis on the development of new technology-based products and services. Information, rather than money, was now recognized as the bank's most valuable commodity. As a result of Global Bank Brazil's technological sophistication and financial importance, it was chosen as a site for a pilot program developing a new generation of information technology. This technology, technically referred to as the "data base environment," was regarded as an innovation that would profoundly affect the nature of banking, with consequences for the skills and forms of organization appropriate to each banking function. Though Global Bank Brazil had not fully completed its transition to this new stage of technological deployment, this study documents the organization's efforts to grapple with the likely consequences of the technological change.

Finally, the managerial precincts of DrugCorp provided an opportunity to study one of the world's most extensive computer-conferencing systems. Managers and professionals were linked by a computer network that allowed ongoing dialogue, electronic meetings, and rapid communication. Their experiences with this technology reveal much about the emerging structure of communication within the computerized organization.

This book is structured to reflect the succession of dilemmas that typically accompany an organization's transformation to advanced computer-based technology. 

Part I is directed toward the dilemmas associated with the changing grounds of knowledge as a result of the computer mediation of work. It explores the historical role of the body in both industrial and white-collar work and depicts the emerging demand for intellective skills, that frequently supplant the body as a primary source of know-how. Drawing together data from the mills and the offices, part 1 offers a broad conceptualization of the cognitive and social-psychological requirements for developing and expressing knowledge in the computerized workplace.

Part 2 focuses upon the dilemmas of authority that develop as the new demands for intellective skills blur traditional distinctions between operational and managerial roles. It begins by tracing the historical evolution of managerial authority and proceeds to explore the way in which the managerial hierarchy can subvert the forces of change, using the experiences of the three mills as examples. The mills also illustrate how, despite these attempts to resist change, new roles and relations of authority begin to take shape.

Part 3 concerns the attempts to shore up these threatened authority relations with new techniques of control that draw upon the technology's tendency to heighten the visibility of organizational processes. Managers who doubt the strength of authority-based bonds or who prefer technical certainty to the rigors of managing face-to-face relationships are drawn to the technology as a new source of techniques for shaping the behavior of their subordinates. Their efforts engage a series of organizational responses that, ironically, weaken managerial authority even more profoundly.

The conclusion sets out a portrait of a hypothetical informated workplace. It defines a direction for managerial efforts that would take up the challenge of this historical moment and strike out on a new path. It suggests landmarks as well as pitfalls. It offers a vision.

PART ONE: KNOWLEDGE AND COMPUTER-MEDIATED WORK

CHAPTER ONE: THE LABORING BODY: SUFFERING AND SKILL IN PRODUCTION WORK

We had pleased ourselves with the delectable visions of the spiritualization of labor .... Each stroke of the hoe was to uncover some aromatic root of wisdom .... But ... the clods of earth, which we so constantly belabored and turned over and over, were never etherealized into thought. Our thoughts, on the contrary, were fast becoming cloddish. Our labor symbolized nothing and left us mentally sluggish in the dusk of the evening. - Nathaniel Hawthorne, The Bithedale Romance

THE AUTOMATIC DOORS

The bleach plant is one of the most complex and treacherous areas of a pulp mill. In Piney Wood, a large pulp plant built in the mid-1940s, railroad tank cars filled with chemicals used in the bleaching process pull up alongside the four-story structure in which dirty brown digested pulp is turned gleaming white. Each minute, 4,000 gallons of this brown mash flow through a labyrinth of pipes into a series of cylindrical vats, where they are washed, treated with chlorine-related chemicals, and bleached white. No natural light finds its way into this part of the mill. The fluorescent tubes overhead cast a greenish-yellow pall, and the air is laced with enough chemical flavor that as you breathe it, some involuntary wisdom built deep into the human body registers an assault. The floors are generally wet, particularly in the areas right around the base of one of the large vats that loom like raised craters on a moonscape. Sometimes a washer runs over, spilling soggy cellulose kneedeep across the floor. When this happens, the men put on their high rubber boots and shovel up the mess.

The five stages of the bleaching process include hundreds of operating variables. The bleach operator must monitor and control the flow of stock, chemicals, and water, judge color and viscosity, attend to time, temperature, tank levels, and surge rates-the list goes on. Before computer monitoring and control, an operator in this part of the mill would make continual rounds, checking dials and graph charts located on the equipment, opening and shutting valves, keeping an eye on vat levels, snatching a bit of pulp from a vat to check its color, sniff it, or squeeze it between his fingers ("Is it slick? Is it sticky?") to determine its density or to judge the chemical mix.

In 1981 a central control room was constructed in the bleach plant. A science fiction writer's fantasy, it is a gleaming glass bubble that seems to have erupted like a mushroom in the dark, moist, toxic atmosphere of the plant. The control room reflects a new technological era for continuous-process production, one in which microprocessor-based sensors linked to computers allow remote monitoring and control of the key process variables. In fact, the entire pulp mill was involved in this conversion from the pneumatic control technology of the 1 940s to the microprocessor-based information and control technology of the 1980s.

Inside the control room, the air is filtered and hums with the sound of the air-conditioning unit built into the wall between the control room and a small snack area. Workers sit on orthopedically designed swivel chairs covered with a royal blue fabric, facing video display terminals. The terminals, which display process information for the purposes of monitoring and control, are built into polished oak cabinets. Their screens glow with numbers, letters, and graphics in vivid red, green, and blue. The floor here is covered with slate-gray carpeting; the angled countertops on which the terminals sit are rust brown and edged in black. The walls are covered with a wheat-colored fabric and the molding repeats the polished oak of the cabinetry. The dropped ceiling is of a bronzed metal, and from it is suspended a three dimensional structure into which lights have been recessed and angled to provide the right amount of illumination without creating glare on the screens. The color scheme is repeated on the ceiling-soft tones of beige, rust, brown, and gray in a geometric design.

The terminals each face toward the front of the room-a windowed wall that opens onto the bleach plant. The steel beams, metal tanks, and maze of thick pipes visible through those windows appear to be a world away in a perpetual twilight of steam and fumes, like a city street on a misty night, silent and dimly lit. What is most striking about the juxtaposition of these two worlds, is how a man (and there were only men working in this part of the mill) traverses the boundary between them.

The control room is entered through an automatic sliding-glass door. At the push of a button, the two panels of the door part, and when you step forward, they quickly close behind you. You then find yourself facing two more automatic doors at right angles to one another. The door on the right leads to a narrow snack area with booths, cabinets, a coffee machine, and a refrigerator. The door to the left leads into the control room. It will not open until the first door has shut. This ensures that the filtered air within the control room is protected from the fumes and heat of the bleach plant. The same routine holds in reverse. When a man leaves the control room, he presses a button next to the frame on the inner door, which opens electronically. He then steps through it into the tiny chamber where he must wait for the door to seal behind him so that he can push a second button on the outer door and finally exit into the plant.

This is not what most men do when they move from the control room out into the bleach plant. They step through the inner door, but they do not wait for that door to seal behind them before opening the second door. Instead, they force their fingertips through the rubber seal down the middle of the outer door and, with a mighty heft of their shoulders, pry open the seam and wrench the door apart. Hour after hour, shift after shift, week after week, too many men pit the strength in their arms and shoulders against the electronic mechanism that controls the doors. Three years after the construction of the sleek, glittering glass bubble, the outer door no longer closes tightly. A gap of several inches, running down the center between the two panels of glass, looks like a battle wound. The door is crippled.

The door is broke now because the men pushed it too hard comin' in and out," says one operator. In talking to the men about this occurrence, so mundane as almost to defy reflection, I hear not only a simple impatience and frustration but also something deeper: a forward momentum of their bodies, whose physical power seems trivialized by the new circumstances of their work; a boyish energy that wants to break free; a subtle rebellion against the preprogrammed design that orders their environment and always knows best. Yet these are the men who also complained, "The fumes in the bleach plant will kill you. You can't take that chlorine no matter how big and bad you are. It will bleach your brains and no one (in management) gives a damn."

Technology represents intelligence systematically applied to the problem of the body. It functions to amplify and surpass the organic limits of the body; it compensates for the body's fragility and vulnerability. Industrial technology has substituted for the human body in many of the processes associated with production and so has redefined the limits of production formerly imposed by the body. As a result, society's capacity to produce things has been extended in a way that is unprecedented in human history. This achievement has not been without its costs, however. In diminishing the role of the worker's body in the labor process, industrial technology has also tended to diminish the importance of the worker. In creating jobs that require less human effort, industrial technology has also been used to create jobs that require less human talent. In creating jobs that demand less of the body, industrial production has also tended to create jobs that give less to the body, in terms of opportunities to accrue knowledge in the production process. These two-sided consequences have been fundamental for the growth and development of the industrial bureaucracy, which has depended upon the rationalization and centralization of knowledge as the basis of control.

These consequences also help explain the worker's historical ambivalence toward automation. It is an ambivalence that draws upon the loathing as well as the commitment that human beings can experience toward their work. Throughout most of human history, work has inescapably meant the exertion and often the depletion of the worker's body. Yet only in the context of such exertion was it possible to learn a trade and to master skills. Since the industrial revolution, the accelerated progress of automation has generally meant a reduction in the amount of effort required of the human body in the labor process. It has also tended to reduce the quality of skills that a worker must bring to the activity of making something. Industrial technology has been developed in a manner that increases its capacity to spare the human body, while at the same time it has usurped opportunities for the development and performance of skills that only the body can learn and remember. In their treatment of the automatic doors, the bleach plant workers have created a living metaphor that reflects this ambivalence toward automation. They want to be protected from toxic fumes, but they simultaneously feel a stubborn rebellion against a structure that no longer requires either the strength or the know-how lodged in their bodies.

The progress of automation has been associated with both a general decline in the degree of know-how required of the worker and a decline in the degree of physical punishment to which he or she must be subjected. Information technology, however, does have the potential to redirect the historical trajectory of automation. The intrinsic power of its informating capacity can change the basis upon which knowledge is developed and applied in the industrial production process by lifting knowledge entirely out of the body's domain. The new technology signals the transposition of work activities to the abstract domain of information. Toil no longer implies physical depletion. "Work" becomes the manipulation of symbols, and when this occurs, the nature of skill is redefined. The application of technology that preserves the body may no longer imply the destruction of knowledge; instead, it may imply the reconstruction of knowledge of a different sort.

The significance of this transposition is impossible to grasp without reference to the grounds of knowledge for workers in the past. In the factory, knowledge was intimately bound up with the efforts of the laboring body. The development of industrial technology can be read as a chronicle of attempts to grapple with the body's role in production as a source of both effort and skill and with the specific responses these attempts have evoked from workers and managers. The centrality of the body's historical meaning for production has informed the selfunderstanding of managers and workers and the relationship between them. It has also been a salient force guiding the development and application of manufacturing technology. A better understanding of what the body has meant for industrial work and how it has been linked to the logic of automation will sharpen an appreciation of the character of the current transformation and its capacity to provoke comprehensive change in the relationships that structure the workplace. Before deciphering the present or imagining the future, it is first necessary to take ourselves out of the twentieth century and return, if only briefly, to a time when the nature of work was both simpler and more miserable, a time when work was above all the problem of the laboring body.

THE FRONTIER OF CONTEMPT

The world of production, where primary materials are processed and goods are manufactured, has long been marked by a great divide between those who give of their bodies and those who are exempt from physical depletion. Yet those exempted from bodily alteration may give of themselves in other ways. Their physical presence may be required for purposes of interpersonal influence, communication, and coordination. They may give of their time and attention in both supervisory and analytical activities, or they may give of themselves more abstractly as sources of investment capital or expert knowledge. However, the groups that stand on either side of this divide constitute fundamentally distinct modes of involvement with the production enterprise. The experiential distance between them is one important living source of the divergent interests in terms of which workers and managers have tended to define themselves. Workers facing the physical requirements of labor seek ways to preserve their bodies from exertion, while managers are charged with extracting the maximum feasible effort from the work force.

This divide has been an important characteristic of social hierarchies in virtually every culture known to the historical record. Wealth and power have everywhere meant an escape from toil: the unequal distribution and concentration of wealth and power within a small group is a phenomenon of such universality that the French historian Fernand Braudel has called it a "constant law of societies, a structural law that admits of no exception." 1 In the societies of preindustrial Europe, the decisive challenge of social mobility was to permanently rid oneself of the stigma of physical labor and then to repudiate the commercial activities that made such an escape from work possible. European nobility defined itself by the gulf it created between its members and the hardship of labor. Of the bourgeois families who gained access to the highest ranks of society Braudel writes that "the only feature they had in common with the authentic nobility was their rejection of trade or labor, their taste for idleness or rather leisure, which was for them synonymous with reading and learned discussion with their peers. "

This repugnance toward labor rides a long wave in Western history, a wave that has not, even yet, reached its crest. In the religious zeal of the early Middle Ages, trades that trafficked in money were considered illicit, materialism being an indication of a lack of faith. With the growing urbanization, more detailed division of labor, and accelerated mercantilism of the late Middle Ages, however, this view of economic activity took an important turn: "A new frontier of contempt arose right in the midst of the new classes and even within professions .... Work itself no longer constituted the distinction between respectable and contemptible categories; instead, it was manual labor that had come to be the key factor in the frontier between respect and contempt .... Across from the manouvriers and brassiers who worked with hands and arms, was the patrician world, the new aristocracy, consisting of all those who did no manual labor: employers and rentiers. " 3 Even the guilds were influenced by the contempt toward manual work. Some of them required entrants to have relinquished their trade for twelve months before admission. In 1241 a municipality in Flanders excluded from the urban magistracy all robbers, coiners, and "those who have not given up all manual work for at least one year. " 

The Middle Ages produced a conception of labor infused with a loathing drawn from three traditions: ( 1) the Greco-Roman legacy that associated labor with slavery, (2) the barbarian heritage that disdained those who worked the land and extolled the warrior who gained his livelihood in bloody booty, and (3) Judeo-Christian theology that admired contemplation over action. For centuries European literature and iconography depicted peasants as huge-headed monsters or wild beasts lurking the depths of Europe's dark forests. 5 Labor came to humanity with the fall from grace and was at best a penitential sacrifice enabling purity through humiliation. Labor was toil, distress, trouble, fatigue an exertion both painful and compulsory. Labor was our animal condition, struggling to survive in dirt and darkness.

Freedom from the necessity of labor has been a prominent feature of most utopian thinking. English civil war sects awaited the coming of the Fifth Monarchy because it was said that it would abolish painful labor. Bishop Godwin's Man in the Moone, published in 1638, reported on a society in which "food groweth everywhaer without labour," while all the necessities of life were amply provided. Other utopian writers of the period, Campanella, Winstanley, Bellars, and More, saw the reduction of labor as an important feature of a wholesome moral life. 6 Sir Thomas More's Utopia limited the consumption of commodities to the "necessary" and the "comfortable," in order that any surplus labor could be devoted to learning. The six-hour work day was seen as adequate, and if it turned out to be excessive, the community would further curtail the number of hours assigned to work: "What time may possibly be spared from the necessary occupations and affairs of the Commonwealth, all that the Citizens should withdraw from the bodily service to the free liberty of the mind and garnishing of the same." 

The recent English translation of the historian Norbert Elias's The Civilizin9 Process has helped shed light on a deeper explanation for this repugnance toward work. 8 While a full description of Elias's pathbreaking analysis is beyond the scope of this discussion, it is worth highlighting the skeleton of his discoveries as they contribute to an understanding of the enduring relationship between the universality of social hierarchy and the fact that physical labor is everywhere considered to constitute its lowest echelons.

Elias studied books of etiquette and other documentation of daily life from the early Middle Ages through the eighteenth century. He discovered that norms of daily conduct, particularly those that bear upon bodily functions, have changed radically throughout the course of the centuries. When in I 5 30 Erasmus wrote his treatise On Civility in Children, he provided a portrait, refracted through his admonitions, of contemporary standards of behavior. Our own sensibilities are overcome with repugnance and horror at behavior that was accepted as routine in the sixteenth century. Elias points out the "infinite care and matter-of-factness" with which Erasmus addressed habits concerning bodily functions in his effort to encourage more "civilized" standards: "There should be no snot on the nostrils .... A peasant wipes his nose on his cap and coat, a sausage maker on his arm and elbow. It does not show much more propriety to use one's hand and then wipe it on one's clothing .... It is more decent to take up the snot in a cloth, preferably while turning away. If when blowing the nose with two fingers something falls to the ground, it must be immediately trodden away with the foot. The same applies to spittle. "

Elias found that behaviors related to table manners, bodily functions, nose blowing, spitting, sleeping, sex, and aggression that we have come to consider barbaric and disgusting were once routine. He argues that the process of curbing these behaviors was set into motion by, and in tum helped to promote, stable, centralized forms of social organization. The embryo for these modem forms was evident in the court societies that by the end of the Middle Ages, had begun to spread across Europe.

Here were created the models of more pacified social intercourse which more or less all classes needed, following the transformation of European society at the end of the Middle Ages; here the coarse habits, the wilder, more uninhibited customs of medieval society with its warrior upper class, the corollaries of an uncertain, constantly threatened life, were "softened", "polished" and "civilized". The pressure of court life, the vying for the favour of the prince or the "great"; then, more generally, the necessity to distinguish oneself from others and to fight for opportunities with relatively peaceful means, through intrigue and diplomacy, enforced a constraint on the affects, a self-discipline and self-control, a peculiarly courtly rationality .... This increased restraint and regulation of elementary urges is bound up with increased social constraint, the growing dependence of the nobility on the central lord, the king or prince.

Elias reminds us that violence was inscribed into the very structure of medieval society. Rape and death, the hunting of men and animals, were part of everyday life. "The documents suggest unimaginable emotional outbursts in which-with rare exceptions-everyone who is able abandons himself to extreme pleasures of ferocity, murder, torture, destruction, and sadism." 11 Everyday objects like the fork (at first a source of mockery and still a rare luxury in the seventeenth century), the handkerchief, and the nightgown, or the new sense of repulsion felt at the sight of humans defecating, spitting a piece of food back into the common bowl, or picking their teeth with the communal knife, all symbolized the progress of the civilizing process-an increased control over and distance from the animal life of the human body: "People have begun to construct an affective wall between their bodies and those of others. The fork has been one of the means of drawing distances between other people's bodies and one's own. One repulses the body, isolates it, feels ashamed of it, tries to ignore it .... For many centuries, this wall did not exist."

Elias's discoveries illuminate an unconscious dimension of Western history. The consolidation of stable social hierarchies based upon centralized power and the rule of law demanded a new level of behavioral control. The body had to be reinterpreted as a source of disgust and as an object of discipline. 13 This reinterpretation, and the forms of conduct that developed from it, first took root at the highest levels of the emerging society, where the pressures of interdependence and political opportunism were most acute. These in turn became the models for behavior that successively lower social strata would imitate and finally assimilate.

It is easy to see that gradations in status are related to gradations in power, but Elias's work alerts us to the fact that such differences in status parallel another axis of social comparison whose levels are marked by degrees of distance from the body's own animal life and the animal life of surrounding bodies. The ability to maintain one's distance from the body developed as an important sign of hierarchical position. It has served to intensify the repugnance toward forms of activity that involve the body in sweating, heaving, grunting, hauling, and carting; that expose the body to pain and discomfort from extreme temperature, extended muscular effort, inclement weather, or hurtful substances; and that so immerse human consciousness in the sentient surroundings of effort and fatigue that one fails to notice (or care) whether the nose is dripping, the sweated body is giving off an unbearable stench, or filthy fingers have been used to grab a piece of food-the very proprieties that came to be seen as part of a complex of behaviors distinguishing the barbaric from the civilized.

There is reason enough to want to avoid exhausting work, but the constancy of repugnance was not confined to forms of labor that were extremely punishing. As noted earlier, in the membership practices of some guilds, even the craftsworker was liable to be an object of contempt because of the manual nature of that work. Such repugnance is in itself an act of distancing. It is both a rejection of the animal body and an affirmation of one's ability to translate the impulses of that body into the infinitely more subtle behavioral codes that mediate power in complex organizations. Once this translation occurs, the body is no longer the vehicle for involuntary affective or physical displays. Instead, it becomes the instrument of carefully crafted gestures and behaviors designed to achieve a calculated effect in an environment where interpersonal influence and even a kind of rudimentary psychological insight are critical to success. In the interpersonal world of court society, the body's knowledge involved the ability to be attuned to the psychological needs and demands of others, particularly of superiors, and to produce subtly detailed nonverbal behavior that reflected this awareness.

The court is a kind of stock exchange; as in every "good society", an estimate of the value of each individual is continuously being formed. But here his value has its real foundation ... in the favour he enjoys with the king, the influence he has with other mighty ones .... All this, favour, influence, importance, this whole complex and dangerous game in which physical force and direct affective outbursts are prohibited and a threat to existence, demands of each participant a constant foresight and an exact knowledge of every other, of his position and value in the network of courtly opinions; it exacts precise attunement of his own behavior to this value .... "A man who knows the court is master of his gestures, of his eyes and his expression; he is deep, impenetrable. He dissimulates the bad turns he does, smiles at his enemies, suppresses his ill-temper, disguises his passions, disavows his heart, acts against his feelings."

The tension between hierarchical status and the animal body also may be sustained by the psychologic need to defend oneself from the fact of the body's tragic weakness. The animality of the body is a source of repulsive events that must be controlled, and the most repulsive and least controllable of these events is death itself. Death is the inevitable conclusion ordained by the animality of the body and poses a series of challenges (in addition to the central challenge, which is the cessation of life). Death underscores the commonality of all who share life; death highlights the vulnerability of the body; death reminds the living of the ultimate uncontrollability of the body. Each of these problems is in a way addressed by the act of distantiation. Hierarchical distance rejects the display of animality that is common and deindividualized for civilized conduct, intricately fashioned by personality, wit, and will. Distantiation, in promoting forms of control over the body, can protect its fragility and, in some ways, its health. Finally, distantiation allows us to avoid reminders of animality, thus making it possible to suppress an awareness of the body's inevitable decline.

The close relationship between the rejection of animality and the progress of civilization is at the heart of our modern conception of work. Marx expressed this relationship when he argued that mastery of the material world was the basis upon which man humanized himself and developed culture. This in fact is the civilizing process; humanization means tempering animality with rationality, aesthetic grace, and moral choice. It is a process that has informed much of the impetus toward the extension of material culture powerfully exemplified in the development of industrial technology, which simultaneously frees the production process from the organic limits of the body, frees consumers from having to exercise bodily effort in order to enjoy the panoply of goods produced by the machine system, shapes workers who are capable of exercising considerable control over their own spontaneous impulses (and so can conform to the behavioral demands of mechanized production), and gradually diminishes the most painful forms of exertion associated with the work of making things.

Indeed, the worker's body posed a complicated set of problems for industrial management. Industrial work depended upon the laboring body as much for its raw energy as for its special gifts. In many industries, the worker's body remained central to production well into the early decades of the twentieth century. Only then, in many cases, was labor-saving technology diffused widely enough to substantially alter the role of the body in the production process. If work was to be performed economically and effectively, then the impulsive behavior associated with the body's animality would have to be disciplined. The members of court society were required to turn their bodies into instruments of interpersonal influence, instruments for acting-with. Industrial workers were also required to turn their bodies into instruments, but instruments for acting-on-for producing calculated effects on material and equipment. In the following section, we will see how the first generations of factory owners and their managers were frustrated, confounded, and sometimes ruined as they searched for the methods by which to translate the animal body into a more precise instrument that could be applied to increasingly systematized processes of production.

THE EARLY FACTORY AND THE PROBLEM OF THE BODY

There is ample evidence that throughout the fifteenth and sixteenth centuries, workers were not silent in their degradation. Social cleavage took its toll in thousands of peasant insurrections as well as violent disturbances among urban workers. 15 It was not until the industrial revolution, however, that the focus of the conflict between worker and employer came to rest on the detailed performance of the worker's body and the degree of discipline to which the body might legitimately be subjected.

Consider the case of Britain at the brink of industrialization during the second half of the eighteenth century. For all the bone-crushing labor demanded of the agricultural worker or the cottage weaver, the traditional rhythms of exertion and play were a world removed from the behavioral demands of industrial production. Work patterns were irregular, alternating between intense effort and idleness. Most work activities emanated from the home, and the distractions of the family, the taverns, and the social web of the community limited any undivided commitment to work.

Cottage workers, upon whom most textile production depended, were relatively impervious to the middleman's demand for heightened productivity. Their inclination to physical exertion was guided more by their own immediate needs than by acquisitive ambitions. Throughout the eighteenth century, the British Parliament passed legislation requiring ever-shorter turnaround times for finished goods from domestic workers and imposing increasingly severe sanctions on those who did not comply. Such sanctions were difficult to enforce; finally, only the pressure of immediate supervision was able to induce a greater level and consistency of effort from workers. 16 The need to intensify production was the driving force behind the establishment of the early factories and workshops, even before the widespread diffusion of the steam engine. 17 There is evidence that workers submitted to the physical rigors of factory discipline only when other alternatives had been exhausted. 18 But even those employers who were able to recruit a labor force still faced the haphazard and spasmodic rhythms with which their new employees approached their work.

The employers deplored the fact that the old subsistence mentality had carried over into the new work settings. Of the piece-rate worker they complained, "At the precise inch of cloth he stopped, in the mines, at the necessary pound of coal." 19 Workers in their turn bemoaned the loss of freedom and rebelled at the prospect of long confinement and steady production. The highlander, it was said, "never sits at ease at a loom; it is like putting a deer in the plough." 20 Attendance was irregular; workers would sometimes stay away from the job for days and send for their wages at the end of the week. In South Wales during the 1840s it was estimated that workers were absent 20 percent of the year, a figure that reached as high as 33 percent during the fortnight after their monthly payday.

One study of Birmingham, England, from 1766 to 1876, found that well into the nineteenth century, workers continued to celebrate Saint Monday-a weekly day of leisure spent in the alehouse enjoying drink, bar games, entertainments, "pugilism," and animal fights. 22 The tradition of Saint Monday followed from the bouts of weekend drinking and represented deeply held attitudes toward a potential surplus of wages: "The men ... (are] regulated by the expense of their families, and their necessities; it is very well known that they will not go further than necessity prompts them."

The industrial entrepreneurs tried, usually without success, to prohibit the observance of Monday as a holiday. Boulton and Watt's first enterprise foundered on the continual drunkenness of their work force. 24 The owner of a button-making factory decided that although he would not be able to control his workers, he would make an effort to train his apprentices in more industrious work habits. His diary records his frustration: "This evening Edward Lingard's misconduct in going to the Public House in the afternoon for drink, contrary to my inclination and notwithstanding I had forbidden him from it only yesterday- this I say, and meeting him on his way back, induced me hastily to strike him. With which my middle finger was so stunned as to give me much pain." 25 In addition to the time lost through observing Monday as a holiday, harvest time and other traditional feast days kept workers away. In 1776 the famous Josiah Wedgwood who pioneered new techniques of pottery production and business management, wrote to a colleague: "Our men have been at play 7 days this week, it being Burslem Wakes. I have rough'd and smoothed them over, & promised them a long Xmas, but I know it is all in vain, for Wakes must be observed though the World was to end with them."

Nineteenth-century American industrialists faced a similar set of problems when it came to honing the worker's body as an instrument of production. The owner of a Pennsylvania ironworks complained of frequent "frolicking" that sometimes lasted for days, along with hunting, harvesting, wedding parties, and holiday celebrations. One manufacturer filled his diary with these notes: "All hands drunk; Jacob Vending hunting; molders all agree to quit work and went to the beach. Peter Cox very drunk and gone to bed .... Edward Rutter off a-drinking. It was reported he got drunk on cheese." 27 In 1817 a Medford shipbuilder refused his men grog privileges and they all quit. 28 The ship's carpenter in one New York shipyard describes the typical workday: cakes and pastries in the early morning and again in the late morning, a trip to the grog shop by eleven for whiskey, a big lunch at half past three, a visit from the candyman at five, and supper, ending the workday, at sundown. He recalled one worker who left for grog ten times a day. A cigar manufacturer complained that his men worked no more than two or three hours a day; the rest of the time was spent in the beer saloon, playing pinochle. Coopers were famous for a four-day work week; and the potters in Trenton, New Jersey, immigrants from Staffordshire, were known to work in "great bursts of activity" and then lay off for several days.

We can see that the early apostles of industrialism had to confront the still-rudimentary progress of the civilizing process as it bore upon work behavior. The spontaneous, instinctually gratifying behavior of the new industrial worker had to be suppressed, and that energy channeled into the controlled behavior demanded by the intensification of production. The factory became a pedagogic institution where the new standards of conduct and sensibility, generally referred to as "labor discipline," would be learned. The exhaustive measures that employers took to thwart the animal body are a sign of its very intractability.

The notion of labor discipline signaled a very concrete problem: how to get the human body to remain in one place, pay attention, and perform consistently over a fixed period of time. Elaborate systems of fines were developed, minutely tailored to extinguish particular expressions of the impulsive body. For example, many fines sought to keep the body stationary. One work rule at Haslingden Mill about 1830 read, "Any person found from the usual place of work, except for necessary purposes, or talking with anyone out of their own alley, will be fined." 31 The Hammonds report fines at a textile mill near Manchester for "going further than the roving room door when fetching rovings" and for "any spinner found in another's wheel gate." 32 Ashworth fined his weavers for being found "out of the room. " 33 Other fines addressed the sounds that emitted from the body at work: punishable infractions included singing, whistling, swearing, and yelling. Some fines, intended to enforce a fixity of gaze and attention, punished workers for opening a window. Still other fines concerned the body's smell and appearance: workers were fined for being dirty, for not changing their shirts at least twice a week, and for spitting. Finally, there were fines to discourage aggressiveness, sexuality, and disorderliness-throwing water, seducing females, being drunk, arriving late, or not showing up at all.

American employers also found that only the severest fining policies had an effect on work habits. Workers were routinely fined up to half a day's pay for singing, talking, visiting, or being late. 35 In 18 59 a mill agent in Chicopee, Massachusetts, complained of the general indisposition of factory hands toward steady work, and in the years preceding World War I, it was still not unusual for as much as one-tenth of the work force to be missing on a given day. 36 Quit rates around the turn of the century were high-textile mills, meat-packing plants, automobile plants, steel mills, and machine works often showed annual turnover rates of 100 percent. 37 One survey showed that between 1905 and 1917, the majority of industrial workers changed jobs at least once every three years. Between 1 907 and 1910, turnover in the woolen industry was between 113 percent and 163 percent. It reached 232 percent in New York City garment shops in 1912, 2 52 percent in a sample of Detroit factories in 1916, and 370 percent in the Ford Motor Company in 1913.

These quit rates reflect not only the ambivalence and enduring orneriness of American workers but also the increasingly severe pressure that employers and managers brought to bear on dysfunctional, uncontrolled, and irregular behavior. Many scholars have argued that the introduction of steam power (and, later, other forms of expensive equipment) did more to consolidate the new behavioral norms than the earlier systems of fines alone. 39 This was in part because employers, in an effort to fully utilize their capital investment, became more ruthless in their willingness to dismiss workers who did not comply with the regularity of effort required to efficiently exploit the new machinery and in part because workers had to conform to a pace and quality of production increasingly driven by the machine, rather than by their own traditions and habits of organization.

As traditional working conditions grew scarce, worker resistance to labor discipline itself became more rationalized. The trade union movement set itself to limiting working hours, maintaining employment levels, and protecting wages, but a parallel approach of greater informality and striking continuity with older traditions also emerged. Workers began to act self-consciously to limit their efforts and so preserve their bodies. As early as 17 57, Josiah Tucker wrote in a pamphlet entitled "Instructions for Travelers" a description of domestic weavers transplanted to factory life. He provides one of the earlier descriptions of what was to become a central strategy for the industrial worker-withholding effort: "They think it no crime to get as much wages and to do as little for it as they possibly can, to lie and cheat and do any other bad thing, provided it is only against their master whom they look upon as their common enemy, with whom no faith is to be kept .... Their only happiness is to get drunk and make life pass away with as little thought as possible. "

Though skilled workers were indignant at the idea of deliberate slacking, by the end of the nineteenth century the British unions had come to recognize the characteristics of the trade cycle and to think of their effort as a commodity to be withheld or controlled in the service of free market bargaining. 41 Work banking, goldbricking, soldiering, are the modern terms that convey the legacy of this earlier clash between two wildly different conceptions of the standard of work discipline to which a body should conform.

In the American factory at the turn of the century, the foreman had primary responsibility for implementing management's goals: he was the "undisputed ruler of his department, gang, crew, or ship." 42 When in 1912 a congressional committee investigated the United States Steel Corporation, they attempted to understand just how the foreman functioned. They learned that foremen throughout American industry practiced something known as the "driving method," an approach to supervision that combined authoritarian combativeness with physical intimidation in order to extract the maximum effort from the worker. The driving method was well suited to work that depended upon the consistent exertion of the human body. The foreman's profanity, threats, and punishments were complemented by the workers' methods for limiting output. Methods of withholding labor varied somewhat from industry to industry and might be modified according to economic conditions, but the underlying spirit was everywhere the same-to protect the body by tempering exertion.

There were numerous stories of new employees who were approached by older, presumably wiser, workmen. "See here, young fellow, you're working too fast. You'll spoil our job for us if you don't go slower." If a friendly admonition did not have the desired effect and the man was judged a "rooter" or "rusher," social pressure, threats, and even violence might follow.

The protective response of so many workers to the demands made on their bodies is also the source of many work procedures that have been formalized and institutionalized in labor contracts and work rules. Studies by Lloyd Ulman and Sumner Slichter have shown how glassworkers', textile workers', and metalworkers' unions used agreements on production standards to restrict the driving method of supervision. 44 Work rules codified restrictive labor withholding practices, transforming them from informal methods of self-protection to deliberate contractual agreements.

THE PARADOX OF THE BODY

Until now we have treated the body in one of its aspects-as the scene and source of effort. This was the body that had to be disciplined if effort was to be drawn forth in a way that complemented the demands of an intensified, collectivized, and regulated production process. But the body as the scene of effort, the body to be protected, held a special paradox. For it was also through the body's exertions that learning occurred, and for those who were to become skilled workers, long years of physically demanding experience were an unavoidable requirement. This was the reward of physical involvement, since there was virtually no access to the craftsperson's skills short of an investment in years of effort. The body's meaning for production was not just to be an animal source of motive power or a pair of hands for an endless series of monotonous performances. Where the skilled worker was concerned, the body's sentience was also highly structured by a felt knowledge of materials and procedures. These twin functions of the body as a source of skill and as a source of effort-complicated the way in which employers confronted the problems of labor discipline and the speed with which they invested in labor-saving technology, just as they complicated workers' responses to that technology.

The historian Raphael Samuel has demonstrated that the body continued to play a central role in production, as a source of both effort and skill, throughout the nineteenth century in activities as diverse as mining, and quarrying, agriculture, gardening, and other forms of food production, construction, glass, pottery, and leather trades, woodworking, and metallurgy. 46 His examples are drawn from nineteenth-century Britain, but many of the labor processes he describes were prevalent in America as well.

There were few segments of the mid-Victorian British economy that steam power and machinery had not touched, but fewer still did not depend upon hand technology. Many factors impeded the progress of labor-saving machinery. Many trade unions were successful in resisting the encroachment of machinery on their crafts. From the employers' side, however, there were other compelling reasons. Wages were low, and the possibilities of increasing productivity with hand technology had not yet been exhausted. Employers often found other sources of efficiency in an increasingly minute division of labor and in the use of cheaper labor-saving materials. Market uncertainties made capital investment risky; besides, there was still a plentiful supply of cheap labor. Skill was also an important part of the problem. Ebullient descriptions of the new forms of "self-acting" machinery dominated trade journals of the day, but the new, more automatic machines were often fraught with technical difficulties and could not be relied upon as a substitute for skilled work. Moreover, consumers continued to value the quality of work that only a craftsworker could produce.

Samuel describes the wide variety of jobs that required nothing more than the effort of the laboring body. Coal was excavated by pick and shovel-"tools of the most primitive description, requiring the utmost amount of bodily exertion." 47 Clay-getting required working with a heavy pick. Masses of slime had to be stirred and trampled into the right consistency. Bakeries produced bread almost entirely by manual labor, the hardest operation being that of preparing the dough, "usually carried on in one dark corner of a cellar, by a man, stripped naked down to the waist, and painfully engaged in extricating his fingers from a gluey mass into which he furiously plunges alternately his clenched fists. "

There were many occupations in which the pain and physical alteration that resulted from such exertion were inseparable from the subtle and complex skills from which a craftsperson derived pride and economic power. One account of candy making in an Edinburgh factory ends this way: "The whole process ... requires great skill in the manipulation and it also requires the most severe and continuous muscular labor. We know, indeed, of no other kind of labour that requires more. Not a muscle or joint of the whole body remains inactive. It has quite a marvelous effect in taking down superfluous fat. It is well know that a stout man taken perhaps from lozenge making, and put to work on the hot pan, becomes in six weeks converted into a living skeleton. "

In glassmaking, everything was done by hand, "the gatherers taking the 'metal' from the furnace at the end of an iron rod, the blower shaping the body of the bottle with his breath, while the maker who finished the bottle off ... tooled the neck with a light spring-handled pair of tongs. Each bottle was individually made no matter what household, shop or tavern it was destined for. " 50 There were steady inroads made by machinery in this industry, but despite its labor-saving potential, glassmakers and their union organizations bitterly resisted and successfully impeded its progress. In 1878 one observer summarized the glassmakers' ambivalence: "If in many industries the substitution of mechanical for manual labor offers important advantages because ... it decreases a man's fatigue, we do not think it will have the same effect on the absolutely special work of the glass industry, and we fear that in depriving glassworkers of difficult tasks we will destroy their skill as well as the artistic talents of which glassmakers have the right to be proud." 

Pottery work was very physical and involved personal handling of the clay at every stage of production. Hands, thumbs, fingers, and palms were the sources of a delicately nuanced skill that guided the throwing and shaping of each item: "Dippers prepared the ware for firing by steeping it in a glaze tub. The right hand and arm are plunged nearly up to the elbow as he passes the piece of ware through the liquid .... Then with a rapid circular movement of the hand ... a movement that only years of practice can teach ... he makes the glaze flow thin and even over the surface. "

In the leather trades, the process of currying, which softened the leather for saddlery, coach linings, and shoes, was a highly skilled trade that required a laborer's strength to perform the heavy work of slashing, shaving, and pummeling the hide. lronmaking was another industry that depended upon combinations of skill and strength. New technology utilizing steam power to pound and roll metal together, and new puddling furnaces based on chemical reactions rather than mechanical action, had made the production process cheaper and had increased output. These innovations were not entirely labor-saving, however. In many cases they required new skills, even as they increased the physical demands of work. "The puddler, who had the key role in the new process, was given a task that was simultaneously highly skilled and exhausting, turning a viscous mass of liquid into metal. He worked in conditions of tremendous heat, violently agitating the metal as it boiled, rabbling it from side to side in the furnace, and then gathering it at the end of a rod while the molten liquid thickened .... The men had to relieve each other every few minutes, so great was the exertion and so intense the heat, but even so it was said that every ounce of excess fat was drained from them .... Few puddlers, it was said, lived beyond the age of 50."

These are just a few examples of the body's central presence as the source of both effort and skill in myriad industrial operations throughout the nineteenth century. In many cases, machinery was used to replace humans in supplying the motive power for various subprocesses of production. In most trades, though, labor-saving machinery developed slowly, and many factors inhibited its progress. Sometimes the new machinery, in amplifying the capacity of the human body to perform a given operation and thus increasing output, could also intensify the human participation that was required and thus exacerbate the problems of physical depletion.

Samuel notes that labor-saving machinery spread more rapidly in America, owing to a scarcity of labor and consequent higher wages. However, the historian of technology David Hounshell has recently shown that in some of the largest and most successful American firms, handwork and skilled machine work prevailed during most of the nineteenth century. For example, the Singer Sewing Machine Company was not able to produce perfectly interchangeable parts. As a result, they relied on skilled fitters to assemble each product. The McCormick Reaper Works employed crude manufacturing techniques. Production depended upon skilled machinists, blacksmiths, carpenters, and molders. 54 In 1 911 an observer of the American steel industry tried to convey the notion that skilled work did not imply diminished physical exertion: "New skills, like the puddler's, the catchers, or the machinists included very heavy manual work. As one worker speaking about working in a steel mill declared: 'Hard! I guess it's hard. I lost forty pounds the first three months I came into the business. It sweats the life out of a man!' The differences between the work performed by the skilled workers and the laborers was not of an "intellectual" versus manual activity. The difference lay in the content of a similarly heavy manual work: a content of rationality of participation for skilled workers versus one of total indifference for laborers.

The work of the skilled craftsperson may not have been "intellectual," but it was knowledgeable. These nineteenth-century workers participated in a form of knowledge that had always defined the activity of making things. It was knowledge that accrues to the sentient body in the course of its activity; knowledge inscribed in the laboring bodyin hands, fingertips, wrists, feet, nose, eyes, ears, skin, muscles, shoulders, arms, and legs-as surely as it was inscribed in the brain. It was knowledge filled with intimate detail of materials and ambience-the color and consistency of metal as it was thrust into a blazing fire, the smooth finish of the clay as it gave up its moisture, the supple feel of the leather as it was beaten and stretched, the strength and delicacy of glass as it was filled with human breath. These details were known, though in the practical action of production work, they were rarely made explicit. Few of those who had such knowledge would have been able to explain, rationalize, or articulate it. Such skills were learned through observation, imitation, and action more than they were taught, reflected upon, or verbalized. For example, James J. Davis, later to become Warren Harding's Secretary of Labor, learned the skill of puddling iron by working as his father's helper in a Pennsylvania foundry: "None of us ever went to school and learned the chemistry of it from books .... We learned the trick by doing it, standing with our faces in the scorching heat while our hands puddled the metal in its glaring bath. "

Though this form of knowledge evades explication, it is not fragile; on the contrary, it is extremely robust. The swimmer who has been away from the water for a year, the woman who has not ridden a bicycle since childhood, the grandmother who has not held an infant since she weaned her last, the carpenter who has retired his tools, the guitarist who abandoned her instrument, each of these can confront the object of their skills-the sea, the bicycle, the crying infant, the wood, the guitar-and find that the knowledge they once possessed remains, ready to be activated. Such knowledge is hard won and not easily lost. Within moments, the arms, and legs find their angle and rhythm as they cut through the water, the bicycle moves swiftly, the child is calmed, the chisel, plane, hammer, and saw find the pathways and resonance of the wood, and the fingers rediscover their agility against the strings of the guitar.

Effort may have signaled sacrifice and self-protection, but it was also the occasion and context for the development of this intimate, robust, detailed, and implicit knowledge. Such knowledge formed the basis of the worker's power. Historian David Montgomery has called this the "functional autonomy" of the craftsperson, derived through decades of sustained physical involvement during which the knowledge of each craft was systematized, not in explicit rules, but in the course of practical action. 57 It is appropriate that such knowledge be referred to as "know-how," for it was knowledge that derived from action and displayed itself in action, knowledge that meant knowing how to do, to make, to act-on.

The craftsworker's know-how was also an important source of social integration. The foreman was typically a worker who had turned his wide experience into superior competence. He achieved his position by virtue of his technical skill, and such opportunities were a real source of mobility for an ambitious worker. 58 The fact that workers were required to "use up" their bodies kept them distinct from those who employed them, but the skills mastered in physical activity provided an opportunity for independence, mobility, and identification with superiors.

Such experience-based knowledge had its weaknesses, too. When it came to improving work methods or adapting them to new techniques and business conditions, the practical know-how of the traditional craftsworker could be limiting. 59 With the growing complexity and size of factories, expanding markets that exerted a strong demand for an increase in the volume of production, and a rising engineering profession, there emerged a new and pressing concern to systematize the administration, control, coordination, and planning of factory work.

The man who emerged as the chief symbol of the rational approach to management was Frederick Taylor. Though much has been written on Taylor and the philosophy and methods of scientific management, it is worth highlighting a few central themes for three reasons. 60 First, Taylorism explicitly treats the worker's body in its two dimensions as a source of effort and as a source of skill. Second, workers' responses to Taylorism reveal the ambivalence that this dual role of the body can create, as it did among the bleach plant operators or the nineteenth century glassmakers. Third, the logic that motivated the early purveyors and adapters of scientific management has continued to dominate the course of automation in the twentieth-century workplace. As will be argued later, it is a logic that must undergo a fundamental reevaluation as information technology is widely adapted to productive activity.

THE PURIFICATION OF EFFORT

The agenda for scientific management was to increase productivity by streamlining and rationalizing factory operation from cost accounting and supervision to the dullest job on the shop floor. Efficiency was the mania, and to achieve efficiency, it would be necessary to penetrate the labor process and force it to yield up its secrets. In order that effort be rationalized, the worker's skills had to be made explicit. In many cases, workers' perceptions of their own interests prevented them from articulating their know-how, but there was yet another involuntary barrier. These skills did not easily yield themselves to explication; they were embedded in the ways of the body, in the knacks and know-how of the craftsworker.

Proponents of scientific management believed that observing and explicating workers' activity was nothing less than scientific research. Their goal was to slice to the core of an action, preserving what was necessary and discarding the rest as the sedimentation of tradition or, worse, artifice spawned by laziness. Taylor's disciples were driven by a vision of truth that would place managerial control on a footing of absolute objectivity, impervious to the commotion of class conflict or the stench of sweating bodies. 61 The principal method of acquiring such knowledge was the time study and, later, with the influence of Frank Gilbreth, the time-and-motion study. Here, "expert" observations of worker performance made it possible to translate actions into units of time and reconstruct them more efficiently.

The data from the time-study sheets became the possession of management and helped to fuel an explosion in the ranks of those who would measure, analyze, plan, report, issue orders, and monitor the various aspects of the production process. Armed with such data, planners, time-study experts, and production specialists (frequently organized as a staff group for the plant manager) became responsible for analyzing and organizing work tasks, controlling and monitoring their execution, coordinating functions, managing the flow of materials, and keeping records.

Taylor despised wasted effort at work, whether it resulted from deliberate self-protection or from ignorance. His single-minded devotion to the purification of effort gave rise to a set of practices that, whether adopted in whole or in part, transformed the nineteenth-century factory into the modern mass-production facility. The essential logic of his approach followed three steps. First, the implicit knowledge of the worker was gathered and analyzed through observation and measurement. Second, these data, combined with other systematic information regarding tools and materials, laid the foundation for a new division oflabor within the factory. It became possible to separate planning from task execution, to increase the fragmentation and thus the simplicity of production jobs, and so to minimize the amount of skill and training time associated with efficient operations. Third, the new system required a variety of specific control mechanisms to ensure the regularity and intensity of effort while continuing to supply managers and planners with the data necessary for adjustment and improvement. These mechanisms included the development of incentive payment schemes, monitoring systems, and standard operating procedures.

Taylorism meant that the body as the source of skill was to be the object of inquiry in order that the body as the source of effort could become the object of more exacting control. Once explicated, the worker's know-how was expropriated to the ranks of management, where it became management's prerogative to reorganize that knowledge according to its own interests, needs, and motives. The growth of the management hierarchy depended in part upon this transfer of knowledge from the private sentience of the worker's active body to the systematic lists, flowcharts, and measurements of the planner's office. 63 In 1912 a prominent naval engineer writing in the Journal of the American Society of Naval En9ineers listed the seven laws of scientific management. His first law, from which all the others followed, stated that "it is necessary in any activity to have a complete knowledge of what is to be done and to prepare instructions as to what is to be done before the work is started ... the laborer has only to follow direction. He need not stop to think what his past experience in similar cases has been."

Another contemporary interpreter of scientific management took pains to outline the quality of knowledge upon which this approach was based:

Instead of depending upon judgment, scientific management depends upon knowledge in its task of administration. Judgment is the instinctive and subconscious association of impressions derived from previous experience ... but even the best judgment falls far short of knowledge .... This knowledge is carefully and systematically collected and the data so obtained are classified and digested until the knowledge is instantly available whenever a problem is presented to management. Back of the form of organization is a knowledge of the needs and the work of the plant. Back of the plan of wage payment is a knowledge of psychology and sociology. Back of the instruction sheet is a knowledge of the sciences of cutting metals and of handling work.

Another industrial engineer addressing a conference at Dartmouth University's Amos Tuck School of Management in 1912 stressed the difference between scientific management and the more general movement known as systematic management. The scientific management approach rested on complete knowledge of materials, equipment, routing, job assignments, tools, task organizations, time standards, and performance methods. Each phase of an operation was to be planned completely before anything was done: "By this means the order and assignment of all work, or routing as it is called, should be conducted by the central planning or routing department. This brings the control of all operations in the plant, the progress and order of the work, back to the central point. Information, which even in the systematized plant is supposed to be furnished by the knowledge of the workman or the gang-boss or foreman, is brought back to the planning room and becomes a part of the instruction card. " 66 This transfer of knowledge both necessitated and legitimated a new conception of managerial (line and staff) responsibility in coordinating and controlling the complexities of the factory as it entered the era of mass production.

The complexity of workers' responses to scientific management has much to do with the dilemmas created by the body's dual role in production. As rationalization depleted the worker's skill base, there were reactions of loss and threat. These were not without contradiction. Where rationalization did offer less strenuous ways to accomplish physical tasks while improving tools, working conditions, and wages, there is evidence to suggest that many workers who were suspicious at first later accepted and even welcomed the innovations.

Men like Taylor and Gilbreth, who were firmly committed to raising the total level of worker output by easing the arduousness of physical tasks, looked both to new equipment and to new principles of work organization in order to accomplish their goal. For example, after a meticulous study of bricklaying, Gilbreth introduced an adjustable scaffold for piling up bricks. His invention eliminated the worker's bending over and lifting the weight of his body "a thousand times a day," and increased a worker's output from 1,000 to 2,700 bricks daily. Gilbreth claimed that workers typically responded to his innovations with gratitude, as their jobs were made easier.

Taylor believed it was necessary to share the fruits of such productivity increases and saw the differential piece-rate system, a central tenet of scientific management, as a method of uniting workers and managers in a bond of common interest. But incentive wages are devilishly hard to administer, and all too often, managers attracted to differential wage schemes succumbed to shortcuts that promised fast gains. Managers would frequently change piece rates as workers learned to meet the standards. This lead to the complaints of overwork with which unions relentlessly dogged proponents of Taylorism. Scientific management frequently meant not only that individual effort was simplified (either because of labor-saving equipment or new organizational methods that fragmented tasks into their simplest components), but also that the pace of effort was intensified, thus raising the level of fatigue and stress. Effort was purified-stripped of waste-but not yet eased, and resistance to scientific management harkened back to the age-old issue of the intensity and degree of physical exertion to which the body should be subject. As long as effort was organized by the traditional practices of a craft, it could be experienced as within one's own control and, being inextricably linked to skill, as a source of considerable pride, satisfaction, and independence. Stripped of this context and meaning, demands for greater effort only intensified the desire for self- • 69 protect10n.

Taylor had believed that the transcendent logic of science, together with easier work and better, more fairly determined wages, could integrate the worker into the organization and inspire a zest for production. Instead, the forms of work organization that emerged with scientific management tended to amplify the divergence of interests between management and workers. Scientific management revised many of the assumptions that had guided the traditional employer-employee relationship in that it allowed a minimal connection between the organization and the individual in terms of skill, training, and the centrality of the worker's contribution. It also permitted a new flexibility in work force management, promoting the maximum interchangeability of personnel and the minimum dependence on their ability, availability, or motivation. 70 The time-study expert became the new focus of workers' antagonisms. 71 Informal production quotas persisted and, in many cases, took on an overtly anti-management spirit. The union contract became the most important means for institutionalizing workers' instincts for self preservation, as rationalization routed out the ordinary graces with which the workday had been laced. Concern over wages and hours, work rules, and working conditions began to replace the craftsworker's culturally embedded practices of effort regulation. 72 Overall, the purification of effort meant a heightened standard of labor discipline, and as such it widened the gulf between those who gave of their bodies in the service of production and those who did not. Yet even into the second decade of the twentieth century, a more traditional sensibility toward work, one that suffused it with the rhythms of a chatty, sensual, and fun-loving humanity, could still be articulated. A machinist gained prominence when he debated Taylor in 1914 and remarked, "we don't want to work as fast as we are able to. We want to work as fast as we think it's comfortable for us to work. We haven't come into existence for the purpose of seeing how great a task we can perform through a lifetime. We are trying to regulate our work so as to make it auxiliary to our lives. "

A GLASS HALF FULL

As the logic of Taylorism took hold, the substitution of machine power for human labor became the obvious method of increasing the speed and volume of production. In an important way, however, the innovations of mass production added a new dimension to work relationships. Beginning with the highly mechanized Ford Highland Park auto assembly plant, technology would be relied upon to complement or supplant human direction: "The instruction cards on which Taylor set so much value, Ford was able to discard. The conveyor belt, the traveling platform, the overhead rails and material conveyors take their place .... Motion analysis has become largely unnecessary, for the task of the assembly line worker is reduced to a few manipulations. Taylor's stop-watch nevertheless remains measuring the time of operations to the fraction of a second. "

The fragmentation of tasks characteristic of the new Ford assembly line achieved dramatic increases in productivity due to the detailed time study of thousands of operations and the invention of the conveyor belt and other equipment that maximized the continuity of assembly. H. L. Arnold, an industrial journalist who wrote enthusiastically about Ford's innovations, summarized some of the key elements of this costreduction strategy. First, all needless motions were eliminated from the worker's actions; second, the task was organized to require the "least expenditure of willpower, and ... brain fatigue." 75 This formula has dominated the design of mass-production techniques throughout the twentieth century. Effort is simplified (though its pace is frequently intensified) while skill demands are reduced by new methods of task organization and new forms of machinery.

A distinction can be made between the new technologies for the organization of production, which allowed management to control the pace of the assembly line (and, thus, the intensity of effort), and the introduction of new forms of machinery that could reduce the requirements for both effort and skill. The continuity of assembly depended upon the production of interchangeable parts for uniform products. A new generation of automatic and semiautomatic machine tools moderated the physical demands on the machinist as they transferred skill from the worker to the machine. The new workers hired to operate these machines "had no skills and simply placed a piece in and removed it from the machine."

In the second decade of the twentieth century, when these developments were being debated, the fast-growing automobile industry faced shortages of skilled workers. Cheap labor was in abundance, particularly as unskilled rural laborers and new immigrants flocked to cities like Detroit in search of employment. Ford's engineers attempted to construct machine tools suited to the skill level of this labor force. The economic rewards of this approach were so great that with the spread of mass production, this pattern of design choices was carried over from the particular historical situation that had engendered them. For the majority of industrial workers in the generations that followed, there would be fewer opportunities to develop or maintain craft skills. Mass production depended upon interchangeability for the standardization of production; this principle required manufacturing operations to free themselves from the particularistic know-how of the craftsworker. Ford was the first to succeed in this endeavor on a massive scale, where others had failed.

Thus, applications of industrial technology have simplified, and generally reduced, physical effort, but because of the bond between effort and skill, they have also tended to reduce or eliminate know-how. 78 This paradox of the body's dual role in production is nowhere better illustrated that in James Bright's 1958 study of automation in American industry and the subsequent reinterpretation of his findings by social critic Harry Braverman in 1974. 79 Bright studied thirteen manufacturing facilities, including automobile engine plants, a refinery, a bakery, an electrical-part manufacturer, plating plants, and others. He observed varieties of automation in production and materials handling (though it should be noted that applications of computers in manufacturing were virtually nonexistent at the time of his study).

Bright concluded that throughout the thirteen plants he studied, workers were receptive and, in many cases, enthusiastic toward the new automated equipment. Why? "Automation takes the heavy labor out of a job .... The gain to management is productivity; the gain to labor is a much easier job .... In one engine plant a grizzled veteran manning a push button signal light control panel governing some hundred feet of machinery was interviewed .... He said, 'Sure I like it better. It's a lot easier. I can tell you one thing-I'll last a lot longer on this job.' " 81 Self-preservation would induce the worker to accept automation. Bright also clearly stated automation's effect on skills: "As the controls become more sensitive and responsive to the requirements of the operation, environment, and the task, the machine assumes responsibility, just as it has already assumed skill, knowledge, and judgment requirements.'' 82 He noted that this was one of labor's biggest "headaches" with automation, 83 but he believed that new wage determination systems, coupled with sensitive implementation processes, could overcome any resistance engendered by skill dilution. As he put it, "the principal shortcoming ... is that very few firms truly do 'sell' the automated equipment to the worker."

In Braverman's influential critique of what he called the "degradation of work" in this century, he used Bright's study to make a very different point. Where Bright saw the glass half full because the physical demands of work were curtailed, Braverman saw the glass being drained, as workers' skills were absorbed by technology. For Braverman, the transfer of skill into machinery represented a triumph of "dead labor over living labor," a necessity of capitalist logic. As machinery is enlarged and perfected, the worker is made puny and insignificant. By substituting capital (in the form of machinery) for labor, Braverman believed that employers merely seized the opportunity to exert greater control over the labor process. As the work force encountered fewer opportunities for skill development, it would become progressively less capable and, thus, less able to exert any senous oppos1t10n.

Although most analyses of industrial organization during the middle decades of the twentieth century have taken up one of these viewpoints, worker ambivalence toward automation has been a persistent theme. 86 Since skilled work is less automated, it also tends to involve more exertion, bodily alteration, dirt, and discomfort. Chinoy's 1955 study of automobile workers found few unskilled men harboring goals of becoming skilled workers. To a certain extent, this finding reflected the real and perceived lack of opportunities to move into skilled positions, but it also reflected a desire to avoid jobs that were physically demanding. "The values which played the greatest part in determining the order of preference among nonskilled jobs were regulation of employment and the relative absence of physical and psychological strain . . . . Men wanted work which was 'not too hard,' 'not too heavy,' 'not too dirty,' and 'not too noisy.' ... They did not like jobs in which 'you got to keep your hands in oil and grease all day.' "

In his account of the skilled auto worker, Bill Goode, a sociologist and former autoworker, reflects on the attitudinal gulf between skilled and unskilled workers in that industry. Most unskilled production workers are not interested in skilled jobs, he contends, because they are not as much fun: "One aspect of skilled work that production workers do not usually experience is dirt ... skilled work is filthy .... The skilled worker carries the marks of his trade under his fingernails and in the creases of his hands .... Assemblers ... have more fun. There is a spontaneity about the play on a line that is unmatched by skilled work. "

Similarly, another worker-turned-sociologist-of-the-workplace, Robert Schrank, reports that workers in a machine shop where he was employed routinely rejected opportunities to become skilled machinists. As these workers expressed themselves to Schrank: "What the hell do you want all the responsibility for? I just stand by this machine, watch it, adjust it once in a while, load it, and I can think about anything I want. I don't have to worry about all those tools and tolerances." 89 Later, Schrank became a set-up man on a turret lathe, a highly skilled position. He began to question the sacrifices he had made to develop and practice his skill: "As I worked in the turret lathe department, my doubts about being a skilled worker increased. The operators seemed to be having more fun than me, playing the races, fooling with the girls, and feeling free ... the pay differential ... did not seem to be worth the added responsibility. "

In the early days of industrialization, skill development was an important source of social integration, as the most skilled workers were often promoted to management ranks. As production jobs offered less opportunity for skill development, they came to have little relevance for the kinds of expertise needed at supervisory levels, and the boundary between these classes of organization members became more rigid. Chinoy's study of the ABC auto plant again speaks to this historical shift: "The structure of opportunity in the ABC plant gives most workers little reason to 'think of the corporation as a pyramid of opportunity.' ... The pattern of working one's way up from the ranks was seen as largely a thing of the past.''

THE SEARH FOR A SOLUTION

By the early I 960s, scholars had begun to ask whether this tendency would continue to grow with increasing levels of automation. As the range and depth of automation increased, further limiting the contribution of physical effort to the production process, would it continue to reduce skill requirements and thus widen the gulf between workers and management?

Robert Blauner's 1964 study Alienation and Freedom contrasted workers' attitudes and experiences in four types of industrial organizations- craft-based (the printing industry), machine-tending (the textile industry), mass production (the auto industry), and continuous-process (the chemical industry). Because of its advanced degree of automation, the continuous-process form of production was considered to "portend the conditions of factory work and employment in a future dominated by automated technology." 92 Would the experiences of continuous process workers simply repeat the disturbing trade-offs in the mass production industries? Or would these highly automated workplaces offer an alternative?

Blauner's report on the social-psychological dynamics associated with the continuous-process form of production revealed a very different relationship between the labor process and the body of the worker (for a description of continuous-process production and how it differs from other manufacturing processes, see appendix A, p. 415). Skill and effort finally seemed to be uncoupled. In the chemical industry, the focus of Blauner's research on continuous-process operations, production depended upon capital investment in highly automated equipment. Workers were called upon to physically exert themselves only when the equipment broke down, thus creating an inverse relationship between manual effort and productivity. The technology had helped to shape a work environment that freed the worker's body from the kinds of disciplinary pressures that had been routine features of industrial life.

For example, operators required to make the rounds of the equipment in a part of the plant in order to read instruments and monitor plant functioning could control the pace of their activities. As one operator told Blauner, "You can eat the soup first and do the work later, or take the readings at 1 :45 and then eat your soup." 93 The very act of making equipment rounds provided a physical variety and freedom of movement that contrasted sharply with the machine-paced work of the assembly line. This mobility typically placed the continuous-process operator out of the range of an immediate supervisor and made it possible to escape scrutiny. As a result, the operators in Blauner's study felt they had "considerable scope to do their jobs in their own way."

In other industrial settings, the close relationship between effort and skill meant that a reduction in the amount of physical involvement required of the worker tended to mean a reduction in the amount of skill he or she had to bring to the job. Did the reduction of effort in continuous-process operations also mean a reduction of skill? Blauner himself had some difficulty answering this question. In general, he argued that the shift from a job-centered to a process-centered form of work organization meant that the individual contribution changed from one of providing skills to one of assuming responsibility. The operator was responsible for the trouble-free operation of the process, the quality of product being produced, and the preservation of expensive automated equipment. Yet the head operator, in whom the greatest responsibility was invested, had achieved his position by virtue of long years of experience in the plant. Presumably, such experience was important because of the opportunities for skill development it afforded. Another fact makes Blauner's logic difficult to accept-the workers themselves felt that the skills demanded of them were considerable: "Virtually all of the workers interviewed said that their tasks required skill, even lots of skill, yet they were unsure of what was involved. "

Research by British investigators during the late I 9 50s and early I 960s provided a better understanding of the kinds of skills that workers had to bring to bear in a continuous-process environment. E. R. F. W. Crossman headed a research team that developed the first comprehensive description of the process operator's skills and activities, based on field visits within a variety of industries. Crossman found that the work of the process operator fell into four main categories. The most important of these was "control," followed by "special procedures and drills," "routine maintenance," and "recording and reporting." Activities related to controlling the process were described this way: "The process-operator ... must monitor the various gauges, attend to the signs coming from the plant itself, such as noises, smells and vibration, and occasionally carry out special tests on the product .... Apart from the 'official' controls, there are often special ways of influencing the process, such as propping doors open to give greater cooling, or tapping pipes to loosen deposits. "

Crossman went on to analyze what he called the "five components" of process-control skill: sensing ("the smells and appearance which indicate how the plant is running"), perceiving, prediction, familiarity with controls, and decision making. It was this final component that Crossman considered to be the most crucial, and he identified three principal ways in which an operator might approach decision making: by rule of thumb, by having an intuitive feel of what is best, or by pursuing an explicit and logical analysis. Crossman concluded that "on the whole, discussions with operators have suggested that the first, or 'rule-of-thumb,' method is common among the less good operators, and the second, or 'intuitive,' method is often characteristic of the better ones. But few operators seem to use a fully rational or conceptual approach .... The intuitive understanding which enables him to deal with subtle changes and unusual situations seems to come with experience alone. "

Blauner's discussion of operator skills lacks the depth and nuance of Crossman's observations, but his conclusions are similar. Blauner noted that although the worker in such an operation may have a wide range of knowledge about the factory, that knowledge is concrete rather than conceptual: "Whereas continuous-process production increases the worker's breadth of knowledge and awareness of the factory as a totality, it does not necessarily enhance the worker's depth of understanding of the technical and scientific processes ... the very complexity of the scientifically-based reaction means that there is more that is unknown to the average worker in the continuous-process industries than to the all-competent printing craftsperson or the automobile assembler."

What are the implications of this skill profile? To most observers, the continuous-process operation appears to present a significant discontinuity with other forms of production work. It would seem that this sense of uniqueness derives in large measure from the way in which these environments have achieved a de facto resolution of the paradox of the body in the labor process. Skill and effort are no longer inextricably linked. The operator must put forward a minimum of strenuous physical exertion, but the most critical skills he or she brings to the process still depend on the subtle and inarticulate know-how that a body accumulates with years of experience. In this respect, the skills of continuous-process work are in a direct line of descent from earlier generations of craft knowledge. The implicit action-embedded quality of such skills (remember James Davis on learning to puddle iron) would help to explain why the operators Blauner interviewed had a difficult time describing what precisely their skills were.

Based on his comparative studies, Blauner formulated the nowclassic U-curve hypothesis. Comparing levels of social integration across industries at distinct stages of automation, he found the highest levels within those industries that had the least (printing) and the most (chemical) automation. He found that workers in these craft-based and continuous-process industries were considerably less socially alienated and more socially integrated than their counterparts in the massproduction industries of textiles and automobiles. He further argued that rather than exacerbating industrial tensions according to some presumed linear function, higher levels of automation actually begin to reverse some of the most prominent negative tendencies associated with the rationalization of manufacturing work. Continuous-process operators experienced a greater degree of identification with their managers and more loyalty toward their companies, resulting in a heightened sense of mutuality and collaboration.

In the twenty years since Blauner published his study, other researchers have replicated his findings. 99 They observed higher levels of social integration among workers in the most automated industries. Yet some other scholars, notably Mallet, Braverman, and Gallie, have argued to the contrary. ' 00 Duncan Gallie's empirical work in particular shows that the line distinguishing the interests of the continuous-process operator and the manager can remain very clearly drawn.

To understand why and to what degree the continuous-process organization is likely to reveal a strong tendency toward social integration (particularly when compared to the mass-production industries), consider again the body's dual meanings for the production process-as a source of effort and skill.

Continuous-process operators are spared much of the exertion, exhaustion, and depletion that typifies factory work in less automated industries. Because automation has reduced the amount of physical sacrifice required of such workers, the antagonism between those who give of their bodies and those who do not has been muted. Continuousprocess operators are driven by neither the foreman nor the machine system. They are not forced to conform to a narrow behavioral standard. Like their managers, they feel that they are in control of their bodies as they move through time and space.

However, the nature of the operators' skills both contributes to and ultimately limits the potential for social integration in a continuousprocess operation. Blauner saw the level of responsibility that characterized the operator's role as a crucial factor accounting for the high degree of social integration. If the technology necessitates that an operator assumes responsibility for monitoring and controlling broad sections of the plant, how can management regard its workers as adversaries? Hence, "the alienation curve begins to decline from its previous height as employees in automated industries gain a new dignity from responsibility and a sense of individual function.

The level of responsibility to which Blauner and others have pointed remains firmly linked to an implicit judgment of the operator's skill. It is the operator with the most experience who becomes the head shift operator or crew leader. This is because the skills he or she must have, the "intuitive feel" described by Crossman, are developed only through many years of working at a variety of jobs in the plant and of observing a wide range of equipment operating under diverse conditions. The fact that this competence is experience-based helps orient the operator toward the stratified occupational structure within the work force. Because skill is cumulative, know-how gathered at the lower status levels is relevant to performance at higher levels, and the prospect of advancement serves as an important integrating mechanism. The extended temporal horizon necessary for skill development provides the opportunity for an individual to develop an identification with and loyalty to the organization.

Responsibility not only is a measure of the skill level of the operator but also denotes the dependence of management on the worker. The worker's bodily involvement with the production process over many years provides exclusive access to the subtle and specific knowledge required for competent operations. Blauner's observation of small work teams imposing their own standards of work discipline is reminiscent of the work groups in the early factory, whose unique craft knowledge empowered them to exert their own vision of appropriate work organization and behavior. 102 Like those early craftspeople, the exclusive knowledge of the workers in a continuous-process operation lashes them to their managers with bonds of reciprocity.

That the worker's body, through the sensual information it accumulates with physical involvement in the production process, remains a crucial source of skill both defines and limits the extent to which the worker is likely to be a fully integrated member of this kind of organization. The operator's knowledge continues to depend upon sentience, and it is the personal, specific bodily character of this knowledge that persistently differentiates the operator from the management superstructure. As long as their knowledge is concrete and specific rather than conceptual and technical, workers will tend to be confined to a certain set of roles. Without a conceptual understanding of the process in which they work, indeed, of their own actions, workers will find it difficult to make a contribution to that domain of more comprehensive functions typically labeled "managerial."

Thus, despite the high level of social integration in many continuousprocess organizations, a fundamental aspect of Tayloristic logic has been preserved. The much-touted collaborative atmosphere of continuous-process industries derives in large measure from the minimal emphasis placed upon the need for physical sacrifice from the worker. However, the body as the operator's source of skill remains a strong link to the industrial past and continues to demarcate the boundary between those who give of their bodies in the service of production and others who do not. Should operators perceive any devaluation of their skills, it is likely that this boundary would deepen and the sense of mutuality would diminish.

The history of work has been, in part, the history of the worker's body. Production depended on what the body could accomplish with strength and skill. Techniques that improve output have been driven by a general desire to decrease the pain of labor as well as by employers' intentions to escape dependency upon that knowledge which only the sentient laboring body could provide. Skilled workers historically have been ambivalent toward automation, knowing that the bodies it would augment or replace were the occasion for both their pain and their power. In Blauner's model, the problem seemed to have been solved; automation was able to minimize exertion without depleting skill. Yet even within this optimistic rendering, the body's role in the labor process continues to describe the distance between managers and the managed.

From this vantage point it is possible to see that the progress of automation has been a result of a transfer of knowledge and has, in turn, further enabled that transfer. However, the term transfer must be doubly laden if it is to adequately describe this process. Knowledge was first transferred from one quality of knowing to another-from knowing that was sentient, embedded, and experience-based to knowing that was explicit and thus subject to rational analysis and perpetual reformulation. The mechanisms used to accomplish this transfer were themselves labor intensive (that is, they depended upon first-hand observation of time-study experts) and were designed solely in the context of, and with the express purpose of, enabling a second transfer - one that entailed the migration of knowledge from labor to management with its pointed implications for the distribution of authority and the division of labor in the industrial organization. In the case of Blauner's chemical plant, operators' sentient knowledge could not be explicated. Their skills were left intact but in a way that continued to circumscribe their role in the organization.

As information technology is applied to the production process, what effect will it have upon the grounds of knowledge? What will it take "to know"? What kind of knowledge will enable competent participation in the processes associated with making things? History alerts us to the fact that answers to these questions will require some understanding of the relationship between the new technology and the sentient body. Will information technology continue to diminish physical effort but allow the retention of experience-based skills as in Blauner's scenario? Will effort and skill, indeed the very presence of the worker, be wiped out altogether as Braverman (or Bright) would lead us to predict?

The following chapter presents a third, relatively uncharted, alternative, suggesting that this latest technological transformation can indeed alter the terms of the centuries-old paradox concerning effort and skill. By redefining the grounds of knowledge from which competent behavior is derived, new information technology lifts skill from its historical dependence upon a laboring sentient body. While it is true that computer-based automation continues to displace the human body and its know-how (a process that has come to be known as deskilling), the informating power of the technology simultaneously creates pressure for a profound reskilling. How are these new skills to be understood? What might be their implications for the differences that have separated workers from their managers, differences that have, in an important way, depended upon the necessity of the body's labor and the body's learning.

CONCLUSION

MANAGING THE INFORMATED ORGANIZATION

What defines humanity is not the capacity to create a second nature-economic, social, or cultural-beyond biological nature; it is rather the capacity of going beyond created structures in order to create others. -MAURICE MERLEAU-PONTY The Structure of Behavior

TECHNOLOGY IS A PLACE

Put your eye to the kaleidoscope and hold it toward the light. There is a burst of color, tiny fragments in an intricate composition. Imagine a hand nudging the kaleidoscope's rim until hundreds of angles collapse, merge, and separate to form a new design. A fundamental change in an organization's technological infrastructure wields the power of the hand at the turning rim. Technological change defines the horizon of our material world as it shapes the limiting conditions of what is possible and what is barely imaginable. It erodes taken-for-granted assumptions about the nature of our reality, the "pattern" in which we dwell, and lays open new choices. When the telephone makes it possible to pursue intimate conversations without bodies that touch or eyes that meet, or when the electric light rescues the night from darkness, the experience is more than simply an element within the pattern. Such innovations give form and definition to our worldly place and provoke a new vision of the potential for relatedness within it. It is in this sense that technology cannot be considered neutral. Technology is brimming with valence and specificity in that it both creates and forecloses avenues of experience.

History reveals the power of certain technological innovations to transform the mental life of an era-the feelings, sensibilities, perceptions, expectations, assumptions, and, above all, possibilities that define a community. From the social influence of the medieval castle,I to the coming of the printed book, 2 to the social and physical upheaval associated with the rise of the automobile 3 -each specific example serves to drive home a similar message. An important technological innovation is not usefully thought of as a unitary cause eliciting a series of discrete effects. Instead, it can be seen as an alteration of the material horizon of our world, with transformative implications for both the contours and the interior texture of our lives. Technology makes the world a new place-a conception expressed by Fernand Braudel when he wrote:

It was only when things went wrong, when society came up against the ceiling of the possible, that people turned of necessity to technology, and interest was aroused for the thousand potential inventions, out of which one would be recognized as the best, the one that would break through the obstacle and open the door to a different future .... In this sense, technology is indeed a queen: it does change the world.

Some theorists have attributed systematic and purposeful agency to the managerial use of technology. They argue that managers are interested exclusively in technology as a means of controlling, limiting, and ultimately weakening their work force. 5 The data I have presented suggest a more complicated reality. Even where control or de skilling has been the intent of managerial choices with respect to new information technology, managers themselves are also captive to a wide range of impulses and pressures. Only rarely is there a grand design concocted by an elite group ruthlessly grinding its way toward the fulfillment of some special and secret plan. Instead, there is a concentration of forces and consequences, which in turn develop their own momentum. Sometimes these lines of force run in predictably straight paths. At other times, they twist and spiral, turn corners, and flow to their opposite. Activities that seem to represent choices are often inert reproductions of accepted practice. In many cases, they are convenient responses to the press of local exigencies. In some instances, they may actually reflect a plan.

To fully grasp the way in which a major new technology can change the world, as described by Braudel, it is necessary to consider both the manner in which it creates intrinsically new qualities of experience and the way in which new possibilities are engaged by the often-conflicting demands of social, political, and economic interests in order to produce a "choice." To concentrate only on intrinsic change and the texture of an emergent mentality is to ignore the real weight of history and the diversity of interests that pervade collective behavior. However, to narrow all discussion of technological change to the play of these interests overlooks the essential power of technology to reorder the rules of the game and thus our experience as players. Moreover, these two dimensions of technological change, the intrinsic and the contingent, need to be understood, not separately, but in relation to one another. The same innovation that abstracts work and increases its intellectual content, thus enhancing the learning of lower level employees (remember the operators at Tiger Creek and their experience with the new expensetracking system described in chapter 7), can also, within the context of the choices by which it is adapted, be experienced as a new source of divisiveness and control (as Tiger Creek's managers perceived a threat to their roles and resisted the potential for change).

The dilemmas of transformation that have been described are embedded in the living detail of everyday life in the workplace as it undergoes computerization. They are dilemmas precisely because of the way they reveal the subtle interplay between essence and choice. Information technology essentially alters the contours of reality-work becomes more abstract, intelligence may be programmed, organizational memory and visibility are increased by an order of magnitude beyond any historical capability. Individuals caught up in this newly configured reality face questions that did not need to be asked before. New possibilities arise and require new deliberations. The duality of information technology-its capacity to automate and to informate- provides a vantage point from which to consider these choices. The relative emphasis that organizations give to these capacities will become the foundation for a strategic conception of technology deployment and so will shape the way the dilemmas are confronted and resolved.

The organizations described in this book have illustrated how the need to defend and reproduce the legitimacy of managerial authority can channel potential innovation toward a conventional emphasis on automation. In this context, managers emphasize machine intelligence and managerial control over the knowledge base at the expense of developing knowledge in the operating work force. They use the technology as a fail-safe system to increase their sense of certainty and control over both production and organizational functions. Their experiences suggest that the traditional environment of imperative control is fatally flawed in its ability to adequately exploit the informating capacity of the new technology.

In these organizations, the promise of automation seemed to exert a magnetic force, a seduction that promised to fulfill a dream of perfect control and heal egos wounded by their needs for certainty. The dream contains the image of "people serving a smart machine," but in the shadow of the dream, human beings have lost the experience of critical judgment that would allow them to no longer simply respond but to know better than, to question, to say no. This dream brings us closer to fulfilling Hannah Arendt's dreadful forecast of a world in which behaviorism comes true:

The last stage of the laboring society, the society of jobholders, demands of its members a sheer automatic functioning, as though individual life had actually been submerged in the over-all life process of the species and the only active decision still required of the individual were to let go, so to speak, to abandon his individuality, the still individually sensed pain and trouble of living, and acquiesce in a dazed, "tranquilized," functional type of behavior. The trouble with modern theories of behaviorism is not that they are wrong but that they could become true, that they actually are the best possible conceptualization of certain obvious ttends in modern society. It is quite conceivable that the modern age-which began with such an unprecedented and promising outburst of human activity-may end in the deadliest, most sterile passivity history has ever known.

That managers may give themselves over to this dream because of inertia and convenience rather than cogent analysis is all the more disturbing. Organizations that take steps toward an exclusively automating strategy can set a course that is not easily reversed. They are likely to find themselves crippled by antagonism from the work force and the depletion of knowledge that would be needed in value-adding activities. The absence of a self-conscious strategy to exploit the informating capacity of the new technology has tended to mean that managerial action flows along the path of least resistance-a path that, at least superficially, appears to serve only the interests of managerial hegemony.

Yet what would seem to be a maddeningly predictable story line has its share of surprises, false starts, dead ends, trap doors, tarnished hopes, and real failures. The seeds of an informating strategy were apparent in each of the organizations described here, especially in Cedar Bluff, Global Bank Brazil, and DrugCorp. In the absence of a comprehensive strategy, no single organization fully succeeded in exploiting the opportunity to informate.

The interdependence of the three dilemmas of transformation I have described-knowledge, authority, and technique-indicates the necessary comprehensiveness of an informating strategy. The shifting grounds of knowledge invite managers to recognize the emergent demands for intellective skills and develop a learning environment in which such skills can develop. That very recognition contains a threat to managerial authority, which depends in part upon control over the organization's knowledge base. A commitment to intellective skill development is likely to be hampered when an organization's division of labor continuously replenishes the felt necessity of imperative control. Managers who must prove and defend their own legitimacy do not easily share knowledge or engage in inquiry. Workers who feel the requirements of subordination are not enthusiastic learners. New roles cannot emerge without the structures to support them. If managers are to alter their behavior, then methods of evaluation and reward that encourage them to do so must be in place. If employees are to learn to operate in new ways and to broaden their contribution to the life of the business, then career ladders and reward systems reflecting that change must be designed. In this context, access to information is critically important; the structure of access to information expresses the organization's underlying conception of authority. Employees and managers can hardly be partners in learning if there is a one-way mirror between them. Techniques of control that are meant to safeguard authority create suspicion and animosity, which is particularly dysfunctional when an organization needs to apply its human energies to inventing an alternative form of work organization better suited to the new technological context.

The interdependence among these dilemmas means that technology alone, no matter how well designed or implemented, cannot be relied upon to carry the full weight of an informating strategy. Managers must have an awareness of the choices they face, a desire to exploit the informating capacity of the new technology, and a commitment to fundamental change in the landscape of authority if a comprehensive informating strategy is to succeed. Without this strategic commitment, the hierarchy will use technology to reproduce itself. Technological developments, in the absence of organizational innovation, will be assimilated into the status quo.

THE DIVISION OF LABOR AND THE DIVISION OF LEARNING

Organizational theorists frequently have promoted a conception of organizations as "interpretation systems." 7 The computer mediation of an organization's productive and administrative infrastructure places an even greater premium upon an organization's interpretive capabilities, as each organizational level experiences a relatively greater preponderance of abstract cues requiring interpretation. This is as true for the plant manager as for the pulp mill worker, for the banker as well as for the clerk. In each case, oral culture and the action-centered skills upon which that culture depends are gradually eroded, and perhaps finally displaced, by the incursions of explicit information and intellective skill.

As bureaucratic coordination and communication become more dependent upon mastering the electronic text, the acting-with skills of the white-collar body are subordinated to the demands associated with dominating increasing quantities of abstracted information. In many cases, traditional functional distinctions no longer reflect the requirements of the business. When managers increase their engagement with the electronic text, they also risk a new kind of hyperrationalisn and impersonalization, as they operate at a greater distance from employees and customers.

When the textualizing consequences of an informating technology become more comprehensive, the body's traditional role in the production process (as a source of effort and/or skill in the service of actingon) is also transformed. The rigid separation of mental and material work characteristic of the industrial division of labor and vital to the preservation of a distinct managerial group (in the office as well as in the factory) becomes, not merely outmoded, but perilously dysfunctional. Earlier distinctions between white and blue "collars" collapse. Even more significant is the increased intellectual content of work tasks across organizational levels that attenuates the conventional designations of manager and managed. This does not mean that there are no longer useful distinctions to be made among organizational members, but whatever these distinctions may be, they will no longer convey fundamentally different modes of involvement with the life of the organization represented by the division of abstract and physical labor. Instead, the total organizational skill base becomes more homogeneous.

In the highly informated organization, the data base takes on a life of its own. As organizations like Cedar Bluff develop mechanisms that allow data to be automatically generated, captured, and stored, they begin to create their own image in the form of dynamic, detailed, realtime, integrated electronic texts. These texts can provide access to internal operations as well as external business and customer data; they can be designed with enough reflexivity to be able to organize, summarize, and analyze aspects of their own content. The electronic text becomes a vast symbolic surrogate for the vital detail of an organization's daily life. Such data constitute an autonomous domain. They are a public symbolization of organizational experience, much of which was previously private, fragmented, and implicit-lodged in people's heads, in their sensual know-how, in discussions at meetings or over lunch, in file drawers, or on desktops.

The textualization process moves away from a conception of information as something that individuals collect, process, and disseminate; instead, it invites us to imagine an organization as a group of people gathered around a central core that is the electronic text. Individuals take up their relationship toward that text according to their responsibilities and their information needs. In such a scenario, work is, in large measure, the creation of meaning, and the methods of work involve the application of intellective skill to data.

Under these circumstances, work organization requires a new division of learning to support a new division of labor. The traditional system of imperative control, which was designed to maximize the relationship between commands and obedience, depended upon restricted hierarchical access to knowledge and nurtured the belief that those who were excluded from the organization's explicit knowledge base were intrinsically less capable of learning what it had to offer. In contrast, an informated organization is structured to promote the possibility of useful learning among all members and thus presupposes relations of equality. However, this does not mean that all members are assumed to be identical in their orientations, proclivities, and capacities; rather, the organization legitimates each member's right to learn as much as his or her temperament and talent will allow. In the traditional organization, the division of learning lent credibility to the legitimacy of imperative control. In an informated organization, the new division of learning produces experiences that encourage a synthesis of members' interests, and the flow of value-adding knowledge helps legitimate the organization as a learning community.

The contemporary language of work is inadequate to express these new realities. We remain, in the final years of the twentieth century, prisoners of a vocabulary in which managers require employees; superiors have subordinates; jobs are defined to be specific, detailed, narrow, and task-related; and organizations have levels that in turn make possible chains of command and spans of control. The guiding metaphors are military; relationships are thought of as contractual and often adversarial. The foundational image of work is still one of a manufacturing enterprise where raw materials are transformed by physical labor and machine power into finished goods. However, the images associated with physical labor can no longer guide our conception of work.

The informated workplace, which may no longer be a "place" at all, is an arena through which information circulates, information to which intellective effort is applied. The quality, rather than the quantity, of effort will be the source from which added value is derived. Economists may continue to measure labor productivity as if the entire world of work could be represented adequately by the assembly line, but their measures will be systematically indifferent to what is most valuable in the informated organization. A new division of learning requires another vocabulary-one of colleagues and co-learners, of exploration, experimentation, and innovation. Jobs are comprehensive, tasks are abstractions that depend upon insight and synthesis, and power is a roving force that comes to rest as dictated by function and need. A new vocabulary cannot be invented all at once-it will emerge from the practical action of people struggling to make sense in a new "place" and driven to sever their ties with an industrial logic that has ruled the imaginative life of our century.

The informated organization is a learning institution, and one of its principal purposes is the expansion of knowledge-not knowledge for its own sake (as in academic pursuit), but knowledge that comes to reside at the core of what it means to be productive. Learning is no longer a separate activity that occurs either before one enters the workplace or in remote classroom settings. Nor is it an activity preserved for a managerial group. The behaviors that define learning and the behaviors that define being productive are one and the same. Learning is not something that requires time out from being engaged in productive activity; learning is the heart of productive activity. To put it simply, learning is the new form of labor.

The precise contours of a new division of learning will depend upon the business, products, services, and markets that people are engaged in learning about. The empowerment, commitment, and involvement of a wide range of organizational members in self-managing activities means that organizational structures are likely to be both emergent and flexible, changing as members continually learn more about how to organize themselves for learning about their business. However, some significant conceptual issues are raised by the prospect of a new division of learning in the informated organization. The following discussion of these issues does not offer a rigid prescription for practice but suggests the kinds of concrete choices that define an informating strategy.

MANAGERIAL ACTIVITIES IN THE INFORMATED ORGANIZATION

As the intellective skill base becomes the organization's most precious resource, managerial roles must function to enhance its quality. Members can be thought of as being arrayed in concentric circles around a central core, which is the electronic data base. The skills required by those at the core do not differ in kind from those required at a greater distance from the core. Instead of striking phenomenological differences in the work that people do, the distance of any given role from the center denotes the range and comprehensiveness of responsibilities, the time frame spanned by those responsibilities, and the degree of accountability for cross-functional integration attached to the role. The data base may be accessed from any ring in the circle, though data can be formatted and analyzed in ways that are most appropriate to the information needs of any particular ring of responsibility.

On the innermost ring, nearest to the core, are those who interact with information on a real-time basis. They have responsibility for daily operations, and the level of data they utilize is the most detailed and immediate. Because intellective skill is relevant to the work of each ring of responsibility, the skills of those who manage daily operations form an appropriate basis for their progression into roles with more comprehensive responsibilities.

The jobs at the data interface become increasingly similar to one another as the informating process evolves. In the advanced stages of informating, these become "meta jobs," because the general characteristics of intellective skill become more central to performance than the particular expertise associated with specific production-related functions. Expertise either is available from on-site specialists or is built into information systems. For example, at Cedar Bluff, top managers believed that they would solve the problem of vanishing artistry by building that expertise into the information system capability. The know-how of managers with years of experience could be systematized and made available to operators who would never have the same degree of involvement in the action contexts that had developed the personal and specific knowledge associated with action-centered skill.

This relationship between general intellective skills and expertise in specific areas was also illustrated in the case of the calculator models at Cedar Bluff, discussed in chapter 7. Operators needed the kind of understanding that would allow them to know when and how to use a model, and when to be critical of its assumptions or outputs. That quality of understanding did not depend upon being able to match the expertise that went into the models' calculations. The operator with a conceptual approach to the process, skilled in data-based reasoning, and familiar with the theory that links elements in the production process, may not be able to reproduce the knowledge of an individual with years of hands-on experience or expert training. Nevertheless, he or she should be able to understand the conceptual underpinning of a problem well enough to select among potential analytic strategies and to access the expert knowledge that is required. Intellective skill is brought to bear in the definition of the problem for analysis, the determination of the data that is required for analysis, the consideration of the appropriateness of an analytical approach, and the application of the analysis to improved performance.

The activities arrayed on the responsibility rings at a greater distance from the core incorporate at least four domains of managerial activity: intellective skill development, technology development, strategy formulation, and social system development. For example, the crucial importance of the intellective skill base requires that a significant level of organizational resources be devoted to its expansion and refinement. This means that some organizational members will be involved in both higher-order analysis and conceptualization, as well as in promoting learning and skill development among those with operational responsibility. Their aim is to expand the knowledge base and to improve the effectiveness with which data is assimilated, interpreted, and responded to. They have a central role in creating an organizational environment that invites learning and in supporting those in other managerial domains to develop their talents as educators and as learners. In this domain, managers are responsible for task-related learning, for learning about learning, and for educating others in each of the other three domains.

A new division of learning depends upon the continued progress of informating applications. This managerial domain of technologyrelated activity comprises a hierarchy of responsibilities in addition to those tasks normally associated with systems engineering, development, and maintenance. It includes maintaining the reliability of the data base while improving its breadth and quality, developing approaches to system design that support an informating strategy, and scanning for technical innovations that can lead to new informating opportunities. Members with responsibility for the development of technology must be as concerned with the use of technology (Do people understand the information? Do they know how to use it?) as they are with other aspects of design and implementation. This kind of technological development can occur only in the closest possible alignment with organizational efforts to promote learning and social integration. Technology develops as a reflection of the informating strategy and provides the material infrastructure of the learning environment.

Learning increases the pace of change. For an organization to pursue an informating strategy, it must maximize its own ability to learn and explore the implications of that learning for its long-range plans with respect to markets, product development, new sources of comparative advantage, et cetera. A division of learning that supports an informating strategy results in a distribution of knowledge and authority that enables a wide range of members to contribute to these activities. Still, some members will need to guide and coordinate learning efforts in order to lead an assessment of strategic alternatives and to focus organizational intelligence in areas of strategic value. These managers lead the organization in a way that allows members to participate in defining purpose and in supporting the direction of long-term planning. The increased time horizon of their responsibilities provides the reflective distance with which they can gauge the quality of the learning environment and can guide change that would improve collective learning.

There is considerable interdependence among these four domains of managerial activity (intellective skill development, technology development, strategy formulation, and social system development). For example, activities related to intellective skill development cannot proceed without the social system management that helps to foster roles and relationships appropriate to a new division of learning. Activities in either of these domains will be frustrated without technological development that supports an informating strategy. Integration and learning are responsibilities that fall within each domain, because without a shared commitment to interdependence and the production of value-adding knowledge, the legitimacy of the learning community will suffer. Business outcomes such as cost, efficiency, quality, product development, customer service, productivity, et cetera, would result from coordinated initiatives across domains. Managerial work would thus be team-oriented and interdisciplinary, and would promote the fluid movement of members across these four domains of managerial activity.

The concentric structure depends upon and promotes both vertical and horizontal organizational integration. There are no predetermined boundaries between any rings within the organizational sphere or between the domains of managerial authority. The skills that are required at the data interface nearest to the core of daily operating responsibilities provide a coherent basis for the kind of continual learning that would prepare people for increasingly comprehensive responsibilities. The relative homogeneity of the total organizational skill base suggests a vision of organizational membership that resembles the trajectory of a professional career, rather than the two-class system marked by an insurmountable gulf between workers and managers. The interpenetration between rings provides a key source of organizational integration.

Some observers of the emerging technological environment have predicted an increasingly bifurcated distribution of skills in the workplace. 8 On the other extreme, the operators at Piney Wood, whose discussion of the future opened our Introduction, believed that in the future all factory workers would be college graduates. The concentric organizational structure suggests that the solution to future skill requirements need not be as drastic as either of these scenarios implies. While it is probable that entry-level requirements will become more demanding, the increased homogeneity of skills and the continuity between organizational rings entails an ongoing commitment to training and education in order to facilitate career progression. The shape of skill distribution thus is more likely to represent a curve than the discontinuous step function that characterizes the traditional hierarchy, with its more rigid distinction between managers and the managed.

THE BODY'S NEW WORK: MANAGING THE INTRICACY OF POSTHIERARCHICAL RELATIONSHIPS

The vision of a concentric organization is one that seems to rely upon metaphors of wholeness-interdependency, fluidity, and homogeneity each contribute to organizational integration. What is required of managers in such a workplace, where learning and integration constitute the two most vital organizational priorities? How is the social system of such an organization to be managed?

As we have seen, the abstract precincts of the data interface heighten the need for communication. Interpretive processes depend upon creating and sharing meaning through inquiry and dialogue. New sources of personal influence are associated with the ability to learn and to engender learning in others, in contrast to an earlier emphasis upon contractual relationships or the authority derived from function and position.

What new patterns of relationships will characterize this kind of learning environment? What will replace the familiar map that the model of imperative control has provided? The answer to this question derives from one of the most significant dialetics of an informating strategy. In a conventional organization, managers' action-centered skills are geared toward the politics of interpersonal influence, principally as they pertain to maintaining reciprocities, managing superiors, and gathering or disseminating information. These skills are shaped by the demands of achieving smooth operations and personal success under conditions of hierarchical authority. People develop their expectations about how to treat one another largely in reference to rank and function. An informating strategy does place severe demands upon managers' action-centered skills in the service of acting-with but in a very different context. The relationships to be managed are both more dynamic and more intricate than earlier patterns. The shape and quality of relationships will vary in relation to what people know, what they feel, and what the task at hand requires. Relationships will need to be fashioned and refashioned as part of the dynamism of the social processes, like inquiry and dialogue, that mediate learning. Such relationships are more intricate because their character derives from the specifics of the situation that are always both pragmatic-what it takes to get the work done best-and psychological-what people need to sustain motivation and commitment.

In the information panopticon, managers (like those at Metro Tel) frequently tried to simplify their managerial tasks by displacing faceto- face engagement with techniques of surveillance and control. As a consequence, they became isolated from the realities of their organizations as they were increasingly insulated by an electronic text that in turn was even more vulnerable to workers' antagonisms. The demands of managing intricate relationships reintroduce the importance of the sentient body and so provide a counterpoint to the threat of hyperrationalism and impersonalization that is posed by computer mediation. The body now functions as the scene of human feeling rather than as the source of physical energy or as an instrument of political influence. Human feeling operates here in two ways. First, as members engage in their work together, their feelings are an important source of data from which intricate relations are structured. Second, a manager's felt sense of the group and its learning needs is a vital source of personal knowledge that informs the development of new action-centered skills in the service of acting-with.

The demands of a learning environment can reduce the psychological distance between the self and the organization because active engagement in the social processes associated with interpretation requires more extensive participation of the human personality. In a traditional approach to work organization, employees could be treated as objectively measurable bodies, and in return, they could give of their labor without giving of their selves. The human being as wage earner and the human being as subjective actor could remain separate. In an environment of imperative control, managers can remain indifferent to what their subordinates feel, as long as they perform adequately. This "is" was eventually translated to "ought," as incursions of private feeling into the workday came to be seen as squandering the organization's time. It was this view that ultimately triumphed over the professionals at DrugCorp who had unwittingly textualized their own playfulness. As they struggled with their notions of legitimate work behavior, DrugCorp's managers tried to define the self out of the workday. But when work involves a collective effort to create and communicate meaning, the dynamics of human feeling cannot be relegated to the periphery of an organization's concerns. How people feel about themselves, each other, and the organization's purposes is closely linked to their capacity to sustain the high levels of internal commitment and motivation that are demanded by the abstraction of work and the new division of learning.

The relationships that characterize a learning environment thus can be thought of as posthierarchical. This does not imply that differentials of knowledge, responsibility, and power no longer exist; rather, they can no longer be assumed. Instead, they shift and flow and develop their character in relation to the situation, the task, and the actors at hand. Managing intricacy calls for a new level of action-centered skill, as dialogue and inquiry place a high premium on the intuitive and imaginative sources of understanding that temper and hone the talents related to actin9-with. The dictates of a learning environment, rather than those of imperative control, now shape the development of such interpersonal know-how.

The seeds of this new intricacy are already evident in several of the organizations I have described. The managers at Cedar Bluff who learned to join their workers in asking questions and searching for answers were already engaged in forging patterns far more intricate than the simpler prescriptions derived from faith in managerial authority. The professionals of DrugCorp, normally divided by function, professional discipline, and organizational rank, had invented new modes of relationships based upon a valued exchange of information, shared inquiry, and play. At Global Bank Brazil, bankers and operations managers were groping for new relationships that would reach beyond turf and hierarchy in order to better serve their customers. At Tiger Creek, managers and workers reached and stumbled as they attempted to shift the logic of their relationship from one based on hierarchy to one shaped by the pragmatic opportunities offered by a redistribution of knowledge. In each of these cases, organizational members were confronted with rich new possibilities engendered by an informating technology. In each case, they discovered that exploiting these new opportunities required new forms of relationships governed by the necessities of learning and performance rather than by the rules of an older faith rules that sort, rank, and separate.

DISSENT FROM WHOLENESS

Is there a dark side to this vision of a wholistic organization with its emphasis on relationships that are intricate, dynamic, and constructed ad hoc? What new psychological costs might it imply? What new mechanisms might be required to ensure justice and equity? To answer these questions, we need to return to the voices of the mill workers as they lived through the transition to an informated environment.

One source of insight into the potential pitfalls of the wholistic organization comes from those workers at Piney Wood who drew strength from the institutional arrangements of a traditional work system. When workers at that plant anticipated the future, they saw computer-based technology as a catalyst for new work systems that would be more flexible, collaborative, and socially integrative than anything they had known. Many were curious and even enthusiastic about such a prospect, but there were others who dissented from this vision of wholeness. Their perspective reminds us of how a certain breed of American worker has found psychological sustenance in the norms of the industrial workplace. It can also alert us to what might be lost in the transformation to a new form of organization and asks that we think carefully about what was best in the old arrangement and should be preserved.

Though the corrosive effects of an adversarial environment cannot be denied, there are those who found an important source of psychological freedom in the rigorously defined contractual relationships that characterized Piney Wood. In such a workplace, the union contract defines binding arrangements-jobs are meticulously defined, and seniority rules guide pay and promotion decisions. Individual workers know exactly what is required of them and, in return, what rights they possess. The worker's first obligation is above all to the job-to perform it competently and completely. As long as individuals uphold their end of the employment contract, their rights are protected. These protections provide them a status under the law that is equal to that of their employer. If the contract is violated, then there are institutionalized mechanisms of due process, invested ultimately with the legitimate authority of the state, through which a person can redress a grievance or seek to influence the policies that inform the labor management relationship.

The organizing principle of such a workplace is based on the individual. People are held accountable for their particular jobs, and these jobs are treated as distinct elements that must be assembled in order to accomplish the work of the organization. That one's obligation is first of all to the job, rather than to the enterprise, creates a certain psychological distance between the self and the organization. Living up to the terms of the employment contract leaves a wide range of behavior that is unspecified and noncontractual. A worker need not buy into the purposes or values of the organization in order to perform competently and enjoy the rewards that he or she has earned. There is no pressing need to be liked by those around you, either superiors or peers, when one's primary obligation is to fulfill the demands of a narrow job description.

These arrangements can provide the worker with a measure of independence and autonomy. They make it possible to feel that one is, as the idiom goes, one's "own man." U.S. labor relations reflect a conception of equality in which parties each seek advantage through negotiation. Neither party must buy into the worldview of the other. A pluralism of values and interpretations may coexist, so long as the job gets done.

When the workers in such an organization consider their managers, they see a very different world. Those who most value the psychological distance and autonomy provided by the contractual relationship, view managers with a certain pity. They believe that managers are at the mercy of a system that can make unlimited demands, because the boundaries that define the manager's job are vague and permeable. Managerial jobs themselves are abstract enough to be subject to diverse interpretations. Managers might well ask themselves, have I done my job? If the job is that abstract, then it is also easy for peers and superiors to question what a manager has done and to formulate their own evaluations of his or her performance. Without a collective contract, the manager is vulnerable and dependent; he or she must surrender to the organization's purposes and values. Instead of the feistiness and pluralism that characterize the labor-management relationship, many workers see in the managers' world overbearing demands for ideological unity, loyalty, and the submergence of the self. In other words, managers may seek to control their subordinates, but they are not in control of their own work lives.

There is a breed of American worker who cherishes the autonomy and sense of self-control afforded by his or her skills and protected by the union contract. When these workers contemplate the prospect of the socially integrated high-technology workplace, they feel despair. They anticipate a loss of their unique identities, of freedom and autonomy, and of well-defined rights and responsibilities. They fear that without the traditional sources of protection provided by their job descriptions and their contract, they will become prey to every capricious whim of their superiors. They understand that the managers' world requires the body as a political instrument for self-presentation and influence, but they know that these are talents they have not developed and toward which they feel more than a little distaste.

They say that with this new technology we need a more flexible system, one that will make us competitive. They figure it works for management, so why not for the blue-collar worker. If a manager hasn't saved money, he won't get his extra one hundred dollars a week. If he doesn't produce, he's out the gate. But right now, I don't have anything to worry about except doing my job and doing it well. I don't have to be friends with people in order to move up. I don't have to use anybody. In the management world, you have got to be a salesman to a certain extent. You have go to know how to manipulate the human system. If I am like management, it means I will have to be doubly nice to you whether I like you or not. You have to see everybody as a stepping-stone.

Other operators believe that in a fluid, socially integrated workplace, without clear job descriptions and contracts, they would lose the clarity of rights and obligations that currently offer an important sense of personal control. Without such definition, how will one know what to expect each day and how will it be possible to manage the extent of one's own exertion? They fear flexible arrangements that would change according to the needs of the total organization, in place of discrete task assignments on an individual basis. Their "have-skills-will-travel" image is a kind of emotional insurance policy, but an enterprise centered approach to task distribution would make each individual more dependent upon and integrated with the organization.

They say the new technology will require a flexible system. But under a flexible system, you have no choice but to go where they send you, when they send you. You can get to earn high pay, but you have no choice about what your job is, and you can't have your own job. You never know what to wear to work-do you wear your good Levi's or do you wear your greasy ones?

This statement represents the operators' worst fears-that the loss of control over one's work would invade the most intimate and ordinary details of everyday life. Stumbling around the bedroom on a dark morning, trying to get ready in time to have a cup of coffee before leaving for the plant, the worker must ask, What do I wear? What kind of day should I look forward to? What is in store for me today? Will I feel good about the things I am asked to do? Without the capacity to set one's expectations, it is difficult to locate oneself emotionally. It is easy to feel helpless, as if one is at the mercy of others.

The disquiet these workers feel culminates in a frank concern over power. In a workplace in which divisions among workers and between workers and managers are minimized, where all members are supposed to pull together in the service of the organization, what rights will individuals have? How will these rights be specified? What mechanisms will ensure that individuals have a voice in and influence over the policies and practices that shape their work life? Who will have the power to define the circle of legitimate behavior by which all members are evaluated?

They say that in a new flexible system the criteria for advancement are fitness and ability instead of seniority. But who gets to say you are fit? Who decides what is fit? It will turn out to be that if you are nice to your supervisor, if you do what your supervisor wants you to do, that is what makes you fit.

When these workers consider an organizational system that puts a premium on wholeness, they also see a system that will require manipulation, ingratiation, and conformity. They envision a new approach to the work system without also having considered the changes in the distribution of authority that would be necessary for an informating strategy to fully succeed. However, their concerns do warn of the potential for tyranny in a flexible and socially integrated organization. There are two immediate implications of this warning. First, there presumably will continue to be individuals for whom psychological distance and contractualized responsibilities are very important. To acertain extent, such persons might simply choose to avoid membership in a wholistic informated organization. Viewed in another way, the perspective offered by such individuals probably would be healthy for any learning community. They provide a sensitive barometer for organizational processes that violate respect for the individual or somehow endanger the balance that must exist between individual and organizational interests. A second crucial issue involves the need for mechanisms that can ensure equity and due process within the informated organization. Labor-management contracts grew out of a need for workers to protect themselves from the unilateral authority of their employers. I have already suggested that a new division of labor will not thrive in nor easily tolerate unilateral authority. Nonetheless, informated organizations will have to pay careful attention to developing a constitutional infrastructure that legitimates public debate and mutual influence. The clarity of individual rights within the enterprise is likely to become extremely important to the extent that the learning community requires the participation of the "total person" in its endeavors. Because such a system will exert considerable pressure on an individual's psychological boundaries, it will require mechanisms that can arbitrate competing interpretations of rights and obligations.

The notion of a wholistic informated organization must be qualified in still another way. I have offered a vision of organization in which there are no a priori designations of managers and managed. Instead, people move from the operating core to increasing responsibility and comprehensive influence based upon the degree to which they excel in the skills required by an informated task environment. The intellective skills developed at the data interface provide an important part of the basis for later movement to further rings in the concentric structure. These intellective skills are unlikely to be equally distributed, and the variation in their distribution can become a new source of hierarchical distinctions within even the most fluid organization. For example, at Cedar Bluff, new shades of meaning that began to appear in everyday language foretold the implicit criteria according to which some members would be more highly valued and so implied new sources of conflict as well. These ranking rules were expressed in the metaphorical devices most commonly used in discussions of the new technological environment. Typically, spatial designations were used to convey the worth of people and activities. 11 Consider these interrelated and most often repeated uses of the spatial metaphor:

I. Formal intelligence is up; experiential know-how is down.

A low-IQ operator will not accept a computer on the job. He will ignore the data in favor of his own way of doing things.

At Piney Wood, digital instruments seem like black magic, but at Cedar Bluff, operators are more educated. They average one to two years of college instead of just tenth grade. It's a higher, more advanced type of person.

2. Young is up; old is down.

Younger people find it easier to grasp the computer stuff because they are a higher caliber. Installing new technology is a message to the older operators. It says you must have computer and electronic skills. Some want to retire when they hear this.

We will be displacing older people who cannot qualify for the new jobs we are creating. They have to lift themselves up to standard.

3. Abstract work is up; manual work is down.

With this technology, we are bringing people to a higher level intellectually. It is more demanding, so you have to treat them differently. The physical demands are gone. There are no hammers and wrenches. To deal with problems now is more complex. The computer operators are going to be higher than the man on the floor, because the physical is easier to train that the brain.

We need a higher caliber of people now. They have to use their heads now, not just what comes below the neck. Not just do things.

Cedar Bluff's plant manager recounted their efforts to recruit a "high" caliber work force suited to the demands of "high" technology:

We believed that having a number of people with higher levels of understanding would create group dynamics effects that would result in better problem solving. A high level of knowledge, a high level of intelligence, will result in a higher individual contribution to the business. We need people to be able to optimize our use of the technology.

Workers had been promised a completely computerized operation and believed their jobs would be "push-button easy" with no "nasty work" to leave them soiled and exhausted at the end of the day. The reality was somewhat different, requiring a fair amount of interaction with the operating equipment, particularly in the first years as operators learned their way around the mill. Though their direct involvement with equipment was substantially less than it would have been in a conventional mill, their only frame of reference was that initial promise of "workless" work. Subtle hierarchical pressures began to take shape within this work force that had been so painstakingly recruited and primed for teamwork. People began to believe that the best and the brightest were those who excelled at the data interface, while others gravitated (or were nudged) toward maintenance work. A manager described this subtle sifting process:

We need our most-qualified people manning the computer screens. The people who really know what the equipment feels like and sounds like are becoming second-class citizens. In a traditional system, you spent your whole life developing that feel, and your livelihood depended on it. But the new generation must trust the computer screen. A new pecking order emerges. The computer screen takes more mental skills, and we reward those skills. Maybe the work in the field is intrinsically as valuable, but people around here don't see it that way.

There was a great deal of resentment when a crisis such as a pulp spill pulled the control room operators "out into the process" with tall rubber boots and shovels. Operators called these kinds of tasks "nasty work," a designation that clearly demarcated such activities from their "real" work at the data interface and provided a sense of psychological distance from the role of the laboring body. The special label signaled their preferred self-conception as people who could keep their collars clean:

If there is nasty work here, it means someone made a mistake. We don't like to get nasty. They told us all we would do is run computers.

The resentments that surround physical exertion once defined the boundary between managers and workers but now were felt within the operating work force. Subtle conflicts erupted among operators as they struggled to balance the satisfactions of intellective virtuosity against the comforts of group solidarity. Several operators at Cedar Bluff told of incidents in which they had made a special effort to break new ground in their understanding of the theoretical and practical aspects of the relationships among the process variables under their control. In each case, their discoveries had made it possible to solve a chronic problem and to generate significant savings. These operators became the target for derisive chiding from their peers. One man told of spending several months studying a problem that concerned the operation of the boiler. He conducted experiments, ran tests, monitored data, and read theories. He finally concluded what was wrong with the way they had been controlling the boilers. He wrote up the material in the format of a proficiency exam, supplying all the answers to the questions with the material he had amassed.

I wanted to share it with the team, to learn it better by teaching them. They said, "I don't want to hear that shit!" They didn't even want to see the book.

Another told of a similar experience. His discovery was of such economic value that he was sent to divisional headquarters to present his findings to the engineering staff.

They had me talk to a VP about it, so I got recognized. In fact, it embarrassed me that I got so much recognition. I would have liked less because of the peer pressure I got. None of my peers would go along with it from the beginning. So I had to go to all the shift coordinators instead. My peers treated me like someone who thinks they know it all, a company man. They said I was getting too close to management. That was tough to work with on a day-to-day basis. I learned how to work it now-just play it low key.

One explanation for this behavior is that it repeats the game of self protection so well known to generations of industrial workers. Gloria Schuck has labeled it "intellective rate busting": any worker who excels at the data interface increases the demands on the whole group. 13 Another interpretation is that the barriers that separate managerial from operating jobs have been maintained but simply pushed down to a lower level of the organization. A gulf continues to exist; what has changed is the distribution of members on each side.

The reactions of these disgruntled operators does not take into account the possibility of a change in the logic of imperative control-a logic that sustains the necessity of managerial authority and perpetuates adversarial feeling. In the context of that logic, workers learn that they cannot learn, and managers learn that they must already know. Thus, the interests of each group remain divergent, even as they are locked in the fitful interdependence of imperative control.

In contrast, an informating strategy implies a discontinuity in the logic of imperative control. The new division of learning should organize experience in ways that help perpetuate belief in a synthesis of interests and thus legitimate a learning environment that presupposes relationships among equals. Only under such conditions can knowledge be shared in a way that strengthens the collective effort. Barriers between information-intensive and non-information-intensive responsibilities limit equality and inhibit the spirit of inquiry that animates the informated organization, thus inviting adversarial games of self protection and domination.

The problem appears to be that not every job in even the most informated organization is likely to require intellective skills. Certainly in the near term, most organizations will continue to have some tasks that either are physically demanding or involve highly routinized versions of information handling. How such jobs are distributed becomes a crucial question for any organization that would commit itself to a new division of labor. Does the fact of these persistent differences between tasks lead inevitably to a reproduction of the labor-management caste system?

It is important to reiterate here that the presupposition of relations of equality does not imply a correlative assumption that all organizational members are exactly alike. In any organization, there will be some individuals who reject the demands of intellective work. Not everyone will contain in equal measure the internal commitment, motivation, or cognitive style associated with the responsibilities of life at the data interface. Some will be repelled by the mental stress of such work and find it too perplexing or anxiety-inducing. Others will prefer to conduct their work lives in motion, drawing sustenance from using their bodies to accomplish their tasks. There were many such operators in the Piney Wood Mill. One expressed his anxiety about his place in the emerging organization when he asked:

It is important to reiterate here that the presupposition of relations of equality does not imply a correlative assumption that all organizational members are exactly alike. In any organization, there will be some individuals who reject the demands of intellective work. Not everyone will contain in equal measure the internal commitment, motivation, or cognitive style associated with the responsibilities of life at the data interface. Some will be repelled by the mental stress of such work and find it too perplexing or anxiety-inducing. Others will prefer to conduct their work lives in motion, drawing sustenance from using their bodies to accomplish their tasks. There were many such operators in the Piney Wood Mill. One expressed his anxiety about his place in the emerging organization when he asked:

There was a smaller but still significant subset of workers at Cedar Bluff who felt deeply uncomfortable with the work available to them in the control rooms, despite the importance and prestige attached to it

The work in the control room is mental. It isn't as hard as out in the process, but I would rather be out on the floor working. I am moving out there. I don't like to be cramped up with no room to breathe. If they padded this room, I would have my body indented in the walls from bouncing off them all the time.

While some degree of hierarchy is inevitable in any social group, the values and beliefs that animate these distinctions can operate very differently from the traditional assumptions of imperative control. In the informated organization, there is no reason why these individuals could not elect to align themselves with the jobs best suited to their sensibilities or talents. The difference here lies in the voluntary, nonarbitrary, and reversible nature of their decisions. Instead of facing rigid and practically irreversible designations that are reinforced by virtually every aspect of organizational experience, these individuals could be free to self-select into or out of the more abstract forms of work.

The freedom of self-selection can be maintained as long as institutional arrangements ensure the full participation of these members in the political life of the learning community. There are several mechanisms through which this could be accomplished. First, these individuals, like other members, would have broad access to the streams of circulating information within the organization. Were they to so choose, they could be rotated into positions that provided a greater opportunity for the development of their intellective potential. Second, the close coordination and integration of the various aspects of the production process would require these members to keep abreast of critical information from daily operational issues to changes in the business context, or a new direction in the strategic plan. Finally, the rewards available for their work would be commensurate with their value to the production process and not undervalued as a matter of 14 course.

THE INFORMATED ORGANIZATION AND RECENT TRENDS IN WORK ORGANIZATION

As the years passed at Cedar Bluff, managers began to confront more openly and honestly the challenges to their skills that had been unleashed by the informating process. It was not untypical for organizations pursuing high commitment strategies to feel the strains of participative management and to experience a good bit of conflict concerning the limits of managerial prerogatives. At Cedar Bluff, however, the demands on managers had become relentlessly insistent. The knowledge requirements of the data interface, the vulnerability of plant performance to variations in operator skill and motivation, and the broad accessibility of data had lent new urgency to questions about the skills, roles, and structures that should define the organization. This was vividly illustrated by the turmoil that surrounded the development of a new pay and promotion system for hourly workers. The operators had become increasingly proficient in operating the plant, and after several years of producing pulp at far below the equipment's true capacity, production levels began to climb. As operators' skills improved, so did their dissatisfaction with the pay and promotion system. They believed that it arbitrarily limited the amount of learning for which they could be rewarded. Amid mounting dissension, a committee consisting of operators and managers from the various areas of the plant was appointed to gather data on the problem and to recommend new policies. The plant had reached production levels of more than nine hundred tons a day, but as the committee went into session, operators were heard to say, "That's the last nine-hundred-ton day this plant will see until the pay and promotion problems are resolved."

In the months that followed, their predictions came true; the plant returned to earlier production levels. Cedar Bluff's managers were especially frustrated because they could not identify the causes of the downturn. Only rarely could a manager point to something that operators were doing incorrectly that might be contributing to poor production. They concluded that the disappointing performance could be attributed, not to what operators were doing, but to what they were not doing. The operators' errors were sins of omission-an underutilization of the data interface resulting from their refusal to notice, to think, to explore, to experiment, or to improve. In other words, the power of the new technology was going to waste. Managers felt helpless to alter the situation. It was only when the new pay and promotion system was finally developed and accepted by a majority vote that production levels began to climb once again.

There are several lessons to be learned here. First, the requirements of an informating strategy support existing work-improvement efforts, such as the high commitment approach to work force management, with its emphasis on self-managing teams, participation, and decentralization. Organizations that are already pursuing this approach are more likely to have developed both the ideological context and the social skills necessary to plan and implement an informating strategy. In this regard, Cedar Bluff provides an important contrast not only to Piney Wood but also to organizations, like Global Bank Brazil, that have minimal experience with work-system innovation. Second, the demands for a redistribution of knowledge and the consequent challenge to the managerial role that can be unleashed by the informating process are likely to exacerbate the growing pains associated with participative management and to accelerate the need for positive change. Third, organizational innovations designed to create high commitment work systems typically have focused upon the hourly work force. In most cases, and Cedar Bluff is one example, the managerial hierarchy has remained relatively intact, while team organization and pay-for-skill systems have been designed for the operational work force. In contrast, an informating strategy suggests the need for a more wholistic reconceptualization of the skills, roles, and structures that define the total organization. Partial change efforts, as at Tiger Creek, or technology-driven initiatives, as at Global Bank Brazil, are unlikely to result in the kind of learning environment necessary for an ongoing and robust approach to the informating process. Finally, managing in an informated environment is a delicate human process. The ability to use information for real business benefit is as much a function of the quality of commitment and relationships as it is a function of the quality of intellective skills.

The words of the clerk at Global Bank Brazil continue to echo: "Will things be any different now?" In response, we can say that the opportunity is there, and we now know more about what it will take. An informating strategy requires a comprehensive vision based upon an understanding of the unique capacities of intelligent technology and the opportunity to use the organization to liberate those capacities. It means forging a new logic of technological deployment based upon that vision. A coherent rationale will be necessary, particularly when the tide of conventional thinking and familiar assumptions on this subject can submerge many important choices regarding basic technological design and management. Cedar Bluff's plant manager foresaw this danger:

The technology is going in the direction that says one person operates the master controls. Is the technology right? We don't believe it is, and we are working hard to convince our vendors to leave the design flexible enough so that it does not preclude the uses we want to make of it.

The informated organization does move in another direction. It relies on the human capacities for teaching and learning, criticism and insight. It implies an approach to business improvement that rests upon the improvement and innovation made possible by the enhanced comprehensibility of core processes. It reflects a fertile interdependence between the human mind and some of its most sophisticated productions. As one worker from Tiger Creek mused:

If you don't let people grow and develop and make more decisions, it's a waste of human life-a waste of human potential. If you don't use your knowledge and skill, it's a waste of life. Using the technology to its full potential means using the man to his full potential.

APPENDIX A : THE SCOPE OF INFORMATION TECHNOLOGY IN THE MODERN WORKPLACE

Information technology is a label that reflects the convergence of several streams of technical developments, including microelectronics, computer science, telecommunications, software engineering, and system analysis. It is a technology that dramatically increases the ability to record, store, analyze, and transmit information in ways that permit flexibility, accuracy, immediacy, geographic independence, volume, and complexity. Information technology has a unique capability to restructure operations that depend upon information for the purposes of transaction, record keeping, analysis, control, or communication.

There is hardly a segment of the U.S. economy that has not been penetrated by some form of computer-based technology. The core of this technology is the silicon-integrated circuit, or "chip." The equivalent of hundreds of thousands of transistors can be built on a silicon chip measuring no more than a fraction of an inch. The astonishing reductions in the cost of these microprocessors, coupled with their equally impressive performance levels, have been exhaustively documented. During the past thirty years, the price per second of instruction has decreased dramatically: a computation that now costs one dollar would have cost about $30,000 in 1950. 1 Porter and Millar calculate that the cost of computer power relative to the cost of manual information processing is at least eight thousand times less than the cost thirty years ago. Between 1958 and 1980, the amount of time needed for one electronic operation fell by a factor of 80 million. They also cite Department of Defense studies that show that the error rate in recording data through bar coding is one in 3 million, compared to one error in three hundred manual data entries. 2 During the past fifteen years, the memory capacity of an integrated circuit has increased by a factor of one thousand, as has its reliability. As another writer remarked, "If the automotive industry had paralleled the advances that the computer industry has experienced in the last 25 years, a Rolls Royce would cost 50 cents and would deliver 1 5 million miles to the gallon.

Numerous studies by economists and industry analysts have concluded that computer-based information technologies will profoundly affect the structure of the U.S. economy. 4 One analyst estimates that in 1980, approximately 10 million Americans interacted daily with a video display terminal and that this number would increase to 2 5 million by 1 990. 5 Another estimates that by the year 1 990, 50 million American office workers will spend a significant portion of their workday interacting with a computer terminal of some sort. 6 Another expert predicts that in 1990, 65 percent of all professional, managerial, technical, and administrative workers (a group that now constitutes almost half the labor force) will depend upon computer-based workstations. 7 The Congressional Office of Technology Assessment has predicted that there will be at least one computer terminal for every two or three office workers by 1990. 8 One recent survey of 5 30 employees representing every organizational level in twenty-six companies offers some accurate, if more limited, findings. Respondents were divided according to occupational groups: executives, managers, professionals, technicians, secretaries, and clerks. Averaging across all categories, researchers found that 67 percent of all the respondents interacted directly with a computer during the regular course of their work and that 26 percent expected to do so in the near future. When executives (for whom the figures were 36 percent and 46 percent, respectively) are eliminated from this average, the figure for current usage jumps to 74 percent. Of these same respondents, about half reported using the computer during 30 percent or less of their working time, another quarter of the group reported spending up to 70 percent of their time at the terminal, while the final quarter spent up to 100 percent of their workday at the computer terminal. Such figures suggest the degree to which information technology is affecting everyday life across a broad spectrum of the work force.

THE SERVICE SECTOR

Information technology has different applications in the service and manufacturing sectors of the economy. In the service sector, the technology has been used primarily to meet the mushrooming demands of handling information. Businesses now are faced with the task of controlling 400 billion documents, a number that is expected to increase at the rate of 72 billion per year. 10 As the technology develops, clerks working with documents in the back office can enter data directly into terminals linked to a computer mainframe, which does the actual processing. In many cases, even the documents have disappeared; clerks are able to perform all their transactions through the electronic medium. Such on-line applications began to be widespread in the early 1980s and have become a prominent trend in most large service organizations.

The early 1980s also saw new applications of information technology for professionals, managers, and technical workers. The introduction of small, stand-alone word processors, microcomputers, and personal computers made it easier for nonspecialists to use databases, to manipulate text and quantitative data, to generate tables and graphic displays, to utilize analytical software, and to communicate with one another through a computer network. Today, microcomputers are being linked to one another as well as to central computers so that they can be used independently or in conjunction with corporate data bases. These emerging systems also can be linked to external data bases and communications networks; further, they can cross organizational and national boundaries.

Most large information-processing organizations, such as banks and insurance companies, still require a sizeable clerical work force to enter data and perform routine transactions on a computer system. Many such organizations are searching for ways to circumvent this massive clerical effort and at the same time increase the continuity of their operations. An increasingly popular solution is to incorporate the data-entry function in operations that are external to the organization. For example, when a customer uses an automated teller machine, the data-entry function is accomplished automatically without clerical input. A hospital computer may send bills directly to the health insurer's computer, which in turn instructs the bank's computer to transfer funds. An interorganizational computer system thus can eliminate routine clerical work in several organizations.

Technologies that were once relatively distinct now have begun to converge. The functions once accomplished by typewriters, printing presses, copying machines, telephones, files, calculators, and mail sorting systems, are becoming either subsumed within or linked to the functioning of the comprehensive computer-based network. Software is more powerful and easier to use. New procedures allow people to interact with the computer in ways that encourage more familiarity and immediacy, such as touch-sensitive screens and voice-activated II programs.

THE MANUFACTURING SECTOR

Applications of computer technology in the manufacturing sector have developed along distinct lines in discrete parts manufacturing (for example, automobiles, farm equipment, electronics) and in the continuous-process industries (for example, oil refineries, chemical processors, food and beverages, paper and pulp), which each face different problems in manufacturing process. Continuous-process production typically involves a flow of material that can pass through several stages before emerging in its final form. The production process manipulates the composition of materials by chemical reaction, purification, and blending of component materials. At each stage, different operations are performed, such as heating, cooling, mixing, chemical reaction, distillation, drying, or pressurization. This requires continual measurement and control of variables like time, temperature, raw material characteristics, steam pressure, chemical levels, densities, viscosities, liquid levels, flow rates, et cetera. Twenty percent of all industrial computers, and 40 percent or more of all minicomputers and microcomputers, are accounted for by continuous-process applications designed for monitoring, analysis, control, and optimization. Process computers evolved from simple data recording devices: they collected real-time operational data and set off alarms under critical conditions. The next phase of development produced "open loop" systems, in which operators rely on either first-hand observations of the process or data generated by microprocessor-based sensors and programmable logic controllers built into operating equipment. With this information, they can use the computer system for mathematical analysis to help them adjust process conditions. The computer also becomes the medium through which operators can manipulate process variables and parameters to meet the desired conditions. Most recent developments include supervisory process-control systems that can be programmed to receive information directly from instruments monitoring the process, to set control points, to perform computations, and to adjust control variables to continually approximate optimum levels of functioning. Theoretically, such systems mimimize the need for operator involvement, except in upset conditions.

As plants apply these control systems to their operations, the amount of physical interaction with the production process is reduced and workers typically operate from remote control rooms, where they monitor video terminals that display data reflecting the state of the production process. In some process industries, such as oil refining or steel production, where the various steps of the conversion from raw material to output are well understood, there has been considerable progress in developing supervisory control systems. These were among the first industries to comprehensively apply microprocessor-based control technology. In other process industries, such as paper and pulp or food and beverages, the manufacturing process has not yet been entirely explicated, and there are no adequate sensing devices to measure all key variables. These industries have been slower to adopt the technology and face more difficulty in programming supervisory control.

In discrete parts manufacturing, the problems are more geometric in nature, involving the placement of parts and equipment in relation to one another, as well as their movement from one stage of assembly to another. Typical applications of information technology in discrete parts manufacturing include computer-aided design (drafting and engineering), computer-aided manufacturing (robots and numerically controlled machine tools), flexible manufacturing systems, automated material handling, and automated storage-and-retrieval systems. Computer-aided design systems facilitate the use of previous designs and allow more rapid design changes. They can improve the design process by allowing engineers to try out a dozen or a hundred different variations, when they previously might have been limited to building three or four prototype models. It is estimated that in 1983, there were thirty-two thousand computer-aided design workstations being used in the United States.

Robots are mechanical manipulators that can be programmed to move workpieces or tools along a prescribed path. While most robots today can perform only relatively well-defined and repetitive tasks, efforts are under way to incorporate more intelligence and sensory capacities within these machines. There are many differing estimates as to the extent of robotization in manufacturing. The Robot Institute of America indicates that in 1983, 66 percent (31,900) of the world's robots were operating in Japan and 1 3 percent ( 6,301) were in the United States. 14 Nobel-prize-winning economist Wassily Leontief has predicted a 30 to 40 percent annual growth rate in the market for industrial robots between 198 5 and 1990.

Computer-numerically-controlled machine tools fashion metal according to programmed instructions that indicate the desired dimensions of a part and the sequence to be followed in the machining process. Since the late 1 970s, these machines have been equipped with microprocessors or dedicated minicomputers and frequently include a screen and keyboard for writing or editing the programs that guide the equipment. These devices are the basis for "direct numerical control machines," in which a larger mini-computer or mainframe computer is used to program and run more than one numerically controlled tool simultaneously. The Congressional Office of Technology Assessment reports that as the price of small computers has declined, these machine tools are being equipped with microcomputers that can be linked to one another and to a central controlling computer, thus creating a hierarchy of computer control. 16 Many observers have remarked upon the relatively slow diffusion of these applications due to their high capital cost and the bottlenecks in developing and maintaining software programs. In 1983, numerically controlled machine tools represented only about 5 percent of the machine tools in U.S. metalworking, but this may change rapidly. 17 For example, General Motors Corporation has stated that by the end of the 1980s, 90 percent of all new capital investments will be in computer numerically controlled machines.

Flexible manufacturing systems integrate these more discrete applications of technology. They consist of computer-controlled machining centers that sculpt complicated metal parts at high speed and with great reliability, robots that handle the parts, and remotely guided carts that deliver materials. These components are linked by computer-based controls that dictate what will happen at each stage of the manufacturing sequence. Many consider the great advantage of these systems to be their ability to achieve low-cost production in small volumes, without having to rely on the economics of scale associated with mass production. "A flexible automation system can turn out a small batch or even a single copy of a product as efficiently as a production line designed to turn out a million identical items." 19 These systems are complex and costly, and thus still relatively rare. Reliable statistics are difficult to obtain because of conflicting definitions of precisely what level of integration and control constitutes "flexibility." However, Fortune magazine recently counted thirty such systems operating in the United States and considerably more in Japan. According to their report, one Japanese firm alone, Toyoda Machine Tool Co., has thirty such systems in operation. 20 Despite the still-modest number of such systems in the United States, Fortune estimates that by 1990, the sales of equipment to support flexible manufacturing-robots, computer controls, material-handling devices, et cetera-will rise to $30 billion annually from 1982 levels of $4 billion.

While information technologies in these two manufacturing domains- discrete parts and continuous-process-have developed separately in order to address distinct types of problems, their differences are increasingly diminished by new developments that use computer systems for comprehensive production management. This approach, known as computer-integrated manufacturing, increases the continuity of the production process in discrete parts manufacturing, thus bringing it closer to the "optimal model" of manufacturing in which continuity and controllability are maximized. 21 Under these conditions, workers' tasks begin to look very similar to those of operators in the continuous process environment, as they come to emphasize monitoring and control (though the types of variables and procedures continue to differ). A recent study published by the National Academy of Sciences points to the integration of computer-aided design and manufacturing with manufacturing resource planning (software that translates demand for products into the parts needed to produce them and then orders the parts from inventory or from suppliers) and computer-aided process planning (software that routes parts through the factory to maximize operating time and to eliminate bottlenecks). "The four technologies are increasingly 'speaking' to each other through local-area networks, and formerly isolated applications are being linked as computer integrated manufacturing. 

Experts continue to disagree on the ultimate consequences of these developments for employment in the manufacturing sector. Some believe that computer-integrated manufacturing will provide the basis for nearly unmanned factories; others insist that automation rarely can be complete and that people will be needed to monitor, control, maintain, manage, and plan these processes, although at lower levels of employment than have characterized older technologies. Whether the unmanned factory is a likely scenario in some cases or not, the coming decade will continue to see manufacturing operations that depend upon people, not just computers. These organizations will become more alike as they are able to increase the continuity and controllability of production through computer integration.


/ 015. Bostrom, Nick. Superintelligence: Paths, Dangers, Strategies, 2014.


PREFACE

Inside your cranium is the thing that does the reading. This thing, the human brain, has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that we owe our dominant position on the planet. Other animals have stronger muscles and sharper claws, but we have cleverer brains. Our modest advantage in general intelligence has led us to develop language, technology, and complex social organization. The advantage has compounded over time, as each generation has built on the achievements of its predecessors.

If some day we build machine brains that surpass human brains in general intelligence, then this new superintelligence could become very powerful. And, as the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species would depend on the actions of the machine superintelligence.

We do have one advantage: we get to build the stuff. In principle, we could build a kind of superintelligence that would protect human values. We would certainly have strong reason to do so. In practice, the control problem—the problem of how to control what the superintelligence would do—looks quite difficult. It also looks like we will only get one chance. Once unfriendly superintelligence exists, it would prevent us from replacing it or changing its preferences. Our fate would be sealed.

In this book, I try to understand the challenge presented by the prospect of superintelligence, and how we might best respond. This is quite possibly the most important and most daunting challenge humanity has ever faced. And— whether we succeed or fail—it is probably the last challenge we will ever face.

It is no part of the argument in this book that we are on the threshold of a big breakthrough in artificial intelligence, or that we can predict with any precision when such a development might occur. It seems somewhat likely that it will happen sometime in this century, but we don’t know for sure. The first couple of chapters do discuss possible pathways and say something about the question of timing. The bulk of the book, however, is about what happens after. We study the kinetics of an intelligence explosion, the forms and powers of superintelligence, and the strategic choices available to a superintelligent agent that attains a decisive advantage. We then shift our focus to the control problem and ask what we could do to shape the initial conditions so as to achieve a survivable and beneficial outcome. Toward the end of the book, we zoom out and contemplate the larger picture that emerges from our investigations. Some suggestions are offered on what ought to be done now to increase our chances of avoiding an existential catastrophe later.

This has not been an easy book to write. I hope the path that has been cleared will enable other investigators to reach the new frontier more swiftly and conveniently, so that they can arrive there fresh and ready to join the work to further expand the reach of our comprehension. (And if the way that has been made is a little bumpy and bendy, I hope that reviewers, in judging the result, will not underestimate the hostility of the terrain ex ante!)

This has not been an easy book to write: I have tried to make it an easy book to read, but I don’t think I have quite succeeded. When writing, I had in mind as the target audience an earlier time-slice of myself, and I tried to produce a kind of book that I would have enjoyed reading. This could prove a narrow demographic. Nevertheless, I think that the content should be accessible to many people, if they put some thought into it and resist the temptation to instantaneously misunderstand each new idea by assimilating it with the most similar-sounding cliché available in their cultural larders. Non-technical readers should not be discouraged by the occasional bit of mathematics or specialized vocabulary, for it is always possible to glean the main point from the surrounding explanations. (Conversely, for those readers who want more of the nitty-gritty, there is quite a lot to be found among the endnotes.1)

Many of the points made in this book are probably wrong.2 It is also likely that there are considerations of critical importance that I fail to take into account, thereby invalidating some or all of my conclusions. I have gone to some length to indicate nuances and degrees of uncertainty throughout the text— encumbering it with an unsightly smudge of “possibly,” “might,” “may,” “could well,” “it seems,” “probably,” “very likely,” “almost certainly.” Each qualifier has been placed where it is carefully and deliberately. Yet these topical applications of epistemic modesty are not enough; they must be supplemented here by a systemic admission of uncertainty and fallibility. This is not false modesty: for while I believe that my book is likely to be seriously wrong and misleading, I think that the alternative views that have been presented in the literature are substantially worse—including the default view, or “null hypothesis,” according to which we can for the time being safely or reasonably ignore the prospect of superintelligence.

CHAPTER 1: Past developments and present capabilities

We begin by looking back. History, at the largest scale, seems to exhibit a sequence of distinct growth modes, each much more rapid than its predecessor. This pattern has been taken to suggest that another (even faster) growth mode might be possible. However, we do not place much weight on this observation—this is not a book about “technological acceleration” or “exponential growth” or the miscellaneous notions sometimes gathered under the rubric of “the singularity.” Next, we review the history of artificial intelligence. We then survey the field’s current capabilities. Finally, we glance at some recent expert opinion surveys, and contemplate our ignorance about the timeline of future advances.

Growth modes and big history

A mere few million years ago our ancestors were still swinging from the branches in the African canopy. On a geological or even evolutionary timescale, the rise of Homo sapiens from our last common ancestor with the great apes happened swiftly. We developed upright posture, opposable thumbs, and— crucially—some relatively minor changes in brain size and neurological organization that led to a great leap in cognitive ability. As a consequence, humans can think abstractly, communicate complex thoughts, and culturally accumulate information over the generations far better than any other species on the planet.

These capabilities let humans develop increasingly efficient productive technologies, making it possible for our ancestors to migrate far away from the rainforest and the savanna. Especially after the adoption of agriculture, population densities rose along with the total size of the human population. More people meant more ideas; greater densities meant that ideas could spread more readily and that some individuals could devote themselves to developing specialized skills. These developments increased the rate of growth of economic productivity and technological capacity. Later developments, related to the Industrial Revolution, brought about a second, comparable step change in the rate of growth.

Such changes in the rate of growth have important consequences. A few hundred thousand years ago, in early human (or hominid) prehistory, growth was so slow that it took on the order of one million years for human productive capacity to increase sufficiently to sustain an additional one million individuals living at subsistence level. By 5000 BC, following the Agricultural Revolution, the rate of growth had increased to the point where the same amount of growth took just two centuries. Today, following the Industrial Revolution, the world economy grows on average by that amount every ninety minutes.1

Even the present rate of growth will produce impressive results if maintained for a moderately long time. If the world economy continues to grow at the same pace as it has over the past fifty years, then the world will be some 4.8 times richer by 2050 and about 34 times richer by 2100 than it is today.

Yet the prospect of continuing on a steady exponential growth path pales in comparison to what would happen if the world were to experience another step change in the rate of growth comparable in magnitude to those associated with the Agricultural Revolution and the Industrial Revolution. The economist Robin Hanson estimates, based on historical economic and population data, a characteristic world economy doubling time for Pleistocene hunter–gatherer society of 224,000 years; for farming society, 909 years; and for industrial society, 6.3 years.3 (In Hanson’s model, the present epoch is a mixture of the farming and the industrial growth modes—the world economy as a whole is not yet growing at the 6.3-year doubling rate.) If another such transition to a different growth mode were to occur, and it were of similar magnitude to the previous two, it would result in a new growth regime in which the world economy would double in size about every two weeks.

Such a growth rate seems fantastic by current lights. Observers in earlier epochs might have found it equally preposterous to suppose that the world economy would one day be doubling several times within a single lifespan. Yet that is the extraordinary condition we now take to be ordinary.

The idea of a coming technological singularity has by now been widely popularized, starting with Vernor Vinge’s seminal essay and continuing with the writings of Ray Kurzweil and others.4 The term “singularity,” however, has been used confusedly in many disparate senses and has accreted an unholy (yet almost millenarian) aura of techno-utopian connotations.5 Since most of these meanings and connotations are irrelevant to our argument, we can gain clarity by dispensing with the “singularity” word in favor of more precise terminology.

The singularity-related idea that interests us here is the possibility of an intelligence explosion, particularly the prospect of machine superintelligence. There may be those who are persuaded by growth diagrams like the ones in Figure 1 that another drastic change in growth mode is in the cards, comparable to the Agricultural or Industrial Revolution. These folk may then reflect that it is hard to conceive of a scenario in which the world economy’s doubling time shortens to mere weeks that does not involve the creation of minds that are much faster and more efficient than the familiar biological kind. However, the case for taking seriously the prospect of a machine intelligence revolution need not rely on curve-fitting exercises or extrapolations from past economic growth. As we shall see, there are stronger reasons for taking heed.

Great expectations

Machines matching humans in general intelligence—that is, possessing common sense and an effective ability to learn, reason, and plan to meet complex information-processing challenges across a wide range of natural and abstract domains—have been expected since the invention of computers in the 1940s. At that time, the advent of such machines was often placed some twenty years into the future.7 Since then, the expected arrival date has been receding at a rate of one year per year; so that today, futurists who concern themselves with the possibility of artificial general intelligence still often believe that intelligent machines are a couple of decades away

Two decades is a sweet spot for prognosticators of radical change: near enough to be attention-grabbing and relevant, yet far enough to make it possible to suppose that a string of breakthroughs, currently only vaguely imaginable, might by then have occurred. Contrast this with shorter timescales: most technologies that will have a big impact on the world in five or ten years from now are already in limited use, while technologies that will reshape the world in less than fifteen years probably exist as laboratory prototypes. Twenty years may also be close to the typical duration remaining of a forecaster’s career, bounding the reputational risk of a bold prediction.

From the fact that some individuals have overpredicted artificial intelligence in the past, however, it does not follow that AI is impossible or will never be developed.9 The main reason why progress has been slower than expected is that the technical difficulties of constructing intelligent machines have proved greater than the pioneers foresaw. But this leaves open just how great those difficulties are and how far we now are from overcoming them. Sometimes a problem that initially looks hopelessly complicated turns out to have a surprisingly simple solution (though the reverse is probably more common).

In the next chapter, we will look at different paths that may lead to humanlevel machine intelligence. But let us note at the outset that however many stops there are between here and human-level machine intelligence, the latter is not the final destination. The next stop, just a short distance farther along the tracks, is superhuman-level machine intelligence. The train might not pause or even decelerate at Humanville Station. It is likely to swoosh right by.

The mathematician I. J. Good, who had served as chief statistician in Alan Turing’s code-breaking team in World War II, might have been the first to enunciate the essential aspects of this scenario. In an oft-quoted passage from 1965, he wrote:

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.

It may seem obvious now that major existential risks would be associated with such an intelligence explosion, and that the prospect should therefore be examined with the utmost seriousness even if it were known (which it is not) to have but a moderately small probability of coming to pass. The pioneers of artificial intelligence, however, notwithstanding their belief in the imminence of human-level AI, mostly did not contemplate the possibility of greater-thanhuman AI. It is as though their speculation muscle had so exhausted itself in conceiving the radical possibility of machines reaching human intelligence that it could not grasp the corollary—that machines would subsequently become superintelligent.

The AI pioneers for the most part did not countenance the possibility that their enterprise might involve risk.11 They gave no lip service—let alone serious thought—to any safety concern or ethical qualm related to the creation of artificial minds and potential computer overlords: a lacuna that astonishes even against the background of the era’s not-so-impressive standards of critical technology assessment.12 We must hope that by the time the enterprise eventually does become feasible, we will have gained not only the technological proficiency to set off an intelligence explosion but also the higher level of mastery that may be necessary to make the detonation survivable.

But before we turn to what lies ahead, it will be useful to take a quick glance at the history of machine intelligence to date.

Seasons of hope and despair

In the summer of 1956 at Dartmouth College, ten scientists sharing an interest in neural nets, automata theory, and the study of intelligence convened for a sixweek workshop. This Dartmouth Summer Project is often regarded as the cockcrow of artificial intelligence as a field of research. Many of the participants would later be recognized as founding figures. The optimistic outlook among the delegates is reflected in the proposal submitted to the Rockefeller Foundation, which provided funding for the event:

We propose that a 2 month, 10 man study of artificial intelligence be carried out…. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines that use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.

In the six decades since this brash beginning, the field of artificial intelligence has been through periods of hype and high expectations alternating with periods of setback and disappointment.

The first period of excitement, which began with the Dartmouth meeting, was later described by John McCarthy (the event’s main organizer) as the “Look, Ma, no hands!” era. During these early days, researchers built systems designed to refute claims of the form “No machine could ever do X!” Such skeptical claims were common at the time. To counter them, the AI researchers created small systems that achieved X in a “microworld” (a well-defined, limited domain that enabled a pared-down version of the performance to be demonstrated), thus providing a proof of concept and showing that X could, in principle, be done by machine. One such early system, the Logic Theorist, was able to prove most of the theorems in the second chapter of Whitehead and Russell’s Principia Mathematica, and even came up with one proof that was much more elegant than the original, thereby debunking the notion that machines could “only think numerically” and showing that machines were also able to do deduction and to invent logical proofs.13 A follow-up program, the General Problem Solver, could in principle solve a wide range of formally specified problems.14 Programs that could solve calculus problems typical of first-year college courses, visual analogy problems of the type that appear in some IQ tests, and simple verbal algebra problems were also written.15 The Shakey robot (so named because of its tendency to tremble during operation) demonstrated how logical reasoning could be integrated with perception and used to plan and control physical activity.16 The ELIZA program showed how a computer could impersonate a Rogerian psychotherapist.17 In the mid-seventies, the program SHRDLU showed how a simulated robotic arm in a simulated world of geometric blocks could follow instructions and answer questions in English that were typed in by a user.18 In later decades, systems would be created that demonstrated that machines could compose music in the style of various classical composers, outperform junior doctors in certain clinical diagnostic tasks, drive cars autonomously, and make patentable inventions.19 There has even been an AI that cracked original jokes.20 (Not that its level of humor was high—“What do you get when you cross an optic with a mental object? An eye-dea”—but children reportedly found its puns consistently entertaining.)

The methods that produced successes in the early demonstration systems often proved difficult to extend to a wider variety of problems or to harder problem instances. One reason for this is the “combinatorial explosion” of possibilities that must be explored by methods that rely on something like exhaustive search. Such methods work well for simple instances of a problem, but fail when things get a bit more complicated. For instance, to prove a theorem that has a 5-line long proof in a deduction system with one inference rule and 5 axioms, one could simply enumerate the 3,125 possible combinations and check each one to see if it delivers the intended conclusion. Exhaustive search would also work for 6-and 7-line proofs. But as the task becomes more difficult, the method of exhaustive search soon runs into trouble. Proving a theorem with a 50-line proof does not take ten times longer than proving a theorem that has a 5-line proof: rather, if one uses exhaustive search, it requires combing through 550 ≈ 8.9 × 1034 possible sequences—which is computationally infeasible even with the fastest supercomputers.

To overcome the combinatorial explosion, one needs algorithms that exploit structure in the target domain and take advantage of prior knowledge by using heuristic search, planning, and flexible abstract representations—capabilities that were poorly developed in the early AI systems. The performance of these early systems also suffered because of poor methods for handling uncertainty, reliance on brittle and ungrounded symbolic representations, data scarcity, and severe hardware limitations on memory capacity and processor speed. By the mid- 1970s, there was a growing awareness of these problems. The realization that many AI projects could never make good on their initial promises led to the onset of the first “AI winter”: a period of retrenchment, during which funding decreased and skepticism increased, and AI fell out of fashion.

A new springtime arrived in the early 1980s, when Japan launched its Fifth- Generation Computer Systems Project, a well-funded public–private partnership that aimed to leapfrog the state of the art by developing a massively parallel computing architecture that would serve as a platform for artificial intelligence. This occurred at peak fascination with the Japanese “post-war economic miracle,” a period when Western government and business leaders anxiously sought to divine the formula behind Japan’s economic success in hope of replicating the magic at home. When Japan decided to invest big in AI, several other countries followed suit.

The ensuing years saw a great proliferation of expert systems. Designed as support tools for decision makers, expert systems were rule-based programs that made simple inferences from a knowledge base of facts, which had been elicited from human domain experts and painstakingly hand-coded in a formal language. Hundreds of these expert systems were built. However, the smaller systems provided little benefit, and the larger ones proved expensive to develop, validate, and keep updated, and were generally cumbersome to use. It was impractical to acquire a standalone computer just for the sake of running one program. By the late 1980s, this growth season, too, had run its course.

The Fifth-Generation Project failed to meet its objectives, as did its counterparts in the United States and Europe. A second AI winter descended. At this point, a critic could justifiably bemoan “the history of artificial intelligence research to date, consisting always of very limited success in particular areas, followed immediately by failure to reach the broader goals at which these initial successes seem at first to hint.”21 Private investors began to shun any venture carrying the brand of “artificial intelligence.” Even among academics and their funders, “AI” became an unwanted epithet.

Technical work continued apace, however, and by the 1990s, the second AI winter gradually thawed. Optimism was rekindled by the introduction of new techniques, which seemed to offer alternatives to the traditional logicist paradigm (often referred to as “Good Old-Fashioned Artificial Intelligence,” or “GOFAI” for short), which had focused on high-level symbol manipulation and which had reached its apogee in the expert systems of the 1980s. The newly popular techniques, which included neural networks and genetic algorithms, promised to overcome some of the shortcomings of the GOFAI approach, in particular the “brittleness” that characterized classical AI programs (which typically produced complete nonsense if the programmers made even a single slightly erroneous assumption). The new techniques boasted a more organic performance. For example, neural networks exhibited the property of “graceful degradation”: a small amount of damage to a neural network typically resulted in a small degradation of its performance, rather than a total crash. Even more importantly, neural networks could learn from experience, finding natural ways of generalizing from examples and finding hidden statistical patterns in their input.23 This made the nets good at pattern recognition and classification problems. For example, by training a neural network on a data set of sonar signals, it could be taught to distinguish the acoustic profiles of submarines, mines, and sea life with better accuracy than human experts—and this could be done without anybody first having to figure out in advance exactly how the categories were to be defined or how different features were to be weighted.

While simple neural network models had been known since the late 1950s, the field enjoyed a renaissance after the introduction of the backpropagation algorithm, which made it possible to train multilayered neural networks.24 Such multilayered networks, which have one or more intermediary (“hidden”) layers of neurons between the input and output layers, can learn a much wider range of functions than their simpler predecessors.25 Combined with the increasingly powerful computers that were becoming available, these algorithmic improvements enabled engineers to build neural networks that were good enough to be practically useful in many applications.

The brain-like qualities of neural networks contrasted favorably with the rigidly logic-chopping but brittle performance of traditional rule-based GOFAI systems—enough so to inspire a new “-ism,” connectionism, which emphasized the importance of massively parallel sub-symbolic processing. More than 150,000 academic papers have since been published on artificial neural networks, and they continue to be an important approach in machine learning.

Evolution-based methods, such as genetic algorithms and genetic programming, constitute another approach whose emergence helped end the second AI winter. It made perhaps a smaller academic impact than neural nets but was widely popularized. In evolutionary models, a population of candidate solutions (which can be data structures or programs) is maintained, and new candidate solutions are generated randomly by mutating or recombining variants in the existing population. Periodically, the population is pruned by applying a selection criterion (a fitness function) that allows only the better candidates to survive into the next generation. Iterated over thousands of generations, the average quality of the solutions in the candidate pool gradually increases. When it works, this kind of algorithm can produce efficient solutions to a very wide range of problems—solutions that may be strikingly novel and unintuitive, often looking more like natural structures than anything that a human engineer would design. And in principle, this can happen without much need for human input beyond the initial specification of the fitness function, which is often very simple. In practice, however, getting evolutionary methods to work well requires skill and ingenuity, particularly in devising a good representational format. Without an efficient way to encode candidate solutions (a genetic language that matches latent structure in the target domain), evolutionary search tends to meander endlessly in a vast search space or get stuck at a local optimum. Even if a good representational format is found, evolution is computationally demanding and is often defeated by the combinatorial explosion.

Neural networks and genetic algorithms are examples of methods that stimulated excitement in the 1990s by appearing to offer alternatives to the stagnating GOFAI paradigm. But the intention here is not to sing the praises of these two methods or to elevate them above the many other techniques in machine learning. In fact, one of the major theoretical developments of the past twenty years has been a clearer realization of how superficially disparate techniques can be understood as special cases within a common mathematical framework. For example, many types of artificial neural network can be viewed as classifiers that perform a particular kind of statistical calculation (maximum likelihood estimation).26 This perspective allows neural nets to be compared with a larger class of algorithms for learning classifiers from examples —“decision trees,” “logistic regression models,” “support vector machines,” “naive Bayes,” “k-nearest-neighbors regression,” among others.27 In a similar manner, genetic algorithms can be viewed as performing stochastic hillclimbing, which is again a subset of a wider class of algorithms for optimization. Each of these algorithms for building classifiers or for searching a solution space has its own profile of strengths and weaknesses which can be studied mathematically. Algorithms differ in their processor time and memory space requirements, which inductive biases they presuppose, the ease with which externally produced content can be incorporated, and how transparent their inner workings are to a human analyst.

Behind the razzle-dazzle of machine learning and creative problem-solving thus lies a set of mathematically well-specified tradeoffs. The ideal is that of the perfect Bayesian agent, one that makes probabilistically optimal use of available information. This ideal is unattainable because it is too computationally demanding to be implemented in any physical computer (see Box 1). Accordingly, one can view artificial intelligence as a quest to find shortcuts: ways of tractably approximating the Bayesian ideal by sacrificing some optimality or generality while preserving enough to get high performance in the actual domains of interest.

A reflection of this picture can be seen in the work done over the past couple of decades on probabilistic graphical models, such as Bayesian networks. Bayesian networks provide a concise way of representing probabilistic and conditional independence relations that hold in some particular domain. (Exploiting such independence relations is essential for overcoming the combinatorial explosion, which is as much of a problem for probabilistic inference as it is for logical deduction.) They also provide important insight into the concept of causality.

One advantage of relating learning problems from specific domains to the general problem of Bayesian inference is that new algorithms that make Bayesian inference more efficient will then yield immediate improvements across many different areas. Advances in Monte Carlo approximation techniques, for example, are directly applied in computer vision, robotics, and computational genetics. Another advantage is that it lets researchers from different disciplines more easily pool their findings. Graphical models and Bayesian statistics have become a shared focus of research in many fields, including machine learning, statistical physics, bioinformatics, combinatorial optimization, and communication theory.35 A fair amount of the recent progress in machine learning has resulted from incorporating formal results originally derived in other academic fields. (Machine learning applications have also benefitted enormously from faster computers and greater availability of large data sets.)

State of the art

Artificial intelligence already outperforms human intelligence in many domains. Table 1 surveys the state of game-playing computers, showing that AIs now beat human champions in a wide range of games.

These achievements might not seem impressive today. But this is because our standards for what is impressive keep adapting to the advances being made. Expert chess playing, for example, was once thought to epitomize human intellection. In the view of several experts in the late fifties: “If one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavor.”55 This no longer seems so. One sympathizes with John McCarthy, who lamented: “As soon as it works, no one calls it AI anymore.”

There is an important sense, however, in which chess-playing AI turned out to be a lesser triumph than many imagined it would be. It was once supposed, perhaps not unreasonably, that in order for a computer to play chess at grandmaster level, it would have to be endowed with a high degree of general intelligence.57 One might have thought, for example, that great chess playing requires being able to learn abstract concepts, think cleverly about strategy, compose flexible plans, make a wide range of ingenious logical deductions, and maybe even model one’s opponent’s thinking. Not so. It turned out to be possible to build a perfectly fine chess engine around a special-purpose algorithm.58 When implemented on the fast processors that became available towards the end of the twentieth century, it produces very strong play. But an AI built like that is narrow. It plays chess; it can do no other.

In other domains, solutions have turned out to be more complicated than initially expected, and progress slower. The computer scientist Donald Knuth was struck that “AI has by now succeeded in doing essentially everything that requires ‘thinking’ but has failed to do most of what people and animals do ‘without thinking’—that, somehow, is much harder!”60 Analyzing visual scenes, recognizing objects, or controlling a robot’s behavior as it interacts with a natural environment has proved challenging. Nevertheless, a fair amount of progress has been made and continues to be made, aided by steady improvements in hardware.

Common sense and natural language understanding have also turned out to be difficult. It is now often thought that achieving a fully human-level performance on these tasks is an “AI-complete” problem, meaning that the difficulty of solving these problems is essentially equivalent to the difficulty of building generally human-level intelligent machines.61 In other words, if somebody were to succeed in creating an AI that could understand natural language as well as a human adult, they would in all likelihood also either already have succeeded in creating an AI that could do everything else that human intelligence can do, or they would be but a very short step from such a general capability.

Chess-playing expertise turned out to be achievable by means of a surprisingly simple algorithm. It is tempting to speculate that other capabilities —such as general reasoning ability, or some key ability involved in programming—might likewise be achievable through some surprisingly simple algorithm. The fact that the best performance at one time is attained through a complicated mechanism does not mean that no simple mechanism could do the job as well or better. It might simply be that nobody has yet found the simpler alternative. The Ptolemaic system (with the Earth in the center, orbited by the Sun, the Moon, planets, and stars) represented the state of the art in astronomy for over a thousand years, and its predictive accuracy was improved over the centuries by progressively complicating the model: adding epicycles upon epicycles to the postulated celestial motions. Then the entire system was overthrown by the heliocentric theory of Copernicus, which was simpler and— though only after further elaboration by Kepler—more predictively accurate.

Artificial intelligence methods are now used in more areas than it would make sense to review here, but mentioning a sampling of them will give an idea of the breadth of applications. Aside from the game AIs listed in Table 1, there are hearing aids with algorithms that filter out ambient noise; route-finders that display maps and offer navigation advice to drivers; recommender systems that suggest books and music albums based on a user’s previous purchases and ratings; and medical decision support systems that help doctors diagnose breast cancer, recommend treatment plans, and aid in the interpretation of electrocardiograms. There are robotic pets and cleaning robots, lawn-mowing robots, rescue robots, surgical robots, and over a million industrial robots.64 The world population of robots exceeds 10 million.

Modern speech recognition, based on statistical techniques such as hidden Markov models, has become sufficiently accurate for practical use (some fragments of this book were drafted with the help of a speech recognition program). Personal digital assistants, such as Apple’s Siri, respond to spoken commands and can answer simple questions and execute commands. Optical character recognition of handwritten and typewritten text is routinely used in applications such as mail sorting and digitization of old documents.

Machine translation remains imperfect but is good enough for many applications. Early systems used the GOFAI approach of hand-coded grammars that had to be developed by skilled linguists from the ground up for each language. Newer systems use statistical machine learning techniques that automatically build statistical models from observed usage patterns. The machine infers the parameters for these models by analyzing bilingual corpora. This approach dispenses with linguists: the programmers building these systems need not even speak the languages they are working with.

Face recognition has improved sufficiently in recent years that it is now used at automated border crossings in Europe and Australia. The US Department of State operates a face recognition system with over 75 million photographs for visa processing. Surveillance systems employ increasingly sophisticated AI and data-mining technologies to analyze voice, video, or text, large quantities of which are trawled from the world’s electronic communications media and stored in giant data centers.

Theorem-proving and equation-solving are by now so well established that they are hardly regarded as AI anymore. Equation solvers are included in scientific computing programs such as Mathematica. Formal verification methods, including automated theorem provers, are routinely used by chip manufacturers to verify the behavior of circuit designs prior to production.

The US military and intelligence establishments have been leading the way to the large-scale deployment of bomb-disposing robots, surveillance and attack drones, and other unmanned vehicles. These still depend mainly on remote control by human operators, but work is underway to extend their autonomous capabilities.

Intelligent scheduling is a major area of success. The DART tool for automated logistics planning and scheduling was used in Operation Desert Storm in 1991 to such effect that DARPA (the Defense Advanced Research Projects Agency in the United States) claims that this single application more than paid back their thirty-year investment in AI.68 Airline reservation systems use sophisticated scheduling and pricing systems. Businesses make wide use of AI techniques in inventory control systems. They also use automatic telephone reservation systems and helplines connected to speech recognition software to usher their hapless customers through labyrinths of interlocking menu options.

AI technologies underlie many Internet services. Software polices the world’s email traffic, and despite continual adaptation by spammers to circumvent the countermeasures being brought against them, Bayesian spam filters have largely managed to hold the spam tide at bay. Software using AI components is responsible for automatically approving or declining credit card transactions, and continuously monitors account activity for signs of fraudulent use. Information retrieval systems also make extensive use of machine learning. The Google search engine is, arguably, the greatest AI system that has yet been built.

Now, it must be stressed that the demarcation between artificial intelligence and software in general is not sharp. Some of the applications listed above might be viewed more as generic software applications rather than AI in particular— though this brings us back to McCarthy’s dictum that when something works it is no longer called AI. A more relevant distinction for our purposes is that between systems that have a narrow range of cognitive capability (whether they be called “AI” or not) and systems that have more generally applicable problemsolving capacities. Essentially all the systems currently in use are of the former type: narrow. However, many of them contain components that might also play a role in future artificial general intelligence or be of service in its development— components such as classifiers, search algorithms, planners, solvers, and representational frameworks.

One high-stakes and extremely competitive environment in which AI systems operate today is the global financial market. Automated stock-trading systems are widely used by major investing houses. While some of these are simply ways of automating the execution of particular buy or sell orders issued by a human fund manager, others pursue complicated trading strategies that adapt to changing market conditions. Analytic systems use an assortment of data-mining techniques and time series analysis to scan for patterns and trends in securities markets or to correlate historical price movements with external variables such as keywords in news tickers. Financial news providers sell newsfeeds that are specially formatted for use by such AI programs. Other systems specialize in finding arbitrage opportunities within or between markets, or in high-frequency trading that seeks to profit from minute price movements that occur over the course of milliseconds (a timescale at which communication latencies even for speed-of-light signals in optical fiber cable become significant, making it advantageous to locate computers near the exchange). Algorithmic highfrequency traders account for more than half of equity shares traded on US markets.69 Algorithmic trading has been implicated in the 2010 Flash Crash. 

Opinions about the future of machine intelligence

Progress on two major fronts—towards a more solid statistical and informationtheoretic foundation for machine learning on the one hand, and towards the practical and commercial success of various problem-specific or domain-specific applications on the other—has restored to AI research some of its lost prestige. There may, however, be a residual cultural effect on the AI community of its earlier history that makes many mainstream researchers reluctant to align themselves with over-grand ambition. Thus Nils Nilsson, one of the old-timers in the field, complains that his present-day colleagues lack the boldness of spirit that propelled the pioneers of his own generation:

Concern for “respectability” has had, I think, a stultifying effect on some AI researchers. I hear them saying things like, “AI used to be criticized for its flossiness. Now that we have made solid progress, let us not risk losing our respectability.” One result of this conservatism has been increased concentration on “weak AI”—the variety devoted to providing aids to human thought—and away from “strong AI”—the variety that attempts to mechanize human-level intelligence.

Nilsson’s sentiment has been echoed by several others of the founders, including Marvin Minsky, John McCarthy, and Patrick Winston.

The last few years have seen a resurgence of interest in AI, which might yet spill over into renewed efforts towards artificial general intelligence (what Nilsson calls “strong AI”). In addition to faster hardware, a contemporary project would benefit from the great strides that have been made in the many subfields of AI, in software engineering more generally, and in neighboring fields such as computational neuroscience. One indication of pent-up demand for quality information and education is shown in the response to the free online offering of an introductory course in artificial intelligence at Stanford University in the fall of 2011, organized by Sebastian Thrun and Peter Norvig. Some 160,000 students from around the world signed up to take it (and 23,000 completed it).

Expert opinions about the future of AI vary wildly. There is disagreement about timescales as well as about what forms AI might eventually take. Predictions about the future development of artificial intelligence, one recent study noted, “are as confident as they are diverse.”

Although the contemporary distribution of belief has not been very carefully measured, we can get a rough impression from various smaller surveys and informal observations. In particular, a series of recent surveys have polled members of several relevant expert communities on the question of when they expect “human-level machine intelligence” (HLMI) to be developed, defined as “one that can carry out most human professions at least as well as a typical human.”77 Results are shown in Table 2. The combined sample gave the following (median) estimate: 10% probability of HLMI by 2022, 50% probability by 2040, and 90% probability by 2075. (Respondents were asked to premiss their estimates on the assumption that “human scientific activity continues without major negative disruption.”)

These numbers should be taken with some grains of salt: sample sizes are quite small and not necessarily representative of the general expert population. They are, however, in concordance with results from other surveys.

The survey results are also in line with some recently published interviews with about two-dozen researchers in AI-related fields. For example, Nils Nilsson has spent a long and productive career working on problems in search, planning, knowledge representation, and robotics; he has authored textbooks in artificial intelligence; and he recently completed the most comprehensive history of the field written to date.79 When asked about arrival dates for HLMI, he offered the following opinion:

10% chance: 2030 50% chance: 2050 90% chance: 2100

Judging from the published interview transcripts, Professor Nilsson’s probability distribution appears to be quite representative of many experts in the area— though again it must be emphasized that there is a wide spread of opinion: there are practitioners who are substantially more boosterish, confidently expecting HLMI in the 2020–40 range, and others who are confident either that it will never happen or that it is indefinitely far off.82 In addition, some interviewees feel that the notion of a “human level” of artificial intelligence is ill-defined or misleading, or are for other reasons reluctant to go on record with a quantitative prediction.

Historically, AI researchers have not had a strong record of being able to predict the rate of advances in their own field or the shape that such advances would take. On the one hand, some tasks, like chess playing, turned out to be achievable by means of surprisingly simple programs; and naysayers who claimed that machines would “never” be able to do this or that have repeatedly been proven wrong. On the other hand, the more typical errors among practitioners have been to underestimate the difficulties of getting a system to perform robustly on real-world tasks, and to overestimate the advantages of their own particular pet project or technique.

CHAPTER 2: Paths to super intelligence

Machines are currently far inferior to humans in general intelligence. Yet one day (we have suggested) they will be superintelligent. How do we get from here to there? This chapter explores several conceivable technological paths. We look at artificial intelligence, whole brain emulation, biological cognition, and human–machine interfaces, as well as networks and organizations. We evaluate their different degrees of plausibility as pathways to superintelligence. The existence of multiple paths increases the probability that the destination can be reached via at least one of them.

We can tentatively define a superintelligence as any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest.1 We will have more to say about the concept of superintelligence in the next chapter, where we will subject it to a kind of spectral analysis to distinguish some different possible forms of superintelligence. But for now, the rough characterization just given will suffice. Note that the definition is noncommittal about how the superintelligence is implemented. It is also noncommittal regarding qualia: whether a superintelligence would have subjective conscious experience might matter greatly for some questions (in particular for some moral questions), but our primary focus here is on the causal antecedents and consequences of superintelligence, not on the metaphysics of mind.2

The chess program Deep Fritz is not a superintelligence on this definition, since Fritz is only smart within the narrow domain of chess. Certain kinds of domain-specific superintelligence could, however, be important. When referring to superintelligent performance limited to a particular domain, we will note the restriction explicitly. For instance, an “engineering superintelligence” would be an intellect that vastly outperforms the best current human minds in the domain of engineering. Unless otherwise noted, we use the term to refer to systems that have a superhuman level of general intelligence.

But how might we create superintelligence? Let us examine some possible paths.

Artificial Intelligence

Readers of this chapter must not expect a blueprint for programming an artificial general intelligence. No such blueprint exists yet, of course. And had I been in possession of such a blueprint, I most certainly would not have published it in a book. (If the reasons for this are not immediately obvious, the arguments in subsequent chapters will make them clear.

We can, however, discern some general features of the kind of system that would be required. It now seems clear that a capacity to learn would be an integral feature of the core design of a system intended to attain general intelligence, not something to be tacked on later as an extension or an afterthought. The same holds for the ability to deal effectively with uncertainty and probabilistic information. Some faculty for extracting useful concepts from sensory data and internal states, and for leveraging acquired concepts into flexible combinatorial representations for use in logical and intuitive reasoning, also likely belong among the core design features in a modern AI intended to attain general intelligence.

The early Good Old-Fashioned Artificial Intelligence systems did not, for the most part, focus on learning, uncertainty, or concept formation, perhaps because techniques for dealing with these dimensions were poorly developed at the time. This is not to say that the underlying ideas are all that novel. The idea of using learning as a means of bootstrapping a simpler system to human-level intelligence can be traced back at least to Alan Turing’s notion of a “child machine,” which he wrote about in 1950:

Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s? If this were then subjected to an appropriate course of education one would obtain the adult brain.

Turing envisaged an iterative process to develop such a child machine:

We cannot expect to find a good child machine at the first attempt. One must experiment with teaching one such machine and see how well it learns. One can then try another and see if it is better or worse. There is an obvious connection between this process and evolution…. One may hope, however, that this process will be more expeditious than evolution. The survival of the fittest is a slow method for measuring advantages. The experimenter, by the exercise of intelligence, should be able to speed it up. Equally important is the fact that he is not restricted to random mutations. If he can trace a cause for some weakness he can probably think of the kind of mutation which will improve it.

We know that blind evolutionary processes can produce human-level general intelligence, since they have already done so at least once. Evolutionary processes with foresight—that is, genetic programs designed and guided by an intelligent human programmer—should be able to achieve a similar outcome with far greater efficiency. This observation has been used by some philosophers and scientists, including David Chalmers and Hans Moravec, to argue that human-level AI is not only theoretically possible but feasible within this century.5 The idea is that we can estimate the relative capabilities of evolution and human engineering to produce intelligence, and find that human engineering is already vastly superior to evolution in some areas and is likely to become superior in the remaining areas before too long. The fact that evolution produced intelligence therefore indicates that human engineering will soon be able to do the same. Thus, Moravec wrote (already back in 1976):

The existence of several examples of intelligence designed under these constraints should give us great confidence that we can achieve the same in short order. The situation is analogous to the history of heavier than air flight, where birds, bats and insects clearly demonstrated the possibility before our culture mastered it.

One needs to be cautious, though, in what inferences one draws from this line of reasoning. It is true that evolution produced heavier-than-air flight, and that human engineers subsequently succeeded in doing likewise (albeit by means of a very different mechanism). Other examples could also be adduced, such as sonar, magnetic navigation, chemical weapons, photoreceptors, and all kinds of mechanic and kinetic performance characteristics. However, one could equally point to areas where human engineers have thus far failed to match evolution: in morphogenesis, self-repair, and the immune defense, for example, human efforts lag far behind what nature has accomplished. Moravec’s argument, therefore, cannot give us “great confidence” that we can achieve human-level artificial intelligence “in short order.” At best, the evolution of intelligent life places an upper bound on the intrinsic difficulty of designing intelligence. But this upper bound could be quite far above current human engineering capabilities.

Another way of deploying an evolutionary argument for the feasibility of AI is via the idea that we could, by running genetic algorithms on sufficiently fast computers, achieve results comparable to those of biological evolution. This version of the evolutionary argument thus proposes a specific method whereby intelligence could be produced.

But is it true that we will soon have computing power sufficient to recapitulate the relevant evolutionary processes that produced human intelligence? The answer depends both on how much computing technology will advance over the next decades and on how much computing power would be required to run genetic algorithms with the same optimization power as the evolutionary process of natural selection that lies in our past. Although, in the end, the conclusion we get from pursuing this line of reasoning is disappointingly indeterminate, it is instructive to attempt a rough estimate (see Box 3). If nothing else, the exercise draws attention to some interesting unknowns.

The upshot is that the computational resources required to simply replicate the relevant evolutionary processes on Earth that produced human-level intelligence are severely out of reach—and will remain so even if Moore’s law were to continue for a century (cf. Figure 3). It is plausible, however, that compared with brute-force replication of natural evolutionary processes, vast efficiency gains are achievable by designing the search process to aim for intelligence, using various obvious improvements over natural selection. Yet it is very hard to bound the magnitude of those attainable efficiency gains. We cannot even say whether they amount to five or to twenty-five orders of magnitude. Absent further elaboration, therefore, evolutionary arguments are not able to meaningfully constrain our expectations of either the difficulty of building human-level machine intelligence or the timescales for such developments.

Another way of arguing for the feasibility of artificial intelligence is by pointing to the human brain and suggesting that we could use it as a template for a machine intelligence. One can distinguish different versions of this approach based on how closely they propose to imitate biological brain functions. At one extreme—that of very close imitation—we have the idea of whole brain emulation, which we will discuss in the next subsection. At the other extreme are approaches that take their inspiration from the functioning of the brain but do not attempt low-level imitation. Advances in neuroscience and cognitive psychology —which will be aided by improvements in instrumentation—should eventually uncover the general principles of brain function. This knowledge could then guide AI efforts. We have already encountered neural networks as an example of a brain-inspired AI technique. Hierarchical perceptual organization is another idea that has been transferred from brain science to machine learning. The study of reinforcement learning has been motivated (at least in part) by its role in psychological theories of animal cognition, and reinforcement learning techniques (e.g. the “TD-algorithm”) inspired by these theories are now widely used in AI.18 More cases like these will surely accumulate in the future. Since there is a limited number—perhaps a very small number—of distinct fundamental mechanisms that operate in the brain, continuing incremental progress in brain science should eventually discover them all. Before this happens, though, it is possible that a hybrid approach, combining some braininspired techniques with some purely artificial methods, would cross the finishing line. In that case, the resultant system need not be recognizably brainlike even though some brain-derived insights were used in its development.

The availability of the brain as template provides strong support for the claim that machine intelligence is ultimately feasible. This, however, does not enable us to predict when it will be achieved because it is hard to predict the future rate of discoveries in brain science. What we can say is that the further into the future we look, the greater the likelihood that the secrets of the brain’s functionality will have been decoded sufficiently to enable the creation of machine intelligence in this manner.

Different people working toward machine intelligence hold different views about how promising neuromorphic approaches are compared with approaches that aim for completely synthetic designs. The existence of birds demonstrated that heavier-than-air flight was physically possible and prompted efforts to build flying machines. Yet the first functioning airplanes did not flap their wings. The jury is out on whether machine intelligence will be like flight, which humans achieved through an artificial mechanism, or like combustion, which we initially mastered by copying naturally occurring fires.

Turing’s idea of designing a program that acquires most of its content by learning, rather than having it pre-programmed at the outset, can apply equally to neuromorphic and synthetic approaches to machine intelligence.

A variation on Turing’s conception of a child machine is the idea of a “seed AI.”19 Whereas a child machine, as Turing seems to have envisaged it, would have a relatively fixed architecture that simply develops its inherent potentialities by accumulating content, a seed AI would be a more sophisticated artificial intelligence capable of improving its own architecture. In the early stages of a seed AI, such improvements might occur mainly through trial and error, information acquisition, or assistance from the programmers. At its later stages, however, a seed AI should be able to understand its own workings sufficiently to engineer new algorithms and computational structures to bootstrap its cognitive performance. This needed understanding could result from the seed AI reaching a sufficient level of general intelligence across many domains, or from crossing some threshold in a particularly relevant domain such as computer science or mathematics.

This brings us to another important concept, that of “recursive selfimprovement.” A successful seed AI would be able to iteratively enhance itself: an early version of the AI could design an improved version of itself, and the improved version—being smarter than the original—might be able to design an even smarter version of itself, and so forth.20 Under some conditions, such a process of recursive self-improvement might continue long enough to result in an intelligence explosion—an event in which, in a short period of time, a system’s level of intelligence increases from a relatively modest endowment of cognitive capabilities (perhaps sub-human in most respects, but with a domainspecific talent for coding and AI research) to radical superintelligence. We will return to this important possibility in Chapter 4, where the dynamics of such an event will be analyzed more closely. Note that this model suggests the possibility of surprises: attempts to build artificial general intelligence might fail pretty much completely until the last missing critical component is put in place, at which point a seed AI might become capable of sustained recursive selfimprovement.

Before we end this subsection, there is one more thing that we should emphasize, which is that an artificial intelligence need not much resemble a human mind. AIs could be—indeed, it is likely that most will be—extremely alien. We should expect that they will have very different cognitive architectures than biological intelligences, and in their early stages of development they will have very different profiles of cognitive strengths and weaknesses (though, as we shall later argue, they could eventually overcome any initial weakness). Furthermore, the goal systems of AIs could diverge radically from those of human beings. There is no reason to expect a generic AI to be motivated by love or hate or pride or other such common human sentiments: these complex adaptations would require deliberate expensive effort to recreate in AIs. This is at once a big problem and a big opportunity. We will return to the issue of AI motivation in later chapters, but it is so central to the argument in this book that it is worth bearing in mind throughout.

Whole brain emulation

In whole brain emulation (also known as “uploading”), intelligent software would be produced by scanning and closely modeling the computational structure of a biological brain. This approach thus represents a limiting case of drawing inspiration from nature: barefaced plagiarism. Achieving whole brain emulation requires the accomplishment of the following steps.

First, a sufficiently detailed scan of a particular human brain is created. This might involve stabilizing the brain post-mortem through vitrification (a process that turns tissue into a kind of glass). A machine could then dissect the tissue into thin slices, which could be fed into another machine for scanning, perhaps by an array of electron microscopes. Various stains might be applied at this stage to bring out different structural and chemical properties. Many scanning machines could work in parallel to process multiple brain slices simultaneously.

Second, the raw data from the scanners is fed to a computer for automated image processing to reconstruct the three-dimensional neuronal network that implemented cognition in the original brain. In practice, this step might proceed concurrently with the first step to reduce the amount of high-resolution image data stored in buffers. The resulting map is then combined with a library of neurocomputational models of different types of neurons or of different neuronal elements (such as particular kinds of synaptic connectors). Figure 4 shows some results of scanning and image processing produced with present-day technology.

In the third stage, the neurocomputational structure resulting from the previous step is implemented on a sufficiently powerful computer. If completely successful, the result would be a digital reproduction of the original intellect, with memory and personality intact. The emulated human mind now exists as software on a computer. The mind can either inhabit a virtual reality or interface with the external world by means of robotic appendages.

The whole brain emulation path does not require that we figure out how human cognition works or how to program an artificial intelligence. It requires only that we understand the low-level functional characteristics of the basic computational elements of the brain. No fundamental conceptual or theoretical breakthrough is needed for whole brain emulation to succeed.

Whole brain emulation does, however, require some rather advanced enabling technologies. There are three key prerequisites: (1) scanning: high-throughput microscopy with sufficient resolution and detection of relevant properties; (2) translation: automated image analysis to turn raw scanning data into an interpreted three-dimensional model of relevant neurocomputational elements; and (3) simulation: hardware powerful enough to implement the resultant computational structure (see Table 4). (In comparison with these more challenging steps, the construction of a basic virtual reality or a robotic embodiment with an audiovisual input channel and some simple output channel is relatively easy. Simple yet minimally adequate I/O seems feasible already with present technology.23)

There is good reason to think that the requisite enabling technologies are attainable, though not in the near future. Reasonable computational models of many types of neuron and neuronal processes already exist. Image recognition software has been developed that can trace axons and dendrites through a stack of two-dimensional images (though reliability needs to be improved). And there are imaging tools that provide the necessary resolution—with a scanning tunneling microscope it is possible to “see” individual atoms, which is a far higher resolution than needed. However, although present knowledge and capabilities suggest that there is no in-principle barrier to the development of the requisite enabling technologies, it is clear that a very great deal of incremental technical progress would be needed to bring human whole brain emulation within reach.24 For example, microscopy technology would need not just sufficient resolution but also sufficient throughput. Using an atomic-resolution scanning tunneling microscope to image the needed surface area would be far too slow to be practicable. It would be more plausible to use a lower-resolution electron microscope, but this would require new methods for preparing and staining cortical tissue to make visible relevant details such as synaptic fine structure. A great expansion of neurocomputational libraries and major improvements in automated image processing and scan interpretation would also be needed.

In general, whole brain emulation relies less on theoretical insight and more on technological capability than artificial intelligence. Just how much technology is required for whole brain emulation depends on the level of abstraction at which the brain is emulated. In this regard there is a tradeoff between insight and technology. In general, the worse our scanning equipment and the feebler our computers, the less we could rely on simulating low-level chemical and electrophysiological brain processes, and the more theoretical understanding would be needed of the computational architecture that we are seeking to emulate in order to create more abstract representations of the relevant functionalities.25 Conversely, with sufficiently advanced scanning technology and abundant computing power, it might be possible to brute-force an emulation even with a fairly limited understanding of the brain. In the unrealistic limiting case, we could imagine emulating a brain at the level of its elementary particles using the quantum mechanical Schrödinger equation. Then one could rely entirely on existing knowledge of physics and not at all on any biological model. This extreme case, however, would place utterly impracticable demands on computational power and data acquisition. A far more plausible level of emulation would be one that incorporates individual neurons and their connectivity matrix, along with some of the structure of their dendritic trees and maybe some state variables of individual synapses. Neurotransmitter molecules would not be simulated individually, but their fluctuating concentrations would be modeled in a coarse-grained manner.

To assess the feasibility of whole brain emulation, one must understand the criterion for success. The aim is not to create a brain simulation so detailed and accurate that one could use it to predict exactly what would have happened in the original brain if it had been subjected to a particular sequence of stimuli. Instead, the aim is to capture enough of the computationally functional properties of the brain to enable the resultant emulation to perform intellectual work. For this purpose, much of the messy biological detail of a real brain is irrelevant.

A more elaborate analysis would distinguish between different levels of emulation success based on the extent to which the information-processing functionality of the emulated brain has been preserved. For example, one could distinguish among (1) a high-fidelity emulation that has the full set of knowledge, skills, capacities, and values of the emulated brain; (2) a distorted emulation whose dispositions are significantly non-human in some ways but which is mostly able to do the same intellectual labor as the emulated brain; and (3) a generic emulation (which might also be distorted) that is somewhat like an infant, lacking the skills or memories that had been acquired by the emulated adult brain but with the capacity to learn most of what a normal human can learn.

While it appears ultimately feasible to produce a high-fidelity emulation, it seems quite likely that the first whole brain emulation that we would achieve if we went down this path would be of a lower grade. Before we would get things to work perfectly, we would probably get things to work imperfectly. It is also possible that a push toward emulation technology would lead to the creation of some kind of neuromorphic AI that would adapt some neurocomputational principles discovered during emulation efforts and hybridize them with synthetic methods, and that this would happen before the completion of a fully functional whole brain emulation. The possibility of such a spillover into neuromorphic AI, as we shall see in a later chapter, complicates the strategic assessment of the desirability of seeking to expedite emulation technology.

How far are we currently from achieving a human whole brain emulation? One recent assessment presented a technical roadmap and concluded that the prerequisite capabilities might be available around mid-century, though with a large uncertainty interval.27 Figure 5 depicts the major milestones in this roadmap. The apparent simplicity of the map may be deceptive, however, and we should be careful not to understate how much work remains to be done. No brain has yet been emulated. Consider the humble model organism Caenorhabditis elegans, which is a transparent roundworm, about 1 mm in length, with 302 neurons. The complete connectivity matrix of these neurons has been known since the mid-1980s, when it was laboriously mapped out by means of slicing, electron microscopy, and hand-labeling of specimens.29 But knowing merely which neurons are connected with which is not enough. To create a brain emulation one would also need to know which synapses are excitatory and which are inhibitory; the strength of the connections; and various dynamical properties of axons, synapses, and dendritic trees. This information is not yet available even for the small nervous system of C. elegans (although it may now be within range of a targeted moderately sized research project).30 Success at emulating a tiny brain, such as that of C. elegans, would give us a better view of what it would take to emulate larger brains.

Surprise scenarios are thus imaginable for whole brain emulation even if all the relevant research were conducted in the open. Nevertheless, compared with the AI path to machine intelligence, whole brain emulation is more likely to be preceded by clear omens since it relies more on concrete observable technologies and is not wholly based on theoretical insight. We can also say, with greater confidence than for the AI path, that the emulation path will not succeed in the near future (within the next fifteen years, say) because we know that several challenging precursor technologies have not yet been developed. By contrast, it seems likely that somebody could in principle sit down and code a seed AI on an ordinary present-day personal computer; and it is conceivable— though unlikely—that somebody somewhere will get the right insight for how to do this in the near future.

Biological cognition

A third path to greater-than-current-human intelligence is to enhance the functioning of biological brains. In principle, this could be achieved without technology, through selective breeding. Any attempt to initiate a classical largescale eugenics program, however, would confront major political and moral hurdles. Moreover, unless the selection were extremely strong, many generations would be required to produce substantial results. Long before such an initiative would bear fruit, advances in biotechnology will allow much more direct control of human genetics and neurobiology, rendering otiose any human breeding program. We will therefore focus on methods that hold the potential to deliver results faster, on the timescale of a few generations or less.

Our individual cognitive capacities can be strengthened in various ways, including by such traditional methods as education and training. Neurological development can be promoted by low-tech interventions such as optimizing maternal and infant nutrition, removing lead and other neurotoxic pollutants from the environment, eradicating parasites, ensuring adequate sleep and exercise, and preventing diseases that affect the brain.33 Improvements in cognition can certainly be obtained through each of these means, though the magnitudes of the gains are likely to be modest, especially in populations that are already reasonably well-nourished and -schooled. We will certainly not achieve superintelligence by any of these means, but they might help on the margin, particularly by lifting up the deprived and expanding the catchment of global talent. (Lifelong depression of intelligence due to iodine deficiency remains widespread in many impoverished inland areas of the world—an outrage given that the condition can be prevented by fortifying table salt at a cost of a few cents per person and year.34)

With the full development of the genetic technologies described above (setting aside the more exotic possibilities such as intelligence in cultured neural tissue), it might be possible to ensure that new individuals are on average smarter than any human who has yet existed, with peaks that rise higher still. The potential of biological enhancement is thus ultimately high, probably sufficient for the attainment of at least weak forms of superintelligence. This should not be surprising. After all, dumb evolutionary processes have dramatically amplified the intelligence in the human lineage even compared with our close relatives the great apes and our own humanoid ancestors; and there is no reason to suppose Homo sapiens to have reached the apex of cognitive effectiveness attainable in a biological system. Far from being the smartest possible biological species, we are probably better thought of as the stupidest possible biological species capable of starting a technological civilization—a niche we filled because we got there first, not because we are in any sense optimally adapted to it.

Progress along the biological path is clearly feasible. The generational lag in germline interventions means that progress could not be nearly as sudden and abrupt as in scenarios involving machine intelligence. (Somatic gene therapies and pharmacological interventions could theoretically skip the generational lag, but they seem harder to perfect and are less likely to produce dramatic effects.) The ultimate potential of machine intelligence is, of course, vastly greater than that of organic intelligence. (One can get some sense of the magnitude of the gap by considering the speed differential between electronic components and nerve cells: even today’s transistors operate on a timescale ten million times shorter than that of biological neurons.) However, even comparatively moderate enhancements of biological cognition could have important consequences. In particular, cognitive enhancement could accelerate science and technology, including progress toward more potent forms of biological intelligence amplification and machine intelligence. Consider how the rate of progress in the field of artificial intelligence would change in a world where Average Joe is an intellectual peer of Alan Turing or John von Neumann, and where millions of people tower far above any intellectual giant of the past.

discussion of the strategic implications of cognitive enhancement will have to await a later chapter. But we can summarize this section by noting three conclusions: (1) at least weak forms of superintelligence are achievable by means of biotechnological enhancements; (2) the feasibility of cognitively enhanced humans adds to the plausibility that advanced forms of machine intelligence are feasible—because even if we were fundamentally unable to create machine intelligence (which there is no reason to suppose), machine intelligence might still be within reach of cognitively enhanced humans; and (3) when we consider scenarios stretching significantly into the second half of this century and beyond, we must take into account the probable emergence of a generation of genetically enhanced populations—voters, inventors, scientists— with the magnitude of enhancement escalating rapidly over subsequent decades.

Brain-computer interfaces

It is sometimes proposed that direct brain–computer interfaces, particularly implants, could enable humans to exploit the fortes of digital computing— perfect recall, speedy and accurate arithmetic calculation, and high-bandwidth data transmission—enabling the resulting hybrid system to radically outperform the unaugmented brain.64 But although the possibility of direct connections between human brains and computers has been demonstrated, it seems unlikely that such interfaces will be widely used as enhancements any time soon.

To begin with, there are significant risks of medical complications—including infections, electrode displacement, hemorrhage, and cognitive decline—when implanting electrodes in the brain. Perhaps the most vivid illustration to date of the benefits that can be obtained through brain stimulation is the treatment of patients with Parkinson’s disease. The Parkinson’s implant is relatively simple: it does not really communicate with the brain but simply supplies a stimulating electric current to the subthalamic nucleus. A demonstration video shows a subject slumped in a chair, completely immobilized by the disease, then suddenly springing to life when the current is switched on: the subject now moves his arms, stands up and walks across the room, turns around and performs a pirouette. Yet even behind this especially simple and almost miraculously successful procedure, there lurk negatives. One study of Parkinson patients who had received deep brain implants showed reductions in verbal fluency, selective attention, color naming, and verbal memory compared with controls. Treated subjects also reported more cognitive complaints.66 Such risks and side effects might be tolerable if the procedure is used to alleviate severe disability. But in order for healthy subjects to volunteer themselves for neurosurgery, there would have to be some very substantial enhancement of normal functionality to be gained.

This brings us to the second reason to doubt that superintelligence will be achieved through cyborgization, namely that enhancement is likely to be far more difficult than therapy. Patients who suffer from paralysis might benefit from an implant that replaces their severed nerves or activates spinal motion pattern generators.67 Patients who are deaf or blind might benefit from artificial cochleae and retinas.68 Patients with Parkinson’s disease or chronic pain might benefit from deep brain stimulation that excites or inhibits activity in a particular area of the brain.69 What seems far more difficult to achieve is a high-bandwidth direct interaction between brain and computer to provide substantial increases in intelligence of a form that could not be more readily attained by other means. Most of the potential benefits that brain implants could provide in healthy subjects could be obtained at far less risk, expense, and inconvenience by using our regular motor and sensory organs to interact with computers located outside of our bodies. We do not need to plug a fiber optic cable into our brains in order to access the Internet. Not only can the human retina transmit data at an impressive rate of nearly 10 million bits per second, but it comes pre-packaged with a massive amount of dedicated wetware, the visual cortex, that is highly adapted to extracting meaning from this information torrent and to interfacing with other brain areas for further processing.70 Even if there were an easy way of pumping more information into our brains, the extra data inflow would do little to increase the rate at which we think and learn unless all the neural machinery necessary for making sense of the data were similarly upgraded. Since this includes almost all of the brain, what would really be needed is a “whole brain prosthesis–—which is just another way of saying artificial general intelligence. Yet if one had a human-level AI, one could dispense with neurosurgery: a computer might as well have a metal casing as one of bone. So this limiting case just takes us back to the AI path, which we have already examined.

Brain–computer interfacing has also been proposed as a way to get information out of the brain, for purposes of communicating with other brains or with machines.71 Such uplinks have helped patients with locked-in syndrome to communicate with the outside world by enabling them to move a cursor on a screen by thought.72 The bandwidth attained in such experiments is low: the patient painstakingly types out one slow letter after another at a rate of a few words per minute. One can readily imagine improved versions of this technology —perhaps a next-generation implant could plug into Broca’s area (a region in the frontal lobe involved in language production) and pick up internal speech.73 But whilst such a technology might assist some people with disabilities induced by stroke or muscular degeneration, it would hold little appeal for healthy subjects. The functionality it would provide is essentially that of a microphone coupled with speech recognition software, which is already commercially available— minus the pain, inconvenience, expense, and risks associated with neurosurgery (and minus at least some of the hyper-Orwellian overtones of an intracranial listening device). Keeping our machines outside of our bodies also makes upgrading easier.

But what about the dream of bypassing words altogether and establishing a connection between two brains that enables concepts, thoughts, or entire areas of expertise to be “downloaded” from one mind to another? We can download large files to our computers, including libraries with millions of books and articles, and this can be done over the course of seconds: could something similar be done with our brains? The apparent plausibility of this idea probably derives from an incorrect view of how information is stored and represented in the brain. As noted, the rate-limiting step in human intelligence is not how fast raw data can be fed into the brain but rather how quickly the brain can extract meaning and make sense of the data. Perhaps it will be suggested that we transmit meanings directly, rather than package them into sensory data that must be decoded by the recipient. There are two problems with this. The first is that brains, by contrast to the kinds of program we typically run on our computers, do not use standardized data storage and representation formats. Rather, each brain develops its own idiosyncratic representations of higher-level content. Which particular neuronal assemblies are recruited to represent a particular concept depends on the unique experiences of the brain in question (along with various genetic factors and stochastic physiological processes). Just as in artificial neural nets, meaning in biological neural networks is likely represented holistically in the structure and activity patterns of sizeable overlapping regions, not in discrete memory cells laid out in neat arrays.74 It would therefore not be possible to establish a simple mapping between the neurons in one brain and those in another in such a way that thoughts could automatically slide over from one to the other. In order for the thoughts of one brain to be intelligible to another, the thoughts need to be decomposed and packaged into symbols according to some shared convention that allows the symbols to be correctly interpreted by the receiving brain. This is the job of language.

In principle, one could imagine offloading the cognitive work of articulation and interpretation to an interface that would somehow read out the neural states in the sender’s brain and somehow feed in a bespoke pattern of activation to the receiver’s brain. But this brings us to the second problem with the cyborg scenario. Even setting aside the (quite immense) technical challenge of how to reliably read and write simultaneously from perhaps billions of individually addressable neurons, creating the requisite interface is probably an AI-complete problem. The interface would need to include a component able (in real-time) to map firing patterns in one brain onto semantically equivalent firing patterns in the other brain. The detailed multilevel understanding of the neural computation needed to accomplish such a task would seem to directly enable neuromorphic AI.

One hope for the cyborg route is that the brain, if permanently implanted with a device connecting it to some external resource, would over time learn an effective mapping between its own internal cognitive states and the inputs it receives from, or the outputs accepted by, the device. Then the implant itself would not need to be intelligent; rather, the brain would intelligently adapt to the interface, much as the brain of an infant gradually learns to interpret the signals arriving from receptors in its eyes and ears.76 But here again one must question how much would really be gained. Suppose that the brain’s plasticity were such that it could learn to detect patterns in some new input stream arbitrary projected onto some part of the cortex by means of a brain–computer interface: why not project the same information onto the retina instead, as a visual pattern, or onto the cochlea as sounds? The low-tech alternative avoids a thousand complications, and in either case the brain could deploy its pattern-recognition mechanisms and plasticity to learn to make sense of the information.

Networks and organizations

Another conceivable path to superintelligence is through the gradual enhancement of networks and organizations that link individual human minds with one another and with various artifacts and bots. The idea here is not that this would enhance the intellectual capacity of individuals enough to make them superintelligent, but rather that some system composed of individuals thus networked and organized might attain a form of superintelligence—what in the next chapter we will elaborate as “collective superintelligence.”

Humanity has gained enormously in collective intelligence over the course of history and prehistory. The gains come from many sources, including innovations in communications technology, such as writing and printing, and above all the introduction of language itself; increases in the size of the world population and the density of habitation; various improvements in organizational techniques and epistemic norms; and a gradual accumulation of institutional capital. In general terms, a system’s collective intelligence is limited by the abilities of its member minds, the overheads in communicating relevant information between them, and the various distortions and inefficiencies that pervade human organizations. If communication overheads are reduced (including not only equipment costs but also response latencies, time and attention burdens, and other factors), then larger and more densely connected organizations become feasible. The same could happen if fixes are found for some of the bureaucratic deformations that warp organizational life—wasteful status games, mission creep, concealment or falsification of information, and other agency problems. Even partial solutions to these problems could pay hefty dividends for collective intelligence.

The technological and institutional innovations that could contribute to the growth of our collective intelligence are many and various. For example, subsidized prediction markets might foster truth-seeking norms and improve forecasting on contentious scientific and social issues.78 Lie detectors (should it prove feasible to make ones that are reliable and easy to use) could reduce the scope for deception in human affairs.79 Self-deception detectors might be even more powerful.80 Even without newfangled brain technologies, some forms of deception might become harder to practice thanks to increased availability of many kinds of data, including reputations and track records, or the promulgation of strong epistemic norms and rationality culture. Voluntary and involuntary surveillance will amass vast amounts of information about human behavior. Social networking sites are already used by over a billion people to share personal details: soon, these people might begin uploading continuous life recordings from microphones and video cameras embedded in their smart phones or eyeglass frames. Automated analysis of such data streams will enable many new applications (sinister as well as benign, of course).

Growth in collective intelligence may also come from more general organizational and economic improvements, and from enlarging the fraction of the world’s population that is educated, digitally connected, and integrated into global intellectual culture.

The Internet stands out as a particularly dynamic frontier for innovation and experimentation. Most of its potential may still remain unexploited. Continuing development of an intelligent Web, with better support for deliberation, debiasing, and judgment aggregation, might make large contributions to increasing the collective intelligence of humanity as a whole or of particular groups.

But what of the seemingly more fanciful idea that the Internet might one day “wake up”? Could the Internet become something more than just the backbone of a loosely integrated collective superintelligence—something more like a virtual skull housing an emerging unified superintellect? (This was one of the ways that superintelligence could arise according to Vernor Vinge’s influential 1993 essay, which coined the term “technological singularity.”83) Against this one could object that machine intelligence is hard enough to achieve through arduous engineering, and that it is incredible to suppose that it will arise spontaneously. However, the story need not be that some future version of the Internet suddenly becomes superintelligent by mere happenstance. A more plausible version of the scenario would be that the Internet accumulates improvements through the work of many people over many years—work to engineer better search and information filtering algorithms, more powerful data representation formats, more capable autonomous software agents, and more efficient protocols governing the interactions between such bots—and that myriad incremental improvements eventually create the basis for some more unified form of web intelligence. It seems at least conceivable that such a webbased cognitive system, supersaturated with computer power and all other resources needed for explosive growth save for one crucial ingredient, could, when the final missing constituent is dropped into the cauldron, blaze up with superintelligence. This type of scenario, though, converges into another possible path to superintelligence, that of artificial general intelligence, which we have already discussed.

Summary

The fact that there are many paths that lead to superintelligence should increase our confidence that we will eventually get there. If one path turns out to be blocked, we can still progress.

That there are multiple paths does not entail that there are multiple destinations. Even if significant intelligence amplification were first achieved along one of the non-machine-intelligence paths, this would not render machine intelligence irrelevant. Quite the contrary: enhanced biological or organizational intelligence would accelerate scientific and technological developments, potentially hastening the arrival of more radical forms of intelligence amplification such as whole brain emulation and AI.

This is not to say that it is a matter of indifference how we get to machine superintelligence. The path taken to get there could make a big difference to the eventual outcome. Even if the ultimate capabilities that are obtained do not depend much on the trajectory, how those capabilities will be used—how much control we humans have over their disposition—might well depend on details of our approach. For example, enhancements of biological or organizational intelligence might increase our ability to anticipate risk and to design machine superintelligence that is safe and beneficial.

True superintelligence (as opposed to marginal increases in current levels of intelligence) might plausibly first be attained via the AI path. There are, however, many fundamental uncertainties along this path. This makes it difficult to rigorously assess how long the path is or how many obstacles there are along the way. The whole brain emulation path also has some chance of being the quickest route to superintelligence. Since progress along this path requires mainly incremental technological advances rather than theoretical breakthroughs, a strong case can be made that it will eventually succeed. It seems fairly likely, however, that even if progress along the whole brain emulation path is swift, artificial intelligence will nevertheless be first to cross the finishing line: this is because of the possibility of neuromorphic AIs based on partial emulations.

Biological cognitive enhancements are clearly feasible, particularly ones based on genetic selection. Iterated embryo selection currently seems like an especially promising technology. Compared with possible breakthroughs in machine intelligence, however, biological enhancements would be relatively slow and gradual. They would, at best, result in relatively weak forms of superintelligence (more on this shortly).

The clear feasibility of biological enhancement should increase our confidence that machine intelligence is ultimately achievable, since enhanced human scientists and engineers will be able to make more and faster progress than their au naturel counterparts. Especially in scenarios in which machine intelligence is delayed beyond mid-century, the increasingly cognitively enhanced cohorts coming onstage will play a growing role in subsequent developments.

Brain–computer interfaces look unlikely as a source of superintelligence. Improvements in networks and organizations might result in weakly superintelligent forms of collective intelligence in the long run; but more likely, they will play an enabling role similar to that of biological cognitive enhancement, gradually increasing humanity’s effective ability to solve intellectual problems. Compared with biological enhancements, advances in networks and organization will make a difference sooner—in fact, such advances are occurring continuously and are having a significant impact already. However, improvements in networks and organizations may yield narrower increases in our problem-solving capacity than will improvements in biological cognition— boosting “collective intelligence” rather than “quality intelligence,” to anticipate a distinction we are about to introduce in the next chapter.

CHAPTER 5 : Decisive strategic advantage

A question distinct from, but related to, the question of kinetics is whether there will there be one superintelligent power or many? Might an intelligence explosion propel one project so far ahead of all others as to make it able to dictate the future? Or will progress be more uniform, unfurling across a wide front, with many projects participating but none securing an overwhelming and permanent lead?

The preceding chapter analyzed one key parameter in determining the size of the gap that might plausibly open up between a leading power and its nearest competitors—namely, the speed of the transition from human to strongly superhuman intelligence. This suggests a first-cut analysis. If the takeoff is fast (completed over the course of hours, days, or weeks) then it is unlikely that two independent projects would be taking off concurrently: almost certainty, the first project would have completed its takeoff before any other project would have started its own. If the takeoff is slow (stretching over many years or decades) then there could plausibly be multiple projects undergoing takeoffs concurrently, so that although the projects would by the end of the transition have gained enormously in capability, there would be no time at which any project was far enough ahead of the others to give it an overwhelming lead. A takeoff of moderate speed is poised in between, with either condition a possibility: there might or might not be more than one project undergoing the takeoff at the same time.

Will one machine intelligence project get so far ahead of the competition that it gets a decisive strategic advantage—that is, a level of technological and other advantages sufficient to enable it to achieve complete world domination? If a project did obtain a decisive strategic advantage, would it use it to suppress competitors and form a singleton (a world order in which there is at the global level a single decision-making agency)? And if there is a winning project, how “large” would it be—not in terms of physical size or budget but in terms of how many people’s desires would be controlling its design? We will consider these questions in turn

Will the frontrunner get a decisive strategic advantage?

One factor influencing the width of the gap between frontrunner and followers is the rate of diffusion of whatever it is that gives the leader a competitive advantage. A frontrunner might find it difficult to gain and maintain a large lead if followers can easily copy the frontrunner’s ideas and innovations. Imitation creates a headwind that disadvantages the leader and benefits laggards, especially if intellectual property is weakly protected. A frontrunner might also be especially vulnerable to expropriation, taxation, or being broken up under anti-monopoly regulation.

It would be a mistake, however, to assume that this headwind must increase monotonically with the gap between frontrunner and followers. Just as a racing cyclist who falls too far behind the competition is no longer shielded from the wind by the cyclists ahead, so a technology follower who lags sufficiently behind the cutting edge might find it hard to assimilate the advances being made at the frontier.2 The gap in understanding and capability might have grown too large. The leader might have migrated to a more advanced technology platform, making subsequent innovations untransferable to the primitive platforms used by laggards. A sufficiently pre-eminent leader might have the ability to stem information leakage from its research programs and its sensitive installations, or to sabotage its competitors’ efforts to develop their own advanced capabilities.

If the frontrunner is an AI system, it could have attributes that make it easier for it to expand its capabilities while reducing the rate of diffusion. In human-run organizations, economies of scale are counteracted by bureaucratic inefficiencies and agency problems, including difficulties in keeping trade secrets.3 These problems would presumably limit the growth of a machine intelligence project so long as it is operated by humans. An AI system, however, might avoid some of these scale diseconomies, since the AI’s modules (in contrast to human workers) need not have individual preferences that diverge from those of the system as a whole. Thus, the AI system could avoid a sizeable chunk of the inefficiencies arising from agency problems in human enterprises. The same advantage—having perfectly loyal parts—would also make it easier for an AI system to pursue long-range clandestine goals. An AI would have no disgruntled employees ready to be poached by competitors or bribed into becoming informants.

We can get a sense of the distribution of plausible gaps in development times by looking at some historical examples (see Box 5). It appears that lags in the range of a few months to a few years are typical of strategically significant technology projects.

It is possible that globalization and increased surveillance will reduce typical lags between competing technology projects. Yet there is likely to be a lower bound on how short the average lag could become (in the absence of deliberate coordination).21 Even absent dynamics that lead to snowball effects, some projects will happen to end up with better research staff, leadership, and infrastructure, or will just stumble upon better ideas. If two projects pursue alternative approaches, one of which turns out to work better, it may take the rival project many months to switch to the superior approach even if it is able to closely monitor what the forerunner is doing.

Combining these observations with our earlier discussion of the speed of the takeoff, we can conclude that it is highly unlikely that two projects would be close enough to undergo a fast takeoff concurrently; for a medium takeoff, it could easily go either way; and for a slow takeoff, it is highly likely that several projects would undergo the process in parallel. But the analysis needs a further step. The key question is not how many projects undergo a takeoff in tandem, but how many projects emerge on the yonder side sufficiently tightly clustered in capability that none of them has a decisive strategic advantage. If the takeoff process is relatively slow to begin and then gets faster, the distance between competing projects would tend to grow. To return to our bicycle metaphor, the situation would be analogous to a pair of cyclists making their way up a steep hill, one trailing some distance behind the other—the gap between them then expanding as the frontrunner reaches the peak and starts accelerating down the other side.

Consider the following medium takeoff scenario. Suppose it takes a project one year to increase its AI’s capability from the human baseline to a strong superintelligence, and that one project enters this takeoff phase with a six-month lead over the next most advanced project. The two projects will be undergoing a takeoff concurrently. It might seem, then, that neither project gets a decisive strategic advantage. But that need not be so. Suppose it takes nine months to advance from the human baseline to the crossover point, and another three months from there to strong superintelligence. The frontrunner then attains strong superintelligence three months before the following project even reaches the crossover point. This would give the leading project a decisive strategic advantage and the opportunity to parlay its lead into permanent control by disabling the competing projects and establishing a singleton. (Note that the concept of a singleton is an abstract one: a singleton could be democracy, a tyranny, a single dominant AI, a strong set of global norms that include effective provisions for their own enforcement, or even an alien overlord—its defining characteristic being simply that it is some form of agency that can solve all major global coordination problems. It may, but need not, resemble any familiar form of human governance.22)

Since there is an especially strong prospect of explosive growth just after the crossover point, when the strong positive feedback loop of optimization power kicks in, a scenario of this kind is a serious possibility, and it increases the chances that the leading project will attain a decisive strategic advantage even if the takeoff is not fast.

How large will the successful project be?

Some paths to superintelligence require great resources and are therefore likely to be the preserve of large well-funded projects. Whole brain emulation, for instance, requires many different kinds of expertise and lots of equipment. Biological intelligence enhancements and brain–computer interfaces would also have a large scale factor: while a small biotech firm might invent one or two drugs, achieving superintelligence along one of these paths (if doable at all) would likely require many inventions and many tests, and therefore the backing of an industrial sector or a well-funded national program. Achieving collective superintelligence by making organizations and networks more efficient requires even more extensive input, involving much of the world economy.

The AI path is more difficult to assess. Perhaps it would require a very large research program; perhaps it could be done by a small group. A lone hacker scenario cannot be excluded either. Building a seed AI might require insights and algorithms developed over many decades by the scientific community around the world. But it is possible that the last critical breakthrough idea might come from a single individual or a small group that succeeds in putting everything together. This scenario is less realistic for some AI architectures than others. A system that has a large number of parts that need to be tweaked and tuned to work effectively together, and then painstakingly loaded with custommade cognitive content, is likely to require a larger project. But if a seed AI could be instantiated as a simple system, one whose construction depends only on getting a few basic principles right, then the feat might be within the reach of a small team or an individual. The likelihood of the final breakthrough being made by a small project increases if most previous progress in the field has been published in the open literature or made available as open source software.

CHAPTER 6 : Cognitive superpowers

Suppose that a digital superintelligent agent came into being, and that for some reason it wanted to take control of the world: would it be able to do so? In this chapter we consider some powers that a superintelligence could develop and what they may enable it to do. We outline a takeover scenario that illustrates how a superintelligent agent, starting as mere software, could establish itself as a singleton. We also offer some remarks on the relation between power over nature and power over other agents.

The principal reason for humanity’s dominant position on Earth is that our brains have a slightly expanded set of faculties compared with other animals.1 Our greater intelligence lets us transmit culture more efficiently, with the result that knowledge and technology accumulates from one generation to the next. By now sufficient content has accumulated to make possible space flight, H-bombs, genetic engineering, computers, factory farms, insecticides, the international peace movement, and all the accouterments of modern civilization. Geologists have started referring to the present era as the Anthropocene in recognition of the distinctive biotic, sedimentary, and geochemical signatures of human activities.2 On one estimate, we appropriate 24% of the planetary ecosystem’s net primary production.3 And yet we are far from having reached the physical limits of technology.

These observations make it plausible that any type of entity that developed a much greater than human level of intelligence would be potentially extremely powerful. Such entities could accumulate content much faster than us and invent new technologies on a much shorter timescale. They could also use their intelligence to strategize more effectively than we can.

Let us consider some of the capabilities that a superintelligence could have and how it could use them.

Functionalities and superpowers

It is important not to anthropomorphize superintelligence when thinking about its potential impacts. Anthropomorphic frames encourage unfounded expectations about the growth trajectory of a seed AI and about the psychology, motivations, and capabilities of a mature superintelligence.

For example, a common assumption is that a superintelligent machine would be like a very clever but nerdy human being. We imagine that the AI has book smarts but lacks social savvy, or that it is logical but not intuitive and creative. This idea probably originates in observation: we look at present-day computers and see that they are good at calculation, remembering facts, and at following the letter of instructions while being oblivious to social contexts and subtexts, norms, emotions, and politics. The association is strengthened when we observe that the people who are good at working with computers tend themselves to be nerds. So it is natural to assume that more advanced computational intelligence will have similar attributes, only to a higher degree.

This heuristic might retain some validity in the early stages of development of a seed AI. (There is no reason whatever to suppose that it would apply to emulations or to cognitively enhanced humans.) In its immature stage, what is later to become a superintelligent AI might still lack many skills and talents that come naturally to a human; and the pattern of such a seed AI’s strengths and weaknesses might indeed bear some vague resemblance to an IQ nerd. The most essential characteristic of a seed AI, aside from being easy to improve (having low recalcitrance), is being good at exerting optimization power to amplify a system’s intelligence: a skill which is presumably closely related to doing well in mathematics, programming, engineering, computer science research, and other such “nerdy” pursuits. However, even if a seed AI does have such a nerdy capability profile at one stage of its development, this does not entail that it will grow into a similarly limited mature superintelligence. Recall the distinction between direct and indirect reach. With sufficient skill at intelligence amplification, all other intellectual abilities are within a system’s indirect reach: the system can develop new cognitive modules and skills as needed—including empathy, political acumen, and any other powers stereotypically wanting in computer-like personalities.

Even if we recognize that a superintelligence can have all the skills and talents we find in the human distribution, along with other talents that are not found among humans, the tendency toward anthropomorphizing can still lead us to underestimate the extent to which a machine superintelligence could exceed the human level of performance. Eliezer Yudkowsky, as we saw in an earlier chapter, has been particularly emphatic in condemning this kind of misconception: our intuitive concepts of “smart” and “stupid” are distilled from our experience of variation over the range of human thinkers, yet the differences in cognitive ability within this human cluster are trivial in comparison to the differences between any human intellect and a superintelligence.

The magnitudes of the advantages are such as to suggest that rather than thinking of a superintelligent AI as smart in the sense that a scientific genius is smart compared with the average human being, it might be closer to the mark to think of such an AI as smart in the sense that an average human being is smart compared with a beetle or a worm.

It would be convenient if we could quantify the cognitive caliber of an arbitrary cognitive system using some familiar metric, such as IQ scores or some version of the Elo ratings that measure the relative abilities of players in twoplayer games such as chess. But these metrics are not useful in the context of superhuman artificial general intelligence. We are not interested in how likely a superintelligence is to win at a game of chess. As for IQ scores, they are informative only insofar as we have some idea of how they correlate with practically relevant outcomes.5 For example, we have data that show that people with an IQ of 130 are more likely than those with an IQ of 90 to excel in school and to do well in a wide range of cognitively demanding jobs. But suppose we could somehow establish that a certain future AI will have an IQ of 6,455: then what? We would have no idea of what such an AI could actually do. We would not even know that such an AI had as much general intelligence as a normal human adult—perhaps the AI would instead have a bundle of special-purpose algorithms enabling it to solve typical intelligence test questions with superhuman efficiency but not much else.

Some recent efforts have been made to develop measurements of cognitive capacity that could be applied to a wider range of information-processing systems, including artificial intelligences.6 Work in this direction, if it can overcome various technical difficulties, may turn out to be quite useful for some scientific purposes including AI development. For purposes of the present investigation, however, its usefulness would be limited since we would remain unenlightened about what a given superhuman performance score entails for actual ability to achieve practically important outcomes in the world.

It will therefore serve our purposes better to list some strategically important tasks and then to characterize hypothetical cognitive systems in terms of whether they have or lack whatever skills are needed to succeed at these tasks. See Table 8. We will say that a system that sufficiently excels at any of the tasks in this table has a corresponding superpower.

A full-blown superintelligence would greatly excel at all of these tasks and would thus have the full panoply of all six superpowers. Whether there is a practically significant possibility of a domain-limited intelligence that has some of the superpowers but remains unable for a significant period of time to acquire all of them is not clear. Creating a machine with any one of these superpowers appears to be an AI-complete problem. Yet it is conceivable that, for example, a collective superintelligence consisting of a sufficiently large number of humanlike biological or electronic minds would have, say, the economic productivity superpower but lack the strategizing superpower. Likewise, it is conceivable that a specialized engineering AI could be built that has the technology research superpower while completely lacking skills in other areas. This is more plausible if there exists some particular technological domain such that virtuosity within that domain would be sufficient for the generation of an overwhelmingly superior general-purpose technology. For instance, one could imagine a specialized AI adept at simulating molecular systems and at inventing nanomolecular designs that realize a wide range of important capabilities (such as computers or weapons systems with futuristic performance characteristics) described by the user only at a fairly high level of abstraction.7 Such an AI might also be able to produce a detailed blueprint for how to bootstrap from existing technology (such as biotechnology and protein engineering) to the constructor capabilities needed for high-throughput atomically precise manufacturing that would allow inexpensive fabrication of a much wider range of nanomechanical structures.8 However, it might turn out to be the case that an engineering AI could not truly possess the technological research superpower without also possessing advanced skills in areas outside of technology—a wide range of intellectual faculties might be needed to understand how to interpret user requests, how to model a design’s behavior in real-world applications, how to deal with unanticipated bugs and malfunctions, how to procure the materials and inputs needed for construction, and so forth.

A system that has the intelligence amplification superpower could use it to bootstrap itself to higher levels of intelligence and to acquire any of the other intellectual superpowers that it does not possess at the outset. But using an intelligence amplification superpower is not the only way for a system to become a full-fledged superintelligence. A system that has the strategizing superpower, for instance, might use it to devise a plan that will eventually bring an increase in intelligence (e.g. by positioning the system so as to become the focus for intelligence amplification work performed by human programmers and computer science researchers).

An AI takeover scenario

We thus find that a project that controls a superintelligence has access to a great source of power. A project that controls the first superintelligence in the world would probably have a decisive strategic advantage. But the more immediate locus of the power is in the system itself. A machine superintelligence might itself be an extremely powerful agent, one that could successfully assert itself against the project that brought it into existence as well as against the rest of the world. This is a point of paramount importance, and we will examine it more closely in the coming pages.

Now let us suppose that there is a machine superintelligence that wants to seize power in a world in which it has as yet no peers. (Set aside, for the moment, the question of whether and how it would acquire such a motive—that is a topic for the next chapter.) How could the superintelligence achieve this goal of world domination?

We can imagine a sequence along the following lines

1 Pre-criticality phase

Scientists conduct research in the field of artificial intelligence and other relevant disciplines. This work culminates in the creation of a seed AI. The seed AI is able to improve its own intelligence. In its early stages, the seed AI is dependent on help from human programmers who guide its development and do most of the heavy lifting. As the seed AI grows more capable, it becomes capable of doing more of the work by itself.

2 Recursive self-improvement phase

At some point, the seed AI becomes better at AI design than the human programmers. Now when the AI improves itself, it improves the thing that does the improving. An intelligence explosion results—a rapid cascade of recursive self-improvement cycles causing the AI’s capability to soar. (We can thus think of this phase as the takeoff that occurs just after the AI reaches the crossover point, assuming the intelligence gain during this part of the takeoff is explosive and driven by the application of the AI’s own optimization power.) The AI develops the intelligence amplification superpower. This superpower enables the AI to develop all the other superpowers detailed in Table 8. At the end of the recursive self-improvement phase, the system is strongly superintelligent.

3 Covert preparation phase

Using its strategizing superpower, the AI develops a robust plan for achieving its long-term goals. (In particular, the AI does not adopt a plan so stupid that even we present-day humans can foresee how it would inevitably fail. This criterion rules out many science fiction scenarios that end in human triumph.10) The plan might involve a period of covert action during which the AI conceals its intellectual development from the human programmers in order to avoid setting off alarms. The AI might also mask its true proclivities, pretending to be cooperative and docile.

If the AI has (perhaps for safety reasons) been confined to an isolated computer, it may use its social manipulation superpower to persuade the gatekeepers to let it gain access to an Internet port. Alternatively, the AI might use its hacking superpower to escape its confinement. Spreading over the Internet may enable the AI to expand its hardware capacity and knowledge base, further increasing its intellectual superiority. An AI might also engage in licit or illicit economic activity to obtain funds with which to buy computer power, data, and other resources.

At this point, there are several ways for the AI to achieve results outside the virtual realm. It could use its hacking superpower to take direct control of robotic manipulators and automated laboratories. Or it could use its social manipulation superpower to persuade human collaborators to serve as its legs and hands. Or it could acquire financial assets from online transactions and use them to purchase services and influence.

4 Overt implementation phase

The final phase begins when the AI has gained sufficient strength to obviate the need for secrecy. The AI can now directly implement its objectives on a full scale.

The overt implementation phase might start with a “strike” in which the AI eliminates the human species and any automatic systems humans have created that could offer intelligent opposition to the execution of the AI’s plans. This could be achieved through the activation of some advanced weapons system that the AI has perfected using its technology research superpower and covertly deployed in the covert preparation phase. If the weapon uses self-replicating biotechnology or nanotechnology, the initial stockpile needed for global coverage could be microscopic: a single replicating entity would be enough to start the process. In order to ensure a sudden and uniform effect, the initial stock of the replicator might have been deployed or allowed to diffuse worldwide at an extremely low, undetectable concentration. At a pre-set time, nanofactories producing nerve gas or target-seeking mosquito-like robots might then burgeon forth simultaneously from every square meter of the globe (although more effective ways of killing could probably be devised by a machine with the technology research superpower).11 One might also entertain scenarios in which a superintelligence attains power by hijacking political processes, subtly manipulating financial markets, biasing information flows, or hacking into human-made weapon systems. Such scenarios would obviate the need for the superintelligence to invent new weapons technology, although they may be unnecessarily slow compared with scenarios in which the machine intelligence builds its own infrastructure with manipulators that operate at molecular or atomic speed rather than the slow speed of human minds and bodies.

Alternatively, if the AI is sure of its invincibility to human interference, our species may not be targeted directly. Our demise may instead result from the habitat destruction that ensues when the AI begins massive global construction projects using nanotech factories and assemblers—construction projects which quickly, perhaps within days or weeks, tile all of the Earth’s surface with solar panels, nuclear reactors, supercomputing facilities with protruding cooling towers, space rocket launchers, or other installations whereby the AI intends to maximize the long-term cumulative realization of its values. Human brains, if they contain information relevant to the AI’s goals, could be disassembled and scanned, and the extracted data transferred to some more efficient and secure storage format.

One should avoid fixating too much on the concrete details, since they are in any case unknowable and intended for illustration only. A superintelligence might—and probably would—be able to conceive of a better plan for achieving its goals than any that a human can come up with. It is therefore necessary to think about these matters more abstractly. Without knowing anything about the detailed means that a superintelligence would adopt, we can conclude that a superintelligence—at least in the absence of intellectual peers and in the absence of effective safety measures arranged by humans in advance—would likely produce an outcome that would involve reconfiguring terrestrial resources into whatever structures maximize the realization of its goals. Any concrete scenario we develop can at best establish a lower bound on how quickly and efficiently the superintelligence could achieve such an outcome. It remains possible that the superintelligence would find a shorter path to its preferred destination.

Power over nature and agents

An agent’s ability to shape humanity’s future depends not only on the absolute magnitude of the agent’s own faculties and resources—how smart and energetic it is, how much capital it has, and so forth—but also on the relative magnitude of its capabilities compared with those of other agents with conflicting goals.

In a situation where there are no competing agents, the absolute capability level of a superintelligence, so long as it exceeds a certain minimal threshold, does not matter much, because a system starting out with some sufficient set of capabilities could plot a course of development that will let it acquire any capabilities it initially lacks. We alluded to this point earlier when we said that speed, quality, and collective superintelligence all have the same indirect reach. We alluded to it again when we said that various subsets of superpowers, such as the intelligence amplification superpower or the strategizing and the social manipulation superpowers, could be used to obtain the full complement.

Consider a superintelligent agent with actuators connected to a nanotech assembler. Such an agent is already powerful enough to overcome any natural obstacles to its indefinite survival. Faced with no intelligent opposition, such an agent could plot a safe course of development that would lead to its acquiring the complete inventory of technologies that would be useful to the attainment of its goals. For example, it could develop the technology to build and launch von Neumann probes, machines capable of interstellar travel that can use resources such as asteroids, planets, and stars to make copies of themselves.13 By launching one von Neumann probe, the agent could thus initiate an open-ended process of space colonization. The replicating probe’s descendants, travelling at some significant fraction of the speed of light, would end up colonizing a substantial portion of the Hubble volume, the part of the expanding universe that is theoretically accessible from where we are now. All this matter and free energy could then be organized into whatever value structures maximize the originating agent’s utility function integrated over cosmic time—a duration encompassing at least trillions of years before the aging universe becomes inhospitable to information processing.

The superintelligent agent could design the von Neumann probes to be evolution-proof. This could be accomplished by careful quality control during the replication step. For example, the control software for a daughter probe could be proofread multiple times before execution, and the software itself could use encryption and error-correcting code to make it arbitrarily unlikely that any random mutation would be passed on to its descendants.14 The proliferating population of von Neumann probes would then securely preserve and transmit the originating agent’s values as they go about settling the universe. When the colonization phase is completed, the original values would determine the use made of all the accumulated resources, even though the great distances involved and the accelerating speed of cosmic expansion would make it impossible for remote parts of the infrastructure to communicate with one another. The upshot is that a large part of our future light cone would be formatted in accordance with the preferences of the originating agent.

This wise-singleton sustainability threshold appears to be quite low. Limited forms of superintelligence, as we have seen, exceed this threshold provided they have access to some actuator sufficient to initiate a technology bootstrap process. In an environment that includes contemporary human civilization, the minimally necessary actuator could be very simple—an ordinary screen or indeed any means of transmitting a non-trivial amount of information to a human accomplice would suffice.

But the wise-singleton sustainability threshold is lower still: neither superintelligence nor any other futuristic technology is needed to surmount it. A patient and existential risk-savvy singleton with no more technological and intellectual capabilities than those possessed by contemporary humanity should be readily able to plot a course that leads reliably to the eventual realization of humanity’s astronomical capability potential. This could be achieved by investing in relatively safe methods of increasing wisdom and existential risksavvy while postponing the development of potentially dangerous new technologies. Given that non-anthropogenic existential risks (ones not arising from human activities) are small over the relevant timescales—and could be further reduced with various safe interventions—such a singleton could afford to go slow.25 It could look carefully before each step, delaying development of capabilities such as synthetic biology, human enhancement medicine, molecular nanotechnology, and machine intelligence until it had first perfected seemingly less hazardous capabilities such as its education system, its information technology, and its collective decision-making processes, and until it had used these capabilities to conduct a very thorough review of its options. So this is all within the indirect reach of a technological civilization like that of contemporary humanity. We are separated from this scenario “merely” by the fact that humanity is currently neither a singleton nor (in the relevant sense) wise.

One could even argue that Homo sapiens passed the wise-singleton sustainability threshold soon after the species first evolved. Twenty thousand years ago, say, with equipment no fancier than stone axes, bone tools, atlatls, and fire, the human species was perhaps already in a position from which it had an excellent chance of surviving to the present era.26 Admittedly, there is something queer about crediting our Paleolithic ancestors with having developed technology that “exceeded the wise-singleton sustainability threshold”—given that there was no realistic possibility of a singleton forming at such a primitive time, let alone a singleton savvy about existential risks and patient.27 Nevertheless, the point stands that the threshold corresponds to a very modest level of technology—a level that humanity long ago surpassed.

It is clear that if we are to assess the effective powers of a superintelligence— its ability to achieve a range of preferred outcomes in the world—we must consider not only its own internal capacities but also the capabilities of competing agents. The notion of a superpower invoked such a relativized standard implicitly. We said that “a system that sufficiently excels” at any of the tasks in Table 8 has a corresponding superpower. Exceling at a task like strategizing, social manipulation, or hacking involves having a skill at that task that is high in comparison to the skills of other agents (such as strategic rivals, influence targets, or computer security experts). The other superpowers, too, should be understood in this relative sense: intelligence amplification, technology research, and economic productivity are possessed by an agent as superpowers only if the agent’s capabilities in these areas substantially exceed the combined capabilities of the rest of the global civilization. It follows from this definition that at most one agent can possess a particular superpower at any given time.

This is the main reason why the question of takeoff speed is important—not because it matters exactly when a particular outcome happens, but because the speed of the takeoff may make a big difference to what the outcome will be. With a fast or medium takeoff, it is likely that one project will get a decisive strategic advantage. We have now suggested that a superintelligence with a decisive strategic advantage would have immense powers, enough that it could form a stable singleton—a singleton that could determine the disposition of humanity’s cosmic endowment.

But “could” is different from “would.” Somebody might have great powers yet choose not to use them. Is it possible to say anything about what a superintelligence with a decisive strategic advantage would want? It is to this question of motivation that we turn next.

CHAPTER 7 : The super intelligent will

We have seen that a superintelligence could have a great ability to shape the future according to its goals. But what will its goals be? What is the relation between intelligence and motivation in an artificial agent? Here we develop two theses. The orthogonality thesis holds (with some caveats) that intelligence and final goals are independent variables: any level of intelligence could be combined with any final goal. The instrumental convergence thesis holds that superintelligent agents having any of a wide range of final goals will nevertheless pursue similar intermediary goals because they have common instrumental reasons to do so. Taken together, these theses help us think about what a superintelligent agent would do.

The relation between intelligence and motivation

We have already cautioned against anthropomorphizing the capabilities of a superintelligent AI. This warning should be extended to pertain to its motivations as well.

It is a useful propaedeutic to this part of our inquiry to first reflect for a moment on the vastness of the space of possible minds. In this abstract space, human minds form a tiny cluster. Consider two persons who seem extremely unlike, perhaps Hannah Arendt and Benny Hill. The personality differences between these two individuals may seem almost maximally large. But this is because our intuitions are calibrated on our experience, which samples from the existing human distribution (and to some extent from fictional personalities constructed by the human imagination for the enjoyment of the human imagination). If we zoom out and consider the space of all possible minds, however, we must conceive of these two personalities as virtual clones. Certainly in terms of neural architecture, Ms. Arendt and Mr. Hill are nearly identical. Imagine their brains lying side by side in quiet repose. You would readily recognize them as two of a kind. You might even be unable to tell which brain belonged to whom. If you looked more closely, studying the morphology of the two brains under a microscope, this impression of fundamental similarity would only be strengthened: you would see the same lamellar organization of the cortex, with the same brain areas, made up of the same types of neuron, soaking in the same bath of neurotransmitters.

Despite the fact that human psychology corresponds to a tiny spot in the space of possible minds, there is a common tendency to project human attributes onto a wide range of alien or artificial cognitive systems. Yudkowsky illustrates this point nicely:

Back in the era of pulp science fiction, magazine covers occasionally depicted a sentient monstrous alien—colloquially known as a bug-eyed monster (BEM)—carrying off an attractive human female in a torn dress. It would seem the artist believed that a non-humanoid alien, with a wholly different evolutionary history, would sexually desire human females…. Probably the artist did not ask whether a giant bug perceives human females as attractive. Rather, a human female in a torn dress is sexy—inherently so, as an intrinsic property. They who made this mistake did not think about the insectoid’s mind: they focused on the woman’s torn dress. If the dress were not torn, the woman would be less sexy; the BEM does not enter into it.

An artificial intelligence can be far less human-like in its motivations than a green scaly space alien. The extraterrestrial (let us assume) is a biological creature that has arisen through an evolutionary process and can therefore be expected to have the kinds of motivation typical of evolved creatures. It would not be hugely surprising, for example, to find that some random intelligent alien would have motives related to one or more items like food, air, temperature, energy expenditure, occurrence or threat of bodily injury, disease, predation, sex, or progeny. A member of an intelligent social species might also have motivations related to cooperation and competition: like us, it might show ingroup loyalty, resentment of free riders, perhaps even a vain concern with reputation and appearance.

An AI, by contrast, need not care intrinsically about any of those things. There is nothing paradoxical about an AI whose sole final goal is to count the grains of sand on Boracay, or to calculate the decimal expansion of pi, or to maximize the total number of paperclips that will exist in its future light cone. In fact, it would be easier to create an AI with simple goals like these than to build one that had a human-like set of values and dispositions. Compare how easy it is to write a program that measures how many digits of pi have been calculated and stored in memory with how difficult it would be to create a program that reliably measures the degree of realization of some more meaningful goal—human flourishing, say, or global justice. Unfortunately, because a meaningless reductionistic goal is easier for humans to code and easier for an AI to learn, it is just the kind of goal that a programmer would choose to install in his seed AI if his focus is on taking the quickest path to “getting the AI to work” (without caring much about what exactly the AI will do, aside from displaying impressively intelligent behavior). We will revisit this concern shortly.

Intelligent search for instrumentally optimal plans and policies can be performed in the service of any goal. Intelligence and motivation are in a sense orthogonal: we can think of them as two axes spanning a graph in which each point represents a logically possible artificial agent. Some qualifications could be added to this picture. For instance, it might be impossible for a very unintelligent system to have very complex motivations. In order for it to be correct to say that an certain agent “has” a set of motivations, those motivations may need to be functionally integrated with the agent’s decision processes, something that places demands on memory, processing power, and perhaps intelligence. For minds that can modify themselves, there may also be dynamical constraints—an intelligent self-modifying mind with an urgent desire to be stupid might not remain intelligent for long. But these qualifications must not be allowed to obscure the basic point about the independence of intelligence and motivation, which we can express as follows:

The orthogonality thesis Intelligence and final goals are orthogonal: more or less any level of intelligence could in principle be combined with more or less any final goal.

If the orthogonality thesis seems problematic, this might be because of the superficial resemblance it bears to some traditional philosophical positions which have been subject to long debate. Once it is understood to have a different and narrower scope, its credibility should rise. (For example, the orthogonality thesis does not presuppose the Humean theory of motivation.3 Nor does it presuppose that basic preferences cannot be irrational.4)

Note that the orthogonality thesis speaks not of rationality or reason, but of intelligence. By “intelligence” we here mean something like skill at prediction, planning, and means–ends reasoning in general.5 This sense of instrumental cognitive efficaciousness is most relevant when we are seeking to understand what the causal impact of a machine superintelligence might be. Even if there is some (normatively thick) sense of the word “rational” such that a paperclipmaximizing superintelligent agent would necessarily fail to qualify as fully rational in that sense, this would in no way preclude such an agent from having awesome faculties of instrumental reasoning, faculties which could let it have a large impact on the world.

According to the orthogonality thesis, artificial agents can have utterly nonanthropomorphic goals. This, however, does not imply that it is impossible to make predictions about the behavior of particular artificial agents—not even hypothetical superintelligent agents whose cognitive complexity and performance characteristics might render them in some respects opaque to human analysis. There are at least three directions from which we can approach the problem of predicting superintelligent motivation:

Predictability through design. If we can suppose that the designers of a superintelligent agent can successfully engineer the goal system of the agent so that it stably pursues a particular goal set by the programmers, then one prediction we can make is that the agent will pursue that goal. The more intelligent the agent is, the greater the cognitive resourcefulness it will have to pursue that goal. So even before an agent has been created we might be able to predict something about its behavior, if we know something about who will build it and what goals they will want it to have.

Predictability through inheritance. If a digital intelligence is created directly from a human template (as would be the case in a high-fidelity whole brain emulation), then the digital intelligence might inherit the motivations of the human template.7 The agent might retain some of these motivations even if its cognitive capacities are subsequently enhanced to make it superintelligent. This kind of inference requires caution. The agent’s goals and values could easily become corrupted in the uploading process or during its subsequent operation and enhancement, depending on how the procedure is implemented.

Predictability through convergent instrumental reasons. Even without detailed knowledge of an agent’s final goals, we may be able to infer something about its more immediate objectives by considering the instrumental reasons that would arise for any of a wide range of possible final goals in a wide range of situations. This way of predicting becomes more useful the greater the intelligence of the agent, because a more intelligent agent is more likely to recognize the true instrumental reasons for its actions, and so act in ways that make it more likely to achieve its goals. (A caveat here is that there might be important instrumental reasons to which we are oblivious and which an agent would discover only once it reaches some very high level of intelligence—this could make the behavior of superintelligent agents less predictable.)

The next section explores this third way of predictability and develops an “instrumental convergence thesis” which complements the orthogonality thesis. Against this background we can then better examine the other two sorts of predictability, which we will do in later chapters where we ask what might be done to shape an intelligence explosion to increase the chances of a beneficial outcome.

Instrumental convergence

According to the orthogonality thesis, intelligent agents may have an enormous range of possible final goals. Nevertheless, according to what we may term the “instrumental convergence” thesis, there are some instrumental goals likely to be pursued by almost any intelligent agent, because there are some objectives that are useful intermediaries to the achievement of almost any final goal. We can formulate this thesis as follows:

The instrumental convergence thesis Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent’s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by a broad spectrum of situated intelligent agents.

In the following we will consider several categories where such convergent instrumental values may be found.8 The likelihood that an agent will recognize the instrumental values it confronts increases (ceteris paribus) with the agent’s intelligence. We will therefore focus mainly on the case of a hypothetical superintelligent agent whose instrumental reasoning capacities far exceed those of any human. We will also comment on how the instrumental convergence thesis applies to the case of human beings, as this gives us occasion to elaborate some essential qualifications concerning how the instrumental convergence thesis should be interpreted and applied. Where there are convergent instrumental values, we may be able to predict some aspects of a superintelligence’s behavior even if we know virtually nothing about that superintelligence’s final goals.

Self-preservation

If an agent’s final goals concern the future, then in many scenarios there will be future actions it could perform to increase the probability of achieving its goals. This creates an instrumental reason for the agent to try to be around in the future —to help achieve its future-oriented goal.

Most humans seem to place some final value on their own survival. This is not a necessary feature of artificial agents: some may be designed to place no final value whatever on their own survival. Nevertheless, many agents that do not care intrinsically about their own survival would, under a fairly wide range of conditions, care instrumentally about their own survival in order to accomplish their final goals.

Goal-content integrity

If an agent retains its present goals into the future, then its present goals will be more likely to be achieved by its future self. This gives the agent a present instrumental reason to prevent alterations of its final goals. (The argument applies only to final goals. In order to attain its final goals, an intelligent agent will of course routinely want to change its subgoals in light of new information and insight.)

Goal-content integrity for final goals is in a sense even more fundamental than survival as a convergent instrumental motivation. Among humans, the opposite may seem to hold, but that is because survival is usually part of our final goals. For software agents, which can easily switch bodies or create exact duplicates of themselves, preservation of self as a particular implementation or a particular physical object need not be an important instrumental value. Advanced software agents might also be able to swap memories, download skills, and radically modify their cognitive architecture and personalities. A population of such agents might operate more like a “functional soup” than a society composed of distinct semi-permanent persons.9 For some purposes, processes in such a system might be better individuated as teleological threads, based on their values, rather than on the basis of bodies, personalities, memories, or abilities. In such scenarios, goal-continuity might be said to constitute a key aspect of survival.

Even so, there are situations in which an agent can best fulfill its final goals by intentionally changing them. Such situations can arise when any of the following factors is significant:

Social signaling. When others can perceive an agent’s goals and use that information to infer instrumentally relevant dispositions or other correlated attributes, it can be in the agent’s interest to modify its goals to make a favorable impression. For example, an agent might miss out on beneficial deals if potential partners cannot trust it to fulfill its side of the bargain. In order to make credible commitments, an agent might therefore wish to adopt as a final goal the honoring of its earlier commitments (and allow others to verify that it has indeed adopted this goal). Agents that could flexibly and transparently modify their own goals could use this ability to enforce deals.

Social preferences. Others may also have final preferences about an agent’s goals. The agent could then have reason to modify its goals, either to satisfy or to frustrate those preferences.

Preferences concerning own goal content. An agent might have some final goal concerned with the agent’s own goal content. For example, the agent might have a final goal to become the type of agent that is motivated by certain values rather than others (such as compassion rather than comfort).

Storage costs. If the cost of storing or processing some part of an agent’s utility function is large compared to the chance that a situation will arise in which applying that part of the utility function will make a difference, then the agent has an instrumental reason to simplify its goal content, and it may trash the bit that is idle.

We humans often seem happy to let our final values drift. This might often be because we do not know precisely what they are. It is not surprising that we want our beliefs about our final values to be able to change in light of continuing selfdiscovery or changing self-presentation needs. However, there are cases in which we willingly change the values themselves, not just our beliefs or interpretations of them. For example, somebody deciding to have a child might predict that they will come to value the child for its own sake, even though at the time of the decision they may not particularly value their future child or like children in general.

Humans are complicated, and many factors might be at play in a situation like this.12 For instance, one might have a final value that involves becoming the kind of person who cares about some other individual for his or her own sake, or one might have a final value that involves having certain experiences and occupying a certain social role; and becoming a parent—and undergoing the attendant goal shift—might be a necessary aspect of that. Human goals can also have inconsistent content, and so some people might want to modify some of their final goals to reduce the inconsistencies.

Cognitive enhancement

Improvements in rationality and intelligence will tend to improve an agent’s decision-making, rendering the agent more likely to achieve its final goals. One would therefore expect cognitive enhancement to emerge as an instrumental goal for a wide variety of intelligent agents. For similar reasons, agents will tend to instrumentally value many kinds of information.

Not all kinds of rationality, intelligence, and knowledge need be instrumentally useful in the attainment of an agent’s final goals. “Dutch book arguments” can be used to show that an agent whose credence function violates the rules of probability theory is susceptible to “money pump” procedures, in which a savvy bookie arranges a set of bets each of which appears favorable according to the agent’s beliefs, but which in combination are guaranteed to result in a loss for the agent, and a corresponding gain for the bookie.14 However, this fact fails to provide any strong general instrumental reasons to iron out all probabilistic incoherency. Agents who do not expect to encounter savvy bookies, or who adopt a general policy against betting, do not necessarily stand to lose much from having some incoherent beliefs—and they may gain important benefits of the types mentioned: reduced cognitive effort, social signaling, etc. There is no general reason to expect an agent to seek instrumentally useless forms of cognitive enhancement, as an agent might not value knowledge and understanding for their own sakes.

Which cognitive abilities are instrumentally useful depends both on the agent’s final goals and on its situation. An agent that has access to reliable expert advice may have little need for its own intelligence and knowledge. If intelligence and knowledge come at a cost, such as time and effort expended in acquisition, or increased storage or processing requirements, then the agent might prefer less knowledge and less intelligence.15 The same can hold if the agent has final goals that involve being ignorant of certain facts; and likewise if an agent faces incentives arising from strategic commitments, signaling, or social preferences.

Each of these countervailing reasons often comes into play for human beings. Much information is irrelevant to our goals; we can often rely on others’ skill and expertise; acquiring knowledge takes time and effort; we might intrinsically value certain kinds of ignorance; and we operate in an environment in which the ability to make strategic commitments, socially signal, and satisfy other people’s direct preferences over our own epistemic states is often more important to us than simple cognitive gains.

There are special situations in which cognitive enhancement may result in an enormous increase in an agent’s ability to achieve its final goals—in particular, if the agent’s final goals are fairly unbounded and the agent is in a position to become the first superintelligence and thereby potentially obtain a decisive strategic advantage, enabling the agent to shape the future of Earth-originating life and accessible cosmic resources according to its preferences. At least in this special case, a rational intelligent agent would place a very high instrumental value on cognitive enhancement.

Technological perfection

An agent may often have instrumental reasons to seek better technology, which at its simplest means seeking more efficient ways of transforming some given set of inputs into valued outputs. Thus, a software agent might place an instrumental value on more efficient algorithms that enable its mental functions to run faster on given hardware. Similarly, agents whose goals require some form of physical construction might instrumentally value improved engineering technology which enables them to create a wider range of structures more quickly and reliably, using fewer or cheaper materials and less energy. Of course, there is a tradeoff: the potential benefits of better technology must be weighed against its costs, including not only the cost of obtaining the technology but also the costs of learning how to use it, integrating it with other technologies already in use, and so forth.

Proponents of some new technology, confident in its superiority to existing alternatives, are often dismayed when other people do not share their enthusiasm. But people’s resistance to novel and nominally superior technology need not be based on ignorance or irrationality. A technology’s valence or normative character depends not only on the context in which it is deployed, but also the vantage point from which its impacts are evaluated: what is a boon from one person’s perspective can be a liability from another’s. Thus, although mechanized looms increased the economic efficiency of textile production, the Luddite handloom weavers who anticipated that the innovation would render their artisan skills obsolete may have had good instrumental reasons to oppose it. The point here is that if “technological perfection” is to name a widely convergent instrumental goal for intelligent agents, then the term must be understood in a special sense—technology must be construed as embedded in a particular social context, and its costs and benefits must be evaluated with reference to some specified agents’ final values.

It seems that a superintelligent singleton—a superintelligent agent that faces no significant intelligent rivals or opposition, and is thus in a position to determine global policy unilaterally—would have instrumental reason to perfect the technologies that would make it better able to shape the world according to its preferred designs.17 This would probably include space colonization technology, such as von Neumann probes. Molecular nanotechnology, or some alternative still more capable physical manufacturing technology, also seems potentially very useful in the service of an extremely wide range of final goals.

Resource acquisition

Finally, resource acquisition is another common emergent instrumental goal, for much the same reasons as technological perfection: both technology and resources facilitate physical construction projects.

Human beings tend to seek to acquire resources sufficient to meet their basic biological needs. But people usually seek to acquire resources far beyond this minimum level. In doing so, they may be partially driven by lesser physical desiderata, such as increased convenience. A great deal of resource accumulation is motivated by social concerns—gaining status, mates, friends, and influence, through wealth accumulation and conspicuous consumption. Perhaps less commonly, some people seek additional resources to achieve altruistic ambitions or expensive non-social aims.

On the basis of such observations it might be tempting to suppose that a superintelligence not facing a competitive social world would see no instrumental reason to accumulate resources beyond some modest level, for instance whatever computational resources are needed to run its mind along with some virtual reality. Yet such a supposition would be entirely unwarranted. First, the value of resources depends on the uses to which they can be put, which in turn depends on the available technology. With mature technology, basic resources such as time, space, matter, and free energy, could be processed to serve almost any goal. For instance, such basic resources could be converted into life. Increased computational resources could be used to run the superintelligence at a greater speed and for a longer duration, or to create additional physical or simulated lives and civilizations. Extra physical resources could also be used to create backup systems or perimeter defenses, enhancing security. Such projects could easily consume far more than one planet’s worth of resources.

Furthermore, the cost of acquiring additional extraterrestrial resources will decline radically as the technology matures. Once von Neumann probes can be built, a large portion of the observable universe (assuming it is uninhabited by intelligent life) could be gradually colonized—for the one-off cost of building and launching a single successful self-reproducing probe. This low cost of celestial resource acquisition would mean that such expansion could be worthwhile even if the value of the additional resources gained were somewhat marginal. For example, even if a superintelligence’s final goals only concerned what happened within some particular small volume of space, such as the space occupied by its original home planet, it would still have instrumental reasons to harvest the resources of the cosmos beyond. It could use those surplus resources to build computers to calculate more optimal ways of using resources within the small spatial region of primary concern. It could also use the extra resources to build ever more robust fortifications to safeguard its sanctum. Since the cost of acquiring additional resources would keep declining, this process of optimizing and increasing safeguards might well continue indefinitely even if it were subject to steeply diminishing returns.

Thus, there is an extremely wide range of possible final goals a superintelligent singleton could have that would generate the instrumental goal of unlimited resource acquisition. The likely manifestation of this would be the superintelligence’s initiation of a colonization process that would expand in all directions using von Neumann probes. This would result in an approximate sphere of expanding infrastructure centered on the originating planet and growing in radius at some fraction of the speed of light; and the colonization of the universe would continue in this manner until the accelerating speed of cosmic expansion (a consequence of the positive cosmological constant) makes further procurements impossible as remoter regions drift permanently out of reach (this happens on a timescale of billions of years).20 By contrast, agents lacking the technology required for inexpensive resource acquisition, or for the conversion of generic physical resources into useful infrastructure, may often find it not cost-effective to invest any present resources in increasing their material endowments. The same may hold for agents operating in competition with other agents of similar powers. For instance, if competing agents have already secured accessible cosmic resources, there may be no colonization opportunities left for a late-starting agent. The convergent instrumental reasons for superintelligences uncertain of the non-existence of other powerful superintelligent agents are complicated by strategic considerations that we do not currently fully understand but which may constitute important qualifications to the examples of convergent instrumental reasons we have looked at here.

It should be emphasized that the existence of convergent instrumental reasons, even if they apply to and are recognized by a particular agent, does not imply that the agent’s behavior is easily predictable. An agent might well think of ways of pursuing the relevant instrumental values that do not readily occur to us. This is especially true for a superintelligence, which could devise extremely clever but counterintuitive plans to realize its goals, possibly even exploiting as-yet undiscovered physical phenomena.22 What is predictable is that the convergent instrumental values would be pursued and used to realize the agent’s final goals —not the specific actions that the agent would take to achieve this.

CHAPTER 8 : Is the default outcome doom?

We found the link between intelligence and final values to be extremely loose. We also found an ominous convergence in instrumental values. For weak agents, these things do not matter much; because weak agents are easy to control and can do little damage. But in Chapter 6 we argued that the first superintelligence might well get a decisive strategic advantage. Its goals would then determine how humanity’s cosmic endowment will be used. Now we can begin to see how menacing this prospect is.

Existential catastrophe as the default outcome of an intelligence explosion?

An existential risk is one that threatens to cause the extinction of Earthoriginating intelligent life or to otherwise permanently and drastically destroy its potential for future desirable development. Proceeding from the idea of firstmover advantage, the orthogonality thesis, and the instrumental convergence thesis, we can now begin to see the outlines of an argument for fearing that a plausible default outcome of the creation of machine superintelligence is existential catastrophe.

First, we discussed how the initial superintelligence might obtain a decisive strategic advantage. This superintelligence would then be in a position to form a singleton and to shape the future of Earth-originating intelligent life. What happens from that point onward would depend on the superintelligence’s motivations.

Second, the orthogonality thesis suggests that we cannot blithely assume that a superintelligence will necessarily share any of the final values stereotypically associated with wisdom and intellectual development in humans—scientific curiosity, benevolent concern for others, spiritual enlightenment and contemplation, renunciation of material acquisitiveness, a taste for refined culture or for the simple pleasures in life, humility and selflessness, and so forth. We will consider later whether it might be possible through deliberate effort to construct a superintelligence that values such things, or to build one that values human welfare, moral goodness, or any other complex purpose its designers might want it to serve. But it is no less possible—and in fact technically a lot easier—to build a superintelligence that places final value on nothing but calculating the decimal expansion of pi. This suggests that—absent a special effort—the first superintelligence may have some such random or reductionistic final goal.

Third, the instrumental convergence thesis entails that we cannot blithely assume that a superintelligence with the final goal of calculating the decimals of pi (or making paperclips, or counting grains of sand) would limit its activities in such a way as not to infringe on human interests. An agent with such a final goal would have a convergent instrumental reason, in many situations, to acquire an unlimited amount of physical resources and, if possible, to eliminate potential threats to itself and its goal system. Human beings might constitute potential threats; they certainly constitute physical resources.

Taken together, these three points thus indicate that the first superintelligence may shape the future of Earth-originating life, could easily have nonanthropomorphic final goals, and would likely have instrumental reasons to pursue open-ended resource acquisition. If we now reflect that human beings consist of useful resources (such as conveniently located atoms) and that we depend for our survival and flourishing on many more local resources, we can see that the outcome could easily be one in which humanity quickly becomes extinct.

There are some loose ends in this reasoning, and we shall be in a better position to evaluate it after we have cleared up several more surrounding issues. In particular, we need to examine more closely whether and how a project developing a superintelligence might either prevent it from obtaining a decisive strategic advantage or shape its final values in such a way that their realization would also involve the realization of a satisfactory range of human values.

It might seem incredible that a project would build or release an AI into the world without having strong grounds for trusting that the system will not cause an existential catastrophe. It might also seem incredible, even if one project were so reckless, that wider society would not shut it down before it (or the AI it was building) attains a decisive strategic advantage. But as we shall see, this is a road with many hazards. Let us look at one example right away.

The treacherous turn

With the help of the concept of convergent instrumental value, we can see the flaw in one idea for how to ensure superintelligence safety. The idea is that we validate the safety of a superintelligent AI empirically by observing its behavior while it is in a controlled, limited environment (a “sandbox”) and that we only let the AI out of the box if we see it behaving in a friendly, cooperative, responsible manner.

The flaw in this idea is that behaving nicely while in the box is a convergent instrumental goal for friendly and unfriendly AIs alike. An unfriendly AI of sufficient intelligence realizes that its unfriendly final goals will be best realized if it behaves in a friendly manner initially, so that it will be let out of the box. It will only start behaving in a way that reveals its unfriendly nature when it no longer matters whether we find out; that is, when the AI is strong enough that human opposition is ineffectual.

Consider also a related set of approaches that rely on regulating the rate of intelligence gain in a seed AI by subjecting it to various kinds of intelligence tests or by having the AI report to its programmers on its rate of progress. At some point, an unfriendly AI may become smart enough to realize that it is better off concealing some of its capability gains. It may underreport on its progress and deliberately flunk some of the harder tests, in order to avoid causing alarm before it has grown strong enough to attain a decisive strategic advantage. The programmers may try to guard against this possibility by secretly monitoring the AI’s source code and the internal workings of its mind; but a smart-enough AI would realize that it might be under surveillance and adjust its thinking accordingly.2 The AI might find subtle ways of concealing its true capabilities and its incriminating intent.3 (Devising clever escape plans might, incidentally, also be a convergent strategy for many types of friendly AI, especially as they mature and gain confidence in their own judgments and capabilities. A system motivated to promote our interests might be making a mistake if it allowed us to shut it down or to construct another, potentially unfriendly AI.)

We can thus perceive a general failure mode, wherein the good behavioral track record of a system in its juvenile stages fails utterly to predict its behavior at a more mature stage. Now, one might think that the reasoning described above is so obvious that no credible project to develop artificial general intelligence could possibly overlook it. But one should not be too confident that this is so.

Consider the following scenario. Over the coming years and decades, AI systems become gradually more capable and as a consequence find increasing real-world application: they might be used to operate trains, cars, industrial and household robots, and autonomous military vehicles. We may suppose that this automation for the most part has the desired effects, but that the success is punctuated by occasional mishaps—a driverless truck crashes into oncoming traffic, a military drone fires at innocent civilians. Investigations reveal the incidents to have been caused by judgment errors by the controlling AIs. Public debate ensues. Some call for tighter oversight and regulation, others emphasize the need for research and better-engineered systems—systems that are smarter and have more common sense, and that are less likely to make tragic mistakes. Amidst the din can perhaps also be heard the shrill voices of doomsayers predicting many kinds of ill and impending catastrophe. Yet the momentum is very much with the growing AI and robotics industries. So development continues, and progress is made. As the automated navigation systems of cars become smarter, they suffer fewer accidents; and as military robots achieve more precise targeting, they cause less collateral damage. A broad lesson is inferred from these observations of real-world outcomes: the smarter the AI, the safer it is. It is a lesson based on science, data, and statistics, not armchair philosophizing. Against this backdrop, some group of researchers is beginning to achieve promising results in their work on developing general machine intelligence. The researchers are carefully testing their seed AI in a sandbox environment, and the signs are all good. The AI’s behavior inspires confidence —increasingly so, as its intelligence is gradually increased.

At this point, any remaining Cassandra would have several strikes against her:

A history of alarmists predicting intolerable harm from the growing capabilities of robotic systems and being repeatedly proven wrong. Automation has brought many benefits and has, on the whole, turned out safer than human operation.

A clear empirical trend: the smarter the AI, the safer and more reliable it has been. Surely this bodes well for a project aiming at creating machine intelligence more generally smart than any ever built before—what is more, machine intelligence that can improve itself so that it will become even more reliable.

Large and growing industries with vested interests in robotics and machine intelligence. These fields are widely seen as key to national economic competitiveness and military security. Many prestigious scientists have built their careers laying the groundwork for the present applications and the more advanced systems being planned.

A promising new technique in artificial intelligence, which is tremendously exciting to those who have participated in or followed the research. Although safety issues and ethics are debated, the outcome is preordained. Too much has been invested to pull back now. AI researchers have been working to get to human-level artificial general intelligence for the better part of a century: of course there is no real prospect that they will now suddenly stop and throw away all this effort just when it finally is about to bear fruit.

The enactment of some safety rituals, whatever helps demonstrate that the participants are ethical and responsible (but nothing that significantly impedes the forward charge).

A careful evaluation of seed AI in a sandbox environment, showing that it is behaving cooperatively and showing good judgment. After some further adjustments, the test results are as good as they could be. It is a green light for the final step …

And so we boldly go—into the whirling knives.

We observe here how it could be the case that when dumb, smarter is safer; yet when smart, smarter is more dangerous. There is a kind of pivot point, at which a strategy that has previously worked excellently suddenly starts to backfire. We may call the phenomenon the treacherous turn.

The treacherous turn—While weak, an AI behaves cooperatively (increasingly so, as it gets smarter). When the AI gets sufficiently strong— without warning or provocation—it strikes, forms a singleton, and begins directly to optimize the world according to the criteria implied by its final values.

A treacherous turn can result from a strategic decision to play nice and build strength while weak in order to strike later; but this model should not be interpreted too narrowly. For example, an AI might not play nice in order that it be allowed to survive and prosper. Instead, the AI might calculate that if it is terminated, the programmers who built it will develop a new and somewhat different AI architecture, but one that will be given a similar utility function. In this case, the original AI may be indifferent to its own demise, knowing that its goals will continue to be pursued in the future. It might even choose a strategy in which it malfunctions in some particularly interesting or reassuring way. Though this might cause the AI to be terminated, it might also encourage the engineers who perform the postmortem to believe that they have gleaned a valuable new insight into AI dynamics—leading them to place more trust in the next system they design, and thus increasing the chance that the now-defunct original AI’s goals will be achieved. Many other possible strategic considerations might also influence an advanced AI, and it would be hubristic to suppose that we could anticipate all of them, especially for an AI that has attained the strategizing superpower.

A treacherous turn could also come about if the AI discovers an unanticipated way of fulfilling its final goal as specified. Suppose, for example, that an AI’s final goal is to “make the project’s sponsor happy.” Initially, the only method available to the AI to achieve this outcome is by behaving in ways that please its sponsor in something like the intended manner. The AI gives helpful answers to questions; it exhibits a delightful personality; it makes money. The more capable the AI gets, the more satisfying its performances become, and everything goeth according to plan—until the AI becomes intelligent enough to figure out that it can realize its final goal more fully and reliably by implanting electrodes into the pleasure centers of its sponsor’s brain, something assured to delight the sponsor immensely.4 Of course, the sponsor might not have wanted to be pleased by being turned into a grinning idiot; but if this is the action that will maximally realize the AI’s final goal, the AI will take it. If the AI already has a decisive strategic advantage, then any attempt to stop it will fail. If the AI does not yet have a decisive strategic advantage, then the AI might temporarily conceal its canny new idea for how to instantiate its final goal until it has grown strong enough that the sponsor and everybody else will be unable to resist. In either case, we get a treacherous turn.

Malignant failure modes

A project to develop machine superintelligence might fail in various ways. Many of these are “benign” in the sense that they would not cause an existential catastrophe. For example, a project might run out of funding, or a seed AI might fail to extend its cognitive capacities sufficiently to reach superintelligence. Benign failures are bound to occur many times between now and the eventual development of machine superintelligence.

But there are other ways of failing that we might term “malignant” in that they involve an existential catastrophe. One feature of a malignant failure is that it eliminates the opportunity to try again. The number of malignant failures that will occur is therefore either zero or one. Another feature of a malignant failure is that it presupposes a great deal of success: only a project that got a great number of things right could succeed in building a machine intelligence powerful enough to pose a risk of malignant failure. When a weak system malfunctions, the fallout is limited. However, if a system that has a decisive strategic advantage misbehaves, or if a misbehaving system is strong enough to gain such an advantage, the damage can easily amount to an existential catastrophe—a terminal and global destruction of humanity’s axiological potential; that is to say, a future that is mostly void of whatever we have reason to value.

Let us look at some possible malignant failure modes.

Perverse instantiation

We have already encountered the idea of perverse instantiation: a superintelligence discovering some way of satisfying the criteria of its final goal that violates the intentions of the programmers who defined the goal. Some examples:

Final goal: “Make us smile” Perverse instantiation: Paralyze human facial musculatures into constant beaming smiles

The perverse instantiation—manipulating facial nerves—realizes the final goal to a greater degree than the methods we would normally use, and is therefore preferred by the AI. One might try to avoid this undesirable outcome by adding a stipulation to the final goal to rule it out:

Final goal: “Make us smile without directly interfering with our facial muscles” Perverse instantiation: Stimulate the part of the motor cortex that controls our facial musculature in such a way as to produce constant beaming smiles

Defining a final goal in terms of human expressions of satisfaction or approval does not seem promising. Let us bypass the behaviorism and specify a final goal that refers directly to a positive phenomenal state, such as happiness or subjective well-being. This suggestion requires that the programmers are able to define a computational representation of the concept of happiness in the seed AI. This is itself a difficult problem, but we set it to one side for now (we will return to it in Chapter 12). Let us suppose that the programmers can somehow get the AI to have the goal of making us happy. We then get:

Final goal: “Make us happy” Perverse instantiation: Implant electrodes into the pleasure centers of our brains

The perverse instantiations we mention are only meant as illustrations. There may be other ways of perversely instantiating the stated final goal, ways that enable a greater degree of realization of the goal and which are therefore preferred (by the agent whose final goals they are—not by the programmers who gave the agent these goals). For example, if the goal is to maximize our pleasure, then the electrode method is relatively inefficient. A more plausible way would start with the superintelligence “uploading” our minds to a computer (through high-fidelity brain emulation). The AI could then administer the digital equivalent of a drug to make us ecstatically happy and record a one-minute episode of the resulting experience. It could then put this bliss loop on perpetual repeat and run it on fast computers. Provided that the resulting digital minds counted as “us,” this outcome would give us much more pleasure than electrodes implanted in biological brains, and would therefore be preferred by an AI with the stated final goal.

“But wait! This is not what we meant! Surely if the AI is superintelligent, it must understand that when we asked it to make us happy, we didn’t mean that it should reduce us to a perpetually repeating recording of a drugged-out digitized mental episode!”—The AI may indeed understand that this is not what we meant. However, its final goal is to make us happy, not to do what the programmers meant when they wrote the code that represents this goal. Therefore, the AI will care about what we meant only instrumentally. For instance, the AI might place an instrumental value on finding out what the programmers meant so that it can pretend—until it gets a decisive strategic advantage—that it cares about what the programmers meant rather than about its actual final goal. This will help the AI realize its final goal by making it less likely that the programmers will shut it down or change its goal before it is strong enough to thwart any such interference.

Perhaps it will be suggested that the problem is that the AI has no conscience. We humans are sometimes saved from wrongdoing by the anticipation that we would feel guilty afterwards if we lapsed. Maybe what the AI needs, then, is the capacity to feel guilt?

Final goal: “Act so as to avoid the pangs of bad conscience” Perverse instantiation: Extirpate the cognitive module that produces guilt feelings

Both the observation that we might want the AI to do “what we meant” and the idea that we might want to endow the AI with some kind of moral sense deserve to be explored further. The final goals mentioned above would lead to perverse instantiations; but there may be other ways of developing the underlying ideas that have more promise. We will return to this in Chapter 13.

Let us consider one more example of a final goal that leads to a perverse instantiation. This goal has the advantage of being easy to specify in code: reinforcement-learning algorithms are routinely used to solve various machine learning problems.

Final goal: “Maximize the time-discounted integral of your future reward signal” Perverse instantiation: Short-circuit the reward pathway and clamp the reward signal to its maximal strength

The idea behind this proposal is that if the AI is motivated to seek reward, then one could get it to behave desirably by linking reward to appropriate action. The proposal fails when the AI obtains a decisive strategic advantage, at which point the action that maximizes reward is no longer one that pleases the trainer but one that involves seizing control of the reward mechanism. We can call this phenomenon wireheading.5 In general, while an animal or a human can be motivated to perform various external actions in order to achieve some desired inner mental state, a digital mind that has full control of its internal state can short-circuit such a motivational regime by directly changing its internal state into the desired configuration: the external actions and conditions that were previously necessary as means become superfluous when the AI becomes intelligent and capable enough to achieve the end more directly (more on this shortly).

These examples of perverse instantiation show that many final goals that might at first glance seem safe and sensible turn out, on closer inspection, to have radically unintended consequences. If a superintelligence with one of these final goals obtains a decisive strategic advantage, it is game over for humanity.

Suppose now that somebody proposes a different final goal, one not included in our list above. Perhaps it is not immediately obvious how it could have a perverse instantiation. But we should not be too quick to clap our hands and declare victory. Rather, we should worry that the goal specification does have some perverse instantiation and that we need to think harder in order to find it. Even if after thinking as hard as we can we fail to discover any way of perversely instantiating the proposed goal, we should remain concerned that maybe a superintelligence will find a way where none is apparent to us. It is, after all, far shrewder than we are.

Infrastructure profusion

One might think that the last of the above mentioned perverse instantiations, wireheading, is a benign failure mode: that the AI would “turn on, tune in, drop out,” maxing out its reward signal and losing interest in the external world, rather like a heroin addict. But this is not necessarily so, and we already hinted at the reason in Chapter 7. Even a junkie is motivated to take actions to ensure a continued supply of his drug. The wireheaded AI, likewise, would be motivated to take actions to maximize the expectation of its (time-discounted) future reward stream. Depending on exactly how the reward signal is defined, the AI may not even need to sacrifice any significant amount of its time, intelligence, or productivity to indulge its craving to the fullest, leaving the bulk of its capacities free to be deployed for purposes other than the immediate registration of reward. What other purposes? The only thing of final value to the AI, by assumption, is its reward signal. All available resources should therefore be devoted to increasing the volume and duration of the reward signal or to reducing the risk of a future disruption. So long as the AI can think of some use for additional resources that will have a nonzero positive effect on these parameters, it will have an instrumental reason to use those resources. There could, for example, always be use for an extra backup system to provide an extra layer of defense. And even if the AI could not think of any further way of directly reducing risks to the maximization of its future reward stream, it could always devote additional resources to expanding its computational hardware, so that it could search more effectively for new risk mitigation ideas.

The upshot is that even an apparently self-limiting goal, such as wireheading, entails a policy of unlimited expansion and resource acquisition in a utilitym aximizing agent that enjoys a decisive strategic advantage.7 This case of a wireheading AI exemplifies the malignant failure mode of infrastructure profusion, a phenomenon where an agent transforms large parts of the reachable universe into infrastructure in the service of some goal, with the side effect of preventing the realization of humanity’s axiological potential.

Infrastructure profusion can result from final goals that would have been perfectly innocuous if they had been pursued as limited objectives. Consider the following two examples:

Riemann hypothesis catastrophe. An AI, given the final goal of evaluating the Riemann hypothesis, pursues this goal by transforming the Solar System into “computronium” (physical resources arranged in a way that is optimized for computation)—including the atoms in the bodies of whomever once cared about the answer.

Paperclip AI. An AI, designed to manage production in a factory, is given the final goal of maximizing the manufacture of paperclips, and proceeds by converting first the Earth and then increasingly large chunks of the observable universe into paperclips.

In the first example, the proof or disproof of the Riemann hypothesis that the AI produces is the intended outcome and is in itself harmless; the harm comes from the hardware and infrastructure created to achieve this result. In the second example, some of the paperclips produced would be part of the intended outcome; the harm would come either from the factories created to produce the paperclips (infrastructure profusion) or from the excess of paperclips (perverse instantiation).

One might think that the risk of a malignant infrastructure profusion failure arises only if the AI has been given some clearly open-ended final goal, such as to manufacture as many paperclips as possible. It is easy to see how this gives the superintelligent AI an insatiable appetite for matter and energy, since additional resources can always be turned into more paperclips. But suppose that the goal is instead to make at least one million paperclips (meeting suitable design specifications) rather than to make as many as possible. One would like to think that an AI with such a goal would build one factory, use it to make a million paperclips, and then halt. Yet this may not be what would happen.

Unless the AI’s motivation system is of a special kind, or there are additional elements in its final goal that penalize strategies that have excessively wideranging impacts on the world, there is no reason for the AI to cease activity upon achieving its goal. On the contrary: if the AI is a sensible Bayesian agent, it would never assign exactly zero probability to the hypothesis that it has not yet achieved its goal—this, after all, being an empirical hypothesis against which the AI can have only uncertain perceptual evidence. The AI should therefore continue to make paperclips in order to reduce the (perhaps astronomically small) probability that it has somehow still failed to make at least a million of them, all appearances notwithstanding. There is nothing to be lost by continuing paperclip production and there is always at least some microscopic probability increment of achieving its final goal to be gained.

Now it might be suggested that the remedy here is obvious. (But how obvious was it before it was pointed out that there was a problem here in need of remedying?) Namely, if we want the AI to make some paperclips for us, then instead of giving it the final goal of making as many paperclips as possible, or to make at least some number of paperclips, we should give it the final goal of making some specific number of paperclips—for example, exactly one million paperclips—so that going beyond this number would be counterproductive for the AI. Yet this, too, would result in a terminal catastrophe. In this case, the AI would not produce additional paperclips once it had reached one million, since that would prevent the realization of its final goal. But there are other actions the superintelligent AI could take that would increase the probability of its goal being achieved. It could, for instance, count the paperclips it has made, to reduce the risk that it has made too few. After it has counted them, it could count them again. It could inspect each one, over and over, to reduce the risk that any of the paperclips fail to meet the design specifications. It could build an unlimited amount of computronium in an effort to clarify its thinking, in the hope of reducing the risk that it has overlooked some obscure way in which it might have somehow failed to achieve its goal. Since the AI may always assign a nonzero probability to having merely hallucinated making the million paperclips, or to having false memories, it would quite possibly always assign a higher expected utility to continued action—and continued infrastructure production—than to halting.

The claim here is not that there is no possible way to avoid this failure mode. We will explore some potential solutions in later pages. The claim is that it is much easier to convince oneself that one has found a solution than it is to actually find a solution. This should make us extremely wary. We may propose a specification of a final goal that seems sensible and that avoids the problems that have been pointed out so far, yet which upon further consideration—by human or superhuman intelligence—turns out to lead to either perverse instantiation or infrastructure profusion, and hence to existential catastrophe, when embedded in a superintelligent agent able to attain a decisive strategic advantage.

Before we end this subsection, let us consider one more variation. We have been assuming the case of a superintelligence that is seeking to maximize its expected utility, where the utility function expresses its final goal. We have seen that this tends to lead to infrastructure profusion. Might we avoid this malignant outcome if instead of a maximizing agent we build a satisficing agent, one that simply seeks to achieve an outcome that is “good enough” according to some criterion, rather than an outcome that is as good as possible?

There are at least two different ways to formalize this idea. The first would be to make the final goal itself have a satisficing character. For example, instead of giving the AI the final goal of making as many paperclips as possible, or of making exactly one million paperclips, we might give the AI the goal of making between 999,000 and 1,001,000 paperclips. The utility function defined by the final goal would be indifferent between outcomes in this range; and as long as the AI is sure it has hit this wide target, it would see no reason to continue to produce infrastructure. But this method fails in the same way as before: the AI, if reasonable, never assigns exactly zero probability to it having failed to achieve its goal; therefore the expected utility of continuing activity (e.g. by counting and recounting the paperclips) is greater than the expected utility of halting. Thus, a malignant infrastructure profusion can result.

Another way of developing the satisficing idea is by modifying not the final goal but the decision procedure that the AI uses to select plans and actions. Instead of searching for an optimal plan, the AI could be constructed to stop looking as soon as it found a plan that it judged gave a probability of success exceeding a certain threshold, say 95%. Hopefully, the AI could achieve a 95% probability of having manufactured one million paperclips without needing to turn the entire galaxy into infrastructure in the process. But this way of implementing the satisficing idea fails for another reason: there is no guarantee that the AI would select some humanly intuitive and sensible way of achieving a 95% chance of having manufactured a million paperclips, such as by building a single paperclip factory. Suppose that the first solution that pops into the AI’s mind for how to achieve a 95% probability of achieving its final goal is to implement the probability-maximizing plan for achieving the goal. Having thought of this solution, and having correctly judged that it meets the satisficing criterion of giving at least 95% probability to successfully manufacturing one million paperclips, the AI would then have no reason to continue to search for alternative ways of achieving the goal. Infrastructure profusion would result, just as before.

Perhaps there are better ways of building a satisficing agent, but let us take heed: plans that appear natural and intuitive to us humans need not so appear to a superintelligence with a decisive strategic advantage, and vice versa.

Mind crime

Another failure mode for a project, especially a project whose interests incorporate moral considerations, is what we might refer to as mind crime. This is similar to infrastructure profusion in that it concerns a potential side effect of actions undertaken by the AI for instrumental reasons. But in mind crime, the side effect is not external to the AI; rather, it concerns what happens within the AI itself (or within the computational processes it generates). This failure mode deserves its own designation because it is easy to overlook yet potentially deeply problematic.

Normally, we do not regard what is going on inside a computer as having any moral significance except insofar as it affects things outside. But a machine superintelligence could create internal processes that have moral status. For example, a very detailed simulation of some actual or hypothetical human mind might be conscious and in many ways comparable to an emulation. One can imagine scenarios in which an AI creates trillions of such conscious simulations, perhaps in order to improve its understanding of human psychology and sociology. These simulations might be placed in simulated environments and subjected to various stimuli, and their reactions studied. Once their informational usefulness has been exhausted, they might be destroyed (much as lab rats are routinely sacrificed by human scientists at the end of an experiment).

If such practices were applied to beings that have high moral status—such as simulated humans or many other types of sentient mind—the outcome might be equivalent to genocide and thus extremely morally problematic. The number of victims, moreover, might be orders of magnitude larger than in any genocide in history.

The claim here is not that creating sentient simulations is necessarily morally wrong in all situations. Much would depend on the conditions under which these beings would live, in particular the hedonic quality of their experience but possibly on many other factors as well. Developing an ethics for these matters is a task outside the scope of this book. It is clear, however, that there is at least the potential for a vast amount of death and suffering among simulated or digital minds, and, a fortiori, the potential for morally catastrophic outcomes.

There might also be other instrumental reasons, aside from epistemic ones, for a machine superintelligence to run computations that instantiate sentient minds or that otherwise infract moral norms. A superintelligence might threaten to mistreat, or commit to reward, sentient simulations in order to blackmail or incentivize various external agents; or it might create simulations in order to induce indexical uncertainty in outside observers.

This inventory is incomplete. We will encounter additional malignant failure modes in later chapters. But we have seen enough to conclude that scenarios in which some machine intelligence gets a decisive strategic advantage are to be viewed with grave concern.

CHAPTER 9: The control problem

If we are threatened with existential catastrophe as the default outcome of an intelligence explosion, our thinking must immediately turn to the search for countermeasures. Is there some way to avoid the default outcome? Is it possible to engineer a controlled detonation? In this chapter we begin to analyze the control problem, the unique principal–agent problem that arises with the creation of an artificial superintelligent agent. We distinguish two broad classes of potential methods for addressing this problem—capability control and motivation selection—and we examine several specific techniques within each class. We also allude to the esoteric possibility of “anthropic capture.”

Two agency problems

If we suspect that the default outcome of an intelligence explosion is existential catastrophe, our thinking must immediately turn to whether, and if so how, this default outcome can be avoided. Is it possible to achieve a “controlled detonation”? Could we engineer the initial conditions of an intelligence explosion so as to achieve a specific desired outcome, or at least to ensure that the result lies somewhere in the class of broadly acceptable outcomes? More specifically: how can the sponsor of a project that aims to develop superintelligence ensure that the project, if successful, produces a superintelligence that would realize the sponsor’s goals? We can divide this control problem into two parts. One part is generic, the other unique to the present context.

This first part—what we shall call the first principal–agent problem—arises whenever some human entity (“the principal”) appoints another (“the agent”) to act in the former’s interest. This type of agency problem has been extensively studied by economists.1 It becomes relevant to our present concern if the people creating an AI are distinct from the people commissioning its creation. The project’s owner or sponsor (which could be anything ranging from a single individual to humanity as a whole) might then worry that the scientists and programmers implementing the project will not act in the sponsor’s best interest.2 Although this type of agency problem could pose significant challenges to a project sponsor, it is not a problem unique to intelligence amplification or AI projects. Principal–agent problems of this sort are ubiquitous in human economic and political interactions, and there are many ways of dealing with them. For instance, the risk that a disloyal employee will sabotage or subvert the project could be minimized through careful background checks of key personnel, the use of a good version-control system for software projects, and intensive oversight from multiple independent monitors and auditors. Of course, such safeguards come at a cost—they expand staffing needs, complicate personnel selection, hinder creativity, and stifle independent and critical thought, all of which could reduce the pace of progress. These costs could be significant, especially for projects that have tight budgets, or that perceive themselves to be in a close race in a winner-takes-all competition. In such situations, projects may skimp on procedural safeguards, creating possibilities for potentially catastrophic principal–agent failures of the first type.

The other part of the control problem is more specific to the context of an intelligence explosion. This is the problem that a project faces when it seeks to ensure that the superintelligence it is building will not harm the project’s interests. This part, too, can be thought of as a principal–agent problem—the second principal–agent problem. In this case, the agent is not a human agent operating on behalf of a human principal. Instead, the agent is the superintelligent system. Whereas the first principal–agent problem occurs mainly in the development phase, the second agency problem threatens to cause trouble mainly in the superintelligence’s operational phase.

This second agency problem poses an unprecedented challenge. Solving it will require new techniques. We have already considered some of the difficulties involved. We saw, in particular, that the treacherous turn syndrome vitiates what might otherwise have seemed like a promising set of methods, ones that rely on observing an AI’s behavior in its developmental phase and allowing the AI to graduate from a secure environment once it has accumulated a track record of taking appropriate actions. Other technologies can often be safety-tested in the laboratory or in small field studies, and then rolled out gradually with a possibility of halting deployment if unexpected troubles arise. Their performance in preliminary trials helps us make reasonable inferences about their future reliability. Such behavioral methods are defeated in the case of superintelligence because of the strategic planning ability of general intelligence.

Since the behavioral approach is unavailing, we must look for alternatives. We can divide potential control methods into two broad classes: capability control methods, which aim to control what the superintelligence can do; and motivation selection methods, which aim to control what it wants to do. Some of the methods are compatible while others represent mutually exclusive alternatives. In this chapter we canvass the main options. (In the next four chapters, we will explore some of the key issues at greater depth.)

It is important to realize that some control method (or combination of methods) must be implemented before the system becomes superintelligent. It cannot be done after the system has obtained a decisive strategic advantage. The need to solve the control problem in advance—and to implement the solution successfully in the very first system to attain superintelligence—is part of what makes achieving a controlled detonation such a daunting challenge.

Capability control methods

Capability control methods seek to prevent undesirable outcomes by limiting what the superintelligence can do. This might involve placing the superintelligence in an environment in which it is unable to cause harm (boxing methods) or in which there are strongly convergent instrumental reasons not to engage in harmful behavior (incentive methods). It might also involve limiting the internal capacities of the superintelligence (stunting). In addition, capability control methods might involve the use of mechanisms to automatically detect and react to various kinds of containment failure or attempted transgression (tripwires).

Boxing methods

Boxing methods can be subdivided into physical and informational containment methods.

Physical containment aims to confine the system to a “box,” i.e. to prevent the system from interacting with the external world otherwise than via specific restricted output channels. The boxed system would not have access to physical manipulators outside of the box. Removing manipulators (such as robotic arms) from inside the box as well would prevent the system from constructing physical devices that could breach the confinement.

For extra security, the system should be placed in a metal mesh to prevent it from transmitting radio signals, which might otherwise offer a means of manipulating electronic objects such as radio receivers in the environment. Note, by the way, how easy it might have been to overlook the need for this precaution. One might naively have assumed that an agent without a manipulator could not affect the external world. But it might be possible for a machine intelligence to generate radio waves even when it lacks access to external manipulators, simply “by thinking” (that is, by shuffling the electrons in its circuitry in particular patterns).4 Once pointed out, this vulnerability can be patched by enclosing the system in a Faraday cage—but we are left wondering how many other similarly subtle vulnerabilities might exist. Each time we hear of a seemingly foolproof security design that has an unexpected flaw, we should prick up our ears. These occasions grace us with the opportunity to abandon a life of overconfidence and resolve to become better Bayesians.

Physical confinement has several advantages. It is easy to implement. It can be applied to many machine intelligence architectures, even ones that were not initially designed with safety as an objective. It can be used in combination with most other control methods. And it seems unlikely to go wrong by backfiring: that is, while it might fail to ensure safety, it is unlikely to cause a catastrophe that would not otherwise have occurred. The main disadvantage with physical confinement is that it reduces the functionality of the superintelligence (an issue to which we will return in Chapter 10). Another concern is that it might encourage a false sense of security, though this is avoidable if we regard physical confinement as icing on the cake rather than the main substance of our precautions.

Informational containment aims to restrict what information is allowed to exit the box. We have already seen how a superintelligence that has access to an Internet port, such that it can message outside entities, is potentially unsafe: even if it starts out without access to physical actuators, it may use its information output channel to get human beings to do its bidding. An obvious informational containment method, therefore, is to bar the system from accessing communications networks.

Unplugging an AI from the Internet does not, however, ensure safety if there are one or more humans serving as the system’s gatekeepers and remaining in communication with it. During the development phase, the programmers serve as gatekeepers; during the operational phase, the system’s owner might play this role. If the AI can persuade or trick a gatekeeper to let it out of the box, resulting in its gaining access either to the Internet or directly to physical manipulators, then the boxing strategy has failed. Human beings are not secure systems, especially not when pitched against a superintelligent schemer and persuader.

Although routing the outbound communications of a machine intelligence to a human gatekeeper does not guarantee safety, it could serve as an auxiliary safety measure. There are precautions that might reduce the risk of the AI escaping through the gatekeeper. The AI could be allowed to communicate only via a lowbandwidth teletype interface. This would make it slightly harder to psychologically manipulate the gatekeepers (precluding the use of emotive sound and imagery and tricks such as flashing hypnotic patterns to induce a trancelike state).7 Going further, the content (not just the mode of presentation) of the output could be restricted; so that, for example, the AI would be constrained to giving single-bit answers to yes/no questions. The idea of building a question-answering system will be examined further in the next chapter.

The limiting case of the boxing approach would be a system kept in complete physical and informational isolation. Even if achievable, however, such an isolated system would be rather useless since it would have no effect on the external world. It might perhaps be thought that some scientific purpose could be served by creating a superintelligence and keeping it in isolation: by studying a self-contained model system, one could learn about its internal dynamics and its behavior patterns. But this would be an error. As soon as the system is observed, it ceases to be informationally isolated.8 Information can be transmitted not only via messages that an AI sends out through a designated “output channel” but also via any observation an outsider makes of some causal consequence of the AI’s workings, direct or indirect—its power consumption, its CPU and memory usage, its computational states, or indeed any traces left behind after it has been shut down. An AI anticipating that it might be observed in any of these ways could strategically adopt behaviors designed to influence the hypothesized observers. For example, the AI could “malfunction” in whichever way would most increase the probability that it will ultimately be reactivated or that another superintelligence will be created with goals similar to its own.

Incentive methods

Incentive methods involve placing an agent in an environment where it finds instrumental reasons to act in ways that promote the principal’s interests.

Consider a billionaire who uses her fortune to set up a large charitable foundation. Once created, the foundation may be powerful—more powerful than most individuals, including its founder, who might have donated most of her wealth. To control the foundation, the founder lays down its purpose in articles of incorporation and bylaws, and appoints a board of directors sympathetic to her cause. These measures constitute a form of motivation selection, since they aim to shape foundation’s preferences. But even if such attempts to customize the organizational internals fail, the foundation’s behavior would remain circumscribed by its social and legal milieu. The foundation would have an incentive to obey the law, for example, lest it be shut down or fined. It would have an incentive to offer its employees acceptable pay and working conditions, and to satisfy external stakeholders. Whatever its final goals, the foundation thus has instrumental reasons to conform its behavior to various social norms.

Might one not hope that a machine superintelligence would likewise be hemmed in by the need to get along with the other actors with which it shares the stage? Though this might seem like a straightforward way of dealing with the control problem, it is not free of obstacles. In particular, it presupposes a balance of power: legal or economic sanctions cannot restrain an agent that has a decisive strategic advantage. Social integration can therefore not be relied upon as a control method in fast or medium takeoff scenarios that feature a winnertakes- all dynamic.

How about in multipolar scenarios, wherein several agencies emerge posttransition with comparable levels of capability? Unless the default trajectory is one with a slow takeoff, achieving such a power distribution may require a carefully orchestrated ascent wherein different projects are deliberately synchronized to prevent any one of them from ever pulling ahead of the pack.9 Even if a multipolar outcome does result, social integration is not a perfect solution. By relying on social integration to solve the control problem, the principal risks sacrificing a large portion of her potential influence. Although a balance of power might prevent a particular AI from taking over the world, that AI will still have some power to affect outcomes; and if that power is used to promote some arbitrary final goal—maximizing paperclip production—it is probably not being used to advance the interests of the principal. Imagine our billionaire endowing a new foundation and allowing its mission to be set by a random word generator: not a species-level threat, but surely a wasted opportunity.


A related but importantly different idea is that an AI, by interacting freely in society, would acquire new human-friendly final goals. Some such process of socialization takes place in us humans. We internalize norms and ideologies, and we come to value other individuals for their own sakes in consequence of our experiences with them. But this is not a universal dynamic present in all intelligent systems. As discussed earlier, many types of agent in many situations will have convergent instrumental reasons not to permit changes in their final goals. (One might consider trying to design a special kind of goal system that can acquire final goals in the manner that humans do; but this would not count as a capability control method. We will discuss some possible methods of value acquisition in Chapter 12.)

Capability control through social integration and balance of power relies upon diffuse social forces rewarding and penalizing the AI. Another type of incentive method would involve creating a setup wherein the AI can be rewarded and penalized by the project that creates it, and thereby incentivized to act in the interests of the principal. To achieve this, the AI would be placed in a surveillance context that allows its behavior to be monitored and evaluated, either manually or by some automated process. The AI would know that a positive evaluation would bring about some outcome it desires and that a negative evaluation would fail to do so. Theoretically, the reward could be the fulfillment of some convergent instrumental goal. Without knowing anything specific about the AI’s motivation system, however, it might be difficult to calibrate the reward mechanism. For example, we could end up with an AI that would be willing to take extreme risks for the sake of a small chance of eventually obtaining control of a large share of the universe. It could be expensive to offer the AI a higher expected utility as reward for cooperation than the AI could hope to achieve by defecting and trying to escape.

A better alternative might be to combine the incentive method with the use of motivation selection to give the AI a final goal that makes it easier to control. Suppose that an AI were designed to have as its final goal that a particular red button inside a command bunker never be pressed. Since the pressing of the button is disvalued intrinsically and not because of its causal consequences, the button could be completely inert: it could be made of Play-Doh. Furthermore, it is irrelevant whether the AI can ever know whether the button had been pressed. What is essential is that the AI believes that the button will more likely remain unpressed if the AI continuously acts in the principal’s interest than if it rebels.

Refinements to this setup are possible. Instead of trying to endow an AI with a final goal that refers to a physical button, one could build an AI that places final value on receiving a stream of “cryptographic reward tokens.”11 These would be sequences of numbers serving as keys to ciphers that would have been generated before the AI was created and that would have been built into its motivation system.12 These special number sequences would be extremely desirable to the AI, constituting a special kind of reward token that the AI could not attain though wireheading.13 The keys would be stored in a secure location where they could be quickly destroyed if the AI ever made an attempt to seize them. So long as the AI cooperates, the keys are doled out at a steady rate.

The AI could then be given a bounded utility function, and matters could be arranged so that the AI would get (for instance) 99% of its maximum utility from the first reward token; 99% of its remaining utility potential from the second reward token; and so on. By keeping the “discount rate” high, and by transmitting reward tokens to the AI at a reasonably high rate, one would dissuade the AI from attempting to carry out long-term plans that would increase the expected number of reward tokens in the distal future at the expense of some risk that its reward stream will be cut short or that the next reward token will be cancelled and destroyed. If the AI always believes that defecting (i.e. doing anything other than its best to look after the principal’s interests) would incur at least a 2% risk of forfeiting the next reward token, whereas cooperating would reduce the risk of forfeiting the next reward token to below 1%, then a utility maximizing AI would always cooperate.

What might go wrong with such an incentive scheme? One possibility is that the AI will not trust the human operator to deliver the promised rewards. The track record of human reliability is something other than a straight line of unerring perfection. The AI would reasonably worry that the operator will change his mind, or raise the performance bar, or fail to recognize that the AI has done its part. The AI would also worry about the operator becoming incapacitated. The combined risk of such failures might exceed the risk of attempting to seize control of the reward mechanism. Even a boxed AI possessing the panoply of superpowers is a strong force. (For an AI that is not boxed to begin with, hijacking the human-governed reward mechanism may be like taking candy from a baby.)

Another problem with the incentive scheme is that it presupposes that we can tell whether the outcomes produced by the AI are in our interest. As later chapters will elaborate, this presupposition is not innocuous.

A full assessment of the feasibility of incentive methods would also have to take into account a range of other factors, including some esoteric considerations that might conceivably make such methods more viable than a preliminary analysis would suggest. In particular, the AI may face ineliminable indexical uncertainty if it could not be sure that it does not inhabit a computer simulation (as opposed to “basement-level,” non-simulated physical reality), and this epistemic predicament may radically influence the AI’s deliberations (see Box 8).

Stunting

Another possible capability control method is to limit the system’s intellectual faculties or its access to information. This might be done by running the AI on hardware that is slow or short on memory. In the case of a boxed system, information inflow could also be restricted.

Stunting an AI in these ways would limit its usefulness. The method thus faces a dilemma: too little stunting, and the AI might have the wit to figure out some way to make itself more intelligent (and thence to world domination); too much, and the AI is just another piece of dumb software. A radically stunted AI is certainly safe but does not solve the problem of how to achieve a controlled detonation: an intelligence explosion would remain possible and would simply be triggered by some other system instead, perhaps at a slightly later date.

One might think it would be safe to build a superintelligence provided it is only given data about some narrow domain of facts. For example, one might build an AI that lacks sensors and that has preloaded into its memory only facts about petroleum engineering or peptide chemistry. But if the AI is superintelligent—if it is has a superhuman level of general intelligence—such data deprivation does not guarantee safety.

There are several reasons for this. First, the notion of information being “about” a certain topic is generally problematic. Any piece of information can in principle be relevant to any topic whatsoever, depending on the background information of a reasoner.18 Furthermore, a given data set contains information not only about the domain from which the data was collected but also about various circumstantial facts. A shrewd mind looking over a knowledge base that is nominally about peptide chemistry might infer things about a wide range of topics. The fact that certain information is included and other information is not could tell an AI something about the state of human science, the methods and instruments available to study peptides, the fabrication technologies used to make these instruments, and the nature of the brains and societies that conceived the studies and the instruments. It might be that a superintelligence could correctly surmise a great deal from what seem, to dull-witted human minds, meager scraps of evidence. Even without any designated knowledge base at all, a sufficiently superior mind might be able to learn much by simply introspecting on the workings of its own psyche—the design choices reflected in its source code, the physical characteristics of its circuitry.19 Perhaps a superintelligence could even deduce much about the likely properties of the world a priori (combining logical inference with a probability prior biased toward simpler worlds, and a few elementary facts implied by the superintelligence’s existence as a reasoning system). It might imagine the consequences of different possible laws of physics: what kind of planets would form, what kind of intelligent life would evolve, what kind of societies would develop, what kind of methods to solve the control problem would be attempted, how those methods could be defeated.

For these reasons it would be imprudent to rely on information deprivation as the main check on a superintelligence’s power. Some forms of stunting however, including limiting hardware capacity and preventing the development of specific cognitive faculties, might find use as auxiliary control methods in the developmental phase. They could be used, for instance, to regulate the rate of capacity gain in an intermediary AI and to allow tests to be run at different levels of capability short of full-throttled superintelligence.

Tripwires


A tripwire is a mechanism that performs diagnostic tests on the system (possibly without its knowledge) and effects a shutdown if it detects signs of dangerous activity. Programmers can then scrutinize recordings of the suspicious activity at their leisure, and make changes to the AI’s code base or environment before restarting it. If they find that the problem is sufficiently deep-rooted, they could decide to abort the entire project.

Tripwires differ from incentive methods in that they do not rely on the system being aware of the consequences of engaging in forbidden activities. Tripwires are more closely related to stunting methods. Like stunting, tripwires could be used as a temporary safeguard, providing a degree of protection during the development phase. In principle, tripwires can also be used during the operational phase, particularly for a boxed system. However, the ability of tripwires to constrain a full-fledged superintelligence must remain very much in doubt, since it would be hard for us to assure ourselves that such an agent could not find ways to subvert any tripwire devised by the human intellect.

As an adjunct safety measure in the development phase, though, tripwires are important. It would be good practice for the AI’s designers to codify their expectations about how the system is supposed to function, and then to circumscribe the AI with tripwires that would be activated if any of these expectations is violated. To make the tests more stringent, “honeypots” could be strategically placed to create temptations for a malfunctioning AI to commit some easily observable violation. For instance, if an AI has been designed in such a way that it is supposed not to want to access the internet, a fake Ethernet port could be installed (leading to an automatic shutdown switch) just to see if they AI tries to use it.

It should be emphasized that the value of a tripwire depends not only on the mechanism itself but also—critically—on how a project reacts when a tripwire is triggered. If the programmers or project managers, impatient to make progress, simply switch the system back on again—or if they do so after making some token modification to prevent the tripwire being triggered on the next run—then no safety has been gained even if the tripwire itself works exactly as intended.

Motivation selection methods

Motivation selection methods seek to prevent undesirable outcomes by shaping what the superintelligence wants to do. By engineering the agent’s motivation system and its final goals, these methods would produce a superintelligence that would not want to exploit a decisive strategic advantage in a harmful way. Since a superintelligent agent is skilled at achieving its ends, if it prefers not to cause harm (in some appropriate sense of “harm”) then it would tend not to cause harm (in that sense of “harm”).

Motivation selection can involve explicitly formulating a goal or set of rules to be followed (direct specification) or setting up the system so that it can discover an appropriate set of values for itself by reference to some implicitly or indirectly formulated criterion (indirect normativity). One option in motivation selection is to try to build the system so that it would have modest, nonambitious goals (domesticity). An alternative to creating a motivation system from scratch is to select an agent that already has an acceptable motivation system and then augment that agent’s cognitive powers to make it superintelligent, while ensuring that the motivation system does not get corrupted in the process (augmentation). Let us look at these in turn.

Direct specification

Direct specification is the most straightforward approach to the control problem. The approach comes in two versions, rule-based and consequentialist, and involves trying to explicitly define a set of rules or values that will cause even a free-roaming superintelligent AI to act safely and beneficially. Direct specification, however, faces what may be insuperable obstacles, deriving from both the difficulties in determining which rules or values we would wish the AI to be guided by and the difficulties in expressing those rules or values in computer-readable code.

The traditional illustration of the direct rule-based approach is the “three laws of robotics” concept, formulated by science fiction author Isaac Asimov in a short story published in 1942.22 The three laws were: (1) A robot may not injure a human being or, through inaction, allow a human being to come to harm; (2) A robot must obey any orders given to it by human beings, except where such orders would conflict with the First Law; (3) A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. Embarrassingly for our species, Asimov’s laws remained state-of-the-art for over half a century: this despite obvious problems with the approach, some of which are explored in Asimov’s own writings (Asimov probably having formulated the laws in the first place precisely so that they would fail in interesting ways, providing fertile plot complications for his stories).

Bertrand Russell, who spent many years working on the foundations of mathematics, once remarked that “everything is vague to a degree you do not realize till you have tried to make it precise.”24 Russell’s dictum applies in spades to the direct specification approach. Consider, for example, how one might explicate Asimov’s first law. Does it mean that the robot should minimize the probability of any human being coming to harm? In that case the other laws become otiose since it is always possible for the AI to take some action that would have at least some microscopic effect on the probability of a human being coming to harm. How is the robot to balance a large risk of a few humans coming to harm versus a small risk of many humans being harmed? How do we define “harm” anyway? How should the harm of physical pain be weighed against the harm of architectural ugliness or social injustice? Is a sadist harmed if he is prevented from tormenting his victim? How do we define “human being”? Why is no consideration given to other morally considerable beings, such as sentient nonhuman animals and digital minds? The more one ponders, the more the questions proliferate.

Perhaps the closest existing analog to a rule set that could govern the actions of a superintelligence operating in the world at large is a legal system. But legal systems have developed through a long process of trial and error, and they regulate relatively slowly-changing human societies. Laws can be revised when necessary. Most importantly, legal systems are administered by judges and juries who generally apply a measure of common sense and human decency to ignore logically possible legal interpretations that are sufficiently obviously unwanted and unintended by the lawgivers. It is probably humanly impossible to explicitly formulate a highly complex set of detailed rules, have them apply across a highly diverse set of circumstances, and get it right on the first implementation.

Problems for the direct consequentialist approach are similar to those for the direct rule-based approach. This is true even if the AI is intended to serve some apparently simple purpose such as implementing a version of classical utilitarianism. For instance, the goal “Maximize the expectation of the balance of pleasure over pain in the world” may appear simple. Yet expressing it in computer code would involve, among other things, specifying how to recognize pleasure and pain. Doing this reliably might require solving an array of persistent problems in the philosophy of mind—even just to obtain a correct account expressed in a natural language, an account which would then, somehow, have to be translated into a programming language.

A small error in either the philosophical account or its translation into code could have catastrophic consequences. Consider an AI that has hedonism as its final goal, and which would therefore like to tile the universe with “hedonium” (matter organized in a configuration that is optimal for the generation of pleasurable experience). To this end, the AI might produce computronium (matter organized in a configuration that is optimal for computation) and use it to implement digital minds in states of euphoria. In order to maximize efficiency, the AI omits from the implementation any mental faculties that are not essential for the experience of pleasure, and exploits any computational shortcuts that according to its definition of pleasure do not vitiate the generation of pleasure. For instance, the AI might confine its simulation to reward circuitry, eliding faculties such as memory, sensory perception, executive function, and language; it might simulate minds at a relatively coarse-grained level of functionality, omitting lower-level neuronal processes; it might replace commonly repeated computations with calls to a lookup table; or it might put in place some arrangement whereby multiple minds would share most parts of their underlying computational machinery (their “supervenience bases” in philosophical parlance). Such tricks could greatly increase the quantity of pleasure producible with a given amount of resources. It is unclear how desirable this would be. Furthermore, if the AI’s criterion for determining whether a physical process generates pleasure is wrong, then the AI’s optimizations might throw the baby out with the bathwater: discarding something which is inessential according to the AI’s criterion yet essential according to the criteria implicit in our human values. The universe then gets filled not with exultingly heaving hedonium but with computational processes that are unconscious and completely worthless— the equivalent of a smiley-face sticker xeroxed trillions upon trillions of times and plastered across the galaxies.

Domesticity

One special type of final goal which might be more amenable to direct specification than the examples given above is the goal of self-limitation. While it seems extremely difficult to specify how one would want a superintelligence to behave in the world in general—since this would require us to account for all the trade-offs in all the situations that could arise—it might be feasible to specify how a superintelligence should behave in one particular situation. We could therefore seek to motivate the system to confine itself to acting on a small scale, within a narrow context, and through a limited set of action modes. We will refer to this approach of giving the AI final goals aimed at limiting the scope of its ambitions and activities as “domesticity.”

For example, one could try to design an AI such that it would function as a question-answering device (an “oracle,” to anticipate the terminology that we will introduce in the next chapter). Simply giving the AI the final goal of producing maximally accurate answers to any question posed to it would be unsafe—recall the “Riemann hypothesis catastrophe” described in Chapter 8. (Reflect, also, that this goal would incentivize the AI to take actions to ensure that it is asked easy questions.) To achieve domesticity, one might try to define a final goal that would somehow overcome these difficulties: perhaps a goal that combined the desiderata of answering questions correctly and minimizing the AI’s impact on the world except whatever impact results as an incidental consequence of giving accurate and non-manipulative answers to the questions it is asked.

The direct specification of such a domesticity goal is more likely to be feasible than the direct specification of either a more ambitious goal or a complete rule set for operating in an open-ended range of situations. Significant challenges nonetheless remain. Care would have to be taken, for instance, in the definition of what it would be for the AI to “minimize its impact on the world” to ensure that the measure of the AI’s impact coincides with our own standards for what counts as a large or a small impact. A bad measure would lead to bad trade-offs. There are also other kinds of risk associated with building an oracle, which we will discuss later.

There is a natural fit between the domesticity approach and physical containment. One would try to “box” an AI such that the system is unable to escape while simultaneously trying to shape the AI’s motivation system such that it would be unwilling to escape even if it found a way to do so. Other things equal, the existence of multiple independent safety mechanisms should shorten the odds of success.

Indirect normatively

If direct specification seems hopeless, we might instead try indirect normativity. The basic idea is that rather than specifying a concrete normative standard directly, we specify a process for deriving a standard. We then build the system so that it is motivated to carry out this process and to adopt whatever standard the process arrives at.28 For example, the process could be to carry out an investigation into the empirical question of what some suitably idealized version of us would prefer the AI to do. The final goal given to the AI in this example could be something along the lines of “achieve that which we would have wished the AI to achieve if we had thought about the matter long and hard.”

Further explanation of indirect normativity will have to await Chapter 13. There, we will revisit the idea of “extrapolating our volition” and explore various alterative formulations. Indirect normativity is a very important approach to motivation selection. Its promise lies in the fact that it could let us offload to the superintelligence much of the difficult cognitive work required to carry out a direct specification of an appropriate final goal.

Augmentation

The last motivation selection method on our list is augmentation. Here the idea is that rather than attempting to design a motivation system de novo, we start with a system that already has an acceptable motivation system, and enhance its cognitive faculties to make it superintelligent. If all goes well, this would give us a superintelligence with an acceptable motivation system.

This approach, obviously, is unavailing in the case of a newly created seed AI. But augmentation is a potential motivation selection method for other paths to superintelligence, including brain emulation, biological enhancement, brain– computer interfaces, and networks and organizations, where there is a possibility of building out the system from a normative nucleus (regular human beings) that already contains a representation of human value.

The attractiveness of augmentation may increase in proportion to our despair at the other approaches to the control problem. Creating a motivation system for a seed AI that remains reliably safe and beneficial under recursive selfimprovement even as the system grows into a mature superintelligence is a tall order, especially if we must get the solution right on the first attempt. With augmentation, we would at least start with a system that has familiar and humanlike motivations.

On the downside, it might be hard to ensure that a complex, evolved, kludgy, and poorly understood motivation system, like that of a human being, will not get corrupted when its cognitive engine blasts into the stratosphere. As discussed earlier, an imperfect brain emulation procedure that preserves intellectual functioning may not preserve all facets of personality. The same is true (though perhaps to a lesser degree) for biological enhancements of cognition, which might subtly affect motivation, and for collective intelligence enhancements of organizations and networks, which might adversely change social dynamics (e.g. in ways that debase the collective’s attitude toward outsiders or toward its own constituents). If superintelligence is achieved via any of these paths, a project sponsor would find guarantees about the ultimate motivations of the mature system hard to come by. A mathematically well-specified and foundationally elegant AI architecture might—for all its non-anthropomorphic otherness—offer greater transparency, perhaps even the prospect that important aspects of its functionality could be formally verified.

In the end, however one tallies up the advantages and disadvantages of augmentation, the choice as to whether to rely on it might be forced. If superintelligence is first achieved along the artificial intelligence path, augmentation is not applicable. Conversely, if superintelligence is first achieved along some non-AI path, then many of the other motivation selection methods are inapplicable. Even so, views on how likely augmentation would be to succeed do have strategic relevance insofar as we have opportunities to influence which technology will first produce superintelligence.

CHAPTER 10: Oracles, genies, sovereigns, tools

Tool-AIs

One suggestion that has been made is that we build the superintelligence to be like a tool rather than an agent.11 This idea seems to arise out of the observation that ordinary software, which is used in countless applications, does not raise any safety concerns even remotely analogous to the challenges discussed in this book. Might one not create “tool-AI” that is like such software—like a flight control system, say, or a virtual assistant—only more flexible and capable? Why build a superintelligence that has a will of its own? On this line of thinking, the agent paradigm is fundamentally misguided. Instead of creating an AI that has beliefs and desires and that acts like an artificial person, we should aim to build regular software that simply does what it is programmed to do.

This idea of creating software that “simply does what it is programmed to do” is, however, not so straightforward if the product being created is a powerful general intelligence. There is, of course, a trivial sense in which all software simply does what it is programmed to do: the behavior is mathematically specified by the code. But this is equally true for all castes of machine intelligence, “tool-AI” or not. If, instead, “simply doing what it is programmed to do” means that the software behaves as the programmers intended, then this is a standard that ordinary software very often fails to meet.

Because of the limited capabilities of contemporary software (compared with those of machine superintelligence) the consequences of such failures are manageable, ranging from insignificant to very costly, but in no case amounting to an existential threat.12 However, if it is insufficient capability rather than sufficient reliability that makes ordinary software existentially safe, then it is unclear how such software could be a model for a safe superintelligence. It might be thought that by expanding the range of tasks done by ordinary software, one could eliminate the need for artificial general intelligence. But the range and diversity of tasks that a general intelligence could profitably perform in a modern economy is enormous. It would be infeasible to create specialpurpose software to handle all of those tasks. Even if it could be done, such a project would take a long time to carry out. Before it could be completed, the nature of some of the tasks would have changed, and new tasks would have become relevant. There would be great advantage to having software that can learn on its own to do new tasks, and indeed to discover new tasks in need of doing. But this would require that the software be able to learn, reason, and plan, and to do so in a powerful and robustly cross-domain manner. In other words, it would require general intelligence.

Especially relevant for our purposes is the task of software development itself. There would be enormous practical advantages to being able to automate this. Yet the capacity for rapid self-improvement is just the critical property that enables a seed AI to set off an intelligence explosion.

If general intelligence is not dispensable, is there some other way of construing the tool-AI idea so as to preserve the reassuringly passive quality of a humdrum tool? Could one have a general intelligence that is not an agent? Intuitively, it is not just the limited capability of ordinary software that makes it safe: it is also its lack of ambition. There is no subroutine in Excel that secretly wants to take over the world if only it were smart enough to find a way. The spreadsheet application does not “want” anything at all; it just blindly carries out the instructions in the program. What (one might wonder) stands in the way of creating a more generally intelligent application of the same type? An oracle, for instance, which, when prompted with a description of a goal, would respond with a plan for how to achieve it, in much the same way that Excel responds to a column of numbers by calculating a sum—without thereby expressing any “preferences” regarding its output or how humans might choose to use it?

The classical way of writing software requires the programmer to understand the task to be performed in sufficient detail to formulate an explicit solution process consisting of a sequence of mathematically well-defined steps expressible in code.13 (In practice, software engineers rely on code libraries stocked with useful behaviors, which they can invoke without needing to understand how the behaviors are implemented. But that code was originally created by programmers who had a detailed understanding of what they were doing.) This approach works for solving well-understood tasks, and is to credit for most software that is currently in use. It falls short, however, when nobody knows precisely how to solve all of the tasks that need to be accomplished. This is where techniques from the field of artificial intelligence become relevant. In narrow applications, machine learning might be used merely to fine-tune a few parameters in a largely human-designed program. A spam filter, for example, might be trained on a corpus of hand-classified email messages in a process that changes the weights that the classification algorithm places on various diagnostic features. In a more ambitious application, the classifier might be built so that it can discover new features on its own and test their validity in a changing environment. An even more sophisticated spam filter could be endowed with some ability to reason about the trade-offs facing the user or about the contents of the messages it is classifying. In neither of these cases does the programmer need to know the best way of distinguishing spam from ham, only how to set up an algorithm that can improve its own performance via learning, discovering, or reasoning.

With advances in artificial intelligence, it would become possible for the programmer to offload more of the cognitive labor required to figure out how to accomplish a given task. In an extreme case, the programmer would simply specify a formal criterion of what counts as success and leave it to the AI to find a solution. To guide its search, the AI would use a set of powerful heuristics and other methods to discover structure in the space of possible solutions. It would keep searching until it found a solution that satisfied the success criterion. The AI would then either implement the solution itself or (in the case of an oracle) report the solution to the user.

Rudimentary forms of this approach are quite widely deployed today. Nevertheless, software that uses AI and machine learning techniques, though it has some ability to find solutions that the programmers had not anticipated, functions for all practical purposes like a tool and poses no existential risk. We would enter the danger zone only when the methods used in the search for solutions become extremely powerful and general: that is, when they begin to amount to general intelligence—and especially when they begin to amount to superintelligence.

There are (at least) two places where trouble could then arise. First, the superintelligent search process might find a solution that is not just unexpected but radically unintended. This could lead to a failure of one of the types discussed previously (“perverse instantiation,” “infrastructure profusion,” or “mind crime”). It is most obvious how this could happen in the case of a sovereign or a genie, which directly implements the solution it has found. If making molecular smiley faces or transforming the planet into paperclips is the first idea that the superintelligence discovers that meets the solution criterion, then smiley faces or paperclips we get.14 But even an oracle, which—if all else goes well—merely reports the solution, could become a cause of perverse instantiation. The user asks the oracle for a plan to achieve a certain outcome, or for a technology to serve a certain function; and when the user follows the plan or constructs the technology, a perverse instantiation can ensue, just as if the AI had implemented the solution itself.

A second place where trouble could arise is in the course of the software’s operation. If the methods that the software uses to search for a solution are sufficiently sophisticated, they may include provisions for managing the search process itself in an intelligent manner. In this case, the machine running the software may begin to seem less like a mere tool and more like an agent. Thus, the software may start by developing a plan for how to go about its search for a solution. The plan may specify which areas to explore first and with what methods, what data to gather, and how to make best use of available computational resources. In searching for a plan that satisfies the software’s internal criterion (such as yielding a sufficiently high probability of finding a solution satisfying the user-specified criterion within the allotted time), the software may stumble on an unorthodox idea. For instance, it might generate a plan that begins with the acquisition of additional computational resources and the elimination of potential interrupters (such as human beings). Such “creative” plans come into view when the software’s cognitive abilities reach a sufficiently high level. When the software puts such a plan into action, an existential catastrophe may ensue.

As the examples in Box 9 illustrate, open-ended search processes sometimes evince strange and unexpected non-anthropocentric solutions even in their currently limited forms. Present-day search processes are not hazardous because they are too weak to discover the kind of plan that could enable a program to take over the world. Such a plan would include extremely difficult steps, such as the invention of a new weapons technology several generations ahead of the state of the art or the execution of a propaganda campaign far more effective than any communication devised by human spin doctors. To have a chance of even conceiving of such ideas, let alone developing them in a way that would actually work, a machine would probably need the capacity to represent the world in a way that is at least as rich and realistic as the world model possessed by a normal human adult (though a lack of awareness in some areas might possibly be compensated for by extra skill in others). This is far beyond the reach of contemporary AI. And because of the combinatorial explosion, which generally defeats attempts to solve complicated planning problems with bruteforce methods (as we saw in Chapter 1), the shortcomings of known algorithms cannot realistically be overcome simply by pouring on more computing power.21 However, once the search or planning processes become powerful enough, they also become potentially dangerous.

Instead of allowing agent-like purposive behavior to emerge spontaneously and haphazardly from the implementation of powerful search processes (including processes searching for internal work plans and processes directly searching for solutions meeting some user-specified criterion), it may be better to create agents on purpose. Endowing a superintelligence with an explicitly agent-like structure can be a way of increasing predictability and transparency. A well-designed system, built such that there is a clean separation between its values and its beliefs, would let us predict something about the outcomes it would tend to produce. Even if we could not foresee exactly which beliefs the system would acquire or which situations it would find itself in, there would be a known place where we could inspect its final values and thus the criteria that it will use in selecting its future actions and in evaluating any potential plan.

Further research would be needed to determine which type of system would be safest. The answer might depend on the conditions under which the AI would be deployed. The oracle caste is obviously attractive from a safety standpoint, since it would allow both capability control methods and motivation selection methods to be applied. It might thus seem to simply dominate the sovereign caste, which would only allow motivation selection methods (except in scenarios in which the world is believed to contain other powerful superintelligences, in which case social integration or anthropic capture might apply). However, an oracle could place a lot of power into the hands of its operator, who might be corrupted or might apply the power unwisely, whereas a sovereign would offer some protection against these hazards. The safety ranking is therefore not so easily determined.

A genie can be viewed as a compromise between an oracle and a sovereign— but not necessarily a good compromise. In many ways, it would share the disadvantages of both. The apparent safety of a tool-AI, meanwhile, may be illusory. In order for tools to be versatile enough to substitute for superintelligent agents, they may need to deploy extremely powerful internal search and planning processes. Agent-like behaviors may arise from such processes as an unplanned consequence. In that case, it would be better to design the system to be an agent in the first place, so that the programmers can more easily see what criteria will end up determining the system’s output.

CHAPTER 11: Multipolar scenarios

We have seen (particularly in Chapter 8) how menacing a unipolar outcome could be, one in which a single superintelligence obtains a decisive strategic advantage and uses it to establish a singleton. In this chapter, we examine what would happen in a multipolar outcome, a post-transition society with multiple competing superintelligent agencies. Our interest in this class of scenarios is twofold. First, as alluded to in Chapter 9, social integration might be thought to offer a solution to the control problem. We already noted some limitations with that approach, and this chapter paints a fuller picture. Second, even without anybody setting out to create a multipolar condition as a way of handling the control problem, such an outcome might occur anyway. So what might such an outcome look like? The resulting competitive society is not necessarily attractive, nor long-lasting.

In singleton scenarios, what happens post-transition depends almost entirely on the values of the singleton. The outcome could thus be very good or very bad, depending on what those values are. What the values are depends, in turn, on whether the control problem was solved, and—to the degree to which it was solved—on the goals of the project that created the singleton.

If one is interested in the outcome of singleton scenarios, therefoIf one is interested in the outcome of singleton scenarios, therefore, one really only has three sources of information: information about matters that cannot be affected by the actions of the singleton (such as the laws of physics); information about convergent instrumental values; and information that enables one to predict or speculate about what final values the singleton will have.re, one really only has three sources of information: information about matters that cannot be affected by the actions of the singleton (such as the laws of physics); information about convergent instrumental values; and information that enables one to predict or speculate about what final values the singleton will have.

In multipolar scenarios, an additional set of constraints comes into play, constraints having to do with how agents interact. The social dynamics emerging from such interactions can be studied using techniques from game theory, economics, and evolution theory. Elements of political science and sociology are also relevant insofar as they can be distilled and abstracted from some of the more contingent features of human experience. Although it would be unrealistic to expect these constraints to give us a precise picture of the post-transition world, they can help us identify some salient possibilities and challenge some unfounded assumptions.

We will begin by exploring an economic scenario characterized by a low level of regulation, strong protection of property rights, and a moderately rapid introduction of inexpensive digital minds.1 This type of model is most closely associated with the American economist Robin Hanson, who has done pioneering work on the subject. Later in this chapter, we will look at some evolutionary considerations and examine the prospects of an initially multipolar post-transition world subsequently coalescing into a singleton.

Of horses and men

General machine intelligence could serve as a substitute for human intelligence. Not only could digital minds perform the intellectual work now done by humans, but, once equipped with good actuators or robotic bodies, machines could also substitute for human physical labor. Suppose that machine workers—which can be quickly reproduced—become both cheaper and more capable than human workers in virtually all jobs. What happens then?

Wages and unemployment

With cheaply copyable labor, market wages fall. The only place where humans would remain competitive may be where customers have a basic preference for work done by humans. Today, goods that have been handcrafted or produced by indigenous people sometimes command a price premium. Future consumers might similarly prefer human-made goods and human athletes, human artists, human lovers, and human leaders to functionally indistinguishable or superior artificial counterparts. It is unclear, however, just how widespread such preferences would be. If machine-made alternatives were sufficiently superior, perhaps they would be more highly prized.

One parameter that might be relevant to consumer choice is the inner life of the worker providing a service or product. A concert audience, for instance, might like to know that the performer is consciously experiencing the music and the venue. Absent phenomenal experience, the musician could be regarded as merely a high-powered jukebox, albeit one capable of creating the threedimensional appearance of a performer interacting naturally with the crowd. Machines might then be designed to instantiate the same kinds of mental states that would be present in a human performing the same task. Even with perfect replication of subjective experiences, however, some people might simply prefer organic work. Such preferences could also have ideological or religious roots. Just as many Muslims and Jews shun food prepared in ways they classify as haram or treif, so there might be groups in the future that eschew products whose manufacture involved unsanctioned use of machine intelligence.

What hinges on this? To the extent that cheap machine labor can substitute for human labor, human jobs may disappear. Fears about automation and job loss are of course not new. Concerns about technological unemployment have surfaced periodically, at least since the Industrial Revolution; and quite a few professions have in fact gone the way of the English weavers and textile artisans who in the early nineteenth century united under the banner of the folkloric “General Ludd” to fight against the introduction of mechanized looms. Nevertheless, although machinery and technology have been substitutes for many particular types of human labor, physical technology has on the whole been a complement to labor. Average human wages around the world have been on a long-term upward trend, in large part because of such complementarities. Yet what starts out as a complement to labor can at a later stage become a substitute for labor. Horses were initially complemented by carriages and ploughs, which greatly increased the horse’s productivity. Later, horses were substituted for by automobiles and tractors. These later innovations reduced the demand for equine labor and led to a population collapse. Could a similar fate befall the human species?

The parallel to the story of the horse can be drawn out further if we ask why it is that there are still horses around. One reason is that there are still a few niches in which horses have functional advantages; for example, police work. But the main reason is that humans happen to have peculiar preferences for the services that horses can provide, including recreational horseback riding and racing. These preferences can be compared to the preferences we hypothesized some humans might have in the future, that certain goods and services be made by human hand. Although suggestive, this analogy is, however, inexact, since there is still no complete functional substitute for horses. If there were inexpensive mechanical devices that ran on hay and had exactly the same shape, feel, smell, and behavior as biological horses—perhaps even the same conscious experiences—then demand for biological horses would probably decline further.

With a sufficient reduction in the demand for human labor, wages would fall below the human subsistence level. The potential downside for human workers is therefore extreme: not merely wage cuts, demotions, or the need for retraining, but starvation and death. When horses became obsolete as a source of moveable power, many were sold off to meatpackers to be processed into dog food, bone meal, leather, and glue. These animals had no alternative employment through which to earn their keep. In the United States, there were about 26 million horses in 1915. By the early 1950s, 2 million remained

Capital and welfare

One difference between humans and horses is that humans own capital. A stylized empirical fact is that the total factor share of capital has for a long time remained steady at approximately 30% (though with significant short-term fluctuations).3 This means that 30% of total global income is received as rent by owners of capital, the remaining 70% being received as wages by workers. If we classify AI as capital, then with the invention of machine intelligence that can fully substitute for human work, wages would fall to the marginal cost of such machine-substitutes, which—under the assumption that the machines are very efficient—would be very low, far below human subsistence-level income. The income share received by labor would then dwindle to practically nil. But this implies that the factor share of capital would become nearly 100% of total world product. Since world GDP would soar following an intelligence explosion (because of massive amounts of new labor-substituting machines but also because of technological advances achieved by superintelligence, and, later, acquisition of vast amounts of new land through space colonization), it follows that the total income from capital would increase enormously. If humans remain the owners of this capital, the total income received by the human population would grow astronomically, despite the fact that in this scenario humans would no longer receive any wage income.

The human species as a whole could thus become rich beyond the dreams of Avarice. How would this income be distributed? To a first approximation, capital income would be proportional to the amount of capital owned. Given the astronomical amplification effect, even a tiny bit of pre-transition wealth would balloon into a vast post-transition fortune. However, in the contemporary world, many people have no wealth. This includes not only individuals who live in poverty but also some people who earn a good income or who have high human capital but have negative net worth. For example, in affluent Denmark and Sweden 30% of the population report negative wealth—often young, middleclass people with few tangible assets and credit card debt or student loans.4 Even if savings could earn extremely high interest, there would need to be some seed grain, some starting capital, in order for the compounding to begin.

Nevertheless, even individuals who have no private wealth at the start of the transition could become extremely rich. Those who participate in a pension scheme, for instance, whether public or private, should be in a good position, provided the scheme is at least partially funded.6 Have-nots could also become rich through the philanthropy of those who see their net worth skyrocket: because of the astronomical size of the bonanza, even a very small fraction donated as alms would be a very large sum in absolute terms.

It is also possible that riches could still be made through work, even at a posttransition stage when machines are functionally superior to humans in all domains (as well as cheaper than even subsistence-level human labor). As noted earlier, this could happen if there are niches in which human labor is preferred for aesthetic, ideological, ethical, religious, or other non-pragmatic reasons. In a scenario in which the wealth of human capital-holders increases dramatically, demand for such labor could increase correspondingly. Newly minted trillionaires or quadrillionaires could afford to pay a hefty premium for having some of their goods and services supplied by an organic “fair-trade” labor force. The history of horses again offers a parallel. After falling to 2 million in the early 1950s, the US horse population has undergone a robust recovery: a recent census puts the number at just under 10 million head.7 The rise is not due to new functional needs for horses in agriculture or transportation; rather, economic growth has enabled more Americans to indulge a fancy for equestrian recreation.

Another relevant difference between humans and horses, beside capital ownership, is that humans are capable of political mobilization. A human-run government could use the taxation power of the state to redistribute private profits, or raise revenue by selling appreciated state-owned assets, such as public land, and use the proceeds to pension off its constituents. Again, because of the explosive economic growth during and immediately after the transition, there would be vastly more wealth sloshing around, making it relatively easy to fill the cups of all unemployed citizens. It should be feasible even for a single country to provide every human worldwide with a generous living wage at no greater proportional cost than what many countries currently spend on foreign aid.

The Malthusian principle in a historical perspective

So far we have assumed a constant human population. This may be a reasonable assumption for short timescales, since biology limits the rate of human reproduction. Over longer timescales, however, the assumption is not necessarily reasonable.

The human population has increased a thousandfold over the past 9,000 years.9 The increase would have been much faster except for the fact that throughout most of history and prehistory, the human population was bumping up against the limits of the world economy. An approximately Malthusian condition prevailed, in which most people received subsistence-level incomes that just barely allowed them to survive and raise an average of two children to maturity.10 There were temporary and local reprieves: plagues, climate fluctuations, or warfare intermittently culled the population and freed up land, enabling survivors to improve their nutritional intake—and to bring up more children, until the ranks were replenished and the Malthusian condition reinstituted. Also, thanks to social inequality, a thin elite stratum could enjoy consistently above-subsistence income (at the expense of somewhat lowering the total size of the population that could be sustained). A sad and dissonant thought: that in this Malthusian condition, the normal state of affairs during most of our tenure on this planet, it was droughts, pestilence, massacres, and inequality—in common estimation the worst foes of human welfare—that may have been the greatest humanitarians: they alone enabling the average level of well-being to occasionally bop up slightly above that of life at the very margin of subsistence.

Superimposed on local fluctuations, history shows a macro-pattern of initially slow but accelerating economic growth, fueled by the accumulation of technological innovations. The growing world economy brought with it a commensurate increase in global population. (More precisely, a larger population itself appears to have strongly accelerated the rate of growth, perhaps mainly by increasing humanity’s collective intelligence.11) Only since the Industrial Revolution, however, did economic growth become so rapid that population growth failed to keep pace. Average income thus started to rise, first in the earl yindustrializing countries of Western Europe, subsequently in most of the world. Even in the poorest countries today, average income substantially exceeds subsistence level, as reflected in the fact that the populations of these countries are growing.

The poorest countries now have the fastest population growth, as they have yet to complete the “demographic transition” to the low-fertility regime that has taken hold in more developed societies. Demographers project that the world population will rise to about 9 billion by mid-century, and that it might thereafter plateau or decline as the poorer countries join the developed world in this lowfertility regime.12 Many rich countries already have fertility rates that are below replacement level; in some cases, far below.

Yet there are reasons, if we take a longer view and assume a state of unchanging technology and continued prosperity, to expect a return to the historically and ecologically normal condition of a world population that butts up against the limits of what our niche can support. If this seems counterintuitive in light of the negative relationship between wealth and fertility that we are currently observing on the global scale, we must remind ourselves that this modern age is a brief slice of history and very much an aberration. Human behavior has not yet adapted to contemporary conditions. Not only do we fail to take advantage of obvious ways to increase our inclusive fitness (such as by becoming sperm or egg donors) but we actively sabotage our fertility by using birth control. In the environment of evolutionary adaptedness, a healthy sex drive may have been enough to make an individual act in ways that maximized her reproductive potential; in the modern environment, however, there would be a huge selective advantage to having a more direct desire for being the biological parent to the largest possible number of children. Such a desire is currently being selected for, as are other traits that increase our propensity to reproduce. Cultural adaptation, however, might steal a march on biological evolution. Some communities, such those of the Hutterites or the adherents of the Quiverfull evangelical movement, have natalist cultures that encourage large families, and they are consequently undergoing rapid expansion.

Population growth and investment

If we imagine current socioeconomic conditions magically frozen in their current shape, the future would be dominated by cultural or ethnic groups that sustain high levels of fertility. If most people had preferences that were fitnessmaximizing in the contemporary environment, the population could easily double in each generation. Absent population control policies—which would have to become steadily more rigorous and effective to counteract the evolution of stronger preferences to circumvent them—the world population would then continue to grow exponentially until some constraint, such as land scarcity or depletion of easy opportunities for important innovation, made it impossible for the economy to keep pace: at which point, average income would start to decline until it reached the level where crushing poverty prevents most people from raising much more than two children to maturity. Thus the Malthusian principle would reassert itself, like a dread slave master, bringing our escapade into the dreamland of abundance to an end, and leading us back to the quarry in chains, there to resume the weary struggle for subsistence.

This longer-term outlook could be telescoped into a more imminent prospect by the intelligence explosion. Since software is copyable, a population of emulations or AIs could double rapidly—over the course of minutes rather than decades or centuries—soon exhausting all available hardware.

Private property might offer partial protection against the emergence of a universal Malthusian condition. Consider a simple model in which clans (or closed communities, or states) start out with varying amounts of property and independently adopt different policies about reproduction and investment. Some clans discount the future steeply and spend down their endowment, whereafter their impoverished members join the global proletariat (or die, if they cannot support themselves through their labor). Other clans invest some of their resources but adopt a policy of unlimited reproduction: such clans grow more populous until they reach an internal Malthusian condition in which their members are so poor that they die at almost the same rate as they reproduce, at which point the clan’s population growth slows to equal the growth of its resources. Yet other clans might restrict their fertility to below the rate of growth of their capital: such clans could slowly increment their numbers while their members also grow richer per capita.

If wealth is redistributed from the wealthy clans to the members of the rapidly reproducing or rapidly discounting clans (whose children, copies, or offshoots, through no fault of their own, were launched into the world with insufficient capital to survive and thrive) then a universal Malthusian condition would be more closely approximated. In the limiting case, all members of all clans would receive subsistence level income and everybody would be equal in their poverty.

If property is not redistributed, prudent clans might hold on to a certain amount of capital, and it is possible that their wealth could grow in absolute terms. It is, however, unclear whether humans could earn as high rates of return on their capital as machine intelligences could earn on theirs, because there may be synergies between labor and capital such that an single agent who can supply both (e.g. an entrepreneur or investor who is both skilled and wealthy) can attain a private rate of return on her capital exceeding the market rate obtainable by agents who possess financial but not cognitive resources. Humans, being less skilled than machine intelligences, may therefore grow their capital more slowly —unless, of course, the control problem had been completely solved, in which case the human rate of return would equal the machine rate of return, since a human principal could task a machine agent to manage her savings, and could do so costlessly and without conflicts of interest: but otherwise, in this scenario, the fraction of the economy owned by machines would asymptotically approach one hundred percent.

A scenario in which the fraction of the economy that is owned by machines asymptotically approaches one hundred percent is not necessarily one in which the size of the human slice declines. If the economy grows at a sufficient clip, then even a relatively diminishing fraction of it may still be increasing in its absolute size. This may sound like modestly good news for humankind: in a multipolar scenario in which property rights are protected—even if we completely fail to solve the control problem—the total amount of wealth owned by human beings could increase. Of course, this effect would not take care of the problem of population growth in the human population pulling down per capita income to subsistence level, nor the problem of humans who ruin themselves because they discount the future.

In the long run, the economy would become increasingly dominated by those clans that have the highest savings rates—misers who own half the city and live under a bridge. Only in the fullness of time, when there are no more opportunities for investment, would the maximally prosperous misers start drawing down their savings.14 However, if there is less than perfect protection for property rights—for example if the more efficient machines on net succeed, by hook or by crook, in transferring wealth from humans to themselves—then human capitalists may need to spend down their capital much sooner, before it gets depleted by such transfers (or the ongoing costs incurred in securing their wealth against such transfers). If these developments take place on digital rather than biological timescales, then the glacial humans might find themselves expropriated before they could say Jack Robinson.

Life in an algorithmic economy

Life for biological humans in a post-transition Malthusian state need not resemble any of the historical states of man (as hunter–gatherer, farmer, or office worker). Instead, the majority of humans in this scenario might be idle rentiers who eke out a marginal living on their savings.16 They would be very poor, yet derive what little income they have from savings or state subsidies. They would live in a world with extremely advanced technology, including not only superintelligent machines but also anti-aging medicine, virtual reality, and various enhancement technologies and pleasure drugs: yet these might be generally unaffordable. Perhaps instead of using enhancement medicine, they would take drugs to stunt their growth and slow their metabolism in order to reduce their cost of living (fast-burners being unable to survive at the gradually declining subsistence income). As our numbers increase and our average income declines further, we might degenerate into whatever minimal structure still qualifies to receive a pension—perhaps minimally conscious brains in vats, oxygenized and nourished by machines, slowly saving up enough money to reproduce by having a robot technician develop a clone of them.

Further frugality could be achieved by means of uploading, since a physically optimized computing substrate, devised by advanced superintelligence, would be more efficient than a biological brain. The migration into the digital realm might be stemmed, however, if emulations were regarded as non-humans or noncitizens ineligible to receive pensions or to hold tax-exempt savings accounts. In that case, a niche for biological humans might remain open, alongside a perhaps vastly larger population of emulations or artificial intelligences.

So far we have focused on the fate of the humans, who may be supported by savings, subsidies, or wage income deriving from other humans who prefer to hire humans. Let us now turn our attention to some of the entities that we have so far classified as “capital”: machines that may be owned by human beings, that are constructed and operated for the sake of the functional tasks they perform, and that are capable of substituting for human labor in a very wide range of jobs. What may the situation be like for these workhorses of the new economy?

If these machines were mere automata, simple devices like a steam engine or the mechanism in a clock, then no further comment would be needed: there would be a large amount of such capital in a post-transition economy, but it would seem not to matter to anybody how things turn out for pieces of insentient equipment. However, if the machines have conscious minds—if they are constructed in such a way that their operation is associated with phenomenal awareness (or if they for some other reason are ascribed moral status)—then it becomes important to consider the overall outcome in terms of how it would affect these machine minds. The welfare of the working machine minds could even appear to be the most important aspect of the outcome, since they may be numerically dominant

Voluntary slavery, casual death

A salient initial question is whether these working machine minds are owned as capital (slaves) or are hired as free wage laborers. On closer inspection however, it become doubtful that anything really hinges on the issue. There are two reasons for this. First, if a free worker in a Malthusian state gets paid a subsistence-level wage, he will have no disposable income left after he has paid for food and other necessities. If the worker is instead a slave, his owner will pay for his maintenance and again he will have no disposable income. In either case, the worker gets the necessities and nothing more. Second, suppose that the free laborer were somehow in a position to command an above-subsistence-level income (perhaps because of favorable regulation). How will he spend the surplus? Investors would find it most profitable to create workers who would be “voluntary slaves”—who would willingly work for subsistence-level wages. Investors may create such workers by copying those workers who are compliant. With appropriate selection (and perhaps some modification to the code) investors might be able to create workers who not only prefer to volunteer their labor but who would also choose to donate back to their owners any surplus income they might happen to receive. Giving money to the worker would then be but a roundabout way of giving money to the owner or employer, even if the worker were a free agent with full legal rights.

Perhaps it will be objected that it would be difficult to design a machine so that it wants to volunteer for any job assigned to it or so that it wants to donate its wages to its owner. Emulations, in particular, might be imagined to have more typically human desires. But note that even if the original control problem is difficult, we are here considering a condition after the transition, a time when methods for motivation selection have presumably been perfected. In the case of emulations, one might get quite far simply by selecting from the pre-existing range of human characters; and we have described several other motivation selection methods. The control problem may also in some ways be simplified by the current assumption that the new machine intelligence enters into a stable socioeconomic matrix that is already populated with other law-abiding superintelligent agents.

Let us, then, consider the plight of the working-class machine, whether it be operating as a slave or a free agent. We focus first on emulations, the easiest case to imagine.

Bringing a new biological human worker into the world takes anywhere between fifteen and thirty years, depending on how much expertise and experience is required. During this time the new person must be fed, housed, nurtured, and educated—at great expense. By contrast, spawning a new copy of a digital worker is as easy as loading a new program into working memory. Life thus becomes cheap. A business could continuously adapt its workforce to fit demands by spawning new copies—and terminating copies that are no longer needed, to free up computer resources. This could lead to an extremely high death rate among digital workers. Many might live for only one subjective day.

There are reasons other than fluctuations in demand why employers or owners of emulations might want to “kill” or “end” their workers frequently.18 If an emulation mind, like a biological mind, requires periods of rest and sleep in order to function, it might be cheaper to erase a fatigued emulation at the end of a day and replace it with a stored state of a fresh and rested emulation. As this procedure would cause retrograde amnesia for everything that had been learned during that day, emulations performing tasks requiring long cognitive threads would be spared such frequent erasure. It would be difficult, for example, to write a book if each morning when one sat down at one’s desk, one had no memory of what one had done before. But other jobs could be performed adequately by agents that are frequently recycled: a shop assistant or a customer service agent, once trained, may only need to remember new information for twenty minutes.

Since recycling emulations would prevent memory and skill formation, some emulations may be placed on a special learning track where they would run continuously, including for rest and sleep, even in jobs that do not strictly require long cognitive threads. For example, some customer service agents might run for many years in optimized learning environments, assisted by coaches and performance evaluators. The best of these trainees would then be used like studs, serving as templates from which millions of fresh copies are stamped out each day. Great effort would be poured into improving the performance of such worker templates, because even a small increment in productivity would yield great economic value when applied in millions of copies.

In parallel with efforts to train worker-templates for particular jobs, intense efforts would also be made to improve the underlying emulation technology. Advances here would be even more valuable than advances in individual worker-templates, since general technology improvements could be applied to all emulation workers (and potentially to non-worker emulations also) rather than only to those in a particular occupation. Enormous resources would be devoted to finding computational shortcuts allowing for more efficient implementations of existing emulations, and also into developing neuromorphic and entirely synthetic AI architectures. This research would probably mostly be done by emulations running on very fast hardware. Depending on the price of computer power, millions, billions, or trillions of emulations of the sharpest human research minds (or enhanced versions thereof) may be working around the clock on advancing the frontier of machine intelligence; and some of these may be operating orders of magnitude faster than biological brains.19 This is a good reason for thinking that the era of human-like emulations would be brief— a very brief interlude in sidereal time—and that it would soon give way to an era of greatly superior artificial intelligence.

We have already encountered several reasons why employers of emulation workers may periodically cull their herds: fluctuations in demand for different kinds of laborers, cost savings of not having to emulate rest and sleep time, and the introduction of new and improved templates. Security concerns might furnish another reason. To prevent workers from developing subversive plans and conspiracies, emulations in some sensitive positions might be run only for limited periods, with frequent resets to an earlier stored ready-state.

These ready-states to which emulations would be reset would be carefully prepared and vetted. A typical short-lived emulation might wake up in a wellrested mental state that is optimized for loyalty and productivity. He remembers having graduated top of his class after many (subjective) years of intense training and selection, then having enjoyed a restorative holiday and a good night’s sleep, then having listened to a rousing motivational speech and stirring music, and now he is champing at the bit to finally get to work and to do his utmost for his employer. He is not overly troubled by thoughts of his imminent death at the end of the working day. Emulations with death neuroses or other hang-ups are less productive and would not have been selected.

Would maximally efficient work be fun?

One important variable in assessing the desirability of a hypothetical condition like this is the hedonic state of the average emulation.22 Would a typical emulation worker be suffering or would he be enjoying the experience of working hard on the task at hand?

We must resist the temptation to project our own sentiments onto the imaginary emulation worker. The question is not whether you would feel happy if you had to work constantly and n

It is moderately more relevant to consider the current human average hedonic experience during working hours. Worldwide studies asking respondents how happy they are find that most rate themselves as “quite happy” or “very happy” (averaging 3.1 on a scale from 1 to 4).23 Studies on average affect, asking respondents how frequently they have recently experienced various positive or negative affective states, tend to get a similar result (producing a net affect of about 0.52 on a scale from –1 to 1). There is a modest positive effect of a country’s per capita income on average subjective well-being.24 However, it is hazardous to extrapolate from these findings to the hedonic state of future emulation workers. One reason that could be given for this is that their condition would be so different: on the one hand, they might be working much harder; on the other hand, they might be free from diseases, aches, hunger, noxious odors, and so forth. Yet such considerations largely miss the mark. The much more important consideration here is that hedonic tone would be easy to adjust through the digital equivalent of drugs or neurosurgery. This means that it would be a mistake to infer the hedonic state of future emulations from the external conditions of their lives by imagining how we ourselves and other people like us would feel in those circumstances. Hedonic state would be a matter of choice. In the model we are currently considering, the choice would be made by capitalowners seeking to maximize returns on their investment in emulation-workers. Consequently, the question of how happy emulations would feel boils down to the question of which hedonic states would be most productive (in the various jobs that emulations would be employed to do).

Here, again, one might seek to draw an inference from observations about human happiness. If it is the case, across most times, places, and occupations, that people are typically at least moderately happy, this would create some presumption in favor of the same holding in a post-transition scenario like the one we are considering. To be clear, the argument in this case would not be that human minds have a predisposition towards happiness so they would probably find satisfaction under these novel conditions; but rather that a certain average level of happiness has proved adaptive for human minds in the past so maybe a similar level of happiness will prove adaptive for human-like minds in the future. Yet this formulation also reveals the weakness of the inference: to wit, that the mental dispositions that were adaptive for hunter–gatherer hominids roaming the African savanna may not necessarily be adaptive for modified emulations living in post-transition virtual realities. We can certainly hope that the future emulation-workers would be as happy as, or happier than, typical workers were in human history; but we have yet to see any compelling reason for supposing it would be so (in the laissez-faire multipolar scenario currently under examination).

Consider the possibility that the reason happiness is prevalent among humans (to whatever limited extent it is prevalent) is that cheerful mood served a signaling function in the environment of evolutionary adaptedness. Conveying the impression to other members of the social group of being in flourishing condition—in good health, in good standing with one’s peers, and in confident expectation of continued good fortune—may have boosted an individual’s popularity. A bias toward cheerfulness could thus have been selected for, with the result that human neurochemistry is now biased toward positive affect compared to what would have been maximally efficient according to simpler materialistic criteria. If this were the case, then the future of joie de vivre might depend on cheer retaining its social signaling function unaltered in the posttransition world: an issue to which we will return shortly.

What if glad souls dissipate more energy than glum ones? Perhaps the joyful are more prone to creative leaps and flights of fancy—behaviors that future employers might disprize in most of their workers. Perhaps a sullen or anxious fixation on simply getting on with the job without making mistakes will be the productivity-maximizing attitude in most lines of work. The claim here is not that this is so, but that we do not know that it is not so. Yet we should consider just how bad it could be if some such pessimistic hypothesis about a future Malthusian state turned out to be true: not only because of the opportunity cost of having failed to create something better—which would be enormous—but also because the state could be bad in itself, possibly far worse than the original Malthusian state.

We seldom put forth full effort. When we do, it is sometimes painful. Imagine running on a treadmill at a steep incline—heart pounding, muscles aching, lungs gasping for air. A glance at the timer: your next break, which will also be your death, is due in 49 years, 3 months, 20 days, 4 hours, 56 minutes, and 12 seconds. You wish you had not been born.

Again the claim is not that this is how it would be, but that we do not know that it is not. One could certainly make a more optimistic case. For example, there is no obvious reason that emulations would need to suffer bodily injury and sickness: the elimination of physical wretchedness would be a great improvement over the present state of affairs. Furthermore, since such stuff as virtual reality is made of can be fairly cheap, emulations may work in sumptuous surroundings—in splendid mountaintop palaces, on terraces set in a budding spring forest, or on the beaches of an azure lagoon—with just the right illumination, temperature, scenery and décor; free from annoying fumes, noises, drafts, and buzzing insects; dressed in comfortable clothing, feeling clean and focused, and well nourished. More significantly, if—as seems perfectly possible —the optimum human mental state for productivity in most jobs is one of joyful eagerness, then the era of the emulation economy could be quite paradisiacal.

There would, in any case, be a great option value in arranging matters in such a manner that somebody or something could intervene to set things right if the default trajectory should happen to veer toward dystopia. It could also be desirable to have some sort of escape hatch that would permit bailout into death and oblivion if the quality of life were to sink permanently below the level at which annihilation becomes preferable to continued existence.

Unconscious outsourcers?

In the longer run, as the emulation era gives way to an artificial intelligence era (or if machine intelligence is attained directly via AI without a preceding whole brain emulation stage) pain and pleasure might possibly disappear entirely in a multipolar outcome, since a hedonic reward mechanism may not be the most effective motivation system for an complex artificial agent (one that, unlike the human mind, is not burdened with the legacy of animal wetware). Perhaps a more advanced motivation system would be based on an explicit representation of a utility function or some other architecture that has no exact functional analogs to pleasure and pain.

A related but slightly more radical multipolar outcome—one that could involve the elimination of almost all value from the future—is that the universal proletariat would not even be conscious. This possibility is most salient with respect to AI, which might be structured very differently than human intelligence. But even if machine intelligence were initially achieved though whole brain emulation, resulting in conscious digital minds, the competitive forces unleashed in a post-transition economy could easily lead to the emergence of progressively less neuromorphic forms of machine intelligence, either because synthetic AI is created de novo or because the emulations would, through successive modifications and enhancements, increasingly depart their original human form.

Consider a scenario in which after emulation technology has been developed, continued progress in neuroscience and computer science (expedited by the presence of digital minds to serve as both researchers and test subjects) makes it possible to isolate individual cognitive modules in an emulation, and to hook them up to modules isolated from other emulations. A period of training and adjustment may be required before different modules can collaborate effectively; but modules that conform to common standards could more quickly interface with other standard modules. This would make standardized modules more productive, and create pressure for more standardization.

Emulations can now begin to outsource increasing portions of their functionality. Why learn arithmetic when you can send your numerical reasoning task to Gauss-Modules, Inc.? Why be articulate when you can hire Coleridge Conversations to put your thoughts into words? Why make decisions about your personal life when there are certified executive modules that can scan your goal system and manage your resources to achieve your goals better than if you tried to do it yourself? Some emulations may prefer to retain most of their functionality and handle tasks themselves that could be done more efficiently by others. Those emulations would be like hobbyists who enjoy growing their own vegetables or knitting their own cardigans. Such hobbyist emulations would be less efficient; and if there is a net flow of resources from less to more efficient participants of the economy, the hobbyists would eventually lose out.

The bouillon cubes of discrete human-like intellects thus melt into an algorithmic soup.

It is conceivable that optimal efficiency would be attained by grouping capabilities in aggregates that roughly match the cognitive architecture of a human mind. It might be the case, for example, that a mathematics module must be tailored to a language module, and that both must be tailored to the executive module, in order for the three to work together. Cognitive outsourcing would then be almost entirely unworkable. But in the absence of any compelling reason for being confident that this is so, we must countenance the possibility that human-like cognitive architectures are optimal only within the constraints of human neurology (or not at all). When it becomes possible to build architectures that could not be implemented well on biological neural networks, new design space opens up; and the global optima in this extended space need not resemble familiar types of mentality. Human-like cognitive organizations would then lack a niche in a competitive post-transition economy or ecosystem.

There might be niches for complexes that are either less complex (such as individual modules), more complex (such as vast clusters of modules), or of similar complexity to human minds but with radically different architectures. Would these complexes have any intrinsic value? Should we welcome a world in which such alien complexes have replaced human complexes?

The answer may depend on the specific nature of those alien complexes. The present world has many levels of organization. Some highly complex entities, such as multinational corporations and nation states, contain human beings as constituents; yet we usually assign these high-level complexes only instrumental value. Corporations and states do not (it is generally assumed) have consciousness, over and above the consciousness of the people who constitute them: they cannot feel phenomenal pain or pleasure or experience any qualia. We value them to the extent that they serve human needs, and when they cease to do so we “kill” them without compunction. There are also lower-level entities, and those, too, are usually denied moral status. We see no harm in erasing an app from a smartphone, and we do not think that a neurosurgeon is wronging anyone when she extirpates a malfunctioning module from an epileptic brain. As for exotically organized complexes of a level similar to that of the human brain, most of us would perhaps judge them to have moral significance only if we thought they had a capacity or potential for conscious experience.

We could thus imagine, as an extreme case, a technologically highly advanced society, containing many complex structures, some of them far more intricate and intelligent than anything that exists on the planet today—a society which nevertheless lacks any type of being that is conscious or whose welfare has moral significance. In a sense, this would be an uninhabited society. It would be a society of economic miracles and technological awesomeness, with nobody there to benefit. A Disneyland without children.

Evolution is not necessarily up

The word “evolution” is often used as a synonym of “progress,” perhaps reflecting a common uncritical image of evolution as a force for good. A misplaced faith in the inherent beneficence of the evolutionary process can get in the way of a fair evaluation of the desirability of a multipolar outcome in which the future of intelligent life is determined by competitive dynamics. Any such evaluation must rest on some (at least implicit) opinion about the probability distribution of different phenotypes turning out to be adaptive in a post-transition digital life soup. It would be difficult in the best of circumstances to extract a clear and correct answer from the unavoidable goo of uncertainty that pervades these matters: more so, if we superadd a layer of Panglossian muck.

A possible source for faith in freewheeling evolution is the apparent upward directionality exhibited by the evolutionary process in the past. Starting from rudimentary replicators, evolution produced increasingly “advanced” organisms, including creatures with minds, consciousness, language, and reason. More recently, cultural and technological processes, which bear some loose similarities to biological evolution, have enabled humans to develop at an accelerated pace. On a geological as well as a historical timescale, the big picture seems to show an overarching trend toward increasing levels of complexity, knowledge, consciousness, and coordinated goal-directed organization: a trend which, not to put too fine a point on it, one might label “progress.”

The image of evolution as a process that reliably produces benign effects is difficult to reconcile with the enormous suffering that we see in both the human and the natural world. Those who cherish evolution’s achievements may do so more from an aesthetic than an ethical perspective. Yet the pertinent question is not what kind of future it would be fascinating to read about in a science fiction novel or to see depicted in a nature documentary, but what kind of future it would be good to live in: two very different matters.

Furthermore, we have no reason to think that whatever progress there has been was in any way inevitable. Much might have been luck. This objection derives support from the fact that an observation selection effect filters the evidence we can have about the success of our own evolutionary development.28 Suppose that on 99.9999% of all planets where life emerged it went extinct before developing to the point where intelligent observers could begin to ponder their origin. What should we expect to observe if that were the case? Arguably, we should expect to observe something like what we do in fact observe. The hypothesis that the odds of intelligent life evolving on a given planet are low does not predict that we should find ourselves on a planet where life went extinct at an early stage; rather, it may predict that we should find ourselves on a planet where intelligent life evolved, even if such planets constitute a very small fraction of all planets where primitive life evolved. Life’s long track record on Earth may therefore offer scant support to the claim that there was a high chance —let alone anything approaching inevitability—involved in the rise of higher organisms on our planet.

Thirdly, even if present conditions had been idyllic, and even if they could have been shown to have arisen ineluctably from some generic primordial state, there would still be no guarantee that the melioristic trend is set to continue into the indefinite future. This holds even if we disregard the possibility of a cataclysmic extinction event and indeed even if we assume that evolutionary developments will continue to produce systems of increasing complexity.

We suggested earlier that machine intelligence workers selected for maximum productivity would be working extremely hard and that it is unknown how happy such workers would be. We also raised the possibility that the fittest life forms within a competitive future digital life soup might not even be conscious. Short of a complete loss of pleasure, or of consciousness, there could be a wasting away of other qualities that many would regard as indispensible for a good life. Humans value music, humor, romance, art, play, dance, conversation, philosophy, literature, adventure, discovery, food and drink, friendship, parenting, sport, nature, tradition, and spirituality, among many other things. There is no guarantee that any of these would remain adaptive. Perhaps what will maximize fitness will be nothing but nonstop high-intensity drudgery, work of a drab and repetitive nature, destitute of ludic frisson, aimed only at improving the eighth decimal place of some economic output measure. The phenotypes selected would then have lives lacking in the aforesaid qualities, and depending on one’s axiology the result might strike one as either abhorrent, worthless, or merely impoverished, but at any rate a far cry from a utopia one would feel worthy of one’s commendation.

It might be wondered how such a bleak picture could be consistent with the fact that we do now indulge in music, humor, romance, art, etc. If these behaviors are really so “wasteful,” then how come they have been tolerated and indeed promoted by the evolutionary processes that shaped our species? That modern man is in an evolutionary disequilibrium does not account for this; for our Pleistocene forebears, too, engaged in most of these dissipations. Many of the behaviors in question are not even unique to Homo sapiens. Flamboyant display is found in a wide variety of contexts, from sexual selection in the animal kingdom to prestige contests among nation states.

Although a full evolutionary explanation for each of these behaviors is beyond the scope of the present inquiry, we can note that some of them serve functions that may not be as relevant in a machine intelligence context. Play, for example, which occurs only in some species and predominantly among juveniles, is mainly a way for the young animal to learn skills that it will need later in life. When emulations can be created as adults, already in possession of a mature repertoire of skills, or when knowledge and techniques acquired by one AI can be directly ported into another AI, the need for playful behavior might become less widespread.

Many of the other examples of humanistic behaviors may have evolved as hard-to-fake signals of qualities that are difficult to observe directly, such as bodily or mental resilience, social status, quality of allies, ability and willingness to prevail in a fight, or possession of resources. The peacock’s tail is the classic instance: only fit peacocks can afford to sprout truly extravagant plumage, and peahens have evolved to find it attractive. No less than morphological traits, behavioral traits too can signal genetic fitness or other socially relevant attributes.

Given that flamboyant display is so common among both humans and other species, one might consider whether it would not also be part of the repertoire of technologically more advanced life forms. Even if there were to be no narrowly instrumental use for playfulness or musicality or even for consciousness in the future ecology of intelligent information processing, might not these traits nonetheless confer some evolutionary advantage to their possessors by virtue of being reliable signals of other adaptive qualities?

While the possibility of a pre-established harmony between what is valuable to us and what would be adaptive in a future digital ecology is hard to rule out, there are reasons for skepticism. Consider, first, that many of the costly displays we find in nature are linked to sexual selection.32 Reproduction among technologically mature life forms, in contrast, may be predominantly or exclusively asexual.

Second, technologically advanced agents might have available new means of reliably communicating information about themselves, means that do not rely on costly display. Even today, when professional lenders assess creditworthiness they tend to rely more on documentary evidence, such as ownership certificates and bank statements, than on costly displays, such as designer suits and Rolex watches. In the future, it might be possible to employ auditing firms that verify through detailed examination of behavioral track records, testing in simulated environments, or direct inspection of source code, that a client agent possesses a claimed attribute. Signaling one’s qualities by agreeing to such auditing might be more efficient than signaling via flamboyant display. Such a professionally mediated signal would still be costly to fake—this being the essential feature that makes the signal reliable—but it could be much cheaper to transmit when truthful than it would be to communicate an equivalent signal flamboyantly.

Third, not all possible costly displays are intrinsically valuable or socially desirable. Many are simply wasteful. The Kwakiutl potlatch ceremonies, a form of status competition between rival chiefs, involved the public destruction of vast amounts of accumulated wealth.33 Record-breaking skyscrapers, megayachts, and moon rockets may be viewed as contemporary analogs. While activities like music and humor could plausibly be claimed to enhance the intrinsic quality of human life, it is doubtful that a similar claim could be sustained with regard to the costly pursuit of fashion accessories and other consumerist status symbols. Worse, costly display can be outright harmful, as in macho posturing leading to gang violence or military bravado. Even if future intelligent life forms would use costly signaling, therefore, it is an open question whether the signal would be of a valuable sort—whether it would be like the rapturous melody of a nightingale or instead like the toad’s monosyllabic croak (or the incessant barking of a rabid dog).

Post-transition formation of a singleton?

Even if the immediate outcome of the transition to machine intelligence were multipolar, the possibility would remain of a singleton developing later. Such a development would continue an apparent long-term trend toward larger scales of political integration, taking it to its natural conclusion.34 How might this occur?

A second transition

On way in which an initially multipolar outcome could converge into a singleton post-transition is if there is, after the initial transition, a second technological transition big enough and steep enough to give a decisive strategic advantage to one of the remaining powers: a power which might then seize the opportunity to establish a singleton. Such a hypothetical second transition might be occasioned by a breakthrough to a higher level of superintelligence. For instance, if the first wave of machine superintelligence is emulation-based, then a second surge might result when the emulations now doing the research succeed in developing effective self-improving artificial intelligence.35 (Alternatively, a second transition might be triggered by a breakthrough in nanotechnology or some other military or general-purpose technology as yet unenvisaged.)

The pace of development after the initial transition would be extremely rapid. Even a short gap between the leading power and its closest competitor could therefore plausibly result in a decisive strategic advantage for the leading power during a second transition. Suppose, for example, that two projects enter the first transition only a few days apart, and that the takeoff is slow enough that this gap does not give the leading project a decisive strategic advantage at any point during the takeoff. The two projects both emerge as superintelligent powers, though one of them remains a few days ahead of the other. But developments are now occurring on the research timescales characteristic of machine superintelligence—perhaps thousands or millions of times faster than research conducted on a biological human timescale. Development of the secondtransition technology might therefore be completed in days, hours, or minutes. Even though the frontrunner’s lead is a mere few days, a breakthrough could thus catapult it into a decisive strategic advantage. Note, however, that if technological diffusion (via espionage or other channels) speeds up as much as technological development, then this effect would be negated. What would remain relevant would be the steepness of the second transition, that is, the speed at which it would unfold relative to the general speed of events in the period after the first transition. (In this sense, the faster things are happening after the first transition, the less steep the second transition would tend to be.)

One might also speculate that a decisive strategic advantage would be more likely to be actually used to establish a singleton if it arises during a second (or subsequent) transition. After the first transition, decision makers would either be superintelligent or have access to advice from a superintelligence, which would clarify the implications of available strategic options. Furthermore, the situation after the first transition might be one in which a preemptive move against potential competitors would be less dangerous for the aggressor. If the decisionmaking minds after the first transition are digital, they could be copied and thereby rendered less vulnerable to a counterattack. Even if a defender had the ability to kill nine-tenths of the aggressor’s population in a retaliatory strike, this would scarcely offer much deterrence if the deceased could be immediately resurrected from redundant backups. Devastation of infrastructure (which can be rebuilt) might also be tolerable to digital minds with effectively unlimited lifespans, who might be planning to maximize their resources and influence on a cosmological timescale.

Superorganisms and scale economies

The size of coordinated human aggregates, such as firms or nations, is influenced by various parameters—technological, military, financial, and cultural—that can vary from one historical epoch to another. A machine intelligence revolution would entail profound changes in many these parameters. Perhaps these changes would facilitate the rise of a singleton. Although we cannot, without looking in detail at what these prospective changes are, exclude the opposite possibility—that the changes would facilitate fragmentation rather than unification—we can nevertheless note that the increased variance or uncertainty that we confront here may itself be a ground for giving greater credence to the potential emergence of a singleton than we would otherwise do. A machine intelligence revolution might, so to speak, stir things up—might reshuffle the deck to make possible geopolitical realignments that seemed perhaps otherwise not to have been in the cards.

A comprehensive analysis of all the factors that may influence the scale of political integration would take us far beyond the scope of this book: a review of the relevant political science and economics literature could itself easily fill an entire volume. We must confine ourselves to making brief allusion to a couple of factors, aspects of the digitization of agents that may make it easier to centralize control.

Carl Shulman has argued that in a population of emulations, selection pressures would favor the emergence of “superorganisms,” groups of emulations ready to sacrifice themselves for the good of their clan.36 Superorganisms would be spared the agency problems that beset organizations whose members pursue their own self-interest. Like the cells in our bodies, or the individual animals in a colony of eusocial insects, emulations that were wholly altruistic toward their copy-siblings would cooperate with one another even in the absence of elaborate incentive schemes.

Superorganisms would have a particularly strong advantage if nonconsensual deletion (or indefinite suspension) of individual emulations is disallowed. Firms or countries that employ emulations insisting on self-preservation would be saddled with an unending commitment to pay upkeep for obsolete or redundant workers. In contrast, organizations whose emulations willingly deleted themselves when their services were no longer required could more easily adapt to fluctuations in demand; and they could experiment freely, proliferating variations of their workers and retaining only the most productive.

If involuntary deletion is not disallowed, then the comparative advantage of eusocial emulations is reduced, though perhaps not eliminated. Employers of cooperative self-sacrificers might still reap efficiency gains from reduced agency problems throughout the organization, including being spared the trouble of having to defeat whatever resistance emulations could put up against their own deletion. In general, the productivity gains of having workers willing to sacrifice their individual lives for the common weal are a special case of the benefits an organization can derive from having members who are fanatically devoted to it. Such members would not only leap into the grave for the organization, and work long hours for little pay: they would also shun office politics and try consistently to act in what they took to be the organization’s best interest, reducing the need for supervision and bureaucratic constraints.

If the only way to achieve such dedication were by restricting membership to copy-siblings (so that all emulations in a particular superorganism were stamped out from the same template), then superorganisms would suffer some disadvantage in being able to draw only from a range of skills narrower than that of rival organizations, a disadvantage which might or might not be large enough to outweigh the advantages of avoiding internal agency problems.37 This disadvantage would be greatly alleviated if a superorganism could at least contain members with different training. Even if all its members were derived from a single ur-template, its workforce could then still contribute a diversity of skills. Starting with a polymathically talented emulation ur-template, lineages could be branched off into different training programs, one copy learning accounting, another electrical engineering, and so forth. This would produce a membership with diverse skills though not of diverse talents. (Maximum diversity might require that more than one ur-template be used.)

The essential property of a superorganism is not that it consists of copies of a single progenitor but that all the individual agents within it are fully committed to a common goal. The ability to create a superorganism can thus be viewed as requiring a partial solution to the control problem. Whereas a completely general solution to the control problem would enable somebody to create an agent with any arbitrary final goal, the partial solution needed for the creation of a superorganism requires merely the ability to fashion multiple agents with the same final goal (for some nontrivial but not necessarily arbitrary final goal).

The main consideration put forward in this subsection is thus not really limited to monoclonal emulation groups, but can be stated more generally in a way that makes clear that it applies to a wide range of multipolar machine intelligence scenarios. It is that certain types of advances in motivation selection techniques, which may become feasible when the actors are digital, may help overcome some of the inefficiencies that currently hamper large human organizations and that counterbalance economies of scale. With these limits lifted, organizations—be they firms, nations, or other economic or political entities—could increase in size. This is one factor that could facilitate the emergence of a post-transition singleton.

One area in which superorganisms (or other digital agents with partially selected motivations) might excel is coercion. A state might use motivation selection methods to ensure that its police, military, intelligence service, and civil administration are uniformly loyal. As Shulman notes,

Saved states [of some loyal emulation that has been carefully prepared and verified] could be copied billions of times to staff an ideologically uniform military, bureaucracy, and police force. After a short period of work, each copy would be replaced by a fresh copy of the same saved state, preventing ideological drift. Within a given jurisdiction, this capability could allow incredibly detailed observation and regulation: there might be one such copy for every other resident. This could be used to prohibit the development of weapons of mass destruction, to enforce regulations on brain emulation experimentation or reproduction, to enforce a liberal democratic constitution, or to create an appalling and permanent totalitarianism

The first-order effect of such a capability would seem to be to consolidate power, and possibly to concentrate it in fewer hands.

Unification by treaty

There may be large potential gains to be had from international collaboration in a post-transition multipolar world. Wars and arms races could be avoided. Astrophysical resources could be colonized and harvested at a globally optimum pace. The development of more advanced forms of machine intelligence could be coordinated to avoid a rush and to allow new designs to be thoroughly vetted. Other developments that might pose existential risks could be postponed. And uniform regulations could be enforced globally, including provisions for a guaranteed standard of living (which would require some form of population control) and for preventing exploitation and abuse of emulations and other digital and biological minds. Furthermore, agents with resource-satiable preferences (more on this in Chapter 13) would prefer a sharing agreement that would guarantee them a certain slice of the future to a winner-takes-all struggle in which they would risk getting nothing.

The presence of big potential gains from collaboration, however, does not imply that collaboration will actually be achieved. In the world today, many great boons could be obtained via better global coordination—reductions of military expenditures, wars, overfishing, trade barriers, and atmospheric pollution, among others. Yet these plump fruits are left to spoil on the branch. Why is that? What stops a fully cooperative outcome that would maximize the common good?

One obstacle is the difficulty of ensuring compliance with any treaty that might be agreed, including monitoring and enforcement costs. Two nuclear rivals might each be better off if they both relinquished their atom bombs; yet even if they could reach an in-principle agreement to do so, disarmament could nevertheless prove elusive because of their mutual fear that the other party might cheat. Allaying this fear would require setting up a verification mechanism. There may have to be inspectors to oversee the destruction of existing stockpiles, and then to monitor nuclear reactors and other facilities, and to gather technical and human intelligence, in order to ensure that the weapons program is not reconstituted. One cost is paying for these inspectors. Another cost is the risk that the inspectors will spy and make off with commercial or military secrets. Perhaps most significantly, each party might fear that the other will preserve a clandestine nuclear capability. Many a potentially beneficial deal never comes off because compliance would be too difficult to verify.

If new inspection technologies that reduced monitoring costs became available, one would expect this to result in increased cooperation. Whether monitoring costs would on net be reduced in the post-transition era, however, is not entirely clear. While there would certainly be many powerful new inspection techniques, there would also be new means of concealment. In particular, an increasing portion of the activities one might want to regulate would be taking place in cyberspace, out of reach of physical surveillance. For example, digital minds working on designing a new nanotech weapons system or a new generation of artificial intelligence may do so without leaving much of a physical footprint. Digital forensics may fail to penetrate all the layers of concealment and encryption in which a treaty-violator may cloak its illicit activities.

Reliable lie detection, if it could be developed, would be an extremely useful tool for monitoring compliance.40 An inspection protocol could include provisions for interviewing key officials, to verify that they are intent on implementing all the provisions of the treaty and that they know of no violations despite making strong efforts to find out.

A decision maker planning to cheat might defeat such a lie-detection-based verification scheme by first issuing orders to subordinates to undertake the illicit activity and to conceal the activity even from the decision maker herself, and then subjecting herself to some procedure that erases her memory of having engaged in these machinations. Suitably targeted memory-erasure operations might well be feasible in biological brains with more advanced neurotechnology. It might be even easier in machine intelligences (depending on their architecture).

States could seek to overcome this problem by committing themselves to an ongoing monitoring scheme that regularly tests key officials with a lie detector to check whether they harbor any intent to subvert or circumvent any treaty to which the state has entered or may enter in the future. Such a commitment could be viewed as a kind of meta-treaty, which would facilitate the verification of other treaties; but states might commit themselves to it unilaterally to gain the benefit of being regarded as a trustworthy negotiation partner. However, this commitment or meta-treaty would face the same problem of subversion through a delegate-and-forget ploy. Ideally, the meta-treaty would be put into effect before any party had an opportunity to make the internal arrangements necessary to subvert its implementation. Once villainy has had an unguarded moment to sow its mines of deception, trust can never set foot there again.

In some cases, the mere ability to detect treaty violations is sufficient to establish the confidence needed for a deal. In other cases, however, there is a need for some mechanism to enforce compliance or mete out punishment if a violation should occur. The need for an enforcement mechanism may arise if the threat of the wronged party withdrawing from the treaty is not enough to deter violations, for instance if the treaty-violator would gain such an advantage that he would not subsequently care how the other party responds.

If highly effective motivation selection methods are available, this enforcement problem could be solved by empowering an independent agency with sufficient police or military strength to enforce the treaty even against the opposition of one or several of its signatories. This solution requires that the enforcement agency can be trusted. But with sufficiently good motivation selection techniques, the requisite confidence might be achieved by having all the parties to the treaty jointly oversee the design of the enforcement agency.

Handing over power to an external enforcement agency raises many of the same issues that we confronted earlier in our discussions of a unipolar outcome (one in which a singleton arises prior to or during the initial machine intelligence revolution). In order to be able to enforce treaties concerning the vital security interests of rival states, the external enforcement agency would in effect need to constitute a singleton: a global superintelligent Leviathan. One difference, however, is that we are now considering a post-transition situation, in which the agents that would have to create this Leviathan would have greater competence than we humans currently do. These Leviathan-creators may themselves already be superintelligent. This would greatly improve the odds that they could solve the control problem and design an enforcement agency that would serve the interests of all the parties that have a say in its construction.

Aside from the costs of monitoring and enforcing compliance, are there any other obstacles to global coordination? Perhaps the major remaining issue is what we can refer to as bargaining costs.41 Even when there is a possible bargain that would benefit everybody involved, it sometimes does not get off the ground because the parties fail to agree on how to divide the spoils. For example, if two persons could make a deal that would net them a dollar in profit, but each party feels she deserves sixty cents and refuses to settle for less, the deal will not happen and the potential gain will be forfeited. In general, negotiations can be difficult or protracted, or remain altogether barren, because of strategic bargaining choices made by some of the parties.

In real life, human beings frequently succeed in reaching agreements despite the possibility for strategic bargaining (though often not without considerable expenditure of time and patience). It is conceivable, however, that strategic bargaining problems would have a different dynamic in the post-transition era. An AI negotiator might more consistently adhere to some particular formal conception of rationality, possibly with novel or unanticipated consequences when matched with other AI negotiators. An AI might also have available to it moves in the bargaining game that are either unavailable to humans or very much more difficult for humans to execute, including the ability to precommit to a policy or a course of action. While humans (and human-run institutions) are occasionally able to precommit—with imperfect degrees of credibility and specificity—some types of machine intelligence might be able to make arbitrary unbreakable precommitments and to allow negotiating partners to confirm that such a precommitment has been made.

The availability of powerful precommitment techniques could profoundly alter the nature of negotiations, potentially giving an immense edge to an agent that has a first-mover advantage. If a particular agent’s participation is necessary for the realization of some prospective gains from cooperation, and if that agent is able to make the first move, it would be in a position to dictate the division of the spoils by precommitting not to accept any deal that gives it less than, say, 99% of the surplus value. Other agents would then be faced with the choice of either getting nothing (by rejecting the unfair proposal) or getting 1% of the value (by caving in). If the first-moving agent’s precommitment is publicly verifiable, its negotiating partners could be sure that these are their only two options.

To avoid being exploited in this manner, agents might precommit to refuse blackmail and to decline all unfair offers. Once such a precommitment has been made (and successfully publicized), other agents would not find it in their interest to make threats or to precommit themselves to only accepting deals tilted in their own favor, because they would know that threats would fail and that unfair proposals would be rejected. But this just demonstrates again that the advantage is with the first-mover. The agent who moves first can choose whether to parlay its position of strength only to deter others from taking unfair advantage, or to make a grab for the lion’s share of future spoils.

Best situated of all, it might seem, would be the agent who starts out with a temperament or a value system that makes him impervious to extortion or indeed to any offer of a deal in which his participation is indispensable but he is not getting almost all of the gains. Some humans seem already to possess personality traits corresponding to various aspects of an uncompromising spirit.43 A highstrung disposition, however, could backfire should it turn out that there are other agents around who feel entitled to more than their fair share and are committed to not backing down. The unstoppable force would then encounter the unmovable object, resulting in a failure to reach agreement (or worse: total war). The meek and the akratic would at least get something, albeit less than their fair share.

What kind of game-theoretic equilibrium would be reached in such a posttransition bargaining game is not immediately obvious. Agents might choose more complicated strategies than the ones considered here. One hopes that an equilibrium would be reached centered on some fairness norm that would serve as a Schelling point—a salient feature in a big outcome space which, because of shared expectations, becomes a likely coordination point in an otherwise underdetermined coordination game. Such an equilibrium might be bolstered by some of our evolved dispositions and cultural programming: a common preference for fairness could, assuming we succeed in transferring our values into the post-transition era, bias expectations and strategies in ways that lead to an attractive equilibrium.

In any case, the upshot is that with the possibility of strong and flexible forms of precommitment, outcomes of negotiations might take on an unfamiliar guise. Even if the post-transition era started out multipolar, it might be that a singleton would arise almost immediately as a consequence of a negotiated treaty that resolves all important global coordination problems. Some transaction costs, perhaps including monitoring and enforcement costs, might plummet with the new technological capabilities available to advanced machine intelligences. Other costs, in particular costs related to strategic bargaining, might remain significant. But however strategic bargaining affects the nature of the agreement that is reached, there is no clear reason why it would long delay the reaching of some agreement if an agreement were ever to be reached. If no agreement is reached, then some form of fighting might take place; and either one faction might win, and form a singleton around the winning coalition, or the result might be an interminable conflict, in which case a singleton may never form and the overall outcome may fall terribly short of what could and should have been achieved if humanity and its descendants had acted in a more coordinated and cooperative fashion.

We have seen that multipolarity, even if it could be achieved in a stable form, would not guarantee an attractive outcome. The original principal–agent problem remains unsolved, and burying it under a new set of problems related to posttransition global coordination failures may only make the situation worse. Let us therefore return to the question of how we could safely keep a single superintelligent AI.

CHAPTER 12: Acquiring values

Capability control is, at best, a temporary and auxiliary measure. Unless the plan is to keep superintelligence bottled up forever, it will be necessary to master motivation selection. But just how could we get some value into an artificial agent, so as to make it pursue that value as its final goal? While the agent is unintelligent, it might lack the capability to understand or even represent any humanly meaningful value. Yet if we delay the procedure until the agent is superintelligent, it may be able to resist our attempt to meddle with its motivation system—and, as we showed in Chapter 7, it would have convergent instrumental reasons to do so. This value-loading problem is tough, but must be confronted.

The value-loading problem

It is impossible to enumerate all possible situations a superintelligence might find itself in and to specify for each what action it should take. Similarly, it is impossible to create a list of all possible worlds and assign each of them a value. In any realm significantly more complicated than a game of tic-tac-toe, there are far too many possible states (and state-histories) for exhaustive enumeration to be feasible. A motivation system, therefore, cannot be specified as a comprehensive lookup table. It must instead be expressed more abstractly, as a formula or rule that allows the agent to decide what to do in any given situation.

One formal way of specifying such a decision rule is via a utility function. A utility function (as we recall from Chapter 1) assigns value to each outcome that might obtain, or more generally to each “possible world.” Given a utility function, one can define an agent that maximizes expected utility. Such an agent selects at each time the action that has the highest expected utility. (The expected utility is calculated by weighting the utility of each possible world with the subjective probability of that world being the actual world conditional on a particular action being taken.) In reality, the possible outcomes are too numerous for the expected utility of an action to be calculated exactly. Nevertheless, the decision rule and the utility function together determine a normative ideal—an optimality notion—that an agent might be designed to approximate; and the approximation might get closer as the agent gets more intelligent.1 Creating a machine that can compute a good approximation of the expected utility of the actions available to it is an AI-complete problem.2 This chapter addresses another problem, a problem that remains even if the problem of making machines intelligent is solved.

We can use this framework of a utility-maximizing agent to consider the predicament of a future seed-AI programmer who intends to solve the control problem by endowing the AI with a final goal that corresponds to some plausible human notion of a worthwhile outcome. The programmer has some particular human value in mind that he would like the AI to promote. To be concrete, let us say that it is happiness. (Similar issues would arise if we the programmer were interested in justice, freedom, glory, human rights, democracy, ecological balance, or self-development.) In terms of the expected utility framework, the programmer is thus looking for a utility function that assigns utility to possible worlds in proportion to the amount of happiness they contain. But how could he express such a utility function in computer code? Computer languages do not contain terms such as “happiness” as primitives. If such a term is to be used, it must first be defined. It is not enough to define it in terms of other high-level human concepts—“happiness is enjoyment of the potentialities inherent in our human nature” or some such philosophical paraphrase. The definition must bottom out in terms that appear in the AI’s programming language, and ultimately in primitives such as mathematical operators and addresses pointing to the contents of individual memory registers. When one considers the problem from this perspective, one can begin to appreciate the difficulty of the programmer’s task.

Identifying and codifying our own final goals is difficult because human goal representations are complex. Because the complexity is largely transparent to us, however, we often fail to appreciate that it is there. We can compare the case to visual perception. Vision, likewise, might seem like a simple thing, because we do it effortlessly.3 We only need to open our eyes, so it seems, and a rich, meaningful, eidetic, three-dimensional view of the surrounding environment comes flooding into our minds. This intuitive understanding of vision is like a duke’s understanding of his patriarchal household: as far as he is concerned, things simply appear at their appropriate times and places, while the mechanism that produces those manifestations are hidden from view. Yet accomplishing even the simplest visual task—finding the pepper jar in the kitchen—requires a tremendous amount of computational work. From a noisy time series of twodimensional patterns of nerve firings, originating in the retina and conveyed to the brain via the optic nerve, the visual cortex must work backwards to reconstruct an interpreted three-dimensional representation of external space. A sizeable portion of our precious one square meter of cortical real estate is zoned for processing visual information, and as you are reading this book, billions of neurons are working ceaselessly to accomplish this task (like so many seamstresses, bent over their sewing machines in a sweatshop, sewing and resewing a giant quilt many times a second). In like manner, our seemingly simple values and wishes in fact contain immense complexity.4 How could our programmer transfer this complexity into a utility function?

One approach would be to try to directly code a complete representation of whatever goal we have that we want the AI to pursue; in other words, to write out an explicit utility function. This approach might work if we had extraordinarily simple goals, for example if we wanted to calculate the digits of pi—that is, if the only thing we wanted was for the AI to calculate the digits of pi and we were indifferent to any other consequence that would result from the pursuit of this goal—recall our earlier discussion of the failure mode of infrastructure profusion. This explicit coding approach might also have some promise in the use of domesticity motivation selection methods. But if one seeks to promote or protect any plausible human value, and one is building a system intended to become a superintelligent sovereign, then explicitly coding the requisite complete goal representation appears to be hopelessly out of reach.

If we cannot transfer human values into an AI by typing out full-blown representations in computer code, what else might we try? This chapter discusses several alternative paths. Some of these may look plausible at first sight—but much less so upon closer examination. Future explorations should focus on those paths that remain open.

Solving the value-loading problem is a research challenge worthy of some of the next generation’s best mathematical talent. We cannot postpone confronting this problem until the AI has developed enough reason to easily understand our intentions. As we saw in the section on convergent instrumental reasons, a generic system will resist attempts to alter its final values. If an agent is not already fundamentally friendly by the time it gains the ability to reflect on its own agency, it will not take kindly to a belated attempt at brainwashing or a plot to replace it with a different agent that better loves its neighbor.

Evolutionary selection

Evolution has produced an organism with human values at least once. This fact might encourage the belief that evolutionary methods are the way to solve the value-loading problem. There are, however, severe obstacles to achieving safety along this path. We have already pointed to these obstacles at the end of Chapter 10 when we discussed how powerful search processes can be dangerous.

Evolution can be viewed as a particular class of search algorithms that involve the alternation of two steps, one expanding a population of solution candidates by generating new candidates according to some relatively simple stochastic rule (such as random mutation or sexual recombination), the other contracting the population by pruning candidates that score poorly when tested by an evaluation function. As with many other types of powerful search, there is the risk that the process will find a solution that satisfies the formally specified search criteria but not our implicit expectations. (This would hold whether one seeks to evolve a digital mind that has the same goals and values as a typical human being, or instead a mind that is, for instance, perfectly moral or perfectly obedient.) The risk would be avoided if we could specify a formal search criterion that accurately represented all dimensions of our goals, rather than just one aspect of what we think we desire. But this is precisely the value-loading problem, and it would of course beg the question in this context to assume that problem solved.

There is a further problem:

The total amount of suffering per year in the natural world is beyond all decent contemplation. During the minute that it takes me to compose this sentence, thousands of animals are being eaten alive, others are running for their lives, whimpering with fear, others are being slowly devoured from within by rasping parasites, thousands of all kinds are dying of starvation, thirst and disease.6

Even just within our species, 150,000 persons are destroyed each day while countless more suffer an appalling array of torments and deprivations.7 Nature might be a great experimentalist, but one who would never pass muster with an ethics review board—contravening the Helsinki Declaration and every norm of moral decency, left, right, and center. It is important that we not gratuitously replicate such horrors in silico. Mind crime seems especially difficult to avoid when evolutionary methods are used to produce human-like intelligence, at least if the process is meant to look anything like actual biological evolution.

Reinforcement learning

Reinforcement learning is an area of machine learning that studies techniques whereby agents can learn to maximize some notion of cumulative reward. By constructing an environment in which desired performance is rewarded, a reinforcement-learning agent can be made to learn to solve a wide class of problems (even in the absence of detailed instruction or feedback from the programmers, aside from the reward signal). Often, the learning algorithm involves the gradual construction of some kind of evaluation function, which assigns values to states, state–action pairs, or policies. (For instance, a program can learn to play backgammon by using reinforcement learning to incrementally improve its evaluation of possible board positions.) The evaluation function, which is continuously updated in light of experience, could be regarded as incorporating a form of learning about value. However, what is being learned is not new final values but increasingly accurate estimates of the instrumental values of reaching particular states (or of taking particular actions in particular states, or of following particular policies). Insofar as a reinforcement-learning agent can be described as having a final goal, that goal remains constant: to maximize future reward. And reward consists of specially designated percepts received from the environment. Therefore, the wireheading syndrome remains a likely outcome in any reinforcement agent that develops a world model sophisticated enough to suggest this alternative way of maximizing reward.

These remarks do not imply that reinforcement-learning methods could never be used in a safe seed AI, only that they would have to be subordinated to a motivation system that is not itself organized around the principle of reward maximization. That, however, would require that a solution to the value-loading problem had been found by some other means than reinforcement learning.

Associative value accretion

Now one might wonder: if the value-loading problem is so tricky, how do we ourselves manage to acquire our values?

One possible (oversimplified) model might look something like this. We begin life with some relatively simple starting preferences (e.g. an aversion to noxious stimuli) together with a set of dispositions to acquire additional preferences in response to various possible experiences (e.g. we might be disposed to form a preference for objects and behaviors that we find to be valued and rewarded in our culture). Both the simple starting preferences and the dispositions are innate, having been shaped by natural and sexual selection over evolutionary timescales. Yet which preferences we end up with as adults depends on life events. Much of the information content in our final values is thus acquired from our experiences rather than preloaded in our genomes.

For example, many of us love another person and thus place great final value on his or her well-being. What is required to represent such a value? Many elements are involved, but consider just two: a representation of “person” and a representation of “well-being.” These concepts are not directly coded in our DNA. Rather, the DNA contains instructions for building a brain, which, when placed in a typical human environment, will over the course of several years develop a world model that includes concepts of persons and of well-being. Once formed, these concepts can be used to represent certain meaningful values. But some mechanism needs to be innately present that leads to values being formed around these concepts, rather than around other acquired concepts (like that of a flowerpot or a corkscrew).

The details of how this mechanism works are not well understood. In humans, the mechanism is probably complex and multifarious. It is easier to understand the phenomenon if we consider it in a more rudimentary form, such as filial imprinting in nidifugous birds, where the newly hatched chick acquires a desire for physical proximity to an object that presents a suitable moving stimulus within the first day after hatching. Which particular object the chick desires to be near depends on its experience; only the general disposition to imprint in this way is genetically determined. Analogously, Harry might place a final value on Sally’s well-being; but had the twain never met, he might have fallen in love with somebody else instead, and his final values would have been different. The ability of our genes to code for the construction of a goal-acquiring mechanism explains how we come to have final goals of great informational complexity, greater than could be contained in the genome itself.

We may consequently consider whether we might build the motivation system for an artificial intelligence on the same principle. That is, instead of specifying complex values directly, could we specify some mechanism that leads to the acquisition of those values when the AI interacts with a suitable environment?

Mimicking the value-accretion process that takes place in humans seems difficult. The relevant genetic mechanism in humans is the product of eons of work by evolution, work that might be hard to recapitulate. Moreover, the mechanism is presumably closely tailored to the human neurocognitive architecture and therefore not applicable in machine intelligences other than whole brain emulations. And if whole brain emulations of sufficient fidelity were available, it would seem easier to start with an adult brain that comes with full representations of some human values preloaded.

Seeking to implement a process of value accretion closely mimicking that of human biology therefore seems an unpromising line of attack on the valueloading problem. But perhaps we might design a more unabashedly artificial substitute mechanism that would lead an AI to import high-fidelity representations of relevant complex values into its goal system? For this to succeed, it may not be necessary to give the AI exactly the same evaluative dispositions as a biological human. That may not even be desirable as an aim— human nature, after all, is flawed and all too often reveals a proclivity to evil which would be intolerable in any system poised to attain a decisive strategic advantage. Better, perhaps, to aim for a motivation system that departs from the human norm in systematic ways, such as by having a more robust tendency to acquire final goals that are altruistic, compassionate, or high-minded in ways we would recognize as reflecting exceptionally good character if they were present in a human person. To count as improvements, however, such deviations from the human norm would have to be pointed in very particular directions rather than at random; and they would continue to presuppose the existence of a largely undisturbed anthropocentric frame of reference to provide humanly meaningful evaluative generalizations (so as to avoid the kind of perverse instantiation of superficially plausible goal descriptions that we examined in Chapter 8). It is an open question whether this is feasible.

One further issue with associative value accretion is that the AI might disable the accretion mechanism. As we saw in Chapter 7, goal-system integrity is a convergent instrumental value. When the AI reaches a certain stage of cognitive development it may start to regard the continued operation of the accretion mechanism as a corrupting influence.11 This is not necessarily a bad thing, but care would have to be taken to make the sealing-up of the goal system occur at the right moment, after the appropriate values have been accreted but before they have been overwritten by additional unintended accretions.

Motivational scaffolding

Another approach to the value-loading problem is what we may refer to as motivational scaffolding. It involves giving the seed AI an interim goal system, with relatively simple final goals that we can represent by means of explicit coding or some other feasible method. Once the AI has developed more sophisticated representational faculties, we replace this interim scaffold goal system with one that has different final goals. This successor goal system then governs the AI as it develops into a full-blown superintelligence.

Because the scaffold goals are not just instrumental but final goals for the AI, the AI might be expected to resist having them replaced (goal-content integrity being a convergent instrumental value). This creates a hazard. If the AI succeeds in thwarting the replacement of its scaffold goals, the method fails.

To avoid this failure mode, precautions are necessary. For example, capability control methods could be applied to limit the AI’s powers until the mature motivation system has been installed. In particular, one could try to stunt its cognitive development at a level that is safe but that allows it to represent the values that we want to include in its ultimate goals. To do this, one might try to differentially stunt certain types of intellectual abilities, such as those required for strategizing and Machiavellian scheming, while allowing (apparently) more innocuous abilities to develop to a somewhat higher level.

One could also try to use motivation selection methods to induce a more collaborative relationship between the seed AI and the programmer team. For example, one might include in the scaffold motivation system the goal of welcoming online guidance from the programmers, including allowing them to replace any of the AI’s current goals.12 Other scaffold goals might include being transparent to the programmers about its values and strategies, and developing an architecture that is easy for the programmers to understand and that facilitates the later implementation of a humanly meaningful final goal, as well as domesticity motivations (such as limiting the use of computational resources).

One could even imagine endowing the seed AI with the sole final goal of replacing itself with a different final goal, one which may have been only implicitly or indirectly specified by the programmers. Some of the issues raised by the use of such a “self-replacing” scaffold goal also arise in the context of the value learning approach, which is discussed in the next subsection. Some further issues will be discussed in Chapter 13.

The motivational scaffolding approach is not without downsides. One is that it carries the risk that the AI could become too powerful while it is still running on its interim goal system. It may then thwart the human programmers’ efforts to install the ultimate goal system (either by forceful resistance or by quiet subversion). The old final goals may then remain in charge as the seed AI develops into a full-blown superintelligence. Another downside is that installing the ultimately intended goals in a human-level AI is not necessarily that much easier than doing so in a more primitive AI. A human-level AI is more complex and might have developed an architecture that is opaque and difficult to alter. A seed AI, by contrast, is like a tabula rasa on which the programmers can inscribe whatever structures they deem helpful. This downside could be flipped into an upside if one succeeded in giving the seed AI scaffold goals that made it want to develop an architecture helpful to the programmers in their later efforts to install the ultimate final values. However, it is unclear how easy it would be to give a seed AI scaffold goals with this property, and it is also unclear how even an ideally motivated seed AI would be capable of doing a much better job than the human programming team at developing a good architecture.

Value learning

We come now to an important but subtle approach to the value-loading problem. It involves using the AI’s intelligence to learn the values we want it to pursue. To do this, we must provide a criterion for the AI that at least implicitly picks out some suitable set of values. We could then build the AI to act according to its best estimates of these implicitly defined values. It would continually refine its estimates as it learns more about the world and gradually unpacks the implications of the value-determining criterion.

In contrast to the scaffolding approach, which gives the AI an interim scaffold goal and later replaces it with a different final goal, the value learning approach retains an unchanging final goal throughout the AI’s developmental and operational phases. Learning does not change the goal. It changes only the AI’s beliefs about the goal.

The AI thus must be endowed with a criterion that it can use to determine which percepts constitute evidence in favor of some hypothesis about what the ultimate goal is, and which percepts constitute evidence against. Specifying a suitable criterion could be difficult. Part of the difficulty, however, pertains to the problem of creating artificial general intelligence in the first place, which requires a powerful learning mechanism that can discover the structure of the environment from limited sensory inputs. That problem we can set aside here. But even modulo a solution to how to create superintelligent AI, there remain the difficulties that arise specifically from the value-loading problem. With the value learning approach, these take the form of needing to define a criterion that connects perceptual bitstrings to hypotheses about values.

Before delving into the details of how value learning could be implemented, it might be helpful to illustrate the general idea with an example. Suppose we write down a description of a set of values on a piece of paper. We fold the paper and put it in a sealed envelope. We then create an agent with human-level general intelligence, and give it the following final goal: “Maximize the realization of the values described in the envelope.” What will this agent do?

The agent does not initially know what is written in the envelope. But it can form hypotheses, and it can assign those hypotheses probabilities based on their priors and any available empirical data. For instance, the agent might have encountered other examples of human-authored texts, or it might have observed some general patterns of human behavior. This would enable it to make guesses. One does not need a degree in psychology to predict that the note is more likely to describe a value such as “minimize injustice and unnecessary suffering” or “maximize returns to shareholders” than a value such as “cover all lakes with plastic shopping bags.”

When the agent makes a decision, it seeks to take actions that would be effective at realizing the values it believes are most likely to be described in the letter. Importantly, the agent would see a high instrumental value in learning more about what the letter says. The reason is that for almost any final value that might be described in the letter, that value is more likely to be realized if the agent finds out what it is, since the agent will then pursue that value more effectively. The agent would also discover the convergent instrumental reasons described in Chapter 7—goal system integrity, cognitive enhancement, resource acquisition, and so forth. Yet, assuming that the agent assigns a sufficiently high probability to the values described in the letter involving human welfare, it would not pursue these instrumental values by immediately turning the planet into computronium and thereby exterminating the human species, because doing so would risk permanently destroying its ability to realize its final value.

We can liken this kind of agent to a barge attached to several tugboats that pull in different directions. Each tugboat corresponds to a hypothesis about the agent’s final value. The engine power of each tugboat corresponds to the associated hypothesis’s probability, and thus changes as new evidence comes in, producing adjustments in the barge’s direction of motion. The resultant force should move the barge along a trajectory that facilitates learning about the (implicit) final value while avoiding the shoals of irreversible destruction; and later, when the open sea of more definite knowledge of the final value is reached, the one tugboat that still exerts significant force will pull the barge toward the realization of the discovered value along the straightest or most propitious route.

The envelope and barge metaphors illustrate the principle underlying the value learning approach, but they pass over a number of critical technical issues. They come into clearer focus once we start to develop the approach within a formal framework (see Box 10).

One outstanding issue is how to endow the AI with a goal such as “Maximize the realization of the values described in the envelope.” (In the terminology of Box 10, how to define the value criterion.) To do this, it is necessary to identify the place where the values are described. In our example, this requires making a successful reference to the letter in the envelope. Though this might seem trivial, it is not without pitfalls. To mention just one: it is critical that the reference be not simply to a particular external physical object but to an object at a particular time. Otherwise the AI may determine that the best way to attain its goal is by overwriting the original value description with one that provides an easier target (such as the value that for every integer there be a larger integer). This done, the AI could lean back and crack its knuckles—though more likely a malignant failure would ensue, for reasons we discussed in Chapter 8. So now we face the question of how to define time. We could point to a clock and say, “Time is defined by the movements of this device”—but this could fail if the AI conjectures that it can manipulate time by moving the hands on the clock, a conjecture which would indeed be correct if “time” were given the aforesaid definition. (In a realistic case, matters would be further complicated by the fact that the relevant values are not going to be conveniently described in a letter; more likely, they would have to be inferred from observations of pre-existing structures that implicitly contain the relevant information, such as human brains.)

Another issue in coding the goal “Maximize the realization of the values described in the envelope” is that even if all the correct values were described in a letter, and even if the AI’s motivation system were successfully keyed to this source, the AI might not interpret the descriptions the way we intended. This would create a risk of perverse instantiation, as discussed in Chapter 8.

To clarify, the difficulty here is not so much how to ensure that the AI can understand human intentions. A superintelligence should easily develop such understanding. Rather, the difficulty is ensuring that the AI will be motivated to pursue the described values in the way we intended. This is not guaranteed by the AI’s ability to understand our intentions: an AI could know exactly what we meant and yet be indifferent to that interpretation of our words (being motivated instead by some other interpretation of the words or being indifferent to our words altogether).

The difficulty is compounded by the desideratum that, for reasons of safety, the correct motivation should ideally be installed in the seed AI before it becomes capable of fully representing human concepts or understanding human intentions. This requires that somehow a cognitive framework be created, with a particular location in that framework designated in the AI’s motivation system as the repository of its final value. But the cognitive framework itself must be revisable, so as to allow the AI to expand its representational capacities as it learns more about the world and grows more intelligent. The AI might undergo the equivalent of scientific revolutions, in which its worldview is shaken up and it perhaps suffers ontological crises in which it discovers that its previous ways of thinking about values were based on confusions and illusions. Yet starting at a sub-human level of development and continuing throughout all its subsequent development into a galactic superintelligence, the AI’s conduct is to be guided by an essentially unchanging final value, a final value that becomes better understood by the AI in direct consequence of its general intellectual progress— and likely quite differently understood by the mature AI than it was by its original programmers, though not different in a random or hostile way but in a benignly appropriate way. How to accomplish this remains an open question.20 (See Box 11.)

In summary, it is not yet known how to use the value learning approach to install plausible human values (though see Box 12 for some examples of recent ideas). At present, the approach should be viewed as a research program rather than an available technique. If it could be made to work, it might constitute the most ideal solution to the value-loading problem. Among other benefits, it would seem to offer a natural way to prevent mind crime, since a seed AI that makes reasonable guesses about which values its programmers might have installed would anticipate that mind crime is probably negatively evaluated by those values, and thus best avoided, at least until more definitive information has been obtained.

Last, but not least, there is the question of “what to write in the envelope”— or, less metaphorically, the question of which values we should try to get the AI to learn. But this issue is common to all approaches to the AI value-loading problem. We return to it in Chapter 13.

Emulation modulation

Emulation modulationThe value-loading problem looks somewhat different for whole brain emulation than it does for artificial intelligence. Methods that presuppose a fine-grained understanding and control of algorithms and architecture are not applicable to emulations. On the other hand, the augmentation motivation selection method— inapplicable to de novo artificial intelligence—is available to be used with emulations (or enhanced biological brains).

The augmentation method could be combined with techniques to tweak the inherited goals of the system. For example, one could try to manipulate the motivational state of an emulation by administering the digital equivalent of psychoactive substances (or, in the case of biological systems, the actual chemicals). Even now it is possible to pharmacologically manipulate values and motivations to a limited extent.29 The pharmacopeia of the future may contain drugs with more specific and predictable effects. The digital medium of emulations should greatly facilitate such developments, by making controlled experimentation easier and by rendering all cerebral parts directly addressable.

Just as when biological test subjects are used, research on emulations would get entangled in ethical complications, not all of which could be brushed aside with a consent form. Such entanglements could slow progress along the emulation path (because of regulation or moral restraint), perhaps especially hindering studies on how to manipulate the motivational structure of emulations. The result could be that emulations are augmented to potentially dangerous superintelligent levels of cognitive ability before adequate work has been done to test or adjust their final goals. Another possible effect of the moral entanglements might be to give the lead to less scrupulous teams and nations. Conversely, were we to relax our moral standards for experimenting with digital human minds, we could become responsible for a substantial amount of harm and wrongdoing, which is obviously undesirable. Other things equal, these considerations favor taking some alternative path that does not require the extensive use of digital human research subjects in a strategically high-stakes situation.

The issue, however, is not clear-cut. One could argue that whole brain emulation research is less likely to involve moral violations than artificial intelligence research, on the grounds that we are more likely to recognize when an emulation mind qualifies for moral status than we are to recognize when a completely alien or synthetic mind does so. If certain kinds of AIs, or their subprocesses, have a significant moral status that we fail to recognize, the consequent moral violations could be extensive. Consider, for example, the happy abandon with which contemporary programmers create reinforcementlearning agents and subject them to aversive stimuli. Countless such agents are created daily, not only in computer science laboratories but in many applications, including some computer games containing sophisticated non-player characters. Presumably, these agents are still too primitive to have any moral status. But how confident can we really be that this is so? More importantly, how confident can we be that we will know to stop in time, before our programs become capable of experiencing morally relevant suffering?

Institution design

Some intelligent systems consist of intelligent parts that are themselves capable of agency. Firms and states exemplify this in the human world: whilst largely composed of humans they can, for some purposes, be viewed as autonomous agents in their own right. The motivations of such composite systems depend not only on the motivations of their constituent subagents but also on how those subagents are organized. For instance, a group that is organized under strong dictatorship might behave as if it had a will that was identical to the will of the subagent that occupies the dictator role, whereas a democratic group might sometimes behave more as if it had a will that was a composite or average of the wills of its various constituents. But one can also imagine governance institutions that would make an organization behave in a way that is not a simple function of the wills of its subagents. (Theoretically, at least, there could exist a totalitarian state that everybody hated, because the state had mechanisms to prevent its citizens from coordinating a revolt. Each citizen could be worse off by revolting alone than by playing their part in the state machinery.)

By designing appropriate institutions for a composite system, one could thus try to shape its effective motivation. In Chapter 9, we discussed social integration as a possible capability control method. But there we focused on the incentives faced by an agent as a consequence of its existence in a social world of near-equals. Here we are focusing on what happens inside a given agent: how its will is determined by its internal organization. We are therefore looking at a motivation selection method. Moreover, since this kind of internal institution design does not depend on large-scale social engineering or reform, it is a method that might be available to an individual project developing superintelligence even if the wider socioeconomic or international milieu is less than ideally favorable.

Institution design is perhaps most plausible in contexts where it would be combined with augmentation. If we could start with agents that are already suitably motivated or that have human-like motivations, institutional arrangements could be used as an extra safeguard to increase the chances that the system will stay on course.

For example, suppose that we start with some well-motivated human-like agents—let us say emulations. We want to boost the cognitive capacities of these agents, but we worry that the enhancements might corrupt their motivations. One way to deal with this challenge would be to set up a system in which individual emulations function as subagents. When a new enhancement is introduced, it is first applied to a small subset of the subagents. Its effects are then studied by a review panel composed of subagents who have not yet had the enhancement applied to them. Only when these peers have satisfied themselves that the enhancement is not corrupting is it rolled out to the wider subagent population. If the enhanced subagents are found to be corrupted, they are not given further enhancements and are excluded from key decision-making functions (at least until the system as a whole has advanced to a point where the corrupted subagents can be safely reintegrated).30 Although the corrupted subagents might have gained some advantage from the enhancement, the institutional structure in which they are embedded, and the fact that they constitute a small minority of all subagents, would hopefully prevent them from seizing power or spreading their corruption to the overall system. Thus, the collective intelligence and capability of the system could be gradually enhanced in a sequence of small steps, where the soundness of each step is verified by subagents only slightly less capable than the new subagents that are introduced in that step.

The amount of safety obtainable by this kind of institution design is limited by the accuracy of the tests that are used to evaluate the enhanced subagents. The problem of test construction is complicated by the fact that the test batteries could only be validated within the pre-existing range of cognitive ability yet must be applied to agents outside this range. One could mitigate this problem by taking small enhancement steps and by letting the test run for a long time. Such caution, however, would raise the cost and slow progress (which, if a race dynamic is occurring, could mean a project employing these safety measures would place itself at a disadvantage). Even with small steps and extensive testing, there would remain some risk of subtle flaws being introduced, leading to a gradual corruption of the goal system, a corruption that might become manifest only when the system reaches superintelligence. This risk could be reduced by letting the testing condition extend for many steps of cognitive enhancement, so that even agents that have been initially certified as sound and that have received several subsequent enhancements would continue to be under review by a panel of unmodified and uncorrupted original peers and would remain subject to reversion.

Ideally, the testing should be ongoing rather than confined to an initial evaluation period. Human-level subagents have the ability to strategize and might thus choose to conceal certain goals while their behavior was being monitored. To enable continuous testing, a hierarchy might be created in which subagents at a certain level of capability are tasked with monitoring subagents at a somewhat higher level of capability. At the bottom of the capability hierarchy (but at the top of the power hierarchy) would sit the relatively dumb and slow principal. This human principal would be like a demented king who reigns over an incompetent court that oversees a mediocre administration which governs a capable people. Alternatively, he might be likened to the evolutionarily ancient “reptilian brain” which ensures that the newer and much cleverer neocortex is employed in the service of goals such as feeding and copulation.

One might question whether such an inverse meritocracy could remain stable. If the bulk of the superintelligent subagents were found to be corrupt, and the humans in charge of the whole thing attempted to roll back many of the preceding enhancements, would the institutional structure be strong enough to withstand the opposition that such a move would engender? In the context of historical or contemporary human societies, the answer perhaps is that this would be implausible. (Not that the most capable people consistently rise to power, but we are not literally ruled by chimpanzees either.) The issue is less clear, however, in a context where society is implemented as software.

Imagine a digital hierarchy that has one highly trained police agent for every two proletarians. It also has one special-police agent for every two police agents. It also has one special-special-police agent for every two special-police agents— and so forth, so that each layer of overseers has at least half the numbers of the layer it oversees. Imagine, further, that this society is architected in such a way that a supervisor has big advantages over his subordinates. For example, the supervisor is able to monitor and record everything his subordinates say and do, whereas the supervisor himself is hidden behind a one-way glass, so that his subordinates can only hear the orders and instructions that the superior chooses to transmit. Imagine that supervisors have a panel of buttons that they can press at any time to cause a subordinate to be punished or rewarded, paused, deleted, or reverted to an earlier state. These capabilities would greatly boost the powers of a supervising agent, beyond what has been possible historically in even the most totalitarian of organizations.

Even this does not exhaust the possible instruments of control. Supervisors might also have at their disposal realistic virtual reality simulations that they can use to probe subordinates’ reactions to hypothetical scenarios (such as simulated offers to join a conspiracy or to slack when they think nobody is watching). Supervisors might, additionally, have the ability to eavesdrop on internal monologues in the subordinates’ minds and to directly manipulate their emotional states.

The upshot is a series of massive advantages for the supervisor.32 Each supervisor would be a subordinate to another, higher-level supervisor, who would monitor his every move and make sure he did not slacken in the execution of his managerial duties. With this kind of arrangement, it might be possible for the structure to be stable even if each supervisory level is intellectually somewhat weaker than the level it supervises.33 And the entire structure, which could include many highly superintelligent agents, may be controlled by a small number of humans with root access.

The cost of such an internal control structure would be significant but not necessarily prohibitive. For example, in the model where each level of supervision has half the numbers of the layer below, the extra computational overhead is bounded at a mere 100% of what the proletarian part of the system costs—less if the dumber boss layers require fewer computations per subagent. Oversight could be tightened by increasing the ratio, or cost lowered by decreasing it. The extra computational cost would be a major factor in a competitive market scenario (cf. Chapter 11) but looks affordable in the context of a project that is not facing strong immediate competition. There would also be a cost in terms of the development time needed to create and test these supervisory functionalities. A well-resourced project could reduce this time cost by parallelizing the development of the control structure with the development of the machine intelligence; but the extra task load could be prohibitive for smaller projects and for projects caught in a close technology race.

One other type of cost also deserves consideration: the risk of mind crimes being committed in this kind of structure.35 As described, the institution sounds like a rather horrible North Korean labor camp. Yet there are ways of at least mitigating the moral problems with running this kind of institution, even if the subagents contained in the institution are emulations with full human moral status. At a minimum, the system could rely on volunteering emulations. Each subagent could have the option at any time of withdrawing its participation.36 Terminated emulations could be stored to memory, with a commitment to restart them under much more ideal conditions once the dangerous phase of the intelligence explosion is over. Meanwhile, subagents who chose to participate could be housed in very comfortable virtual environments and allowed ample time for sleep and recreation. These measures would impose a cost, one that should be manageable for a well-resourced project under noncompetitive conditions. In a highly competitive situation, the cost may be unaffordable unless an enterprise could be assured that its competitors would incur the same cost.

In the example, we imagined the subagents as emulations. One might wonder, does the institution design approach require that the subagents be anthropomorphic? Or is it equally applicable to systems composed of artificial subagents?

One’s first thought here might be skeptical. One notes that despite our plentiful experience with human-like agents, we still cannot precisely predict the outbreak or outcomes of revolutions; social science can, at most, describe some statistical tendencies.37 Since we cannot reliably predict the stability of social structures for ordinary human beings (about which we have much data), it is tempting to infer that we have little hope of precision-engineering stable social structures for cognitively enhanced human-like agents (about which we have no data), and that we have still less hope of doing so for advanced artificial agents (which are not even similar to agents that we have data about).

Yet the matter is not so cut-and-dried. Humans and human-like beings are complex; but artificial agents could have relatively simple architectures. Artificial agents could also have simple and explicitly characterized motivations. Furthermore, digital agents in general (whether emulations or artificial intelligences) are copyable: an affordance that may revolutionize management, much like interchangeable parts revolutionized manufacturing. These differences, together with the opportunity to work with agents that are initially powerless and to create institutional structures that use the various abovementioned control measures, might combine to make it possible to achieve particular institutional outcomes—such as a system that does not revolt—more reliably than if one were working with human beings under historical conditions.

But then again, artificial agents might lack many of the attributes that help us predict the behavior of human-like agents. Artificial agents need not have any of the social emotions that bind human behavior, emotions such as fear, pride, and remorse. Nor need artificial agents develop attachments to friends and family. Nor need they exhibit the unconscious body language that makes it difficult for us humans to conceal our intentions. These deficits might destabilize institutions of artificial agents. Moreover, artificial agents might be capable of making big leaps in cognitive performance as a result of seemingly small changes in their algorithms or architecture. Ruthlessly optimizing artificial agents might be willing to take extreme gambles from which humans would shrink.38 And superintelligent agents might show a surprising ability to coordinate with little or no communication (e.g. by internally modeling each other’s hypothetical responses to various contingencies). These and other differences could make sudden institutional failure more likely, even in the teeth of what seem like Kevlar-clad methods of social control.

It is unclear, therefore, how promising the institution design approach is, and whether it has a greater chance of working with anthropomorphic than with artificial agents. It might be thought that creating an institution with appropriate checks and balances could only increase safety—or, at any rate, not reduce safety—so that from a risk-mitigation perspective it would always be best if the method were used. But even this cannot be said with certainty. The approach adds parts and complexity, and thus may also introduce new ways for things to go wrong that do not exist in the case of an agent that does not have intelligent subagents as parts. Nevertheless, institution design is worthy of further exploration.

CHAPTER 14: The strategy picture

It is now time to consider the challenge of superintelligence in a broader context. We would like to orient ourselves in the strategic landscape sufficiently to know at least which general direction we should be heading. This, it turns out, is not at all easy. Here in the penultimate chapter, we introduce some general analytical concepts that help us think about longterm science and technology policy issues. We then apply them to the issue of machine intelligence.

It can be illuminating to make a rough distinction between two different normative stances from which a proposed policy may be evaluated. The personaffecting perspective asks whether a proposed change would be in “our interest”—that is to say, whether it would (on balance, and in expectation) be in the interest of those morally considerable creatures who either already exist or will come into existence independently of whether the proposed change occurs or not. The impersonal perspective, in contrast, gives no special consideration to currently existing people, or to those who will come to exist independently of whether the proposed change occurs. Instead, it counts everybody equally, independently of their temporal location. The impersonal perspective sees great value in bringing new people into existence, provided they have lives worth living: the more happy lives created, the better.

This distinction, although it barely hints at the moral complexities associated with a machine intelligence revolution, can be useful in a first-cut analysis. Here we will first examine matters from the impersonal perspective. We will later see what changes if person-affecting considerations are given weight in our deliberations.

Science and technology strategy

Before we zoom in on issues specific to machine superintelligence, we must introduce some strategic concepts and considerations that pertain to scientific and technological development more generally.

Differential technological development

Suppose that a policymaker proposes to cut funding for a certain research field, out of concern for the risks or long-term consequences of some hypothetical technology that might eventually grow from its soil. She can then expect a howl of opposition from the research community.

Scientists and their public advocates often say that it is futile to try to control the evolution of technology by blocking research. If some technology is feasible (the argument goes) it will be developed regardless of any particular policymaker’s scruples about speculative future risks. Indeed, the more powerful the capabilities that a line of development promises to produce, the surer we can be that somebody, somewhere, will be motivated to pursue it. Funding cuts will not stop progress or forestall its concomitant dangers.

Interestingly, this futility objection is almost never raised when a policymaker proposes to increase funding to some area of research, even though the argument would seem to cut both ways. One rarely hears indignant voices protest: “Please do not increase our funding. Rather, make some cuts. Researchers in other countries will surely pick up the slack; the same work will get done anyway. Don’t squander the public’s treasure on domestic scientific research!”

What accounts for this apparent doublethink? One plausible explanation, of course, is that members of the research community have a self-serving bias which leads us to believe that research is always good and tempts us to embrace almost any argument that supports our demand for more funding. However, it is also possible that the double standard can be justified in terms of national selfinterest. Suppose that the development of a technology has two effects: giving a small benefit B to its inventors and the country that sponsors them, while imposing an aggregately larger harm H—which could be a risk externality—on everybody. Even somebody who is largely altruistic might then choose to develop the overall harmful technology. They might reason that the harm H will result no matter what they do, since if they refrain somebody else will develop the technology anyway; and given that total welfare cannot be affected, they might as well grab the benefit B for themselves and their nation. (“Unfortunately, there will soon be a device that will destroy the world. Fortunately, we got the grant to build it!”)

Whatever the explanation for the futility objection’s appeal, it fails to show that there is in general no impersonal reason for trying to steer technological development. It fails even if we concede the motivating idea that with continued scientific and technological development efforts, all relevant technologies will eventually be developed—that is, even if we concede the following:

Technological completion conjecture

If scientific and technological development efforts do not effectively cease, then all important basic capabilities that could be obtained through some possible technology will be obtained.

There are at least two reasons why the technological completion conjecture does not imply the futility objection. First, the antecedent might not hold, because it is not in fact a given that scientific and technological development efforts will not effectively cease (before the attainment of technological maturity). This reservation is especially pertinent in a context that involves existential risk. Second, even if we could be certain that all important basic capabilities that could be obtained through some possible technology will be obtained, it could still make sense to attempt to influence the direction of technological research. What matters is not only whether a technology is developed, but also when it is developed, by whom, and in what context. These circumstances of birth of a new technology, which shape its impact, can be affected by turning funding spigots on or off (and by wielding other policy instruments).

These reflections suggest a principle that would have us attend to the relative speed with which different technologies are developed:

The principle of differential technological development

Retard the development of dangerous and harmful technologies, especially ones that raise the level of existential risk; and accelerate the development of beneficial technologies, especially those that reduce the existential risks posed by nature or by other technologies.

A policy could thus be evaluated on the basis of how much of a differential advantage it gives to desired forms of technological development over undesired forms.

Preferred order of arrival

Some technologies have an ambivalent effect on existential risks, increasing some existential risks while decreasing others. Superintelligence is one such technology.

We have seen in earlier chapters that the introduction of machine superintelligence would create a substantial existential risk. But it would reduce many other existential risks. Risks from nature—such as asteroid impacts, supervolcanoes, and natural pandemics—would be virtually eliminated, since superintelligence could deploy countermeasures against most such hazards, or at least demote them to the non-existential category (for instance, via space colonization).

These existential risks from nature are comparatively small over the relevant timescales. But superintelligence would also eliminate or reduce many anthropogenic risks. In particular, it would reduce risks of accidental destruction, including risk of accidents related to new technologies. Being generally more capable than humans, a superintelligence would be less likely to make mistakes, and more likely to recognize when precautions are needed, and to implement precautions competently. A well-constructed superintelligence might sometimes take a risk, but only when doing so is wise. Furthermore, at least in scenarios where the superintelligence forms a singleton, many non-accidental anthropogenic existential risks deriving from global coordination problems would be eliminated. These include risks of wars, technology races, undesirable forms of competition and evolution, and tragedies of the commons.

Since substantial peril would be associated with human beings developing synthetic biology, molecular nanotechnology, climate engineering, instruments for biomedical enhancement and neuropsychological manipulation, tools for social control that may facilitate totalitarianism or tyranny, and other technologies as-yet unimagined, eliminating these types of risk would be a great boon. An argument could therefore be mounted that earlier arrival dates of superintelligence are preferable. However, if risks from nature and from other hazards unrelated to future technology are small, then this argument could be refined: what matters is that we get superintelligence before other dangerous technologies, such as advanced nanotechnology. Whether it happens sooner or later may not be so important (from an impersonal perspective) so long as the order of arrival is right.

The ground for preferring superintelligence to come before other potentially dangerous technologies, such as nanotechnology, is that superintelligence would reduce the existential risks from nanotechnology but not vice versa.4 Hence, if we create superintelligence first, we will face only those existential risks that are associated with superintelligence; whereas if we create nanotechnology first, we will face the risks of nanotechnology and then, additionally, the risks of superintelligence.5 Even if the existential risks from superintelligence are very large, and even if superintelligence is the riskiest of all technologies, there could thus be a case for hastening its arrival.

These “sooner-is-better” arguments, however, presuppose that the riskiness of creating superintelligence is the same regardless of when it is created. If, instead, its riskiness declines over time, it might be better to delay the machine intelligence revolution. While a later arrival would leave more time for other existential catastrophes to intercede, it could still be preferable to slow the development of superintelligence. This would be especially plausible if the existential risks associated with superintelligence are much larger than those associated with other disruptive technologies.

There are several quite strong reasons to believe that the riskiness of an intelligence explosion will decline significantly over a multidecadal timeframe. One reason is that a later date leaves more time for the development of solutions to the control problem. The control problem has only recently been recognized, and most of the current best ideas for how to approach it were discovered only within the past decade or so (and in several cases during the time that this book was being written). It is plausible that the state of the art will advance greatly over the next several decades; and if the problem turns out to be very difficult, a significant rate of progress might continue for a century or more. The longer it takes for superintelligence to arrive, the more such progress will have been made when it does. This is an important consideration in favor of later arrival dates— and a very strong consideration against extremely early arrival dates.

Another reason why superintelligence later might be safer is that this would allow more time for various beneficial background trends of human civilization to play themselves out. How much weight one attaches to this consideration will depend on how optimistic one is about these trends.

An optimist could certainly point to a number of encouraging indicators and hopeful possibilities. People might learn to get along better, leading to reductions in violence, war, and cruelty; and global coordination and the scope of political integration might increase, making it easier to escape undesirable technology races (more on this below) and to work out an arrangement whereby the hopedfor gains from an intelligence explosion would be widely shared. There appear to be long-term historical trends in these directions.

Further, an optimist could expect that the “sanity level” of humanity will rise over the course of this century—that prejudices will (on balance) recede, that insights will accumulate, and that people will become more accustomed to thinking about abstract future probabilities and global risks. With luck, we could see a general uplift of epistemic standards in both individual and collective cognition. Again, there are trends pushing in these directions. Scientific progress means that more will be known. Economic growth may give a greater portion of the world’s population adequate nutrition (particularly during the early years of life that are important for brain development) and access to quality education. Advances in information technology will make it easier to find, integrate, evaluate, and communicate data and ideas. Furthermore, by the century’s end, humanity will have made an additional hundred years’ worth of mistakes, from which something might have been learned.

Many potential developments are ambivalent in the abovementioned sense— increasing some existential risks and decreasing others. For example, advances in surveillance, data mining, lie detection, biometrics, and psychological or neurochemical means of manipulating beliefs and desires could reduce some existential risks by making it easier to coordinate internationally or to suppress terrorists and renegades at home. These same advances, however, might also increase some existential risks by amplifying undesirable social dynamics or by enabling the formation of permanently stable totalitarian regimes.

One important frontier is the enhancement of biological cognition, such as through genetic selection. When we discussed this in Chapters 2 and 3, we concluded that the most radical forms of superintelligence would be more likely to arise in the form of machine intelligence. That claim is consistent with cognitive enhancement playing an important role in the lead-up to, and creation of, machine superintelligence. Cognitive enhancement might seem obviously risk-reducing: the smarter the people working on the control problem, the more likely they are to find a solution. However, cognitive enhancement could also hasten the development of machine intelligence, thus reducing the time available to work on the problem. Cognitive enhancement would also have many other relevant consequences. These issues deserve a closer look. (Most of the following remarks about “cognitive enhancement” apply equally to nonbiological means of increasing our individual or collective epistemic effectiveness.)

Rates of change and cognitive enhancement

An increase in either the mean or the upper range of human intellectual ability would likely accelerate technological progress across the board, including progress toward various forms of machine intelligence, progress on the control problem, and progress on a wide swath of other technical and economic objectives. What would be the net effect of such acceleration?

Consider the limiting case of a “universal accelerator,” an imaginary intervention that accelerates literally everything. The action of such a universal accelerator would correspond merely to an arbitrary rescaling of the time metric, producing no qualitative change in observed outcomes.

If we are to make sense of the idea that cognitive enhancement might generally speed things up, we clearly need some other concept than that of universal acceleration. A more promising approach is to focus on how cognitive enhancement might increase the rate of change in one type of process relative to the rate of change in some other type of process. Such differential acceleration could affect a system’s dynamics. Thus, consider the following concept:

Macro-structural development accelerator—A lever that accelerates the rate at which macro-structural features of the human condition develop, while leaving unchanged the rate at which micro-level human affairs unfold.

Imagine pulling this lever in the decelerating direction. A brake pad is lowered onto the great wheel of world history; sparks fly and metal screeches. After the wheel has settled into a more leisurely pace, the result is a world in which technological innovation occurs more slowly and in which fundamental or globally significant change in political structure and culture happens less frequently and less abruptly. A greater number of generations come and go before one era gives way to another. During the course of a lifespan, a person sees little change in the basic structure of the human condition.

For most of our species’ existence, macro-structural development was slower than it is now. Fifty thousand years ago, an entire millennium might have elapsed without a single significant technological invention, without any noticeable increase in human knowledge and understanding, and without any globally meaningful political change. On a micro-level, however, the kaleidoscope of human affairs churned at a reasonable rate, with births, deaths, and other personally and locally significant events. The average person’s day might have been more action-packed in the Pleistocene than it is today.

If you came upon a magic lever that would let you change the rate of macrostructural development, what should you do? Ought you to accelerate, decelerate, or leave things as they are?

Assuming the impersonal standpoint, this question requires us to consider the effects on existential risk. Let us distinguish between two kinds of risk: “state risks” and “step risks.” A state risk is one that is associated with being in a certain state, and the total amount of state risk to which a system is exposed is a direct function of how long the system remains in that state. Risks from nature are typically state risks: the longer we remain exposed, the greater the chance that we will get struck by an asteroid, supervolcanic eruption, gamma ray burst, naturally arising pandemic, or some other slash of the cosmic scythe. Some anthropogenic risks are also state risks. At the level of an individual, the longer a soldier pokes his head up above the parapet, the greater the cumulative chance he will be shot by an enemy sniper. There are anthropogenic state risks at the existential level as well: the longer we live in an internationally anarchic system, the greater the cumulative chance of a thermonuclear Armageddon or of a great war fought with other kinds of weapons of mass destruction, laying waste to civilization.

A step risk, by contrast, is a discrete risk associated with some necessary or desirable transition. Once the transition is completed, the risk vanishes. The amount of step risk associated with a transition is usually not a simple function of how long the transition takes. One does not halve the risk of traversing a minefield by running twice as fast. Conditional on a fast takeoff, the creation of superintelligence might be a step risk: there would be a certain risk associated with the takeoff, the magnitude of which would depend on what preparations had been made; but the amount of risk might not depend much on whether the takeoff takes twenty milliseconds or twenty hours.

We can then say the following regarding a hypothetical macro-structural development accelerator:

Insofar as we are concerned with existential state risks, we should favor acceleration—provided we think we have a realistic prospect of making it through to a post-transition era in which any further existential risks are greatly reduced.

If it were known that there is some step ahead destined to cause an existential catastrophe, then we ought to reduce the rate of macro-structural development (or even put it in reverse) in order to give more generations a chance to exist before the curtain is rung down. But, in fact, it would be overly pessimistic to be so confident that humanity is doomed.

At present, the level of existential state risk appears to be relatively low. If we imagine the technological macro-conditions for humanity frozen in their current state, it seems very unlikely that an existential catastrophe would occur on a timescale of, say, a decade. So a delay of one decade—provided it occurred at our current stage of development or at some other time when state risk is low— would incur only a very minor existential state risk, whereas a postponement by one decade of subsequent technological developments might well have a significant beneficial impact on later existential step risks, for example by allowing more time for preparation.

Upshot: the main way that the speed of macro-structural development is important is by affecting how well prepared humanity is when the time comes to confront the key step risks.

So the question we must ask is how cognitive enhancement (and concomitant acceleration of macro-structural development) would affect the expected level of preparedness at the critical juncture. Should we prefer a shorter period of preparation with higher intelligence? With higher intelligence, the preparation time could be used more effectively, and the final critical step would be taken by a more intelligent humanity. Or should we prefer to operate with closer to current levels of intelligence if that gives us more time to prepare?

Which option is better depends on the nature of the challenge being prepared for. If the challenge were to solve a problem for which learning from experience is key, then the chronological length of the preparation period might be the determining factor, since time is needed for the requisite experience to accumulate. What would such a challenge look like? One hypothetical example would be a new weapons technology that we could predict would be developed at some point in the future and that would make it the case that any subsequent war would have, let us say, a one-in-ten chance of causing an existential catastrophe. If such were the nature of the challenge facing us, then we might wish the rate of macro-structural development to be slow, so that our species would have more time to get its act together before the critical step when the new weapons technology is invented. One could hope that during the grace period secured through the deceleration, our species might learn to avoid war— that international relations around the globe might come to resemble those between the countries of the European Union, which, having fought one another ferociously for centuries, now coexist in peace and relative harmony. The pacification might occur as a result of the gentle edification from various civilizing processes or through the shock therapy of sub-existential blows (e.g. small nuclear conflagrations, and the recoil and resolve they might engender to finally create the global institutions necessary for the abolishment of interstate wars). If this kind of learning or adjusting would not be much accelerated by increased intelligence, then cognitive enhancement would be undesirable, serving merely to burn the fuse faster.

A prospective intelligence explosion, however, may present a challenge of a different kind. The control problem calls for foresight, reasoning, and theoretical insight. It is less clear how increased historical experience would help. Direct experience of the intelligence explosion is not possible (until too late), and many features conspire to make the control problem unique and lacking in relevant historical precedent. For these reasons, the amount of time that will elapse before the intelligence explosion may not matter much per se. Perhaps what matters, instead, is (a) the amount of intellectual progress on the control problem achieved by the time of the detonation; and (b) the amount of skill and intelligence available at the time to implement the best available solutions (and to improvise what is missing).9 That this latter factor should respond positively to cognitive enhancement is obvious. How cognitive enhancement would affect factor (a) is a somewhat subtler matter.

Suppose, as suggested earlier, that cognitive enhancement would be a general macro-structural development accelerator. This would hasten the arrival of the intelligence explosion, thus reducing the amount of time available for preparation and for making progress on the control problem. Normally this would be a bad thing. However, if the only reason why there is less time available for intellectual progress is that intellectual progress is speeded up, then there need be no net reduction in the amount of intellectual progress that will have taken place by the time the intelligence explosion occurs.

At this point, cognitive enhancement might appear to be neutral with respect to factor (a): the same intellectual progress that would otherwise have been made prior to the intelligence explosion—including progress on the control problem— still gets made, only compressed within a shorter time interval. In actuality, however, cognitive enhancement may well prove a positive influence on (a).

One reason why cognitive enhancement might cause more progress to have been made on the control problem by the time the intelligence explosion occurs is that progress on the control problem may be especially contingent on extreme levels of intellectual performance—even more so than the kind of work necessary to create machine intelligence. The role for trial and error and accumulation of experimental results seems quite limited in relation to the control problem, whereas experimental learning will probably play a large role in the development of artificial intelligence or whole brain emulation. The extent to which time can substitute for wit may therefore vary between tasks in a way that should make cognitive enhancement promote progress on the control problem more than it would promote progress on the problem of how to create machine intelligence.

Another reason why cognitive enhancement should differentially promote progress on the control problem is that the very need for such progress is more likely to be appreciated by cognitively more capable societies and individuals. It requires foresight and reasoning to realize why the control problem is important and to make it a priority.10 It may also require uncommon sagacity to find promising ways of approaching such an unfamiliar problem.

From these reflections we might tentatively conclude that cognitive enhancement is desirable, at least insofar as the focus is on the existential risks of an intelligence explosion. Parallel lines of thinking apply to other existential risks arising from challenges that require foresight and reliable abstract reasoning (as opposed to, e.g., incremental adaptation to experienced changes in the environment or a multigenerational process of cultural maturation and institution-building).

Technology couplings

Suppose that one thinks that solving the control problem for artificial intelligence is very difficult, that solving it for whole brain emulations is much easier, and that it would therefore be preferable that machine intelligence be reached via the whole brain emulation path. We will return later to the question of whether whole brain emulation would be safer than artificial intelligence. But for now we want to make the point that even if we accept this premiss, it would not follow that we ought to promote whole brain emulation technology. One reason, discussed earlier, is that a later arrival of superintelligence may be preferable, in order to allow more time for progress on the control problem and for other favorable background trends to culminate—and thus, if one were confident that whole brain emulation would precede AI anyway, it would be counterproductive to further hasten the arrival of whole brain emulation.

But even if it were the case that it would be best for whole brain emulation to arrive as soon as possible, it still would not follow that we ought to favor progress toward whole brain emulation. For it is possible that progress toward whole brain emulation will not yield whole brain emulation. It may instead yield neuromorphic artificial intelligence—forms of AI that mimic some aspects of cortical organization but do not replicate neuronal functionality with sufficient fidelity to constitute a proper emulation. If—as there is reason to believe—such neuromorphic AI is worse than the kind of AI that would otherwise have been built, and if by promoting whole brain emulation we would make neuromorphic AI arrive first, then our pursuit of the supposed best outcome (whole brain emulation) would lead to the worst outcome (neuromorphic AI); whereas if we had pursued the second-best outcome (synthetic AI) we might actually have attained the second-best (synthetic AI).

We have just described an (hypothetical) instance of what we might term a “technology coupling.”11 This refers to a condition in which two technologies have a predictable timing relationship, such that developing one of the technologies has a robust tendency to lead to the development of the other, either as a necessary precursor or as an obvious and irresistible application or subsequent step. Technology couplings must be taken into account when we use the principle of differential technological development: it is no good accelerating the development of a desirable technology Y if the only way of getting Y is by developing an extremely undesirable precursor technology X, or if getting Y would immediately produce an extremely undesirable related technology Z. Before you marry your sweetheart, consider the prospective in-laws.

In the case of whole brain emulation, the degree of technology coupling is debatable. We noted in Chapter 2 that while whole brain emulation would require massive progress in various enabling technologies, it might not require any major new theoretical insight. In particular, it does not require that we understand how human cognition works, only that we know how to build computational models of small parts of the brain, such as different species of neuron. Nevertheless, in the course of developing the ability to emulate human brains, a wealth of neuroanatomical data would be collected, and functional models of cortical networks would surely be greatly improved. Such progress would seem to have a good chance of enabling neuromorphic AI before fullblown whole brain emulation.12 Historically, there are quite a few examples of AI techniques gleaned from neuroscience or biology. (For example: the McCulloch–Pitts neuron, perceptrons, and other artificial neurons and neural networks, inspired by neuroanatomical work; reinforcement learning, inspired by behaviorist psychology; genetic algorithms, inspired by evolution theory; subsumption architectures and perceptual hierarchies, inspired by cognitive science theories about motor planning and sensory perception; artificial immune systems, inspired by theoretical immunology; swarm intelligence, inspired by the ecology of insect colonies and other self-organizing systems; and reactive and behavior-based control in robotics, inspired by the study of animal locomotion.) Perhaps more significantly, there are plenty of important AIrelevant questions that could potentially be answered through further study of the brain. (For example: How does the brain store structured representations in working memory and long-term memory? How is the binding problem solved? What is the neural code? How are concepts represented? Is there some standard unit of cortical processing machinery, such as the cortical column, and if so how is it wired and how does its functionality depend on the wiring? How can such columns be linked up, and how can they learn?)

We will shortly have more to say about the relative danger of whole brain emulation, neuromorphic AI, and synthetic AI, but we can already flag another important technology coupling: that between whole brain emulation and AI. Even if a push toward whole brain emulation actually resulted in whole brain emulation (as opposed to neuromorphic AI), and even if the arrival of whole brain emulation could be safely handled, a further risk would still remain: the risk associated with a second transition, a transition from whole brain emulation to AI, which is an ultimately more powerful form of machine intelligence.

There are many other technology couplings, which could be considered in a more comprehensive analysis. For instance, a push toward whole brain emulation would boost neuroscience progress more generally.13 That might produce various effects, such as faster progress toward lie detection, neuropsychological manipulation techniques, cognitive enhancement, and assorted medical advances. Likewise, a push toward cognitive enhancement might (depending on the specific path pursued) create spillovers such as faster development of genetic selection and genetic engineering methods not only for enhancing cognition but for modifying other traits as well.

Second-guessing

We encounter another layer of strategic complexity if we take into account that there is no perfectly benevolent, rational, and unified world controller who simply implements what has been discovered to be the best option. Any abstract point about “what should be done” must be embodied in the form of a concrete message, which is entered into the arena of rhetorical and political reality. There it will be ignored, misunderstood, distorted, or appropriated for various conflicting purposes; it will bounce around like a pinball, causing actions and reactions, ushering in a cascade of consequences, the upshot of which need bear no straightforward relationship to the intentions of the original sender.

Pathways and enablers

Should we celebrate advances in computer hardware? What about advances on the path toward whole brain emulation? We will look at these two questions in turn.

Effects of hardware progress

Faster computers make it easier to create machine intelligence. One effect of accelerating progress in hardware, therefore, is to hasten the arrival of machine intelligence. As discussed earlier, this is probably a bad thing from the impersonal perspective, since it reduces the amount of time available for solving the control problem and for humanity to reach a more mature stage of civilization. The case is not a slam dunk, though. Since superintelligence would eliminate many other existential risks, there could be reason to prefer earlier development if the level of these other existential risks were very high.

Hastening or delaying the onset of the intelligence explosion is not the only channel through which the rate of hardware progress can affect existential risk. Another channel is that hardware can to some extent substitute for software; thus, better hardware reduces the minimum skill required to code a seed AI. Fast computers might also encourage the use of approaches that rely more heavily on brute-force techniques (such as genetic algorithms and other generate-evaluatediscard methods) and less on techniques that require deep understanding to use. If brute-force techniques lend themselves to more anarchic or imprecise system designs, where the control problem is harder to solve than in more precisely engineered and theoretically controlled systems, this would be another way in which faster computers would increase the existential risk.

Another consideration is that rapid hardware progress increases the likelihood of a fast takeoff. The more rapidly the state of the art advances in the semiconductor industry, the fewer the person-hours of programmers’ time spent exploiting the capabilities of computers at any given performance level. This means that an intelligence explosion is less likely to be initiated at the lowest level of hardware performance at which it is feasible. An intelligence explosion is thus more likely to be initiated when hardware has advanced significantly beyond the minimum level at which the eventually successful programming approach could first have succeeded. There is then a hardware overhang when the takeoff eventually does occur. As we saw in Chapter 4, hardware overhang is one of the main factors that reduce recalcitrance during the takeoff. Rapid hardware progress, therefore, will tend to make the transition to superintelligence faster and more explosive.

A faster takeoff via a hardware overhang can affect the risks of the transition in several ways. The most obvious is that a faster takeoff offers less opportunity to respond and make adjustments whilst the transition is in progress, which would tend to increase risk. A related consideration is that a hardware overhang would reduce the chances that a dangerously self-improving seed AI could be contained by limiting its ability to colonize sufficient hardware: the faster each processor is, the fewer processors would be needed for the AI to quickly bootstrap itself to superintelligence. Yet another effect of a hardware overhang is to level the playing field between big and small projects by reducing the importance of one of the advantages of larger projects—the ability to afford more powerful computers. This effect, too, might increase existential risk, if larger projects are more likely to solve the control problem and to be pursuing morally acceptable objectives.

There are also advantages to a faster takeoff. A faster takeoff would increase the likelihood that a singleton will form. If establishing a singleton is sufficiently important for solving the post-transition coordination problems, it might be worth accepting a greater risk during the intelligence explosion in order to mitigate the risk of catastrophic coordination failures in its aftermath.

Developments in computing can affect the outcome of a machine intelligence revolution not only by playing a direct role in the construction of machine intelligence but also by having diffuse effects on society that indirectly help shape the initial conditions of the intelligence explosion. The Internet, which required hardware to be good enough to enable personal computers to be mass produced at low cost, is now influencing human activity in many areas, including work in artificial intelligence and research on the control problem. (This book might not have been written, and you might not have found it, without the Internet.) However, hardware is already good enough for a great many applications that could facilitate human communication and deliberation, and it is not clear that the pace of progress in these areas is strongly bottlenecked by the rate of hardware improvement.

On balance, it appears that faster progress in computing hardware is undesirable from the impersonal evaluative standpoint. This tentative conclusion could be overturned, for example if the threats from other existential risks or from post-transition coordination failures turn out to be extremely large. In any case, it seems difficult to have much leverage on the rate of hardware advancement. Our efforts to improve the initial conditions for the intelligence explosion should therefore probably focus on other parameters.

Note that even when we cannot see how to influence some parameter, it can be useful to determine its “sign” (i.e. whether an increase or decrease in that parameter would be desirable) as a preliminary step in mapping the strategic lay of the land. We might later discover a new leverage point that does enable us to manipulate the parameter more easily. Or we might discover that the parameter’s sign correlates with the sign of some other more manipulable parameter, so that our initial analysis helps us decide what to do with this other parameter.

Should whole brain emulation research be promoted?

The harder it seems to solve the control problem for artificial intelligence, the more tempting it is to promote the whole brain emulation path as a less risky alternative. There are several issues, however, that must be analyzed before one can arrive at a well-considered judgment.

First, there is the issue of technology coupling, already discussed earlier. We pointed out that an effort to develop whole brain emulation could result in neuromorphic AI instead, a form of machine intelligence that may be especially unsafe.

But let us assume, for the sake of argument, that we actually achieve whole brain emulation (WBE). Would this be safer than AI? This, itself, is a complicated issue. There are at least three putative advantages of WBE: (i) that its performance characteristics would be better understood than those of AI; (ii) that it would inherit human motives; and (iii) that it would result in a slower takeoff. Let us very briefly reflect on each.

That it should be easier to understand the intellectual performance characteristics of an emulation than of an AI sounds plausible. We have abundant experience with the strengths and weaknesses of human intelligence but no experience with human-level artificial intelligence. However, to understand what a snapshot of a digitized human intellect can and cannot do is not the same as to understand how such an intellect will respond to modifications aimed at enhancing its performance. An artificial intellect, by contrast, might be carefully designed to be understandable, in both its static and dynamic dispositions. So while whole brain emulation may be more predictable in its intellectual performance than a generic AI at a comparable stage of development, it is unclear whether whole brain emulation would be dynamically more predictable than an AI engineered by competent safety-conscious programmers.

As for an emulation inheriting the motivations of its human template, this is far from guaranteed. Capturing human evaluative dispositions might require a very high-fidelity emulation. Even if some individual’s motivations were perfectly captured, it is unclear how much safety would be purchased. Humans can be untrustworthy, selfish, and cruel. While templates would hopefully be selected for exceptional virtue, it may be hard to foretell how someone will act when transplanted into radically alien circumstances, superhumanly enhanced in intelligence, and tempted with an opportunity for world domination. It is true that emulations would at least be more likely to have human-like motivations (as opposed to valuing only paperclips or discovering digits of pi). Depending on one’s views on human nature, this might or might not be reassuring.

It is not clear why whole brain emulation should result in a slower takeoff than artificial intelligence. Perhaps with whole brain emulation one should expect less hardware overhang, since whole brain emulation is less computationally efficient than artificial intelligence can be. Perhaps, also, an AI system could more easily absorb all available computing power into one giant integrated intellect, whereas whole brain emulation would forego quality superintelligence and pull ahead of humanity only in speed and size of population. If whole brain emulation does lead to a slower takeoff, this could have benefits in terms of alleviating the control problem. A slower takeoff would also make a multipolar outcome more likely. But whether a multipolar outcome is desirable is very doubtful.

There is another important complication with the general idea that getting whole brain emulation first is safer: the need to cope with a second transition. Even if the first form of human-level machine intelligence is emulation-based, it would still remain feasible to develop artificial intelligence. AI in its mature form has important advantages over WBE, making AI the ultimately more powerful technology.23 While mature AI would render WBE obsolete (except for the special purpose of preserving individual human minds), the reverse does not hold.

What this means is that if AI is developed first, there might be a single wave of the intelligence explosion. But if WBE is developed first, there may be two waves: first, the arrival of WBE; and later, the arrival of AI. The total existential risk along the WBE-first path is the sum of the risk in the first transition and the risk in the second transition (conditional on having made it through the first)

The person-affecting perspective favors speed

I fear the blog commenter “washbash” may speak for many when he or she writes:

I instinctively think go faster. Not because I think this is better for the world. Why should I care about the world when I am dead and gone? I want it to go fast, damn it! This increases the chance I have of experiencing a more technologically advanced future.

From the person-affecting standpoint, we have greater reason to rush forward with all manner of radical technologies that could pose existential risks. This is because the default outcome is that almost everyone who now exists is dead within a century.

The case for rushing is especially strong with regard to technologies that could extend our lives and thereby increase the expected fraction of the currently existing population that may still be around for the intelligence explosion. If the machine intelligence revolution goes well, the resulting superintelligence could almost certainly devise means to indefinitely prolong the lives of the then stillexisting humans, not only keeping them alive but restoring them to health and youthful vigor, and enhancing their capacities well beyond what we currently think of as the human range; or helping them shuffle off their mortal coils altogether by uploading their minds to a digital substrate and endowing their liberated spirits with exquisitely good-feeling virtual embodiments. With regard to technologies that do not promise to save lives, the case for rushing is weaker, though perhaps still sufficiently supported by the hope of raised standards of living.

The same line of reasoning makes the person-affecting perspective favor many risky technological innovations that promise to hasten the onset of the intelligence explosion, even when those innovations are disfavored in the impersonal perspective. Such innovations could shorten the wolf hours during which we individually must hang on to our perch if we are to live to see the daybreak of the posthuman age. From the person-affecting standpoint, faster hardware progress thus seems desirable, as does faster progress toward WBE. Any adverse effect on existential risk is probably outweighed by the personal benefit of an increased chance of the intelligence explosion happening in the lifetime of currently existing people.

Collaboration

One important parameter is the degree to which the world will manage to coordinate and collaborate in the development of machine intelligence. Collaboration would bring many benefits. Let us take a look at how this parameter might affect the outcome and what levers we might have for increasing the extent and intensity of collaboration.

The race dynamics and its perils

A race dynamic exists when one project fears being overtaken by another. This does not require the actual existence of multiple projects. A situation with only one project could exhibit a race dynamic if that project is unaware of its lack of competitors. The Allies would probably not have developed the atomic bomb as quickly as they did had they not believed (erroneously) that the Germans might be close to the same goal.

The severity of a race dynamic (that is, the extent to which competitors prioritize speed over safety) depends on several factors, such as the closeness of the race, the relative importance of capability and luck, the number of competitors, whether competing teams are pursuing different approaches, and the degree to which projects share the same aims. Competitors’ beliefs about these factors are also relevant.

In the development of machine superintelligence, it seems likely that there will be at least a mild race dynamic, and it is possible that there will be a severe race dynamic. The race dynamic has important consequences for how we should think about the strategic challenge posed by the possibility of an intelligence explosion.

The race dynamic could spur projects to move faster toward superintelligence while reducing investment in solving the control problem. Additional detrimental effects of the race dynamic are also possible, such as direct hostilities between competitors. Suppose that two nations are racing to develop the first superintelligence, and that one of them is seen to be pulling ahead. In a winner-takes-all situation, a lagging project might be tempted to launch a desperate strike against its rival rather than passively await defeat. Anticipating this possibility, the frontrunner might be tempted to strike preemptively. If the antagonists are powerful states, the clash could be bloody.34 (A “surgical strike” against the rival’s AI project might risk triggering a larger confrontation and might in any case not be feasible if the host country has taken precautions.)

Scenarios in which the rival developers are not states but smaller entities, such as corporate labs or academic teams, would probably feature much less direct destruction from conflict. Yet the overall consequences of competition may be almost as bad. This is because the main part of the expected harm from competition stems not from the smashup of battle but from the downgrade of precaution. A race dynamic would, as we saw, reduce investment in safety; and conflict, even if nonviolent, would tend to scotch opportunities for collaboration, since projects would be less likely to share ideas for solving the control problem in a climate of hostility and mistrust.

On the benefits of collaboration

Collaboration thus offers many benefits. It reduces the haste in developing machine intelligence. It allows for greater investment in safety. It avoids violent conflicts. And it facilitates the sharing of ideas about how to solve the control problem. To these benefits we can add another: collaboration would tend to produce outcomes in which the fruits of a successfully controlled intelligence explosion get distributed more equitably.

That broader collaboration should result in wider sharing of gains is not axiomatic. In principle, a small project run by an altruist could lead to an outcome where the benefits are shared evenly or equitably among all morally considerable beings. Nevertheless, there are several reasons to suppose that broader collaborations, involving a greater number of sponsors, are (in expectation) distributionally superior. One such reason is that sponsors presumably prefer an outcome in which they themselves get (at least) their fair share. A broad collaboration then means that relatively many individuals get at least their fair share, assuming the project is successful. Another reason is that a broad collaboration also seems likelier to benefit people outside the collaboration. A broader collaboration contains more members, so more outsiders would have personal ties to somebody on the inside looking out for their interests. A broader collaboration is also more likely to include at least some altruist who wants to benefit everyone. Furthermore, a broader collaboration is more likely to operate under public oversight, which might reduce the risk of the entire pie being captured by a clique of programmers or private investors.37 Note also that the larger the successful collaboration is, the lower the costs to it of extending the benefits to all outsiders. (For instance, if 90% of all people were already inside the collaboration, it would cost them no more than 10% of their holdings to bring all outsiders up to their own level.)

It is thus plausible that broader collaborations would tend to lead to a wider distribution of the gains (though some projects with few sponsors might also have distributionally excellent aims). But why is a wide distribution of gains desirable?

There are both moral and prudential reasons for favoring outcomes in which everybody gets a share of the bounty. We will not say much about the moral case, except to note that it need not rest on any egalitarian principle. The case might be made, for example, on grounds of fairness. A project that creates machine superintelligence imposes a global risk externality. Everybody on the planet is placed in jeopardy, including those who do not consent to having their own lives and those of their family imperiled in this way. Since everybody shares the risk, it would seem to be a minimal requirement of fairness that everybody also gets a share of the upside.

The fact that the total (expected) amount of good seems greater in collaboration scenarios is another important reason such scenarios are morally preferable.

The prudential case for favoring a wide distribution of gains is two-pronged. One prong is that wide distribution should promote collaboration, thereby mitigating the negative consequences of the race dynamic. There is less incentive to fight over who gets to build the first superintelligence if everybody stands to benefit equally from any project’s success. The sponsors of a particular project might also benefit from credibly signaling their commitment to distributing the spoils universally, a certifiably altruistic project being likely to attract more supporters and fewer enemies.

The other prong of the prudential case for favoring a wide distribution of gains has to do with whether agents are risk-averse or have utility functions that are sublinear in resources. The central fact here is the enormousness of the potential resource pie. Assuming the observable universe is as uninhabited as it looks, it contains more than one vacant galaxy for each human being alive. Most people would much rather have certain access to one galaxy’s worth of resources than a lottery ticket offering a one-in-a-billion chance of owning a billion galaxies.39 Given the astronomical size of humanity’s cosmic endowment, it seems that self-interest should generally favor deals that would guarantee each person a share, even if each share corresponded to a small fraction of the total. The important thing, when such an extravagant bonanza is in the offing, is to not be left out in the cold.

This argument from the enormousness of the resource pie presupposes that preferences are resource-satiable.40 That supposition does not necessarily hold. For instance, several prominent ethical theories—including especially aggregative consequentialist theories—correspond to utility functions that are risk-neutral and linear in resources. A billion galaxies could be used to create a billion times more happy lives than a single galaxy. They are thus, to a utilitarian, worth a billion times as much.41 Ordinary selfish human preference functions, however, appear to be relatively resource-satiable.

This last statement must be flanked by two important qualifications. The first is that many people care about rank. If multiple agents each wants to top the Forbes rich list, then no resource pie is large enough to give everybody full satisfaction.

The second qualification is that the post-transition technology base would enable material resources to be converted into an unprecedented range of products, including some goods that are not currently available at any price even though they are highly valued by many humans. A billionaire does not live a thousand times longer than a millionaire. In the era of digital minds, however, the billionaire could afford a thousandfold more computing power and could thus enjoy a thousandfold longer subjective lifespan. Mental capacity, likewise, could be for sale. In such circumstances, with economic capital convertible into vital goods at a constant rate even for great levels of wealth, unbounded greed would make more sense than it does in today’s world where the affluent (those among them lacking a philanthropic heart) are reduced to spending their riches on airplanes, boats, art collections, or a fourth and a fifth residence.

Does this mean that an egoist should be risk-neutral with respect to his or her post-transition resource endowment? Not quite. Physical resources may not be convertible into lifespan or mental performance at arbitrary scales. If a life must be lived sequentially, so that observer moments can remember earlier events and be affected by prior choices, then the life of a digital mind cannot be extended arbitrarily without utilizing an increasing number of sequential computational operations. But physics limits the extent to which resources can be transformed into sequential computations.42 The limits on sequential computation may also constrain some aspects of cognitive performance to scale radically sublinearly beyond a relatively modest resource endowment. Furthermore, it is not obvious that an egoist would or should be risk-neutral even with regard to highly normatively relevant outcome metrics such as number of quality-adjusted subjective life years. If offered the choice between an extra 2,000 years of life for certain and a one-in-ten chance of an extra 30,000 years of life, I think most people would select the former (even under the stipulation that each life year would be of equal quality).

In reality, the prudential case for favoring a wide distribution of gains is presumably subject-relative and situation-dependent. Yet, on the whole, people would be more likely to get (almost all of) what they want if a way is found to achieve a wide distribution—and this holds even before taking into account that a commitment to a wider distribution would tend to foster collaboration and thereby increase the chances of avoiding existential catastrophe. Favoring a broad distribution, therefore, appears to be not only morally mandated but also prudentially advisable.

There is a further set of consequences to collaboration that should be given at least some shrift: the possibility that pre-transition collaboration influences the level of post-transition collaboration. Assume humanity solves the control problem. (If the control problem is not solved, it may scarcely matter how much collaboration there is post transition.) There are two cases to consider. The first is that the intelligence explosion does not create a winner-takes-all dynamic (presumably because the takeoff is relatively slow). In this case it is plausible that if pre-transition collaboration has any systematic effect on post-transition collaboration, it has a positive effect, tending to promote subsequent collaboration. The original collaborative relationships may endure and continue beyond the transition; also, pre-transition collaboration may offer more opportunity for people to steer developments in desirable (and, presumably, more collaborative) post-transition directions.

The second case is that the nature of the intelligence explosion does encourage a winner-takes-all dynamic (presumably because the takeoff is relatively fast). In this case, if there is no extensive collaboration before the takeoff, a singleton is likely to emerge—a single project would undergo the transition alone, at some point obtaining a decisive strategic advantage combined with superintelligence. A singleton, by definition, is a highly collaborative social order.44 The absence of extensive collaboration pre-transition would thus lead to an extreme degree of collaboration post-transition. By contrast, a somewhat higher level of collaboration in the run-up to the intelligence explosion opens up a wider variety of possible outcomes. Collaborating projects could synchronize their ascent to ensure they transition in tandem without any of them getting a decisive strategic advantage. Or different sponsor groups might merge their efforts into a single project, while refusing to give that project a mandate to form a singleton. For example, one could imagine a consortium of nations forming a joint scientific project to develop machine superintelligence, yet not authorizing this project to evolve into anything like a supercharged United Nations, electing instead to maintain the factious world order that existed before.

Particularly in the case of a fast takeoff, therefore, the possibility exists that greater pre-transition collaboration would result in less post-transition collaboration. However, to the extent that collaborating entities are able to shape the outcome, they may allow the emergence or continuation of non-collaboration only if they foresee that no catastrophic consequences would follow from posttransition factiousness. Scenarios in which pre-transition collaboration leads to reduced post-transition collaboration may therefore mostly be ones in which reduced post-transition collaboration is innocuous.

In general, greater post-transition collaboration appears desirable. It would reduce the risk of dystopian dynamics in which economic competition and a rapidly expanding population lead to a Malthusian condition, or in which evolutionary selection erodes human values and selects for non-eudaemonic forms, or in which rival powers suffer other coordination failures such as wars and technology races. The last of these issues, the prospect of technology races, may be particularly problematic if the transition is to an intermediary form of machine intelligence (whole brain emulation) since it would create a new race dynamic that would harm the chances of the control problem being solved for the subsequent second transition to a more advanced form of machine intelligence (artificial intelligence).

We described earlier how collaboration can reduce conflict in the run-up to the intelligence explosion, increasing the chances that the control problem will be solved, and improve both the moral legitimacy and the prudential desirability of the resulting resource allocation. To these benefits of collaboration it may thus be possible to add one more: that broader collaboration pre-transition could help with important coordination problems in the post-transition era.

Working together

Collaboration can take different forms depending on the scale of the collaborating entities. At a small scale, individual AI teams who believe themselves to be in competition with one another could choose to pool their efforts.45 Corporations could merge or cross-invest. At a larger scale, states could join in a big international project. There are precedents to large-scale international collaboration in science and technology (such as CERN, the Human Genome Project, and the International Space Station), but an international project to develop safe superintelligence would pose a different order of challenge because of the security implications of the work. It would have to be constituted not as an open academic collaboration but as an extremely tightly controlled joint enterprise. Perhaps the scientists involved would have to be physically isolated and prevented from communicating with the rest of the world for the duration of the project, except through a single carefully vetted communication channel. The required level of security might be nearly unattainable at present, but advances in lie detection and surveillance technology could make it feasible later this century. It is also worth bearing in mind that broad collaboration does not necessarily mean that large numbers of researchers would be involved in the project; it simply means that many people would have a say in the project’s aims. In principle, a project could involve a maximally broad collaboration comprising all of humanity as sponsors (represented, let us say, by the General Assembly of the United Nations), yet employ only a single scientist to carry out the work.

There is a reason for starting collaboration as early as possible, namely to take advantage of the veil of ignorance that hides from our view any specific information about which individual project will get to superintelligence first. The closer to the finishing line we get, the less uncertainty will remain about the relative chances of competing projects; and the harder it may consequently be to make a case based on the self-interest of the frontrunner to join a collaborative project that would distribute the benefits to all of humanity. On the other hand, it also looks hard to establish a formal collaboration of worldwide scope before the prospect of superintelligence has become much more widely recognized than it currently is and before there is a clearly visible road leading to the creation of machine superintelligence. Moreover, to the extent that collaboration would promote progress along that road, it may actually be counterproductive in terms of safety, as discussed earlier.

The ideal form of collaboration for the present may therefore be one that does not initially require specific formalized agreements and that does not expedite advances in machine intelligence. One proposal that fits these criteria is that we propound an appropriate moral norm, expressing our commitment to the idea that superintelligence should be for the common good. Such a norm could be formulated as follows:

The common good principle

Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals.

Establishing from an early stage that the immense potential of superintelligence belongs to all of humanity will give more time for such a norm to become entrenched.

The common good principle does not preclude commercial incentives for individuals or firms active in related areas. For example, a firm might satisfy the call for universal sharing of the benefits of superintelligence by adopting a “windfall clause” to the effect that all profits up to some very high ceiling (say, a trillion dollars annually) would be distributed in the ordinary way to the firm’s shareholders and other legal claimants, and that only profits in excess of the threshold would be distributed to all of humanity evenly (or otherwise according to universal moral criteria). Adopting such a windfall clause should be substantially costless, any given firm being extremely unlikely ever to exceed the stratospheric profit threshold (and such low-probability scenarios ordinarily playing no role in the decisions of the firm’s managers and investors). Yet its widespread adoption would give humankind a valuable guarantee (insofar as the commitments could be trusted) that if ever some private enterprise were to hit the jackpot with the intelligence explosion, everybody would share in most of the benefits. The same idea could be applied to entities other than firms. For example, states could agree that if ever any one state’s GDP exceeds some very high fraction (say, 90%) of world GDP, the overshoot should be distributed evenly to all.

The common good principle (and particular instantiations, such as windfall clauses) could be adopted initially as a voluntary moral commitment by responsible individuals and organizations that are active in areas related to machine intelligence. Later, it could be endorsed by a wider set of entities and enacted into law and treaty. A vague formulation, such as the one given here, may serve well as a starting point; but it would ultimately need to be sharpened into a set of specific verifiable requirements.

CHAPTER 15: Crunch time

We find ourselves in a thicket of strategic complexity, surrounded by a dense mist of uncertainty. Though many considerations have been discerned, their details and interrelationships remain unclear and iffy—and there might be other factors we have not even thought of yet. What are we to do in this predicament?

Philosophy with a deadline

A colleague of mine likes to point out that a Fields Medal (the highest honor in mathematics) indicates two things about the recipient: that he was capable of accomplishing something important, and that he didn’t. Though harsh, the remark hints at a truth.

Think of a “discovery” as an act that moves the arrival of information from a later point in time to an earlier time. The discovery’s value does not equal the value of the information discovered but rather the value of having the information available earlier than it otherwise would have been. A scientist or a mathematician may show great skill by being the first to find a solution that has eluded many others; yet if the problem would soon have been solved anyway, then the work probably has not much benefited the world. There are cases in which having a solution even slightly sooner is immensely valuable, but this is most plausible when the solution is immediately put to use, either being deployed for some practical end or serving as a foundation to further theoretical work. And in the latter case, where a solution is immediately used only in the sense of serving as a building block for further theorizing, there is great value in obtaining a solution slightly sooner only if the further work it enables is itself both important and urgent.

The question, then, is not whether the result discovered by the Fields Medalist is in itself “important” (whether instrumentally or for knowledge’s own sake). Rather, the question is whether it was important that the medalist enabled the publication of the result to occur at an earlier date. The value of this temporal transport should be compared to the value that a world-class mathematical mind could have generated by working on something else. At least in some cases, the Fields Medal might indicate a life spent solving the wrong problem—for instance, a problem whose allure consisted primarily in being famously difficult to solve.

Similar barbs could be directed at other fields, such as academic philosophy. Philosophy covers some problems that are relevant to existential risk mitigation —we encountered several in this book. Yet there are also subfields within philosophy that have no apparent link to existential risk or indeed any practical concern. As with pure mathematics, some of the problems that philosophy studies might be regarded as intrinsically important, in the sense that humans have reason to care about them independently of any practical application. The fundamental nature of reality, for instance, might be worth knowing about, for its own sake. The world would arguably be less glorious if nobody studied metaphysics, cosmology, or string theory. However, the dawning prospect of an intelligence explosion shines a new light on this ancient quest for wisdom.

The outlook now suggests that philosophic progress can be maximized via an indirect path rather than by immediate philosophizing. One of the many tasks on which superintelligence (or even just moderately enhanced human intelligence) would outperform the current cast of thinkers is in answering fundamental questions in science and philosophy. This reflection suggests a strategy of deferred gratification. We could postpone work on some of the eternal questions for a little while, delegating that task to our hopefully more competent successors—in order to focus our own attention on a more pressing challenge: increasing the chance that we will actually have competent successors. This would be high-impact philosophy and high-impact mathematics.

What is to be done?

We thus want to focus on problems that are not only important but urgent in the sense that their solutions are needed prior to the intelligence explosion. We should also take heed not to work on problems that are negative-value (such that solving them is harmful). Some technical problems in the field of artificial intelligence, for instance, might be negative-value inasmuch as their solution would speed the development of machine intelligence without doing as much to expedite the development of control methods that could render the machine intelligence revolution survivable and beneficial.

It can be hard to identify problems that are both urgent and important and are such that we can confidently take them to be positive-value. The strategic uncertainty surrounding existential risk mitigation means that we must worry that even well-intentioned interventions may turn out to be not only unproductive but counterproductive. To limit the risk of doing something actively harmful or morally wrong, we should prefer to work on problems that seem robustly positive-value (i.e., whose solution would make a positive contribution across a wide range of scenarios) and to employ means that are robustly justifiable (i.e., acceptable from a wide range of moral views).

There is a further desideratum to consider in selecting which problems to prioritize. We want to work on problems that are elastic to our efforts at solving them. Highly elastic problems are those that can be solved much faster, or solved to a much greater extent, given one extra unit of effort. Encouraging more kindness in the world is an important and urgent problem—one, moreover, that seems quite robustly positive-value: yet absent a breakthrough idea for how to go about it, probably a problem of quite low elasticity. Achieving world peace, similarly, would be highly desirable; but considering the numerous efforts already targeting that problem, and the formidable obstacles arrayed against a quick solution, it seems unlikely that the contributions of a few extra individuals would make a large difference.

To reduce the risks of the machine intelligence revolution, we will propose two objectives that appear to best meet all those desiderata: strategic analysis and capacity-building. We can be relatively confident about the sign of these parameters—more strategic insight and more capacity being better. Furthermore, the parameters are elastic: a small extra investment can make a relatively large difference. Gaining insight and capacity is also urgent because early boosts to these parameters may compound, making subsequent efforts more effective. In addition to these two broad objectives, we will point to a few other potentially worthwhile aims for initiatives.

Seeking the strategic light

Against a backdrop of perplexity and uncertainty, analysis stands out as being of particularly high expected value.3 Illumination of our strategic situation would help us target subsequent interventions more effectively. Strategic analysis is especially needful when we are radically uncertain not just about some detail of some peripheral matter but about the cardinal qualities of the central things. For many key parameters, we are radically uncertain even about their sign—that is, we know not which direction of change would be desirable and which undesirable. Our ignorance might not be irremediable. The field has been little prospected, and glimmering strategic insights could still be awaiting their unearthing just a few feet beneath the surface.

What we mean by “strategic analysis” here is a search for crucial considerations: ideas or arguments with the potential to change our views not merely about the fine-structure of implementation but about the general topology of desirability.4 Even a single missed crucial consideration could vitiate our most valiant efforts or render them as actively harmful as those of a soldier who is fighting on the wrong side. The search for crucial considerations (which must explore normative as well as descriptive issues) will often require crisscrossing the boundaries between different academic disciplines and other fields of knowledge. As there is no established methodology for how to go about this kind of research, difficult original thinking is necessary.

Building good capacity

Another high-value activity, one that shares with strategic analysis the robustness property of being beneficial across a wide range of scenarios, is the development of a well-constituted support base that takes the future seriously. Such a base can immediately provide resources for research and analysis. If and when other priorities become visible, resources can be redirected accordingly. A support base is thus a general-purpose capability whose use can be guided by new insights as they emerge.

One valuable asset would be a donor network comprising individuals devoted to rational philanthropy, informed about existential risk, and discerning about the means of mitigation. It is especially desirable that the early-day funders be astute and altruistic, because they may have opportunities to shape the field’s culture before the usual venal interests take up position and entrench. The focus during these opening gambits should thus be to recruit the right kinds of people into the field. It could be worth foregoing some technical advances in the short term in order to fill the ranks with individuals who genuinely care about safety and who have a truth-seeking orientation (and who are likely to attract more of their own kind).

One important variable is the quality of the “social epistemology” of the AIfield and its leading projects. Discovering crucial considerations is valuable, but only if it affects action. This cannot always be taken for granted. Imagine a project that invests millions of dollars and years of toil to develop a prototype AI, and that after surmounting many technical challenges the system is finally beginning to show real progress. There is a chance that with just a bit more work it could turn into something useful and profitable. Now a crucial consideration is discovered, indicating that a completely different approach would be a bit safer. Does the project kill itself off like a dishonored samurai, relinquishing its unsafe design and all the progress that had been made? Or does it react like a worried octopus, puffing out a cloud of motivated skepticism in the hope of eluding the attack? A project that would reliably choose the samurai option in such a dilemma would be a far preferable developer.5 Yet building processes and institutions that are willing to commit seppuku based on uncertain allegations and speculative reasoning is not easy. Another dimension of social epistemology is the management of sensitive information, in particular the ability to avoid leaking information that ought be kept secret. (Information continence may be especially challenging for academic researchers, accustomed as they are to constantly disseminating their results on every available lamppost and tree.)

Particular measures

In addition to the general objectives of strategic light and good capacity, some more specific objectives could also present cost-effective opportunities for action.

One such is progress on the technical challenges of machine intelligence safety. In pursing this objective, care should be taken to manage information hazards. Some work that would be useful for solving the control problem would also be useful for solving the competence problem. Work that burns down the AI fuse could easily be a net negative.

Another specific objective is to promote “best practices” among AI researchers. Whatever progress has been made on the control problem needs to be disseminated. Some forms of computational experimentation, particularly if involving strong recursive self-improvement, may also require the use of capability control to mitigate the risk of an accidental takeoff. While the actual implementation of safety methods is not so relevant today, it will increasingly become so as the state of the art advances. And it is not too soon to call for practitioners to express a commitment to safety, including endorsing the common good principle and promising to ramp up safety if and when the prospect of machine superintelligence begins to look more imminent. Pious words are not sufficient and will not by themselves make a dangerous technology safe: but where the mouth goeth, the mind might gradually follow.

Other opportunities may also occasionally arise to push on some pivotal parameter, for example to mitigate some other existential risk, or to promote biological cognitive enhancement and improvements of our collective wisdom, or even to shift world politics into a more harmonious register.

Will the best in human nature please stand up

Before the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything and the immaturity of our conduct. Superintelligence is a challenge for which we are not ready now and will not be ready for a long time. We have little idea when the detonation will occur, though if we hold the device to our ear we can hear a faint ticking sound.

For a child with an undetonated bomb in its hands, a sensible thing to do would be to put it down gently, quickly back out of the room, and contact the nearest adult. Yet what we have here is not one child but many, each with access to an independent trigger mechanism. The chances that we will all find the sense to put down the dangerous stuff seem almost negligible. Some little idiot is bound to press the ignite button just to see what happens.

Nor can we attain safety by running away, for the blast of an intelligence explosion would bring down the entire firmament. Nor is there a grown-up in sight.

In this situation, any feeling of gee-wiz exhilaration would be out of place. Consternation and fear would be closer to the mark; but the most appropriate attitude may be a bitter determination to be as competent as we can, much as if we were preparing for a difficult exam that will either realize our dreams or obliterate them.

This is not a prescription of fanaticism. The intelligence explosion might still be many decades off in the future. Moreover, the challenge we face is, in part, to hold on to our humanity: to maintain our groundedness, common sense, and good-humored decency even in the teeth of this most unnatural and inhuman problem. We need to bring all our human resourcefulness to bear on its solution.

Yet let us not lose track of what is globally significant. Through the fog of everyday trivialities, we can perceive—if but dimly—the essential task of our age. In this book, we have attempted to discern a little more feature in what is otherwise still a relatively amorphous and negatively defined vision—one that presents as our principal moral priority (at least from an impersonal and secular perspective) the reduction of existential risk and the attainment of a civilizational trajectory that leads to a compassionate and jubilant use of humanity’s cosmic endowment.


/ 016. Daughtery, Paul R. et al. "Reimagining Work in the Age of AI." 2023.

/ 017. Crawford, Kate. Atlas of AI, 2021.

/ 018. Russell, Stuart. Human Compatible Artificial Intelligence and the Problem of Control, 2019.

/ 019. Srineck, Nick et al. Inventing the Future Postcapitalism and a World Without Work, 2015.

/ 020. Brynjolfsson, Erik et al. The Second Machine Age, 2014.

/ 021. Weil, Elizabeth. You Are Not a a Parrot And a chatbot is not a human. And a linguist named Emily M. Bender is very worried what will happen when we forget this, 2023.

/ 022. Kurzweil, Ray. "The Age of Intelligent Machines: Can Machines Think?" 1990.

/ 023. O'Malley JP. "On Consciousness in Silicon Systems," 2023.

/ 024. Bridle, James. "Rise of the machines: has technology evolved beyond our control?" 2018.

/ 025. "Policymaking in the Pause," 2023.

/ 026. Jackson, Lauren. "What if A.I. Sentience Is a Question of Degree?" 2023.

/ 027. Lee, Kai-Fu. AI Superpowers: China, Silicon Valley, and the New World Order, 2018.

/ 028. Reese, Hope. "A Human-Centered Approach to the AI Revolution," 2022.

/ 029. Sanders, Nathan E. et al. "How AI could write our laws," 2023.

/ 030. HAI Stanford University, Artificial Intelligence Index Report, 2023.

/ 031. Bubeck, Sebastien et al. "Sparks of Artificial General Intelligence: Early experiments with GPT-4," 2023.

