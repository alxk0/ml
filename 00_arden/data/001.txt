COMPUTING MACHINERY AND INTELLIGENCE By A.M. Turing


1. The Imitation Game

I propose to consider the question, "Can machines think?" This should begin with definitions of the meaning of the terms "machine" and "think." The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words "machine" and "think" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, "Can machines think?" is to be sought in a statistical survey such as a Gallup poll. But this is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.

The new form of the problem can be described in terms of a game which we call the 'imitation game." It is played with three people, a man (A), a woman (B), and an interrogator (C) who may be of either sex. The interrogator stays in a room apart front the other two. The object of the game for the interrogator is to determine which of the other two is the man and which is the woman. He knows them by labels X and Y, and at the end of the game he says either "X is A and Y is B" or "X is B and Y is A." The interrogator is allowed to put questions to A and B thus:

C: Will X please tell me the length of his or her hair?

Now suppose X is actually A, then A must answer. It is A's object in the game to try and cause C to make the wrong identification. His answer might therefore be:

"My hair is shingled, and the longest strands are about nine inches long."

In order that tones of voice may not help the interrogator the answers should be written, or better still, typewritten. The ideal arrangement is to have a teleprinter communicating between the two rooms. Alternatively the question and answers can be repeated by an intermediary. The object of the game for the third player (B) is to help the interrogator. The best strategy for her is probably to give truthful answers. She can add such things as "I am the woman, don't listen to him!" to her answers, but it will avail nothing as the man can make similar remarks.

We now ask the question, "What will happen when a machine takes the part of A in this game?" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, "Can machines think?"

2. Critique of the New Problem
As well as asking, "What is the answer to this new form of the question," one may ask, "Is this new question a worthy one to investigate?" This latter question we investigate without further ado, thereby cutting short an infinite regress.
The new problem has the advantage of drawing a fairly sharp line between the physical and the intellectual capacities of a man. No engineer or chemist claims to be able to produce a material which is indistinguishable from the human skin. It is possible that at some time this might be done, but even supposing this invention available we should feel there was little point in trying to make a "thinking machine" more human by dressing it up in such artificial flesh. The form in which we have set the problem reflects this fact in the condition which prevents the interrogator from seeing or touching the other competitors, or hearing -their voices. Some other advantages of the proposed criterion may be shown up by specimen questions and answers. Thus:
Q: Please write me a sonnet on the subject of the Forth Bridge.
A : Count me out on this one. I never could write poetry.
Q: Add 34957 to 70764.
A: (Pause about 30 seconds and then give as answer) 105621.
Q: Do you play chess?
A: Yes.
Q: I have K at my K1, and no other pieces. You have only K at K6 and R at R1. It is your move. What do you play?
A: (After a pause of 15 seconds) R-R8 mate.
The question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include. We do not wish to penalise the machine for its inability to shine in beauty competitions, nor to penalise a man for losing in a race against an aeroplane. The conditions of our game make these disabilities irrelevant. The "witnesses" can brag, if they consider it advisable, as much as they please about their charms, strength or heroism, but the interrogator cannot demand practical demonstrations.
The game may perhaps be criticised on the ground that the odds are weighted too heavily against the machine. If the man were to try and pretend to be the machine he would clearly make a very poor showing. He would be given away at once by slowness and inaccuracy in arithmetic. May not machines carry out something which ought to be described as thinking but which is very different from what a man does? This objection is a very strong one, but at least we can say that if, nevertheless, a machine can be constructed to play the imitation game satisfactorily, we need not be troubled by this objection.

It might be urged that when playing the "imitation game" the best strategy for the machine may possibly be something other than imitation of the behaviour of a man. This may be, but I think it is unlikely that there is any great effect of this kind. In any case there is no intention to investigate here the theory of the game, and it will be assumed that the best strategy is to try to provide answers that would naturally be given by a man.
3. The Machines Concerned in the GameThe question which we put in 1 will not be quite definite until we have specified what we mean by the word "machine." It is natural that we should wish to permit every kind of engineering technique to be used in our machines. We also wish to allow the possibility than an engineer or team of engineers may construct a machine which works, but whose manner of operation cannot be satisfactorily described by its constructors because they have applied a method which is largely experimental. Finally, we wish to exclude from the machines men born in the usual manner. It is difficult to frame the definitions so as to satisfy these three conditions. One might for instance insist that the team of engineers should be all of one sex, but this would not really be satisfactory, for it is probably possible to rear a complete individual from a single cell of the skin (say) of a man. To do so would be a feat of biological technique deserving of the very highest praise, but we would not be inclined to regard it as a case of "constructing a thinking machine." This prompts us to abandon the requirement that every kind of technique should be permitted. We are the more ready to do so in view of the fact that the present interest in "thinking machines" has been aroused by a particular kind of machine, usually called an "electronic computer" or "digital computer." Following this suggestion we only permit digital computers to take part in our game.
This restriction appears at first sight to be a very drastic one. I shall attempt to show that it is not so in reality. To do this necessitates a short account of the nature and properties of these computers.
It may also be said that this identification of machines with digital computers, like our criterion for "thinking," will only be unsatisfactory if (contrary to my belief), it turns out that digital computers are unable to give a good showing in the game.
There are already a number of digital computers in working order, and it may be asked, "Why not try the experiment straight away? It would be easy to satisfy the conditions of the game. A number of interrogators could be used, and statistics compiled to show how often the right identification was given." The short answer is that we are not asking whether all digital computers would do well in the game nor whether the computers at present available would do well, but whether there are imaginable computers which would do well. But this is only the short answer. We shall see this question in a different light later.

4. Digital Computers
The idea behind digital computers may be explained by saying that these machines are intended to carry out any operations which could be done by a human computer. The human computer is supposed to be following fixed rules; he has no authority to deviate from them in any detail. We may suppose that these rules are supplied in a book, which is altered whenever he is put on to a new job. He has also an unlimited supply of paper on which he does his calculations. He may also do his multiplications and additions on a "desk machine," but this is not important.
If we use the above explanation as a definition we shall be in danger of circularity of argument. We avoid this by giving an outline. of the means by which the desired effect is achieved. A digital computer can usually be regarded as consisting of three parts:
(i) Store.
(ii) Executive unit.
(iii) Control.
The store is a store of information, and corresponds to the human computer's paper, whether this is the paper on which he does his calculations or that on which his book of rules is printed. In so far as the human computer does calculations in his bead a part of the store will correspond to his memory.
The executive unit is the part which carries out the various individual operations involved in a calculation. What these individual operations are will vary from machine to machine. Usually fairly lengthy operations can be done such as "Multiply 3540675445 by 7076345687" but in some machines only very simple ones such as "Write down 0" are possible.
We have mentioned that the "book of rules" supplied to the computer is replaced in the machine by a part of the store. It is then called the "table of instructions." It is the duty of the control to see that these instructions are obeyed correctly and in the right order. The control is so constructed that this necessarily happens.
The information in the store is usually broken up into packets of moderately small size. In one machine, for instance, a packet might consist of ten decimal digits. Numbers are assigned to the parts of the store in which the various packets of information are stored, in some systematic manner. A typical instruction might say-
"Add the number stored in position 6809 to that in 4302 and put the result back into the latter storage position."
Needless to say it would not occur in the machine expressed in English. It would more likely be coded in a form such as 6809430217. Here 17 says which of various possible operations is to be performed on the two numbers. In this case the)e operation is that described above, viz., "Add the number. . . ." It will be noticed that the instruction takes up 10 digits and so forms one packet of information, very conveniently. The control will normally take the instructions to be obeyed in the order of the positions in which they are stored, but occasionally an instruction such as"Now obey the instruction stored in position 5606, and continue from there"may be encountered, or again"If position 4505 contains 0 obey next the instruction stored in 6707, otherwise continue straight on."Instructions of these latter types are very important because they make it possible for a sequence of operations to be replaced over and over again until some condition is fulfilled, but in doing so to obey, not fresh instructions on each repetition, but the same ones over and over again. To take a domestic analogy. Suppose Mother wants Tommy to call at the cobbler's every morning on his way to school to see if her shoes are done, she can ask him afresh every morning. Alternatively she can stick up a notice once and for all in the hall which he will see when he leaves for school and which tells him to call for the shoes, and also to destroy the notice when he comes back if he has the shoes with him.
The reader must accept it as a fact that digital computers can be constructed, and indeed have been constructed, according to the principles we have described, and that they can in fact mimic the actions of a human computer very closely.
The book of rules which we have described our human computer as using is of course a convenient fiction. Actual human computers really remember what they have got to do. If one wants to make a machine mimic the behaviour of the human computer in some complex operation one has to ask him how it is done, and then translate the answer into the form of an instruction table. Constructing instruction tables is usually described as "programming." To "programme a machine to carry out the operation A" means to put the appropriate instruction table into the machine so that it will do A.
An interesting variant on the idea of a digital computer is a "digital computer with a random element." These have instructions involving the throwing of a die or some equivalent electronic process; one such instruction might for instance be, "Throw the die and put the-resulting number into store 1000." Sometimes such a machine is described as having free will (though I would not use this phrase myself), It is not normally possible to determine from observing a machine whether it has a random element, for a similar effect can be produced by such devices as making the choices depend on the digits of the decimal for .
Most actual digital computers have only a finite store. There is no theoretical difficulty in the idea of a computer with an unlimited store. Of course only a finite part can have been used at any one time. Likewise only a finite amount can have been constructed, but we can imagine more and more being added as required. Such computers have special theoretical interest and will be called infinitive capacity computers.
The idea of a digital computer is an old one. Charles Babbage, Lucasian Professor of Mathematics at Cambridge from 1828 to 1839, planned such a machine, called the Analytical Engine, but it was never completed. Although Babbage had all the essential ideas, his machine was not at that time such a very attractive prospect. The speed which would have been available would be definitely faster than a human computer but something like I 00 times slower than the Manchester machine, itself one of the slower of the modern machines, The storage was to be purely mechanical, using wheels and cards.
The fact that Babbage's Analytical Engine was to be entirely mechanical will help us to rid ourselves of a superstition. Importance is often attached to the fact that modern digital computers are electrical, and that the nervous system also is electrical. Since Babbage's machine was not electrical, and since all digital computers are in a sense equivalent, we see that this use of electricity cannot be of theoretical importance. Of course electricity usually comes in where fast signalling is concerned, so that it is not surprising that we find it in both these connections. In the nervous system chemical phenomena are at least as important as electrical. In certain computers the storage system is mainly acoustic. The feature of using electricity is thus seen to be only a very superficial similarity. If we wish to find such similarities we should took rather for mathematical analogies of function.
5. Universality of Digital ComputersThe digital computers considered in the last section may be classified amongst the "discrete-state machines." These are the machines which move by sudden jumps or clicks from one quite definite state to another. These states are sufficiently different for the possibility of confusion between them to be ignored. Strictly speaking there, are no such machines. Everything really moves continuously. But there are many kinds of machine which can profitably be thought of as being discrete-state machines. For instance in considering the switches for a lighting system it is a convenient fiction that each switch must be definitely on or definitely off. There must be intermediate positions, but for most purposes we can forget about them. As an example of a discrete-state machine we might consider a wheel which clicks round through 120 once a second, but may be stopped by a ]ever which can be operated from outside; in addition a lamp is to light in one of the positions of the wheel. This machine could be described abstractly as follows. The internal state of the machine (which is described by the position of the wheel) may be q1, q2 or q3. There is an input signal i0. or i1 (position of ]ever). The internal state at any moment is determined by the last state and input signal according to the table
(TABLE DELETED)The output signals, the only externally visible indication of the internal state (the light) are described by the table

State q1 q2 q3
output o0 o0 o1This example is typical of discrete-state machines. They can be described by such tables provided they have only a finite number of possible states.It will seem that given the initial state of the machine and the input signals it is always possible to predict all future states, This is reminiscent of Laplace's view that from the complete state of the universe at one moment of time, as described by the positions and velocities of all particles, it should be possible to predict all future states. The prediction which we are considering is, however, rather nearer to practicability than that considered by Laplace. The system of the "universe as a whole" is such that quite small errors in the initial conditions can have an overwhelming effect at a later time. The displacement of a single electron by a billionth of a centimetre at one moment might make the difference between a man being killed by an avalanche a year later, or escaping. It is an essential property of the mechanical systems which we have called "discrete-state machines" that this phenomenon does not occur. Even when we consider the actual physical machines instead of the idealised machines, reasonably accurate knowledge of the state at one moment yields reasonably accurate knowledge any number of steps later.
As we have mentioned, digital computers fall within the class of discrete-state machines. But the number of states of which such a machine is capable is usually enormously large. For instance, the number for the machine now working at Manchester is about 2 165,000, i.e., about 10 50,000. Compare this with our example of the clicking wheel described above, which had three states. It is not difficult to see why the number of states should be so immense. The computer includes a store corresponding to the paper used by a human computer. It must be possible to write into the store any one of the combinations of symbols which might have been written on the paper. For simplicity suppose that only digits from 0 to 9 are used as symbols. Variations in handwriting are ignored. Suppose the computer is allowed 100 sheets of paper each containing 50 lines each with room for 30 digits. Then the number of states is 10 100x50x30 i.e., 10 150,000 . This is about the number of states of three Manchester machines put together. The logarithm to the base two of the number of states is usually called the "storage capacity" of the machine. Thus the Manchester machine has a storage capacity of about 165,000 and the wheel machine of our example about 1.6. If two machines are put together their capacities must be added to obtain the capacity of the resultant machine. This leads to the possibility of statements such as "The Manchester machine contains 64 magnetic tracks each with a capacity of 2560, eight electronic tubes with a capacity of 1280. Miscellaneous storage amounts to about 300 making a total of 174,380."
Given the table corresponding to a discrete-state machine it is possible to predict what it will do. There is no reason why this calculation should not be carried out by means of a digital computer. Provided it could be carried out sufficiently quickly the digital computer could mimic the behavior of any discrete-state machine. The imitation game could then be played with the machine in question (as B) and the mimicking digital computer (as A) and the interrogator would be unable to distinguish them. Of course the digital computer must have an adequate storage capacity as well as working sufficiently fast. Moreover, it must be programmed afresh for each new machine which it is desired to mimic.
This special property of digital computers, that they can mimic any discrete-state machine, is described by saying that they are universal machines. The existence of machines with this property has the important consequence that, considerations of speed apart, it is unnecessary to design various new machines to do various computing processes. They can all be done with one digital computer, suitably programmed for each case. It 'ill be seen that as a consequence of this all digital computers are in a sense equivalent.
We may now consider again the point raised at the end of §3. It was suggested tentatively that the question, "Can machines think?" should be replaced by "Are there imaginable digital computers which would do well in the imitation game?" If we wish we can make this superficially more general and ask "Are there discrete-state machines which would do well?" But in view of the universality property we see that either of these questions is equivalent to this, "Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?"
6. Contrary Views on the Main Question
We may now consider the ground to have been cleared and we are ready to proceed to the debate on our question, "Can machines think?" and the variant of it quoted at the end of the last section. We cannot altogether abandon the original form of the problem, for opinions will differ as to the appropriateness of the substitution and we must at least listen to what has to be said in this connexion.
It will simplify matters for the reader if I explain first my own beliefs in the matter. Consider first the more accurate form of the question. I believe that in about fifty years' time it will be possible, to programme computers, with a storage capacity of about 109, to make them play the imitation game so well that an average interrogator will not have more than 70 per cent chance of making the right identification after five minutes of questioning. The original question, "Can machines think?" I believe to be too meaningless to deserve discussion. Nevertheless I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted. I believe further that no useful purpose is served by concealing these beliefs. The popular view that scientists proceed inexorably from well-established fact to well-established fact, never being influenced by any improved conjecture, is quite mistaken. Provided it is made clear which are proved facts and which are conjectures, no harm can result. Conjectures are of great importance since they suggest useful lines of research.

I now proceed to consider opinions opposed to my own.
(1) The Theological ObjectionThinking is a function of man's immortal soul. God has given an immortal soul to every man and woman, but not to any other animal or to machines. Hence no animal or machine can think.I am unable to accept any part of this, but will attempt to reply in theological terms. I should find the argument more convincing if animals were classed with men, for there is a greater difference, to my mind, between the typical animate and the inanimate than there is between man and the other animals. The arbitrary character of the orthodox view becomes clearer if we consider how it might appear to a member of some other religious community. How do Christians regard the Moslem view that women have no souls? But let us leave this point aside and return to the main argument. It appears to me that the argument quoted above implies a serious restriction of the omnipotence of the Almighty. It is admitted that there are certain things that He cannot do such as making one equal to two, but should we not believe that He has freedom to confer a soul on an elephant if He sees fit? We might expect that He would only exercise this power in conjunction with a mutation which provided the elephant with an appropriately improved brain to minister to the needs of this sort[. An argument of exactly similar form may be made for the case of machines. It may seem different because it is more difficult to "swallow." But this really only means that we think it would be less likely that He would consider the circumstances suitable for conferring a soul. The circumstances in question are discussed in the rest of this paper. In attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing .mansions for the souls that He creates.
However, this is mere speculation. I am not very impressed with theological arguments whatever they may be used to support. Such arguments have often been found unsatisfactory in the past. In the time of Galileo it was argued that the texts, "And the sun stood still . . . and hasted not to go down about a whole day" (Joshua x. 13) and "He laid the foundations of the earth, that it should not move at any time" (Psalm cv. 5) were an adequate refutation of the Copernican theory. With our present knowledge such an argument appears futile. When that knowledge was not available it made a quite different impression.
(2) The "Heads in the Sand" ObjectionThe consequences of machines thinking would be too dreadful. Let us hope and believe that they cannot do so."This argument is seldom expressed quite so openly as in the form above. But it affects most of us who think about it at all. We like to believe that Man is in some subtle way superior to the rest of creation. It is best if he can be shown to be necessarily superior, for then there is no danger of him losing his commanding position. The popularity of the theological argument is clearly connected with this feeling. It is likely to be quite strong in intellectual people, since they value the power of thinking more highly than others, and are more inclined to base their belief in the superiority of Man on this power.I do not think that this argument is sufficiently substantial to require refutation. Consolation would be more appropriate: perhaps this should be sought in the transmigration of souls.
(3) The Mathematical Objection
There are a number of results of mathematical logic which can be used to show that there are limitations to the powers of discrete-state machines. The best known of these results is known as Godel's theorem ( 1931 ) and shows that in any sufficiently powerful logical system statements can be formulated which can neither be proved nor disproved within the system, unless possibly the system itself is inconsistent. There are other, in some respects similar, results due to Church (1936), Kleene (1935), Rosser, and Turing (1937). The latter result is the most convenient to consider, since it refers directly to machines, whereas the others can only be used in a comparatively indirect argument: for instance if Godel's theorem is to be used we need in addition to have some means of describing logical systems in terms of machines, and machines in terms of logical systems. The result in question refers to a type of machine which is essentially a digital computer with an infinite capacity. It states that there are certain things that such a machine cannot do. If it is rigged up to give answers to questions as in the imitation game, there will be some questions to which it will either give a wrong answer, or fail to give an answer at all however much time is allowed for a reply. There may, of course, be many such questions, and questions which cannot be answered by one machine may be satisfactorily answered by another. We are of course supposing for the present that the questions are of the kind to which an answer "Yes" or "No" is appropriate, rather than questions such as "What do you think of Picasso?" The questions that we know the machines must fail on are of this type, "Consider the machine specified as follows. . . . Will this machine ever answer 'Yes' to any question?" The dots are to be replaced by a description of some machine in a standard form, which could be something like that used in §5. When the machine described bears a certain comparatively simple relation to the machine which is under interrogation, it can be shown that the answer is either wrong or not forthcoming. This is the mathematical result: it is argued that it proves a disability of machines to which the human intellect is not subject.
The short answer to this argument is that although it is established that there are limitations to the Powers If any particular machine, it has only been stated, without any sort of proof, that no such limitations apply to the human intellect. But I do not think this view can be dismissed quite so lightly. Whenever one of these machines is asked the appropriate critical question, and gives a definite answer, we know that this answer must be wrong, and this gives us a certain feeling of superiority. Is this feeling illusory? It is no doubt quite genuine, but I do not think too much importance should be attached to it. We too often give wrong answers to questions ourselves to be justified in being very pleased at such evidence of fallibility on the part of the machines. Further, our superiority can only be felt on such an occasion in relation to the one machine over which we have scored our petty triumph. There would be no question of triumphing simultaneously over all machines. In short, then, there might be men cleverer than any given machine, but then again there might be other machines cleverer again, and so on.
Those who hold to the mathematical argument would, I think, mostly he willing to accept the imitation game as a basis for discussion, Those who believe in the two previous objections would probably not be interested in any criteria.
(4) The Argument from Consciousness
This argument is very, well expressed in Professor Jefferson's Lister Oration for 1949, from which I quote. "Not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain-that is, not only write it but know that it had written it. No mechanism could feel (and not merely artificially signal, an easy contrivance) pleasure at its successes, grief when its valves fuse, be warmed by flattery, be made miserable by its mistakes, be charmed by sex, be angry or depressed when it cannot get what it wants."
This argument appears to be a denial of the validity of our test. According to the most extreme form of this view the only way by which one could be sure that machine thinks is to be the machine and to feel oneself thinking. One could then describe these feelings to the world, but of course no one would be justified in taking any notice. Likewise according to this view the only way to know that a man thinks is to be that particular man. It is in fact the solipsist point of view. It may be the most logical view to hold but it makes communication of ideas difficult. A is liable to believe "A thinks but B does not" whilst B believes "B thinks but A does not." instead of arguing continually over this point it is usual to have the polite convention that everyone thinks.
I am sure that Professor Jefferson does not wish to adopt the extreme and solipsist point of view. Probably he would be quite willing to accept the imitation game as a test. The game (with the player B omitted) is frequently used in practice under the name of viva voce to discover whether some one really understands something or has "learnt it parrot fashion." Let us listen in to a part of such a viva voce:
Interrogator: In the first line of your sonnet which reads "Shall I compare thee to a summer's day," would not "a spring day" do as well or better?
Witness: It wouldn't scan.
Interrogator: How about "a winter's day," That would scan all right.
Witness: Yes, but nobody wants to be compared to a winter's day.

Interrogator: Would you say Mr. Pickwick reminded you of Christmas?
Witness: In a way.
Interrogator: Yet Christmas is a winter's day, and I do not think Mr. Pickwick would mind the comparison.
Witness: I don't think you're serious. By a winter's day one means a typical winter's day, rather than a special one like Christmas.
And so on, What would Professor Jefferson say if the sonnet-writing machine was able to answer like this in the viva voce? I do not know whether he would regard the machine as "merely artificially signalling" these answers, but if the answers were as satisfactory and sustained as in the above passage I do not think he would describe it as "an easy contrivance." This phrase is, I think, intended to cover such devices as the inclusion in the machine of a record of someone reading a sonnet, with appropriate switching to turn it on from time to time.
In short then, I think that most of those who support the argument from consciousness could be persuaded to abandon it rather than be forced into the solipsist position. They will then probably be willing to accept our test.
I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.
(5) Arguments from Various Disabilities
These arguments take the form, "I grant you that you can make machines do all the things you have mentioned but you will never be able to make one to do X." Numerous features X are suggested in this connexion I offer a selection:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humour, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make some one fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.
No support is usually offered for these statements. I believe they are mostly founded on the principle of scientific induction. A man has seen thousands of machines in his lifetime. From what he sees of them he draws a number of general conclusions. They are ugly, each is designed for a very limited purpose, when required for a minutely different purpose they are useless, the variety of behaviour of any one of them is very small, etc., etc. Naturally he concludes that these are necessary properties of machines in general. Many of these limitations are associated with the very small storage capacity of most machines. (I am assuming that the idea of storage capacity is extended in some way to cover machines other than discrete-state machines. The exact definition does not matter as no mathematical accuracy is claimed in the present discussion,) A few years ago, when very little had been heard of digital computers, it was possible to elicit much incredulity concerning them, if one mentioned their properties without describing their construction. That was presumably due to a similar application of the principle of scientific induction. These applications of the principle are of course largely unconscious. When a burnt child fears the fire and shows that he fears it by avoiding it, f should say that he was applying scientific induction. (I could of course also describe his behaviour in many other ways.) The works and customs of mankind do not seem to be very suitable material to which to apply scientific induction. A very large part of space-time must be investigated, if reliable results are to be obtained. Otherwise we may (as most English 'Children do) decide that everybody speaks English, and that it is silly to learn French.
There are, however, special remarks to be made about many of the disabilities that have been mentioned. The inability to enjoy strawberries and cream may have struck the reader as frivolous. Possibly a machine might be made to enjoy this delicious dish, but any attempt to make one do so would be idiotic.
The claim that "machines cannot make mistakes" seems a curious one. One is tempted to retort, "Are they any the worse for that?" But let us adopt a more sympathetic attitude, and try to see what is really meant. I think this criticism can be explained in terms of the imitation game. It is claimed that the interrogator could distinguish the machine from the man simply by setting them a number of problems in arithmetic. The machine would be unmasked because of its deadly accuracy. The reply to this is simple. The machine (programmed for playing the game) would not attempt to give the right answers to the arithmetic problems. It would deliberately introduce mistakes in a manner calculated to confuse the interrogator. A mechanical fault would probably show itself through an unsuitable decision as to what sort of a mistake to make in the arithmetic. Even this interpretation of the criticism is not sufficiently sympathetic. But we cannot afford the space to go into it much further. It seems to me that this criticism depends on a confusion between two kinds of mistake, We may call them "errors of functioning" and "errors of conclusion." Errors of functioning are due to some mechanical or electrical fault which causes the machine to behave otherwise than it was designed to do. In philosophical discussions one likes to ignore the possibility of such errors; one is therefore discussing "abstract machines." These abstract machines are mathematical fictions rather than physical objects. By definition they are incapable of errors of functioning. In this sense we can truly say that "machines can never make mistakes." Errors of conclusion can only arise when some meaning is attached to the output signals from the machine. The machine might, for instance, type out mathematical equations, or sentences in English. When a false proposition is typed we say that the machine has committed an error of conclusion. There is clearly no reason at all for saying that a machine cannot make this kind of mistake. It might do nothing but type out repeatedly "O = I." To take a less perverse example, it might have some method for drawing conclusions by scientific induction. We must expect such a method to lead occasionally to erroneous results.
The claim that a machine cannot be the subject of its own thought can of course only be answered if it can be shown that the machine has some thought with some subject matter. Nevertheless, "the subject matter of a machine's operations" does seem to mean something, at least to the people who deal with it. If, for instance, the machine was trying to find a solution of the equation x2 - 40x - 11 = 0 one would be tempted to describe this equation as part of the machine's subject matter at that moment. In this sort of sense a machine undoubtedly can be its own subject matter. It may be used to help in making up its own programmes, or to predict the effect of alterations in its own structure. By observing the results of its own behaviour it can modify its own programmes so as to achieve some purpose more effectively. These are possibilities of the near future, rather than Utopian dreams.
The criticism that a machine cannot have much diversity of behaviour is just a way of saying that it cannot have much storage capacity. Until fairly recently a storage capacity of even a thousand digits was very rare.
The criticisms that we are considering here are often disguised forms of the argument from consciousness, Usually if one maintains that a machine can do one of these things, and describes the kind of method that the machine could use, one will not make much of an impression. It is thought that tile method (whatever it may be, for it must be mechanical) is really rather base. Compare the parentheses in Jefferson's statement quoted on page 22.
(6) Lady Lovelace's ObjectionOur most detailed information of Babbage's Analytical Engine comes from a memoir by Lady Lovelace ( 1842). In it she states, "The Analytical Engine has no pretensions to originate anything. It can do whatever we know how to order it to perform" (her italics). This statement is quoted by Hartree ( 1949) who adds: "This does not imply that it may not be possible to construct electronic equipment which will 'think for itself,' or in which, in biological terms, one could set up a conditioned reflex, which would serve as a basis for 'learning.' Whether this is possible in principle or not is a stimulating and exciting question, suggested by some of these recent developments But it did not seem that the machines constructed or projected at the time had this property."
I am in thorough agreement with Hartree over this. It will be noticed that he does not assert that the machines in question had not got the property, but rather that the evidence available to Lady Lovelace did not encourage her to believe that they had it. It is quite possible that the machines in question had in a sense got this property. For suppose that some discrete-state machine has the property. The Analytical Engine was a universal digital computer, so that, if its storage capacity and speed were adequate, it could by suitable programming be made to mimic the machine in question. Probably this argument did not occur to the Countess or to Babbage. In any case there was no obligation on them to claim all that could be claimed.
This whole question will be considered again under the heading of learning machines.
A variant of Lady Lovelace's objection states that a machine can "never do anything really new." This may be parried for a moment with the saw, "There is nothing new under the sun." Who can be certain that "original work" that he has done was not simply the growth of the seed planted in him by teaching, or the effect of following well-known general principles. A better variant of the objection says that a machine can never "take us by surprise." This statement is a more direct challenge and can be met directly. Machines take me by surprise with great frequency. This is largely because I do not do sufficient calculation to decide what to expect them to do, or rather because, although I do a calculation, I do it in a hurried, slipshod fashion, taking risks. Perhaps I say to myself, "I suppose the Voltage here ought to he the same as there: anyway let's assume it is." Naturally I am often wrong, and the result is a surprise for me for by the time the experiment is done these assumptions have been forgotten. These admissions lay me open to lectures on the subject of my vicious ways, but do not throw any doubt on my credibility when I testify to the surprises I experience.
I do not expect this reply to silence my critic. He will probably say that h surprises are due to some creative mental act on my part, and reflect no credit on the machine. This leads us back to the argument from consciousness, and far from the idea of surprise. It is a line of argument we must consider closed, but it is perhaps worth remarking that the appreciation of something as surprising requires as much of a "creative mental act" whether the surprising event originates from a man, a book, a machine or anything else.
The view that machines cannot give rise to surprises is due, I believe, to a fallacy to which philosophers and mathematicians are particularly subject. This is the assumption that as soon as a fact is presented to a mind all consequences of that fact spring into the mind simultaneously with it. It is a very useful assumption under many circumstances, but one too easily forgets that it is false. A natural consequence of doing so is that one then assumes that there is no virtue in the mere working out of consequences from data and general principles.
(7) Argument from Continuity in the Nervous System
The nervous system is certainly not a discrete-state machine. A small error in the information about the size of a nervous impulse impinging on a neuron, may make a large difference to the size of the outgoing impulse. It may be argued that, this being so, one cannot expect to be able to mimic the behaviour of the nervous system with a discrete-state system.
It is true that a discrete-state machine must be different from a continuous machine. But if we adhere to the conditions of the imitation game, the interrogator will not be able to take any advantage of this difference. The situation can be made clearer if we consider sonic other simpler continuous machine. A differential analyser will do very well. (A differential analyser is a certain kind of machine not of the discrete-state type used for some kinds of calculation.) Some of these provide their answers in a typed form, and so are suitable for taking part in the game. It would not be possible for a digital computer to predict exactly what answers the differential analyser would give to a problem, but it would be quite capable of giving the right sort of answer. For instance, if asked to give the value of (actually about 3.1416) it would be reasonable to choose at random between the values 3.12, 3.13, 3.14, 3.15, 3.16 with the probabilities of 0.05, 0.15, 0.55, 0.19, 0.06 (say). Under these circumstances it would be very difficult for the interrogator to distinguish the differential analyser from the digital computer.
(8) The Argument from Informality of BehaviourIt is not possible to produce a set of rules purporting to describe what a man should do in every conceivable set of circumstances. One might for instance have a rule that one is to stop when one sees a red traffic light, and to go if one sees a green one, but what if by some fault both appear together? One may perhaps decide that it is safest to stop. But some further difficulty may well arise from this decision later. To attempt to provide rules of conduct to cover every eventuality, even those arising from traffic lights, appears to be impossible. With all this I agree.
From this it is argued that we cannot be machines. I shall try to reproduce the argument, but I fear I shall hardly do it justice. It seems to run something like this. "if each man had a definite set of rules of conduct by which he regulated his life he would be no better than a machine. But there are no such rules, so men cannot be machines." The undistributed middle is glaring. I do not think the argument is ever put quite like this, but I believe this is the argument used nevertheless. There may however be a certain confusion between "rules of conduct" and "laws of behaviour" to cloud the issue. By "rules of conduct" I mean precepts such as "Stop if you see red lights," on which one can act, and of which one can be conscious. By "laws of behaviour" I mean laws of nature as applied to a man's body such as "if you pinch him he will squeak." If we substitute "laws of behaviour which regulate his life" for "laws of conduct by which he regulates his life" in the argument quoted the undistributed middle is no longer insuperable. For we believe that it is not only true that being regulated by laws of behaviour implies being some sort of machine (though not necessarily a discrete-state machine), but that conversely being such a machine implies being regulated by such laws. However, we cannot so easily convince ourselves of the absence of complete laws of behaviour as of complete rules of conduct. The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, "We have searched enough. There are no such laws."
We can demonstrate more forcibly that any such statement would be unjustified. For suppose we could be sure of finding such laws if they existed. Then given a discrete-state machine it should certainly be possible to discover by observation sufficient about it to predict its future behaviour, and this within a reasonable time, say a thousand years. But this does not seem to be the case. I have set up on the Manchester computer a small programme using only 1,000 units of storage, whereby the machine supplied with one sixteen-figure number replies with another within two seconds. I would defy anyone to learn from these replies sufficient about the programme to be able to predict any replies to untried values.
(9) The Argument from Extrasensory PerceptionI assume that the reader is familiar with the idea of extrasensory perception, and the meaning of the four items of it, viz., telepathy, clairvoyance, precognition and psychokinesis. These disturbing phenomena seem to deny all our usual scientific ideas. How we should like to discredit them! Unfortunately the statistical evidence, at least for telepathy, is overwhelming. It is very difficult to rearrange one's ideas so as to fit these new facts in. Once one has accepted them it does not seem a very big step to believe in ghosts and bogies. The idea that our bodies move simply according to the known laws of physics, together with some others not yet discovered but somewhat similar, would be one of the first to go.
This argument is to my mind quite a strong one. One can say in reply that many scientific theories seem to remain workable in practice, in spite of clashing with ESP; that in fact one can get along very nicely if one forgets about it. This is rather cold comfort, and one fears that thinking is just the kind of phenomenon where ESP may be especially relevant.A more specific argument based on ESP might run as follows: "Let us play the imitation game, using as witnesses a man who is good as a telepathic receiver, and a digital computer. The interrogator can ask such questions as 'What suit does the card in my right hand belong to?' The man by telepathy or clairvoyance gives the right answer 130 times out of 400 cards. The machine can only guess at random, and perhaps gets 104 right, so the interrogator makes the right identification." There is an interesting possibility which opens here. Suppose the digital computer contains a random number generator. Then it will be natural to use this to decide what answer to give. But then the random number generator will be subject to the psychokinetic powers of the interrogator. Perhaps this psychokinesis might cause the machine to guess right more often than would be expected on a probability calculation, so that the interrogator might still be unable to make the right identification. On the other hand, he might be able to guess right without any questioning, by clairvoyance. With ESP anything may happen.
If telepathy is admitted it will be necessary to tighten our test up. The situation could be regarded as analogous to that which would occur if the interrogator were talking to himself and one of the competitors was listening with his ear to the wall. To put the competitors into a "telepathy-proof room" would satisfy all requirements.
7. Learning MachinesThe reader will have anticipated that I have no very convincing arguments of a positive nature to support my views. If I had I should not have taken such pains to point out the fallacies in contrary views. Such evidence as I have I shall now give.

Let us return for a moment to Lady Lovelace's objection, which stated that the machine can only do what we tell it to do. One could say that a man can "inject" an idea into the machine, and that it will respond to a certain extent and then drop into quiescence, like a piano string struck by a hammer. Another simile would be an atomic pile of less than critical size: an injected idea is to correspond to a neutron entering the pile from without. Each such neutron will cause a certain disturbance which eventually dies away. If, however, the size of the pile is sufficiently increased, tire disturbance caused by such an incoming neutron will very likely go on and on increasing until the whole pile is destroyed. Is there a corresponding phenomenon for minds, and is there one for machines? There does seem to be one for the human mind. The majority of them seem to be "subcritical," i.e., to correspond in this analogy to piles of subcritical size. An idea presented to such a mind will on average give rise to less than one idea in reply. A smallish proportion are supercritical. An idea presented to such a mind that may give rise to a whole "theory" consisting of secondary, tertiary and more remote ideas. Animals minds seem to be very definitely subcritical. Adhering to this analogy we ask, "Can a machine be made to be supercritical?"
The "skin-of-an-onion" analogy is also helpful. In considering the functions of the mind or the brain we find certain operations which we can explain in purely mechanical terms. This we say does not correspond to the real mind: it is a sort of skin which we must strip off if we are to find the real mind. But then in what remains we find a further skin to be stripped off, and so on. Proceeding in this way do we ever come to the "real" mind, or do we eventually come to the skin which has nothing in it? In the latter case the whole mind is mechanical. (It would not be a discrete-state machine however. We have discussed this.)
These last two paragraphs do not claim to be convincing arguments. They should rather be described as "recitations tending to produce belief."
The only really satisfactory support that can be given for the view expressed at the beginning of §6, will be that provided by waiting for the end of the century and then doing the experiment described. But what can we say in the meantime? What steps should be taken now if the experiment is to be successful?
As I have explained, the problem is mainly one of programming. Advances in engineering will have to be made too, but it seems unlikely that these will not be adequate for the requirements. Estimates of the storage capacity of the brain vary from 1010 to 1015 binary digits. I incline to the lower values and believe that only a very small fraction is used for the higher types of thinking. Most of it is probably used for the retention of visual impressions, I should be surprised if more than 109 was required for satisfactory playing of the imitation game, at any rate against a blind man. (Note: The capacity of the Encyclopaedia Britannica, 11th edition, is 2 X 109) A storage capacity of 107, would be a very practicable possibility even by present techniques. It is probably not necessary to increase the speed of operations of the machines at all. Parts of modern machines which can be regarded as analogs of nerve cells work about a thousand times faster than the latter. This should provide a "margin of safety" which could cover losses of speed arising in many ways, Our problem then is to find out how to programme these machines to play the game. At my present rate of working I produce about a thousand digits of progratiirne a day, so that about sixty workers, working steadily through the fifty years might accomplish the job, if nothing went into the wastepaper basket. Some more expeditious method seems desirable.
In the process of trying to imitate an adult human mind we are bound to think a good deal about the process which has brought it to the state that it is in. We may notice three components.
(a) The initial state of the mind, say at birth,
(b) The education to which it has been subjected,
(c) Other experience, not to be described as education, to which it has been subjected.
Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain. Presumably the child brain is something like a notebook as one buys it from the stationer's. Rather little mechanism, and lots of blank sheets. (Mechanism and writing are from our point of view almost synonymous.) Our hope is that there is so little mechanism in the child brain that something like it can be easily programmed. The amount of work in the education we can assume, as a first approximation, to be much the same as for the human child.
We have thus divided our problem into two parts. The child programme and the education process. These two remain very closely connected. We cannot expect to find a good child machine at the first attempt. One must experiment with teaching one such machine and see how well it learns. One can then try another and see if it is better or worse. There is an obvious connection between this process and evolution, by the identifications
Structure of the child machine = hereditary material
Changes of the child machine = mutation,
Natural selection = judgment of the experimenter
One may hope, however, that this process will be more expeditious than evolution. The survival of the fittest is a slow method for measuring advantages. The experimenter, by the exercise of intelligence, should he able to speed it up. Equally important is the fact that he is not restricted to random mutations. If he can trace a cause for some weakness he can probably think of the kind of mutation which will improve it.
It will not be possible to apply exactly the same teaching process to the machine as to a normal child. It will not, for instance, be provided with legs, so that it could not be asked to go out and fill the coal scuttle. Possibly it might not have eyes. But however well these deficiencies might be overcome by clever engineering, one could not send the creature to school without the other children making excessive fun of it. It must be given some tuition. We need not be too concerned about the legs, eyes, etc. The example of Miss Helen Keller shows that education can take place provided that communication in both directions between teacher and pupil can take place by some means or other.
We normally associate punishments and rewards with the teaching process. Some simple child machines can be constructed or programmed on this sort of principle. The machine has to be so constructed that events which shortly preceded the occurrence of a punishment signal are unlikely to be repeated, whereas a reward signal increased the probability of repetition of the events which led up to it. These definitions do not presuppose any feelings on the part of the machine, I have done some experiments with one such child machine, and succeeded in teaching it a few things, but the teaching method was too unorthodox for the experiment to be considered really successful.The use of punishments and rewards can at best be a part of the teaching process. Roughly speaking, if the teacher has no other means of communicating to the pupil, the amount of information which can reach him does not exceed the total number of rewards and punishments applied. By the time a child has learnt to repeat "Casabianca" he would probably feel very sore indeed, if the text could only be discovered by a "Twenty Questions" technique, every "NO" taking the form of a blow. It is necessary therefore to have some other "unemotional" channels of communication. If these are available it is possible to teach a machine by punishments and rewards to obey orders given in some language, e.g., a symbolic language. These orders are to be transmitted through the "unemotional" channels. The use of this language will diminish greatly the number of punishments and rewards required.
Opinions may vary as to the complexity which is suitable in the child machine. One might try to make it as simple as possible consistently with the general principles. Alternatively one might have a complete system of logical inference "built in."' In the latter case the store would be largely occupied with definitions and propositions. The propositions would have various kinds of status, e.g., well-established facts, conjectures, mathematically proved theorems, statements given by an authority, expressions having the logical form of proposition but not belief-value. Certain propositions may be described as "imperatives." The machine should be so constructed that as soon as an imperative is classed as "well established" the appropriate action automatically takes place. To illustrate this, suppose the teacher says to the machine, "Do your homework now." This may cause "Teacher says 'Do your homework now' " to be included amongst the well-established facts. Another such fact might be, "Everything that teacher says is true." Combining these may eventually lead to the imperative, "Do your homework now," being included amongst the well-established facts, and this, by the construction of the machine, will mean that the homework actually gets started, but the effect is very satisfactory. The processes of inference used by the machine need not be such as would satisfy the most exacting logicians. There might for instance be no hierarchy of types. But this need not mean that type fallacies will occur, any more than we are bound to fall over unfenced cliffs. Suitable imperatives (expressed within the systems, not forming part of the rules of the system) such as "Do not use a class unless it is a subclass of one which has been mentioned by teacher" can have a similar effect to "Do not go too near the edge."
The imperatives that can be obeyed by a machine that has no limbs are bound to be of a rather intellectual character, as in the example (doing homework) given above. important amongst such imperatives will be ones which regulate the order in which the rules of the logical system concerned are to be applied, For at each stage when one is using a logical system, there is a very large number of alternative steps, any of which one is permitted to apply, so far as obedience to the rules of the logical system is concerned. These choices make the difference between a brilliant and a footling reasoner, not the difference between a sound and a fallacious one. Propositions leading to imperatives of this kind might be "When Socrates is mentioned, use the syllogism in Barbara" or "If one method has been proved to be quicker than another, do not use the slower method." Some of these may be "given by authority," but others may be produced by the machine itself, e.g. by scientific induction.
The idea of a learning machine may appear paradoxical to some readers. How can the rules of operation of the machine change? They should describe completely how the machine will react whatever its history might be, whatever changes it might undergo. The rules are thus quite time-invariant. This is quite true. The explanation of the paradox is that the rules which get changed in the learning process are of a rather less pretentious kind, claiming only an ephemeral validity. The reader may draw a parallel with the Constitution of the United States.
An important feature of a learning machine is that its teacher will often be very largely ignorant of quite what is going on inside, although he may still be able to some extent to predict his pupil's behavior. This should apply most strongly to the later education of a machine arising from a child machine of well-tried design (or programme). This is in clear contrast with normal procedure when using a machine to do computations one's object is then to have a clear mental picture of the state of the machine at each moment in the computation. This object can only be achieved with a struggle. The view that "the machine can only do what we know how to order it to do,"' appears strange in face of this. Most of the programmes which we can put into the machine will result in its doing something that we cannot make sense (if at all, or which we regard as completely random behaviour. Intelligent behaviour presumably consists in a departure from the completely disciplined behaviour involved in computation, but a rather slight one, which does not give rise to random behaviour, or to pointless repetitive loops. Another important result of preparing our machine for its part in the imitation game by a process of teaching and learning is that "human fallibility" is likely to be omitted in a rather natural way, i.e., without special "coaching." (The reader should reconcile this with the point of view on pages 23 and 24.) Processes that are learnt do not produce a hundred per cent certainty of result; if they did they could not be unlearnt.


AUTOMATION AND THE FUTURE OF WORK, Aaron Benanav

From the New Left Review 119 September October 2019.


The world is abuzz with talk of automation. Rapid advances in artificial intelligence, machine learning and robotics seem set to transform the world of work. In the most advanced factories, companies like Tesla have been aiming for ‘lights-out’ production, in which fully automated work processes, no longer needing human hands, can run in the dark. Meanwhile, in the illuminated halls of robotics conventions, machines are on display that can play ping-pong, cook food, have sex and even hold conversations. Computers are not only developing new strategies for playing Go, but are said to be writing symphonies that bring audiences to tears. Dressed in white lab coats or donning virtual suits, computers are learning to identify cancers and will soon be developing legal strategies. Trucks are already barrelling across the us without drivers; robotic dogs are carrying military-grade weapons across desolate plains. Are we living in the last days of human toil? Is what Edward Bellamy once called the ‘edict of Eden’ about to be revoked, as ‘men’—or at least, the wealthiest among them—become like gods?

There are many reasons to doubt the hype. For one thing, machines remain comically incapable of opening doors or, alas, folding laundry. Robotic security guards are toppling into mall fountains. Computerized digital assistants can answer questions and translate documents, but not well enough to do the job without human intervention; the same is true of self-driving cars. In the midst of the American ‘Fight for Fifteen’ movement, billboards went up in San Francisco threatening to replace fast-food workers with touchscreens if a law raising the minimum wage were passed. The Wall Street Journal dubbed the bill the ‘robot employment act’. Yet many fast-food workers in Europe already work alongside touchscreens and often earn better pay than in the us. Is the talk of automation overdone?

1. THE AUTOMATION DISCOURSE

In the pages of newspapers and popular magazines, scare stories about automation may remain just idle chatter. However, over the past decade, this talk has crystalized into an influential social theory, which purports not only to analyse current technologies and predict their future, but also to explore the consequences of technological change for society at large. This automation discourse rests on four main propositions. First, workers are already being displaced by ever-more advanced machines, resulting in rising levels of ‘technological unemployment’. Second, this displacement is a sign that we are on the verge of achieving a largely automated society, in which nearly all work will be performed by self-moving machines and intelligent computers. Third: automation should entail humanity’s collective liberation from toil, but because we live in a society where most people must work in order to live, this dream may well turn out to be a nightmare. Fourth, therefore, the only way to prevent a mass-unemployment catastrophe is to provide a universal basic income (ubi), breaking the connection between the incomes people earn and the work they do, as a way to inaugurate a new society.

This argument has been put forward by a number of self-described futurists. In the widely read Second Machine Age (2014), Erik Brynjolfsson and Andrew McAfee argue that we find ourselves ‘at an inflection point—a bend in the curve where many technologies that used to be found only in science fiction are becoming everyday reality.’ New technologies promise an enormous ‘bounty’, but Brynjolfsson and McAfee caution that ‘there is no economic law that says that all workers, or even a majority of workers, will benefit from these advances.’ On the contrary: as the demand for labour falls with the adoption of more advanced technologies, wages are stagnating; a rising share of annual income is therefore being captured by capital rather than by labour. The result is growing inequality, which could ‘slow our journey’ into what they call ‘the second machine age’ by generating a ‘failure mode of capitalism’ in which rentier extraction crowds out technological innovation. In Rise of the Robots (2015), Martin Ford similarly claims that we are pushing ‘towards a tipping point’ that is poised to ‘make the entire economy less labour-intensive.’ Again, ‘the most frightening long-term scenario of all might be if the global economic system eventually manages to adapt to the new reality’, leading to the creation of an ‘automated feudalism’ in which the ‘peasants would be largely superfluous’ and the elite impervious to economic demands. For these authors, education and retraining will not be enough to stabilize the demand for labour in an automated economy; some form of guaranteed non-wage income, such as a negative income tax, must be put in place.

The automation discourse has been enthusiastically adopted by the jeans-wearing elite of Silicon Valley. Bill Gates is advocating for a tax on robots. Mark Zuckerberg told Harvard undergraduate inductees that they should ‘explore ideas like universal basic income’, a policy Elon Musk also thinks will become increasingly ‘necessary’ over time, as robots outcompete humans across a growing range of jobs. Musk has been naming his SpaceX drone vessels after spaceships from Iain M. Banks’s Culture Series, a set of ambiguously utopian science-fiction novels depicting a post-scarcity world in which human beings live fulfilling lives alongside intelligent robots, called ‘minds’, without the need for markets or states.

Politicians and their advisors have equally identified with the automation discourse, which has become one of the leading perspectives on our ‘digital future’. In his farewell presidential address, Obama suggested that the ‘next wave of economic dislocations’ will come not from overseas trade, but rather from ‘the relentless pace of automation that makes a lot of good, middle-class jobs obsolete.’ Robert Reich, former Labour Secretary under Bill Clinton, expressed similar fears: we will soon reach a point ‘where technology is displacing so many jobs, not just menial jobs but also professional jobs, that we’re going to have to take seriously the notion of a universal basic income.’ Clinton’s former Treasury Secretary, Lawrence Summers, made the same admission: once-‘stupid’ ideas about technological unemployment now seem increasingly smart, he said, as workers’ wages stagnate and economic inequality rises. The discourse has become the basis of a long-shot presidential campaign for 2020: Andrew Yang, Obama’s former ‘Ambassador of Global Entrepreneurship’, has penned his own tome on automation, The War on Normal People, and is now running a futuristic campaign on a ‘Humanity First’, ubi platform. Among Yang’s vocal supporters is Andy Stern, former head of the seiu, whose Raising the Floor is yet another example of the discourse.10

Yang and Stern—like all of the other writers named so far—take pains to assure readers that some variant of capitalism is here to stay, even if it must jettison its labour markets; however, they admit to the influence of figures on the far left who offer a more radical version of the automation discourse. In Inventing the Future, Nick Srnicek and Alex Williams argue that the ‘most recent wave of automation is poised’ to transform the labour market ‘drastically, as it comes to encompass every aspect of the economy’.11 They claim that only a socialist government would actually be able to fulfil the promise of full automation by creating a post-work or post-scarcity society. In Four Futures, Peter Frase thoughtfully explores the alternative outcomes for such a post-scarcity society, depending on whether it still had private property and still suffered from resource scarcity, which could persist even if labour scarcity were overcome. Like the liberal proponents of the automation discourse, these left-wing writers stress that, even if the coming of advanced robotics is inevitable, ‘there is no necessary progression into a post-work world’. Srnicek, Williams and Frase are all proponents of ubi, but in a left-wing variant. For them, ubi serves as a bridge to ‘fully automated luxury communism’, a term originally coined in 2014 by Aaron Bastani to name a possible goal of socialist politics, and which flourished for five years as a meme on the internet before his book—outlining an automated future in which artificial intelligence, solar power, gene-editing, asteroid mining and lab-grown meat generate a world of limitless leisure and self-invention—finally appeared.

Recurrent fears

These futurist visions, from all points of the political spectrum, depend upon a common prediction of the trajectory of technological change. Have they got this right? To answer this question, it is helpful to have a couple of working definitions. Automation may be distinguished as a specific form of labour-saving technical innovation: automation technologies fully substitute for human labour, rather than merely augmenting human-productive capacities. With labour-augmenting technologies, a given job category will continue to exist, but each worker in that category will be more productive. For example, adding new machines to an assembly-line producing cars may make line workers more productive without abolishing line work as such. However, fewer workers will be needed in total to produce any given number of automobiles. Whether that results in fewer jobs will then depend on how much output—the total number of cars—also increases.

By contrast, automation may be defined as what Kurt Vonnegut describes in Player Piano: it takes place whenever an entire ‘job classification has been eliminated. Poof.’ No matter how much production might increase, another telephone-switchboard operator or hand-manipulator of rolled steel will never be hired. In these cases, machines have fully substituted for human labour. Much of the debate around the future of workplace automation turns on an evaluation of the degree to which present or near-future technologies are labour-substituting or labour-augmenting in character. Distinguishing between these two types of technical change turns out to be incredibly difficult in practice. One famous study from the Oxford Martin School suggested that 47 per cent of jobs in the us are at high risk of automation; a more recent study from the oecd predicts that 14 per cent of oecd jobs are at high risk, with another 32 per cent at risk of significant change in the way they are carried out (due to labour-augmenting rather than substituting innovations).

It is unclear, however, whether even the highest of these estimates suggests that a qualitative break with the past has taken place. By one count, ‘57 per cent of the jobs workers did in the 1960s no longer exist today’.16 Automation, in fact, turns out to be a constant feature of the history of capitalism. By contrast, the discourse around automation, which extrapolates from instances of technological change to a broader social theory, is not constant; it periodically recurs in modern history. Excitement about a coming age of automation can be traced back to at least the mid-19th century. Charles Babbage published On the Economy of Machinery and Manufactures in 1832; John Adolphus Etzler’s The Paradise Within the Reach of All Men, Without Labour appeared in 1833, Andrew Ure’s The Philosophy of Manufactures in 1835. These books presaged the imminent emergence of largely or fully automated factories, run with minimal or merely supervisory human labour. This vision was a major influence on Marx, whose Capital, Volume One argued that a complex world of interacting machines was in the process of displacing labour at the centre of economic life.

Visions of automated factories then appeared again in the 1930s, 1950s and 1980s, before their re-emergence in the 2010s. Each time, they were accompanied or shortly followed by predictions of a coming age of ‘catastrophic unemployment and social breakdown’, which could be prevented only if society were reorganized.17 To point out the periodicity of this discourse is not to say that its accompanying social visions should be dismissed. For one thing, the technological breakthroughs presaged by automation discourse could still be achieved at any time: just because they were wrong in the past does not necessarily mean that they will always be wrong in the future. More than that, these visions of automation have clearly been generative in social terms: they point to certain utopian possibilities latent within modern capitalist societies. The error in their approach is merely to suppose that, via ongoing technological shifts, these utopian possibilities will imminently be revealed via a catastrophe of mass unemployment.

The basic insight on which automation theory relies was described, most succinctly, by the Harvard economist Wassily Leontief. He pointed out that the ‘effective operation of the automatic price mechanism’ at the core of capitalist societies ‘depends critically’ on a peculiar feature of modern technology, namely that in spite of bringing about ‘an unprecedented rise in total output’, it nevertheless ‘strengthened the dominant role of human labour in most kinds of productive processes’.18 At any time, a breakthrough could destroy this fragile pin, annihilating the social preconditions of functioning market economies. Drawing on this insight—and adding only that such a technological breakthrough now exists—the automation prognosticators often argue that capitalism must be a transitory mode of production, which will eventually give way to a new form of life that does not organize itself around work for wages and monetary exchange.

Taking its periodicity into account, automation theory may be described as a spontaneous discourse of capitalist societies, which, for a mixture of structural and contingent reasons, reappears in those societies time and again as a way of thinking through their limits. What summons the automation discourse periodically into being is a deep anxiety about the functioning of the labour market: there are simply too few jobs for too many people. Proponents of the automation discourse consistently explain the problem of a low demand for labour in terms of runaway technological change.

Declining labour demand
If automation discourse appeals so widely again today, it is because, whatever their causes, the ascribed consequences of automation are all around us: global capitalism clearly is failing to provide jobs for many of the people who need them. There is, in other words, a persistently low demand for labour, reflected not only in higher spikes of unemployment and increasingly jobless recoveries—both frequently cited by automation theorists—but also in a phenomenon with more generic consequences: declining labour shares of income. Many studies have now confirmed that the labour share, whose steadiness was held to be a stylized fact of economic growth, has been falling for decades (Figure 1).
These shifts signal a radical decline in workers’ bargaining power. Realities for the typical worker are worse than these statistics suggest, since wage growth has become increasingly skewed towards the highest earners: the infamous top one per cent. A growing gap has opened up not only between the growth of labour productivity and average wage-incomes, but also between the growth of average wages and that of median wages, with the result that many workers see a vanishingly thin slice of economic growth (Figure 2).20 Under these conditions, rising inequality is contained only by the strength of redistributive programmes. Even critics of automation discourse such as David Autor and Robert Gordon are disturbed by these trends: something has gone wrong with the economy, leading to a low demand for labour.

Is automation the cause of the low demand for labour? I will join the critics of automation discourse in arguing that it is not. However, along the way, I will also criticize the critics—both for producing explanations of low labour demand that only apply in high-income countries and for failing to produce anything like a radical vision of social change that is adequate to the scale of the problems we now confront. Indeed, it should be said from the outset that I am more sympathetic to the left automation theorists than to their critics.

Even if the explanation they offer turns out to be inadequate, the automation theorists have at least focused the world’s attention on the problem of a persistently low demand for labour. They have also excelled in actually trying to imagine solutions to this problem that are broadly emancipatory in character. In Jameson’s terms, the automation theorists are our late capitalist utopians. In a world reeling from the ‘perfect storm’ of climate change, rising inequality, recalcitrant neoliberalism and resurgent ethno-nationalism, the automation theorists are the ones pushing through the catastrophe with a vision of an emancipated future, in which humanity advances to the next stage in our history, whatever that might mean (or whatever we want to make it mean), and technology helps to free us all to discover and follow our passions. That is true in spite of the fact that—like many of the utopians of the past—the actual visions these latest utopians offer need to be freed from their largely technocratic fantasies of how social change to a better future might take place.

Major shifts in the forms of government intervention in the economy are adopted only under massive social pressure, such as, in the course of the 20th century, the threat of communism or of civilizational collapse. Today, policy reforms could emerge in response to pressure coming from a new mass movement, aiming to change the basic makeup of the social order. Instead of fearing that movement, we should see ourselves as part of it, helping articulate its goals and paths forward. If that movement is defeated, maybe the best we will get is basic income, but that should not be our goal. We should be reaching towards a post-scarcity world, which advanced technologies will certainly help us realize, even if full automation is not achievable—or even desirable.The return of automation discourse is a symptom of our era, as it was in times past: it arises when the global economy’s failure to create enough jobs causes people to question its fundamental viability. The breakdown of this market mechanism today is more extreme than at any time in the past. This is because a greater share of the world’s population than ever before depends on selling its labour or the simple products of its labour to survive, in the context of weakening global economic growth. Our present reality is better described by near-future science-fiction dystopias than by standard economic analysis; ours is a hot planet, with micro-drones flying over the heads of the street hawkers and rickshaw pullers, where the rich live in guarded, climate-controlled communities while the rest of us wile away our time in dead-end jobs, playing video games on smartphones. We need to slip out of this timeline and into another.
Reaching towards a post-scarcity world—in which all individuals are guaranteed access to whatever they need to make a life, without exception—can become the basis on which humanity mounts a battle against climate change. It can also be the foundation on which we remake the world, creating the conditions in which, as James Boggs once put it, ‘for the first time in human history, great masses of people will be free to explore and reflect, to question and to create, to learn and to teach, unhampered by the fear of where the next meal is coming from’. Finding our way forward requires a break between work and income, as the automation theorists recognize, but also between profit and income, as many do not.
In responding to the automation discourse, then, I will argue that the decline in the demand for labour is due not to an unprecedented leap in technological innovation, but to ongoing technical change in an environment of deepening economic stagnation. In the second part of this contribution, to be published in nlr 120, I contend that this fall in labour demand manifests not as mass unemployment, but rather as mass under-employment, not necessarily a problem for the elites. On this basis, I mount a critique of technocratic solutions, like basic income. I offer a thought-experiment of how we might imagine a post-scarcity society that centres on humans, not machines, and project a path of how we might get there through social struggle, rather than administrative intervention. But first, in Part One, I provide a diagnosis of the underlying causes of the decline in demand for labour. This involves a detour to consider the fortunes of the global manufacturing sector and the competitive dynamics at work in labour’s ‘deindustrialization’.
2. LABOUR'S GLOBAL DEINDUSTRALIZATION
Automation-discourse theorists recognize that, if technologically induced job-destruction is to have widespread social ramifications, it will have to eliminate employment in the vast and variegated service sector, which absorbs 74 per cent of workers in high-income countries and 52 per cent worldwide. They therefore focus on ‘new forms of service-sector automation’ in retail, transportation and food services, where ‘robotization’ is said to be ‘gathering steam’ with a growing army of machines that take orders, stock shelves, drive cars and flip burgers. Many more service-sector jobs, including some that require years of education and training, will supposedly be rendered obsolete in the coming years due to advances in artificial intelligence. Of course, these claims are mostly predictions about the effects that technologies will have on future patterns of employment. Such predictions can go wrong—as for example when Eatsa, an automated fast-food company which employed neither cashiers nor waiters, was forced to close most of its stores in 2017.
In making their case, automation theorists often point to the manufacturing sector as the precedent for what they imagine is beginning to happen in services—for in manufacturing, the employment-apocalypse has already taken place. To evaluate the theorists’ claims, it therefore makes sense to begin by looking at what role automation has played in that sector’s fate. After all, manufacturing is the area most amenable to automation, since on the shop floor it is possible to ‘radically simplify the environment in which machines work, to enable autonomous operation’. Industrial robotics has been around for a long time: the first robot, the ‘unimate’, was installed in a General Motors plant in 1961. Still, until the 1960s, scholars studying this sector were able to dismiss Luddite fears of long-term technological unemployment out of hand. Manufacturing employment in fact grew most rapidly in those lines where technical innovation was happening at the fastest pace, because it was in those lines that prices fell the fastest, stoking the growth of demand for the products.
Industrialization has long since given way to deindustrialization, and not just in any one line but across the manufacturing sectors of most countries. The share of workers employed in manufacturing fell first across the high-income world: manufacturing employed 22 per cent of all workers in the us in 1970; that share declined to just 8 per cent in 2017. Over the same period, manufacturing employment shares fell from 23 per cent to 9 per cent in France, and from 30 per cent to 8 per cent in the uk. Japan, Germany and Italy have experienced smaller but still substantial declines: in Japan from 25 per cent to 15 per cent, in Germany from 29 per cent to 17 per cent, and in Italy from 25 per cent to 15 per cent. In all cases, the declines were eventually associated with substantial falls in the total number of people employed in manufacturing. In the us, Germany, Italy and Japan, the overall number of manufacturing jobs fell by approximately a third from postwar peaks; in France, by 50 per cent and in the uk, by 67 per cent.

It is commonly assumed that deindustrialization must be the result of production facilities moving offshore. Yet in none of the countries named above has manufacturing job loss been associated with declines in manufacturing output. Real value added in manufacturing more than doubled in the us, France, Germany, Japan and Italy between 1970 and 2017. Even the uk, whose manufacturing sector fared worst of all among this group, saw a 25 per cent increase in manufacturing real value added over this period. To be sure, low- and middle-income countries are producing more and more goods for import into high-income countries; however, deindustrialization in the latter cannot simply be the result of productive capacity moving to the former. In the scholarly literature, deindustrialization is therefore ‘most commonly defined as a decline in the share of manufacturing in total employment’, regardless of corresponding trends in levels of manufactured output. This definition moves in step with automation theorists’ core expectations: more goods are being produced but by fewer workers.

It is on this basis that commentators typically cite rapidly rising labour productivity, rather than an influx of low-cost imports from abroad, as the primary cause of industrial-job loss in advanced economies. On closer inspection, however, this explanation turns out to be inadequate: no upward leap has taken place in manufacturing productivity levels. On the contrary, manufacturing productivity has been growing at a sluggish pace for decades, leading Robert Solow to quip, ‘We see the computer age everywhere, except in the productivity statistics.’ Automation theorists discuss this ‘productivity paradox’ as a problem for their account—explaining it in terms of weak demand for products, or the persistent availability of low-wage workers—but they understate its true significance. This is partly due to the appearance of steady labour-productivity growth in us manufacturing, at an average rate of around 3 per cent per year since 1950. On that basis, Brynjolfsson and McAfee suggest, automation could show up in the compounding effects of exponential growth, rather than an uptick in the growth rate.

However, official us manufacturing growth-rate statistics are overinflated, for example in logging the production of computers with higher processing speeds as equivalent to the production of more computers. On that basis, government statistics claim that productivity levels in the computers and electronics sub-sector rose at an average rate of over 10 per cent per year between 1987 and 2011, even as productivity growth rates outside of that sub-sector fell to around 2 per cent per year over the same period. Since 2011, trends across the manufacturing sector have worsened: real output per hour in the sector as a whole was lower in 2017 than at its peak in 2010. Productivity growth rates in manufacturing collapsed precisely when they were supposed to be rising rapidly due to industrial automation.

Correcting manufacturing-productivity statistics in the us brings them more into line with trends visible in the statistics of other countries. In Germany and Japan, manufacturing-productivity growth rates have fallen dramatically since their postwar peaks. In Germany, for example, manufacturing productivity grew at an average annual rate of 6.3 per cent per year in the 1950s and 60s, falling to 2.4 per cent since 2000. This downward trend is to some extent an expected result of the end of an era of rapid, catch-up growth. However, it should still be surprising to the automation theorists, since Germany and Japan have raced ahead of the us in the field of industrial robotics. Indeed, the robots used in Tesla’s largely automated car factory in California were made by a German robotics company. German and Japanese firms deploy about 60 per cent more industrial robots per 10,000 manufacturing workers, compared to the US.

Yet deindustrialization continues to take place in all these countries, despite lacklustre manufacturing-productivity growth rates: that is, it is taking place as the automation theorists expect, but not for the reasons they offer. To explore the causes of deindustrialization in more detail, I use the following accounting identity. For any given industry, the rate of growth of output (ΔO) minus the rate of growth of labour productivity (ΔP) equals the rate of growth of employment (ΔE). Thus, ΔO – ΔP = ΔE. So, for example, if the output of automobiles grows by 3 per cent per year, and productivity in the automobile industry grows by 2 per cent per year, then employment in that industry must necessarily rise by one per cent per year (3 – 2 = 1). Contrariwise, if output grows by 3 per cent per year and productivity grows by 4 per cent per year, employment will contract by 1 per cent per year (3 – 4 = -1).

Disaggregating manufacturing-output growth rates in France provides us with a sense of the typical pattern playing out across the high-income countries (Figure 3). During the so-called Golden Age of postwar capitalism, productivity growth rates in French manufacturing were much higher than they are today—5.2 per cent per year, on average, between 1950 and 1973—but output growth rates were even higher than that—5.9 per cent per year—as a result of a steady increase in employment of 0.7 per cent per year. Since 1973, both output and productivity rates have declined, but output rates fell much more sharply than productivity rates. By the early years of the 21st century, productivity growth rates—although much slower, at 2.7 per cent per year—were now faster than their corresponding output growth rates—at 0.9 per cent—as manufacturing employment contracted rapidly, by 1.7 per cent per year.

This disaggregation helps explain why automation theorists falsely perceive productivity to be growing at a rapid pace in manufacturing: in fact, productivity growth has been rapid only relative to extremely sluggish output growth. The same pattern can be seen in the statistics of other countries: no absolute decline in levels of manufacturing production has taken place, but there has been a decline in the output growth rate, with the result that output is growing more slowly than productivity (Table 1, overleaf). The simultaneity of limited technological dynamism and worsening economic stagnation combines to generate a progressive decline in industrial employment levels.

As such, ‘output-led’ deindustrialization is impossible to explain in purely technological terms. In searching for alternative perspectives, economists have mostly preferred to describe it as a harmless evolutionary feature of advanced economies. However, that perspective is itself at a loss in explaining extreme variations in the gdp per capita levels at which this supposedly evolutionary economic shift has taken place. Deindustrialization unfolded first in high-income countries in the late 1960s and early 1970s, at the tail-end of a period in which levels of income per person had converged across the us, Europe and Japan. In the decades that followed, deindustrialization then spread ‘prematurely’ to middle- and low-income countries, with larger variations in incomes per capita (Figure 4). In the late 1970s, deindustrialization arrived in southern Europe; much of Latin America, parts of East and Southeast Asia, and southern Africa followed in the 1980s and 1990s. Peak industrialization levels in many poorer countries were so low that it may be more accurate to say that they never industrialized in the first place.

By the end of the 20th century, it was possible to describe deindustrialization as a kind of global epidemic: worldwide manufacturing employment rose in absolute terms by 0.4 per cent per year between 1991 and 2016, but that was much slower than the overall growth of the global labour force, with the result that the manufacturing share of total employment declined by 3 percentage points over the same period. China is a key exception, but only a partial one (Figure 5, overleaf). In the mid 1990s, Chinese state-owned enterprises shed large numbers of workers, sending manufacturing-employment shares on a steady downward trajectory. China re-industrialized, starting in the early 2000s, but then began to deindustrialize once again in the mid 2010s: its manufacturing-employment share has since dropped from 19.3 per cent in 2013 to 17.5 per cent in 2017, with further falls likely. If deindustrialization cannot be explained by either automation or the internal evolution of advanced economies, what could be its source?

3. BLIGHT OF MANUFACTURING OVERCAPACITY

What the economists’ accounts fail to register in explaining deindustrialization is also what is missing from the automation theorists’ accounts. The truth is that rates of output growth in manufacturing have tended to decline, not only in this or that country, but worldwide (Figure 6). In the 1950s and 60s, global manufacturing production expanded at an average annual rate of 7.1 per cent per year, in real terms. That rate fell progressively to 4.8 per cent in the 1970s, and to 3.0 per cent between 1980 and 2007. Since the 2008 crisis and up to 2014, manufacturing output expanded at just 1.6 per cent per year, on a world scale—that is, at less than a quarter of the pace achieved during the so-called postwar Golden Age. It is worth noting that these figures include the dramatic expansion of manufacturing productive capacity in China. Again, it is the incredible degree of slowdown or even stagnation in manufacturing-output growth, visible on the world scale, that explains why manufacturing-productivity growth appears to be advancing at a rapid clip, even though it is actually much slower than before. More and more is produced with fewer workers, as the automation theorists claim, but not because technological change is giving rise to high rates of productivity growth. On the contrary, productivity growth in manufacturing appears rapid today only because the yardstick of output growth, against which it is measured, is shrinking.Seen from this perspective, the global wave of deindustrialization can be said to find its origins not in runaway technical change but rather in worsening overcapacity in world markets for manufactured goods. The rise in overcapacity developed stepwise after World War Two. In the immediate postwar period, the us hosted the most dynamic economy in the world, with the most advanced technologies. Under the threat of communist expansion within Europe, as well as in East and Southeast Asia, the us proved willing to share its technological largesse with its former imperial competitors Germany and Japan, as well as other ‘frontline’ countries, in order to bring them all under the us security umbrella. In the first few decades of the post-wwii era, these technology transfers were a major boost to economic growth in Europe and Japan, opening up opportunities for export-led expansion. This strategy was also supported by the devaluation of European and Japanese currencies against the dollar. However, as Robert Brenner has argued, rising manufacturing capacity across the globe quickly generated overcapacity, issuing in a ‘long downturn’ in manufacturing output growth rates.

What mattered here was not only the later building out of manufacturing capacity in the global South, but the earlier creation of such capacity in countries like Germany, Italy and Japan, which hosted the first low-cost producers in the postwar era who succeeded in taking shares in global markets for industrial goods, and then invading the previously impenetrable us domestic market. That competition caused rates of industrial-output growth in the us to decline in the late 1960s, issuing in deindustrialization in employment terms. As the us responded to heightened import penetration in the 1970s by breaking up the Bretton Woods order and devaluing the dollar, these same problems spread from the highest wage countries in North America and northern Europe to Japan and the rest of Europe. Thereafter, as more and more countries built up manufacturing capacity, adopted export-led growth strategies and entered global markets for manufactured goods, falling rates of manufacturing-output growth and consequent labour deindustrialization also spread to Latin America, the Middle East, Asia and Africa, as well as to the global economy taken as a whole.

Deindustrialization is not only a matter of technological advance, but also of a global redundancy of technological capacities, creating more crowded markets in which rapid rates of industrial-output expansion become more difficult to achieve. The mechanism transmitting this problem across the globe was severely depressed prices in global markets for manufactured goods. That led to falling income-per-unit capital ratios, then to falling rates of profit, then to lower rates of investment, and hence lower rates of output growth. In this environment, firms have faced heightened competition for market share: as overall growth rates slow, the only way to grow quickly is to steal market shares from other firms. Each firm has to do everything it can to keep up with its competitors. Overcapacity explains why, since the early 1970s, productivity-growth rates have fallen less severely than output-growth rates: firms have continued to raise their productivity levels as best they can despite falling rates of output growth (or else have gone under, disappearing from statistical averages). As manufacturing-output growth rates slipped below productivity-growth rates in one country after another, deindustrialization spread worldwide.

Driving globalization

Explaining global waves of deindustrialization in terms of global overcapacity rather than industrial automation allows us to understand a number of features of this phenomenon that otherwise appear paradoxical. For example, rising overcapacity explains why deindustrialization has been accompanied not only by ongoing efforts to develop new labour-saving technologies, but also by the building out of gigantic, labour-using supply chains—usually with a more damaging environmental impact. A key turning point in that story came in the 1960s, when low-cost Japanese and German products invaded the us domestic market, sending the us industrial-import penetration ratio soaring from less than 7 per cent in the mid-60s to 16 per cent in the early 1970s. From that point forward, it became clear that high levels of labour productivity would no longer serve as a shield against competition from lower-wage countries. The us firms that did best in this context were the ones that responded by globalizing production. Facing competition on prices, us multinational firms built international supply chains, shifting the more labour-intensive components of their production processes abroad and playing suppliers off against one another to achieve the best prices. In the mid-60s the first export-processing zones opened in Taiwan and South Korea. Even Silicon Valley, which formerly produced its computer chips locally in the San Jose area, shifted its production to low-wage areas, using lower grades of technology (and also benefitting from laxer laws around pollution and workers’ safety). mncs in Germany and Japan adopted similar strategies, which were everywhere supported by new infrastructures of transportation and communication technologies.

The globalization of production allowed the world’s wealthiest economies to retain manufacturing capacity, but it did not reverse the overall trend towards labour deindustrialization. As supply chains were built out across the world, firms in more and more countries were pulled into the swirl of world-market competition. In some countries, this move was accompanied by shifts in the location of new plants: rustbelts oriented towards production for domestic markets went into decline, while sunbelts integrated into global supply networks expanded dramatically. Chattanooga grew at the expense of Detroit, Ciudad Juárez at the expense of Mexico City, Guangdong at the expense of Dongbei. Yet given the overall slowdown in rates of world manufacturing-market expansion, this re-orientation towards the world market could only result in lacklustre outcomes: the rise of sunbelts failed to balance out the decline of rustbelts, resulting in global deindustrialization.

At the same time, global manufacturing overcapacity explains why the countries that have succeeded in attaining a high degree of robotization are not those that have seen the worst degree of deindustrialization. In the context of intense global competition, high degrees of robotization have given firms competitive advantages, allowing them to take market share from firms in other countries. Thus Germany, Japan and South Korea have some of the highest levels of robotization; they also have the largest trade surpluses in the world. Workers in European and East Asian firms know that automation helps preserve their jobs. China is also a top-four country in terms of trade surpluses, providing its manufacturing sector with a gigantic boost in terms of both output and employment growth. China has advanced on this front not due to high levels of robotization, but rather due to a mix of low wages, moderate to advanced technologies, and strong infrastructural capacities. Yet the result was the same: in spite of system-wide overcapacity and slow growth rates, the prc has industrialized rapidly because Chinese firms have been able to take market share away from other firms—not only in the us, but also in countries like Mexico and Brazil—which lost market share as Chinese firms expanded. It could not have been otherwise, since in an environment where average growth rates are low, firms can only achieve high rates of growth by taking market share from their competitors. Whether China will be able to retain its competitive position as its wage levels rise remains an open question; Chinese firms are now racing to robotize in order to head off this possibility.

4. BEYOND MANUFACTURING

The evidence I have cited so far to explain job loss in the manufacturing sector through worsening overcapacity may appear to have little purchase on the larger, economy-wide patterns—of stagnant wages, falling labour shares of income, declining labour-force participation rates and jobless recoveries after recessions—that the automation theorists have sought to explain by growing technological dynamism. Automation may therefore still seem a good explanation for the decline in demand for labour across the service sectors of each country’s economy, and so across the world economy as a whole. Yet this broader problem of declining labour demand also turns out to be better explained by the worsening industrial stagnation I have described than by widespread technological dynamism.

This is because, as rates of manufacturing-output growth stagnated in one country after another from the 1970s onward, no other sector appeared on the scene to replace industry as a major economic-growth engine. Instead, the slowdown in manufacturing-output growth rates was accompanied by a slowdown in overall growth rates. This trend is visible in the economic statistics of high-income countries. France is again a striking example (Figure 7). In France, real manufacturing value added (mva) rose at 5.9 per cent per year between 1950 and 1973, while real value added in the total economy (gdp) rose at 5.1 per cent per year. Since 1973, both growth measures have declined significantly: by the 2001–17 period, mva was rising at only 0.9 per cent per year, while gdp was rising at a faster but still sluggish pace of 1.2 per cent per year. Note that during the 1950s and 60s, mva growth generally led the overall economy: manufacturing served as the major engine of overall growth. Since 1973, mva growth rates have trailed overall economic growth. Similar patterns can be seen in other high-income countries (Table 2, overleaf).Their export-led growth engines sputtered and slowed to a crawl; and as they did so, overall rates of economic growth slowed considerably.

Economists studying deindustrialization often point out that while manufacturing has declined as a share of nominal gdp, it has maintained, until recently, a more or less steady share of real gdp, which is to say that, between 1973 and 2000, real mva grew at approximately the same pace as real gdp. What that has meant in practice is that, as manufacturing has become less dynamic, so has the overall economy. There was no significant shift in demand from industry to services. Instead, as capital accumulation slowed down in manufacturing, the expansion of aggregate output also slowed significantly across the economy as a whole.

This tendency to economy-wide stagnation, associated with the decline in manufacturing dynamism, then explains the system-wide decline in the demand for labour, and so also the problems that the automation theorists cite: stagnant real wages, falling labour shares of income and so on. This economy-wide pattern of declining labour demand is not the result of rising productivity-growth rates, associated with automation in the service sector. On the contrary, productivity is growing even more slowly outside of the manufacturing sector than inside of it: in France, for example, while productivity in the manufacturing sector was rising at an average annual rate of 2.7 per cent per year between 2001–17, productivity in the service sector was rising at just 0.6 per cent per year. Similar gaps exist in other countries. Once again, the mistake of the automation theorists is to focus on rising productivity growth rather than falling output growth. The environment of slower economic growth explains the low demand for labour all by itself. Workers, and especially workers who are not protected by powerful unions or labour laws, find it difficult to pressure employers to raise their wages when there is so much slack in the labour market.

These trends are as visible in the world economy—including China—as they are in the high-income countries (Figure 8, overleaf). In the 1950s and 60s, global mva growth and gdp growth were expanding at rapid clips of 7.1 per cent and 5.0 per cent respectively, with mva growth leading gdp growth by a significant margin. From the 1970s onward, as global mva growth slowed, so did global gdp growth. In most of the decades that followed, global mva growth continued to lead gdp growth but by a much smaller margin. Since 2008, both rates have been growing at the exceptionally slow pace of 1.6 per cent per year. Again, the implication is that, as manufacturing growth rates declined, nothing emerged to replace industry as a growth engine. Not all regions of the world economy are experiencing this slowdown in the same way or to the same extent, but even countries like China that have grown quickly have to contend with this global slowdown and its consequences. Since the 2008 crisis, China’s economic growth rate has slowed considerably; its economy is deindustrializing.

The clear conclusion is that manufacturing turned out to be a unique engine of overall economic growth. Industrial production tends to be amenable to incremental increases in productivity, achieved via technologies that can be repurposed across numerous lines. Industry also benefits from static and dynamic economies of scale. Meanwhile, there is no necessary boundary to industrial expansion: industry consists of all economic activities that are capable of being rendered via an industrial process. The reallocation of workers from low-productivity jobs in agriculture, domestic industry and domestic services to high-productivity jobs in factories raises levels of income per worker and hence overall economic growth rates. The countries that have caught up with the West in terms of income—such as Japan, South Korea and Taiwan—mostly did so by industrializing: they exploited opportunities to produce for the world market, at increasing scale and using advanced technologies, allowing them to grow at speeds that would have been unachievable had they depended on domestic-market demand alone.

When the growth engine of industrialization sputters—due to the replication of technical capacities, international redundancy and fierce competition for markets—there has been no replacement for it as a source of rapid growth. Instead of workers reallocating from low-productivity jobs to high-productivity ones, the reverse of this process takes place, as workers pool increasingly in low-productivity jobs in the service sector. As countries have deindustrialized, they have also seen a massive build-up of financialized capital, chasing returns to the ownership of relatively liquid assets, rather than investment in new fixed capital. In spite of the high degree of overcapacity in industry, there is nowhere more profitable in the real economy for capital to invest itself. Indeed, if there had been, we would have evidence of it in higher rates of investment and hence higher gdp growth rates. This helps explain why firms have reacted to over-accumulation by trying to make their existing manufacturing capacity more flexible and efficient, rather than ceding territory to lower-cost, higher-productivity firms from other countries.

The lack of an alternative growth engine also explains why governments in poorer countries have encouraged domestic producers to try to break into already oversupplied international markets for manufactures. Nothing has replaced those markets as a major source of globally accessible demand. Overcapacity exists in agriculture, too, and is even worse there than in industry; meanwhile services, which are mostly non-tradable, make up only a tiny share of global exports. If countries are to retain any dependable link to the international market under these conditions, they must find some way to insert themselves into industrial lines, however oversupplied. System-wide overcapacity and the generalized slowdown in economic growth have therefore been devastating for most poorer countries: the amount of foreign exchange they have captured through liberalization has been pitiful; so, too, has been the number of jobs created.

Indeed, global economic downshifts have been particularly devastating for low- and middle-income countries, not only because they are poorer, but also because those downshifts have taken place in an era of rapid labour-force expansion: between 1980 and the present, the world’s waged workforce grew by about 75 per cent, adding more than 1.5 billion people to the world’s labour markets. These labour market entrants, living mostly in poorer countries, had the misfortune of growing up and looking for work at a time when global industrial overcapacity began to shape patterns of economic growth in post-colonial countries: declining rates of manufactured export growth into the us and Europe in the late 1970s and early 1980s ignited the 1982 debt crisis, followed by imf-led structural adjustment, which pushed countries to deepen their imbrications in global markets at a time of ever slower global growth and rising competition from China. In spite of shocks to the demand for labour generated by slowing global growth rates and rising economic turmoil, huge numbers of workers were still forced to seek employment in order to live.

Some may respond that the present low rates of global growth are in fact nothing out of the ordinary, if only we shift our baseline from the exceptional postwar ‘Golden Age’ to previous periods, such as the pre-WWI era. But a global perspective on the decline in the demand for labour provides the answer to this objection. It is true that, during the Belle Epoque, average rates of economic growth were more comparable to growth rates today. However, in that period, large sections of the population still lived in the countryside and produced much of what they needed to live. European empires still overran the globe, not only limiting the diffusion of new manufacturing technologies to a few regions, but also actively deindustrializing the rest of the world economy. Yet in spite of the much more limited sphere in which labour markets were active—and in which industrialization took place—the pre-wwi era, as also the inter-war period, was marked by a persistently low demand for labour, making for employment insecurity, rising inequality and tumultuous social movements aimed at transforming economic relations. In this respect, the world of today does look like the world of the Belle Epoque. The difference is that today, a much larger share of the world’s population depends on finding work in labour markets in order to live.What automation theorists describe as the result of rising technological dynamism is actually the consequence of worsening economic stagnation: productivity-growth rates appear to rise when, in reality, output-growth rates are falling. This mistake is not without reason. The demand for labour is determined by the gap between productivity and output growth rates. Reading the shrinking of this gap the wrong way around—that is, as due to rising productivity rather than falling output rates—is what generates the upside-down world of the automation discourse. Proponents of this discourse then search for the technological evidence that supports their view of the causes for the declining demand for labour. In making this leap, the automation theorists miss the true story of overcrowded markets and economic slowdown that actually explains the decline in labour demand.

Yet even if automation is not itself the primary cause of a low demand for labour, it is nevertheless the case that, in a slow-growing world economy, technological changes within a near-future horizon may still threaten large numbers of jobs with destruction, in a context of economic stagnation and slower rates of job creation. Technological change then acts as a secondary cause of a low labour demand, operating within the context of the first. The concluding section of this essay in nlr 120 will address these technological dynamics, as well as the socio-political problems—and opportunities—generated by a persistently low demand for labour in late-capitalist societies.


Beyond Human Intelligence, The Natural world produces its own supercomputers, Conor Purcell


Anyone with personal experience taking LSD, psilocybin, ayahuasca, or any other mindaltering psychedelic will truly recognise the world’s interconnected nature. Fractalising into white diamonds, the air shatters, trees breathe, and animals speak your language. Some suggest these drugs dismantle an evolved human filter, revealing nature for what it truly is: a connected intelligence.

James Bridle’s new book Ways of Being: Beyond Human Intelligence is an exploration of different forms of intelligence, both biological and artificial. It’s also, as the author says, a call for us humans to start forming new relationships with non-human intelligence. Throughout the book Bridle argues that our common future demands less industrial hubris, and more cooperation with existing and deeply knowledgeable biological systems.

A writer and artist known for coining the term ‘New Aesthetic’ – used to refer to the increasing appearance of the visual language of digital technology in the physical world, and the combination of the virtual and physical – James Bridle advocates for a future characterised by human, animal, and plant reconnectivity for the sake of achieving a better planetary balance.

Our regular contributor, Conor Purcell, PhD, had the opportunity to interview Bridle for FARSIGHT, speaking by video call between Purcell’s home in County Donegal, Ireland, and the interviewee’s in Greece.

What inspired you to write this book?

I studied artificial intelligence almost twenty years ago when it was kind of fading from the curriculum because it wasn’t going anywhere. Since then, there haven’t been any kind of major discoveries. But what has happened is that vast amounts of data have become available, which have been harvested largely by social media giants and governments. At the same time, processing power has massively increased. We’re now seeing how AI is revealing itself to be something not quite human in that it thinks and approaches the world in a very different way than we do. We’re also starting to realise, thanks to decades of research, that intelligence is something much more interesting and greater than our very narrow human idea of it.

With the book, I wanted to understand how we can better accommodate ourselves with everything else that we share the planet with. For me, this question is central to achieving environmental justice and progress. I now see an opportunity with AI for reimagining, firstly, what intelligence is, and secondly, how we impact other forms of intelligence beyond the human.

How do you think people have become so disconnected from these other ways of being, specifically the intelligences of animals and plants in nature?

A good example to demonstrate this is how in medieval times there were cases when animals were accused of committing a crime and tried in courtrooms. There were lawyers there, and the animals were presented to juries. This wasn’t pantomime, but a deeply serious undertaking because non-humans were part of the community. That meant they had rights and responsibilities.

Over time, and especially with industrialisation and urbanisation, attitudes towards nonhuman life in all its forms changed. We started to view them essentially as machines – unfeeling automatons who didn’t have the kind of inner life or higher importance which we ascribe to humans. This became the dominant mode of thought within Western, postenlightenment societies. That’s when the abattoirs began. And now the environmental mess that we find ourselves in is all related to how we’re out of balance with our deeply entangled and interdependent relationships with all other species.

What do we now know about the intelligent behavior of plants?

Recent research has shown several surprising behavioural qualities in plants. I write in the book about scientists who subjected certain plants to repeated shocks and found that quite quickly they learned essentially to ignore the shock and move away from its source. What’s more is that they remembered patterns and continued to avoid the source of the shock in the future.

This is an extraordinary finding that completely changes our understanding of plant behaviour. Even the idea that plants have a thing that we might call behaviour is astonishing because the traditional kind of botanical approach mostly involves cutting them up into small pieces and studying them as if they were machines. What’s interesting too is that these researchers write about working with plant spirits, and their work is informed by both the knowledge that has come from the plants themselves and by treating the plant as already having its own personhood.

This is real science published in legitimate scientific journals. It’s peer reviewed. It’s reproducible. It conforms to all the structures of the scientific method. What that tells me is that there are multiple ways of approaching these intelligences and to do that via a kind of synthesis of these different ways of knowing is incredibly powerful. We can explore the world by observing and connecting with these behaviours, as long as our goal is to truly understand. Ultimately, it all depends on admitting the possibility in the first place that these kinds of alternative intelligences are real.

In a chapter called "Non-Binary Machines" you talk about the fields of cybernetics- which has a long history dating back to the mid-20th century- and how this shows a future alternative to what you call 'corporate artificial intelligence'. Can you explain what you mean by this?

It has to do with thinking of intelligence as a process, rather than as a machine that thinks like a kind of brain in a box. Particularly in Britain, cybernetic researchers – those involved in the science of communication and automation in machines and other living beings – envisioned a kind of intelligence that is active in the world, which is connected to the world around it, which is learning, and which is defined by what it does, rather than what it is. This is different to the corporate artificial intelligence of today which is currently being developed to increase profits.

Cybernetic research continues in various ways. There is very interesting new research around soft robotics, which essentially tries to make robotic systems more adaptive to the world around them. Programmes like the Unconventional Computing Lab at the University of the West of England is a good example. One of the things they study is the computational abilities of various plants and animals. They are doing very interesting things like redesigning computer logic based on the movement of crabs, for example. This points to the fact that what we understand as computation is not something that can only be performed within machines, but in fact is conducted by biological organisms too.

It also appears that biological systems can be calibrated to test variable abilities and to solve mathematical problems – they might even be more efficient than our fastest supercomputers. These abilities exist across the natural world, but since we usually only see the things that we know how to test for, there remains the possibility of a whole range of intelligences which far exceed our own. The problem is that we don’t even know how to ask the appropriate questions yet.

How can we reconnect with non-human intelligence in the future?

Towards the end of the book, I write about the need to provide more shared territory for human and non-human lives. I mean this both in the form of animal reserves, conservation areas, wildlife corridors and shared spaces that allow animals to move in ways that they currently cannot. But I also think that the notion of animal intelligence compels us to think politically.

In the book, I discuss the Irish experience with the introduction of the so-called citizen assemblies, made up of 33 representatives chosen by political parties and 66 randomly chosen citizens, to make recommendations on society’s biggest challenges. One of the things we learned from the citizen assemblies, not just in Ireland, but in other places, is that this is an extraordinary mechanism for mobilising what are essentially multiple forms of intelligence. The assemblies didn’t use animal, plant or artificial intelligence, but by branching out beyond the traditional domain of experts, the range of human intelligence – and personality types – was enlarged. So instead of selecting a very narrow definition of domain experts, it’s acknowledging that what you need for complex thorny problems, particularly novel ones, is a wider diversity of life experiences and ways of thinking.

The same principle can apply to including intelligences beyond the human in our decision making. I believe that only by bringing in diverse ways of thinking and forms of life experience can we address the kind of extraordinary global and pan-species problems that we face.


Is Artificial Intelligence a Myth? Exploring whether human sense making can be surpassed by AI, Klaus Æ. Mogensen


Erik Larson is a tech entrepreneur and pioneering research scientist working at the forefront of natural language processing. He recently published a book called The Myth of Artificial Intelligence: Why Computers Can't Think the Way We Do. FARSIGHT met him online for an interview about the future of AI, and why he believes the field’s current path of development will not lead us to human-level intelligence in machines anytime soon.

Erik, what made you decide to write this book?

My specialty is natural language processing, and I wrote the book from the perspective of understanding the many practical challenges and difficulties there are in making computers understand human language on a deep level. Early in my career, I read a book by Ray Kurzweil, The Age of Intelligent Machines, where he proposed 2029 as the year when computers become as smart as humans. I thought, maybe – it’s 30 years, after all. By 2005, when his book The Singularity is Near came out, I thought that it could not happen in 20 years without some major unexpected scientific breakthrough that we couldn’t anticipate yet. Instead of acting like we’re on an inevitable path to general AI, we should tell the broader public that achieving true computer intelligence is a lot more difficult than many assume. That’s why I wrote the book. 

You argue that we are very far from developing general artificial intelligence. In fact, you believe that the approach we are currently pursuing can never lead us there. Why is that?

The main framework that I use in the book is inference. In AI, the problem is that we’re using the wrong type of inference to ever get to general or commonsense intelligence. Right now, the field is almost exclusively dominated by machine learning using inductive inference, learning from prior examples. Human beings use induction all the time, but it’s not the most important type of inference for us. It can’t handle novelty because it’s based on prior observation. Without a novelty mechanism, you can’t get to certain kinds of intelligence. I don’t mean to say that it’s impossible. Nature has developed general intelligence, so we should be able to eventually do the same thing. However, there’s something currently missing, and that’s why it’s been so difficult to make certain kinds of progress in the field.

Arthur C. Clarke famously thought that to get something like intelligence in a computer, we would need heuristic logic- finding and using solutions that aren't precise, but just good enough, which is how we think. We don't measure the distance across the street with a measuring tape; we guesstimate how far it is This method is a lot faster and works well for everyday stuff. Do you think we could program that kind of heuristic logic into computers?

We do that already. Before deep learning became the dominant paradigm in AI development, classic AI design was more rule-based. One of the great challenges in the classic rules-based paradigm was in fact to find these rules of thumb, or heuristics. Herbert Simon, a pioneer in AI and Nobel Prize winner in economics, has said that people who favour adequacy and efficiency over optimisation generally make better, more responsible, and quicker decisions than those who want to make every decision perfect. Precision can be a barrier. However, the classic AI approach based on common-sense heuristics also failed when the domain wasn’t sufficiently constrained. Even if you have a rule that doesn’t need precision, you have so much context in an unconstrained real-world environment that you need rules to tell other rules when they are relevant. It quickly becomes intractable to try to get intelligent behaviour from such a system.

There are two major, unsolved problems in AI. One is robotics, especially when the robot is not in a very specific environment. A robot arm in an industrial setting with few degrees of freedom works well, but if we have a robot walking down the street in Manhattan, there are just so many peripheral problems that can occur in such a complex environment. Somebody walks in front of the robot; something unexpected happens. If you took the best, smartest robot in the world and set it loose on any city street, within a few minutes it would cause a traffic accident. That’s why you don’t see robots on the street.

The other major problem is having a real conversation with an AI system where it truly understands what you’re saying and responds with understanding. I mentioned inference before, and in addition to deduction and induction, there’s a third type of inference called abduction that people generally aren’t aware of, but which we use all the time. Deduction is, “It’s raining; therefore, the streets are wet.” Abduction is, “I know rain makes streets wet. I see the streets are wet. Perhaps it’s raining.” You generate a hypothesis that explains an observation. It’s not certain knowledge – you could be wrong. Maybe a fire hydrant broke. However, you keep correcting your hypothesis with further observation. The streets are wet, my hypothesis is that it’s raining, and then I confirm it or form another. That’s abduction – hypothesis generation.

You mentioned novelty. A human who has not been in a certain situation before can think it through and still handle it. If you introduce a chess master to Shogi, Japanese chess, which has slightly different rules, they would very quickly be able to adapt their experience with chess to be able to play it well. A chess-playing AI, however, would have to learn from scratch – its inductive deep learning of chess would be useless.

I believe game-playing AIs still use some version of a min-max algorithm, deducing what would be the best move given that it has watched a million games play out before. This is very different from a human, who doesn’t play a million games and then computes the probability. I’m not a neuroscientist, so I couldn’t tell you what’s happening in the brain of chess masters – but I’m pretty sure they don’t mindlessly play a million games before becoming masters.

I've observed that as computers get better than us at something, like chess or trivia knowledge, we tend to move the goalpost and say that this has nothing to do with intelligence. Will we keep redefining intelligence as being whatever we can do that computers can't, or are there some markers of intelligence that we can't explain away?

My response is to go back to Alan Turing’s original 1950 paper, when he said that if a person can converse with a computer and be convinced that it is a real human, then it must be intelligent. I would say that this test still holds. Of course, you can converse with a chatbot that just continues to deflect questions, but to have a conversation that’s empathetic and understanding with the computer – we still can’t do that.

During the summer of 2022, a big news story surfaced of a Google engineer becoming convinced that a program he was developing had gained real sentience and warranted rights akin to human rights. Could we not say that it is passed the Turing test?

The latest language models are quite good, but you can trip them up very easily if you know how. Language has a property called compositionality, how sentences are put together to provide meaning. There’s a big difference between me riding a horse and a horse riding me, but an AI language model is not going to get that because it doesn’t have a sense of compositionality. Natural language is a barrier for artificial intelligence – one of the biggest. A legitimate test of language understanding would convince me that an AI was intelligent.

Another test would be navigation in dynamic environments by autonomous vehicles or robots. Getting to fully autonomous driving will be a lot harder than people think. The small city of Palo Alto, California, is mapped out on a grid, and you get pretty good performance from the vehicles there. But if you’re driving on a rural road and the AI must rely on sensor data, we’re a long way from vehicles being able to autonomously navigate that. Fully capable robotics in openended dynamic environments and fully understanding natural language; those are the two big frontiers.

Could an AI not develop its own language, very different from human language, that is uses to understand its environment and gets around some of the current limitations? We could compare it to communicating with dolphins, which seem to have a complex language that we haven't come close to understanding. They cannot understand our questions, and we cannot understand theirs; yet they are doubtless sentient beings.

I suppose is it possible for a creative AI to somehow achieve a way to frame the world that doesn’t require natural language. I don’t know the answer to that, but the immediate practical problem I see is how do we then interact with those systems? That might create some very, very strange human-machine interactions. I almost completely avoided the question of sentience in my book because, frankly, I don’t have a lot to say about it. It’s an issue that very quickly becomes philosophical. It could be that computers right now have some low level of sentience, like insects, and we just can’t detect it because we don’t know how. As an engineer, I don’t know the entry point into that argument, so I leave it alone.

You argue that we can’t achieve general AI the way we try to do it now, with machine learning and adding more components to computers. However, in physics there’s the phenomenon of ‘emergence’, where new traits develop when the complexity is high enough. One water molecule doesn’t have surface tension, but put enough together, and you get it. A single neuron isn’t sentient, but enough produce human sentience. Would it not be possible, if we add complexity and more components to supercomputers, that they could achieve intelligence and sentience as an emergent trait?

I think it’s an interesting question. It’s like a pile of sand: if you keep adding grains of sand, you get a nice conical shape, until at one point adding just one more grain of sand gets you a cascading effect. We have these thresholds in emergence where something isn’t happening, and then at some level of complexity, a completely different phenomenon emerges. I think it’s interesting whether that could apply to technology or computers, but I don’t have any strong scientific position on that.

Isn't there a danger if we have, say, self-driving cars who all think the same way because we have copied the same machine learning into all of them? If there are several routes from a suburb to the city, they will all choose the same route because that's what the system says they should do, whereas humans might imagine that the main route will probably be too busy and choose another one instead?

I think we’ll solve those sorts of problems. We already have systems where you can see traffic flow. The problems that I worry about are more practical. There have been cases where selfdriving cars don’t stop because a stop sign is slightly damaged and is perceived as something else. There’s a famous example of a system that tried to drive underneath a school bus because it thought it was an overpass. We just can’t eliminate all problems because the natural world is so messy. A bunch of leaves that the wind blows across the street might be interpreted as a solid object, and the AI will slam on the brakes.

We have people worrying that if we achieve general intelligence in computers, they are going to take over the world, or follow some order, like maximizing the production of paperclips, to such extremes that the AI will wipe out humanity to do it more efficiently. Do you think there is any real danger of such things happening, or are we just projecting our own faults onto artificial intelligence?

There’s an interesting contradiction in the paperclip scenario. The system is supposed to have general intelligence, which you would think included common sense, but on the other hand, it’s so narrow and computational that it thinks it can maximise the sale of paperclips by turning all humans into paperclips. Real computer intelligence would realise that it’s not intended to wipe us out. There’s another option, though, which is that it becomes malevolent and actively desires to rid the world of human beings. That gets us into the question of whether something like malevolence could possibly emerge in an AI.

We have AIs today that looks at x-rays of patients, trying to determine if they have cancer. They can be very good at this, but they don't know anything about cancer or what it means to a human being. They lack an understanding of what their task really is about. Do you think we can achieve intelligence in computers without true understanding of what they do?

That’s a great question, but I don’t have a great answer for it. It raises the whole issue, in this case of medical science, of whether an AI can provide proper diagnoses when it doesn’t understand care. Someone should write a PhD about how medicine is best administered and what the role of technology is and can be.

Research shows that even when an AI is better than any doctor at diagnosing cancer, it is even more efficient when it works with a human doctor. They approach the problem in different ways - one with a human understanding, the other from being trained on millions of x-rays. Human-AI partnerships seem to work best.

I think that’s right. In terms of something we care about, like medicine, it sounds like this kind of collaboration may work best. To me that’s a good use of technology. That’s why we make technology – because it furthers human goals. Whether we will have autonomous systems that will replace humans in all domains, that is a completely different question. Whether we get fully sentient AI or not, we’re heading in this direction in the future. That’s for sure.”


Typography, Automation, and the Division of Labor: A Brief History, J. Dakota Brown


Typography was born in the mass-production mechanism of the printing press. It has thus always been implicated in automation and, thereby, in the distinctly modern dynamics of overwork, underemployment, and runaway production. Transformations of labor and technology, however, have received scant attention in graphic design historiography. Philip Meggs' landmark textbook A History of Graphic Design, for example, offers only the briefest hints of the social dislocations that accompanied automation in the printing trades. One reads, for example, that the first steam press in England was operated in a secret location to guard against sabotage, or that vaguely-defined "strikes and violence" greeted the first installations of typesetting machines. Otherwise, such histories tend to treat innovations in print technology as a politically neutral process of technical refinement. But the new machines and methods did not just drop from the heavens: their development was often materially supported by employers who aimed to speed up production, capture control over the work process, and even break strikes.

Modernity, Modernism, and the Graphic Designer

As the design historian Adrian Forty has documented, industrial and graphic design emerged with the capitalist division of labor; both professions, in turn, catalyzed further divisions and fragmentations of work. In eighteenth-century crafts like ceramics and printed fabrics, the erosion of trade knowledge was accompanied by the rise of a new role in production: that of the "modeller. Usually hired from outside of the trade, these early designers were more dependably in touch with bourgeois taste than craftspeople were. In Josiah Wedgwood's ceramic works, stylistic concerns were conditioned by a need to simplify production into a rigid series of straightforward tasks, in which there was little occasion for variation between workers. The contemporaneous vogue for Neoclassicism, with its simplified geometry and restrained ornament, provided an ideal opportunity to streamline production - with the express goal, in Wedgwood's words, of making "such Machines of the Men as cannot err."

As critical historians from Karl Marx to Henry Braverman and David Noble have shown, the progress of capitalism's division of labor entails a gradual transfer of control and planning from the factory floor to management. But the resulting degradation and cheapening of work was noticed almost from the beginning: notably by Wedgwood's contemporary Adam Smith. In the opening chapter to The Wealth of Nations, Smith explains the production process in a new type of pin factory. Here, the capitalist has not simply gathered formerly-independent artisans to practice their trade side-by-side - instead, he has exploded the pin-making process into a line along which each laborer only cuts, sharpens, or polishes. Smith notes the miraculous extension of productivity in a process thus rationalized; elsewhere, however, he worries that the "great body of the people" will increasingly fill their days repeating the same handful of tasks.

The man whose whole life is spent performing a few simple operations, of which the effects too are, perhaps, always the same … has no occasion to exert his understanding, or to exercise his invention.... He naturally loses, therefore, the habit of such exertion, and generally becomes as stupid and ignorant as it is possible for a human creature to become.

This same image was on John Ruskin's mind in 1853, as he formulated what would become a central text for the Arts & Crafts movement's antiindustrial critique. In "The Nature of the Gothic," Ruskin mourns "the little piece of intelligence" rationed out to the factory worker, which "exhausts itself in making the point of a pin." For Ruskin, the division of labor is more accurately the division of the laborers themselves: these abundant pins, he writes, are polished with mere "crumbs" of human capacities. The essay was republished by William Morris' Kelmscott Press in 1892. Arriving at the end of a career rich in the paradoxes of an anti-capitalist design practice, Kelmscott was Morris' attempt to restore aesthetic unity to the book while keeping skilled craftspeople employed at higher-thanaverage wages. Though Morris believed that the form of the book had been betrayed by industrial shoddiness, the scale of his undertaking still necessitated some modernization of the traditional work process. Kelmscott books nonetheless remained so expensive to produce that Morris was trapped, as he lamented, "ministering to the swinish luxury of the rich."

In the early twentieth-century United States, Morris' legacy would be refashioned in a context of accelerating industrial transformation. Frank Lloyd Wright came to believe that Morris' desired reconciliation between art, labor, and leisure was likely to be delivered by mechanization itself. For Wright, the machine—centrally illustrated by the printing press—had to be grasped for what it had become: "intellect mastering the drudgery of the earth." The "meaningless torture" inflicted on workers and materials alike could now be swept away, Wright argued, as long as designers could part with anachronistic practices of ornamentation. Meanwhile, American commercial artists were falling "under the Arts & Crafts spell" and emerging as freelance specialists in book typography. Bruce Rogers and Frederic Goudy, for example, took on Morris' aesthetic standards while largely ignoring his concerns about the social contradictions of large-scale production. It was Goudy's student W.A. Dwiggins who would later popularize the phrase "graphic design" to describe this emerging position in print's division of labor. The workshops of these early graphic designers were characterized by a clarified managerial role for the designer, a more rationalized division of labor below and, finally, an embrace of labor-saving technology in typesetting and printing.

Across the ocean, meanwhile, a more explicitly socialist embrace of industry had produced the modernist "machine aesthetic." Echoing and radicalizing Wright, the Constructivist manifesto of 1922 declared war on traditional art and pledged a conditional allegiance to the machine: Constructivists would be both technology's "first fighting and punitive force" and its "last slave-workers."[17] At the same time, the Bauhaus was moving away from its Arts & Crafts roots. The school had initially been organized along guild lines: composed not of students and professors, but of masters, journeymen, and apprentices.[18] In transitioning to an emphasis on industrial production, Bauhaus designers synthesized compositional lessons from Futurism, Dada, and Constructivism. One uniting theme of these movements had been a desire to alter the experience of reading by exploding the strictures of the letterpress. In each case, photomechanical techniques promised a way out. In the Bauhaus graphics studios, László Moholy-Nagy continued in this vein, developing a a montage practice he termed "Typophoto." 

In the USSR, El Lissitzky theorized the epistemological and technical aspects of this blurring of text and image. The text-image, he surmised, could be put to work perfecting the human sensorium: revolutionized book forms would yield a "perpetual sharpening of the optic nerve." Lissitzky also read early attempts at phototypography in the context of a historical tendency toward lightness and mobility: "The amount of material used is decreasing, we are dematerializing, cumbersome masses of material are being supplanted by released energies."[20] Such a process would culminate, as he cryptically wrote in 1926, in a final transcendence of print itself: "the electro-library." After the midpoint of the century, metal type would indeed be supplanted by photographic media, in systems that were increasingly directed by "electro-libraries" of "dematerialized" data. By the 1990s, the convergences and displacements predicted by Lissitzky had yielded the digital hybridization of writing, typesetting, and imaging. In the end, however, these transformations owed more to the bottom lines of print capitalists than to the efforts of the radical modernists.

Modernism in design began with a vision of socialist industrial transformation -but by mid-century, it had become welded to the public image of the great capitalist conglomerates. Corporate boosters of modernism like the paperboard magnate Walter Paepke, a benefactor of the postwar "New Bauhaus" in Chicago, prophesied a world in which design would meld with management.[22] Paepke argued that design could improve market competition between large firms tied to uniform machinery and wage agreements; internally, it could even be put to work on such "problems" as worker morale. Though modernism's trajectory from utopian potential to capitalist instrumentalization is familiar to design history, parallel themes in the history of the printing trades have received less attention. The initial promise of technological innovations—to end, in Wright's words, the "meaningless torture" of repetitive and inefficient labor -soon gave way to the sobering realities of deskilling and displacement. The contours of twentieth-century print technologies would be shaped in large part by struggles over automation and employment. The "released energies" of print's dematerialization increasingly took the form of outmoded workers.

Industrial Rationalization and the Printer

The growing coherence and confidence of the graphic design profession is accompanied historically by the gradual fragmentation and decline of the printing trades.[24] The job description of "printing" originally encompassed a set of knowledges that extended far beyond the point of contact between ink and paper. Early printers were often also typefounders, publishers, and booksellers. Even as the craft became more specialized, printing still involved typesetting and composing pages, which often extended to a role in writing. According to union typesetter and historian Henry Rosemont, newspaper printers in the mid-nineteenth century relied on a broad but informal education in "language, history, geography and other subjects," which enabled them to produce entire articles from telegrams consisting of little more than the relevant nouns, verbs, and modifiers. 

Print workers thus held a strategic position in the circulation of public discourse, which was simply not possible without them. They often took advantage of this position to educate themselves and to advocate for the interests of their trade. In addition to their obligatory literacy, they had access to the press as an organizing tool-an extreme rarity for manufacturing workers of the early industrial era. Journeyman printers became the first group of workers to go on strike in the United States, just a year after the Revolutionary War. As Régis Debray has documented, print workers would go on to play prominent roles in revolutionary movements around the world during the next two centuries.

The printed book was, if not the first, then certainly the clearest early example of the world of standardized, mass-produced commodities to come. And as with any other commodity, print production required "labor-saving" innovations to keep competing firms profitable. More efficient inventions often rendered the work less taxing and dangerous, but the primary motivation for their adoption was always the reduction of labor costs—resulting in layoffs or slashed hours. Print workers often found themselves fighting technologies that promised to ease the burden of their labor. Rosemont offers the example of a more efficient ink roller, invented in 1814 and vigorously resisted by American printers. The existing standard was a more rudimentary instrument that required periodic soakings in animal urine to keep it from hardening. Printers worked in consistently squalid, poorly- ventilated shops that contributed to lower-than-average life expectancies—but such conditions were clearly preferable to unemployment.

The printed book was, if not the first, then certainly the clearest early example of the world of standardized, mass-produced commodities to come. And as with any other commodity, print production required "labor-saving" innovations to keep competing firms profitable. More efficient inventions often rendered the work less taxing and dangerous, but the primary motivation for their adoption was always the reduction of labor costs—resulting in layoffs or slashed hours. Print workers often found themselves fighting technologies that promised to ease the burden of their labor. Rosemont offers the example of a more efficient ink roller, invented in 1814 and vigorously resisted by American printers. The existing standard was a more rudimentary instrument that required periodic soakings in animal urine to keep it from hardening. Printers worked in consistently squalid, poorly- ventilated shops that contributed to lower-than-average life expectancies—but such conditions were clearly preferable to unemployment.

Though Johannes Gutenberg's fifteenth-century press had scarcely changed in the intervening years, the first decades of the nineteenth century brought transformations far beyond the humble roller. Iron construction and steam power fundamentally changed not only the shape of the machine, but the entire work process that fed and maintained it. However, a a major production bottleneck remained: the reproduction of writing still required the manual assembly of each word and line. Printing firms grew to be heavily reliant upon a workforce of typesetters who were both rigorously trained and militantly organized. During the 1880s, several attempts were made to mechanize the typesetting process. One particularly spectacular failure was the Paige Compositor, which bankrupted its primary investor Mark Twain. As Twain is said to have boasted shortly before the invention proved unfeasible, the Paige could "work like six men and do everything but drink, swear, and go out on strike."

Then in 1886 Ottmar Mergenthaler, a German engineer hired by an investors' group that represented the major New York newspapers, presented the first working model of the Linotype machine. Like type composition by the old method, the Linotype utilized thin bits of metal; here, however, each bit carried the negative impression of a character. Operators typed the characters into a line; justification was then carried out by a spacing mechanism that sealed the channel into which the characters had been set. Molten metal was then injected into the channel to form a full line in positive relief. After cooling, each "line o' type" was stacked into columns and locked into page layouts for the press. While the Linotype was an expensive and somewhat risky investment, it delivered on promises of labor-cost savings, and in time it contributed to a dramatic enlargement of the size and circulation of the periodical press.

The trade at first dismissed these developments. As one printer's newspaper reassuringly put it in 1891, Linotypes were mere "toys" for print capitalists with no practical background in the trade. Over the next decade, however, the threat became palpable. As one unemployed typesetter wrote in 1900, his place at the typecase had been usurped by a "monster" that eerily replicated his movements without need for food or human dignity. While a steady demand for the manual composition of headlines, advertisements, and other display applications muffled the effect slightly, the new work process soon touched off an employment crisis. Younger compositors scrambled to learn machine composition, while thousands of older or more narrowly-trained workers fell through the cracks. However, the International Typographical Union (ITU), which represented manual typesetters, was able to establish jurisdiction over the machines in strategic industrial centers around the turn of the century. In order to stave off the effects of the transformation, the ITU pushed for shorter workdays and encouraged early retirements. 

As the popular press grew during the teens and twenties, typesetting employment stabilized and even expanded. The ITU grew in tandem, and soon became one of the most powerful unions in the United States. However, this position was soon threatened by a number of mutually-reinforcing technical innovations. First, teletypesetting enabled Linotypes to be driven like player pianos; encoded tape was poised to replace human typists. Second, a slew of phototypesetting inventions sought to replace the cumbersome "hot metal" process with typefaces stored on film. Giving typography a photochemical basis, in turn, allowed a more seamless integration of text and image, while also making typesetting more readily compatible with letterpress printing's longtime competitor, offset lithography. 

Teletypesetting evolved from Morse code and the stock ticker; it would go on to form the basis for early computing.

Like the beginnings of mechanized typesetting itself, efforts at moving beyond hot metal were piloted by newspapers. Early experiments with "cold type" were explicitly undertaken to break a wave of ITU strikes following the passage of the Taft-Hartley Act of 1947, which stripped organized labor of many of the bargaining rights it had won over the preceding decades.[36] During a citywide pressroom strike that lasted from November 1947 to September 1949, the Chicago Tribune put its existing clerical staff to work on a new model of justifying typewriter whose output could be "pasted up" as camera-ready paper layouts, as opposed to being "locked up" in countless pieces of backward-reading metal.[37] The Tribune's infamous "Dewey Defeats Truman" edition of November 3, 1948 was typeset by strikebreakers, in a work process that now moved faster than the official ballot counts.38 While the quality of typewriter paste-up left something to be desired, these experiments strongly hinted at the possibility of producing a newspaper without the union. 

The ITU was able to keep these challenges at bay throughout the mid-twentieth century. New contracts forbade machines like the teletypesetter, even though this meant that print-ready stories from the wire services had to be retyped by an ITU member on the premises.[39] It wasn't until 1964 that the New York City local signed a contract allowing Linotypes to be run on "outside tape" on the condition, however, that employers paid 100% of the profits deriving from the new machinery into an "automation fund."[40] While this price was prohibitively steep for many firms, it opened the door to similar agreements on phototypesetting and, eventually, to computer systems. During the 1970s, the ITU began to draw down in exchange for the job and pension security of existing members.[41] In the meantime, the new machines had already crept into areas of the industry with low union representation. A paradoxical result was that capital-intensive metropolitan papers like the New York Times were among last to make the transition. The final night of Linotype composition at the Times—July 1, 1978—is memorialized in the documentary Farewell Etaoin Shrdlu, directed by ITU proofreader David Loeb Weiss. Among the film's interviewees is a compositor who reflects on his 26 years in the industry: 

[T]hat's six years apprenticeship, 20 years journeyman. And these are words that aren't just tossed around. ... All the knowledge I've acquired over these 26 years is all locked up in a little box now called a computer. And I think probably most jobs are gonna end up the same way.

Once more, the newspaper industry led the way in automation, and again the ITU attempted to train people in the new processes or encourage early retirements. In the earlier transformation, the work lost to Linotype composition was compensated by a gradual but decisive expansion of print production. This time, however, the further rationalization of typesetting destroyed older forms of work while narrowing the number of jobs in the new lines. As Lissitzky had predicted, metal gave way to film and paper; the material footprint of typography was shrinking. But as long as each text needed to be retyped to be typeset, labor-time savings were minimal. The widespread adoption of teletypesetting technology, however, allowed the storage and transmission of coded texts and, eventually, their formatting directions as well. By the 1970s, computer systems were beginning to dissolve typesetting into word processing. A centuries-old gap separating writing and printing was beginning to close—and this gap had been the very ground on which the ITU stood. The union suffered a long decline and finally dissolved in 1986, just as the personal computer was completing typography's process of dematerialization. It was, at that time, the longest continuously-running union in U.S. history.

The End of Modernism and the Last Typesetters

In the 1970s, print production involved a complex hierarchy of work processes, the final product of which was never fully visible until it had been printed. Designers could only approximate typographical treatments; directions on spacing, size, and weight were then handed off to phototypesetting shops to interpret in detail. A separate group of prepress specialists followed designers' directions on variables like color density and image placement, and then "stripped" together disparate negatives to create a print-ready master. But despite the many hands through which such work passed, much of the period's modernist-influenced design left the impression that it was the product a singular, detached mind. 

Though there was still a high degree of churn in new machines and processes, this division of labor held stable until the arrival of Apple's Macintosh computer in 1984. The personal computer centralized capacities formerly bound up in massive metal-founding operations, delicate apparatuses of type on film, or astronomically expensive, room-filling computers—to say nothing of the highly specialized workers that attended these machines, or of the systems of education and apprenticeship that such a workforce presupposed. Tasks that were once contracted out with some combination of strict direction and trust were now fully under the control of the individual designer—from the smallest details of letterforms to the organization of entire books. The Macintosh would soon offer image-editing capacities with no existing analogue, which in turn put pressure on commercial photographers and illustrators. The century since the invention of the Linotype had been one of "creative destruction" in the print industry: novel forms of work appeared suddenly and disruptively, only to be rendered obsolete in their turn. Once the brake provided by ITU contracts was removed, this process could accelerate unabated.

By the mid-1980s, typographical technology had reached a height of modernized seamlessness which, ironically, contributed to the decline of modernism's hegemony in graphic design. New design software facilitated effects like layering and distortion, which were quickly put to use in visual polemics against modernist clarity. Formal complexity and semantic confusion in graphic design had a long pre-Macintosh history —stretching at least as far back as the late-1960s letterpress experiments of Wolfgang Weingart. In the 1980s, however, graphic designers raised the stakes of these experiments by linking them to contemporaneous developments in the academy: in particular, to the "linguistic" and "cultural turns" in the humanities.[45] Terms like "deconstruction" and "post-structuralism" were applied to the printed page in ways that often required little familiarity with the theories in question. The grid—increasingly understood as a symbol of authoritarian and, perhaps, Eurocentric rationality—was parodied, skewed, or thrown aside entirely. Designers arranged texts into ambiguous formations, and designed new typefaces that intentionally thwarted legibility.

By the 1990s, the postmodernist critique of modern rationality and power had grown more rigorous. However, the movement's theorists showed little interest in grasping capitalism as a determining context for their theory and practice; transformations in the political economy of print were thus largely ignored. When, in 1997, Emigre published a rare acknowledgment that entire industries were collapsing next door, it was with a heavy dose of schadenfreude 

[M]any of the printers who have gone out of business over the last quarter century deserved their fate. The grassroots of the printing trade is, after all, notoriously conservative, protectionist, and sexist.

While prepress and printing-like most American trades-tended toward a narrowly white male membership and self-image, the heaviest losses in the industry from the 1980s forward were in fact suffered by the largely non-unionized workforce of the cold type shops. Compared to the membership of the ITU, these workers were disproportionately women and people of color.

The postmodernists' focus on cultural intervention often neglected the material contingencies of the practice. Semiotic theory and cultural studies opened vistas to broad contexts of symbolic circulation, but often at the cost of such bare facts as design's own relationship to waged work. It is perhaps not surprising, then, that a new generation of practitioners has taken a more archaeological approach to the labor of design. In the recent documentaries Linotype: The Film (2012) and Graphic Means: A History of Graphic Design Production (2016)-both directed by practicing graphic designers. histories of print production expose deeper issues of deskilling, unemployment, and deindustrialization. These documentaries elegantly organize a complex history of print technology, and the present essay would admittedly have been impossible without them. However, both ultimately elide the capitalist labor dynamics that would explain their own narratives. 

Douglas Wilson's Linotype stirringly evokes the lost world of hot motel through humanizing portraits of the workers who kept it running. For example, in a near-reprise of his role as the narrator of Farewell Etaoin Shrdlu, the late Carl Schlesinger makes frequent appearances. The filmmakers include footage of him singing and tap dancing, and they indulge him as he tells a long-winded story about the time he met Marilyn Monroe. A casual viewer would never know that Schlesinger was also a lifelong member of the ITU, or that he coauthored an important book on the union's automation strategy. Despite its exhaustiveness, in fact, Linotype manages to bracket the union's existence altogether. Briar Levit's Graphic Means takes up where Linotype leaves off-impressively condensing the jumble of machines that bridged the hot type and digital eras. Graphic Means directly addresses the role of the ITU and, further, the gendered division that arose between unionized hot type shops and "open" cold type shops. However, the decline of the union is presented as a technical inevitability and even as a refutation of male privilege; the phototypesetting bosses interviewed seem to be speaking as feminists when they say that "the girls" did equally admirable work for half the wages. The vulnerability of non-unionized women to the next wave of automation, meanwhile, is never addressed. 

While typesetting has disappeared as a distinct job, it would be too simple to say that it was automated out of existence. Rather, since the late twentieth century the job description of the graphic designer has expanded to include tasks once carried out by the earliest printers. Now that we have considered the standpoints of both modernist radicals and extinct print workers, the contemporary situation of the graphic designer should appear somewhat absurd. Capitalist technological development has rendered texts and images almost infinitely reproducible—and has built unfathomable electro-libraries in the process. But despite this gigantic aggregation of productive force, it is still necessary to put people to work moving words and pictures around, most often in the service of brand competition among otherwise identical commodities. What confronts us is not a world in which machines have freed people from work, but one of mass unemployment, in which some of the most celebrated "innovations" are apps that facilitate short-term, low-wage, benefit-less contracts.

If graphic designers became typesetters, they may turn out to be the last typesetters. The design software that repackaged the knowledge and skill of the printing trades seemed at first to deliver a dreamed-of autonomy to graphic design as a profession. But because these technologies were off-the-shelf consumer products, trained and credentialed designers have less and less of a monopoly on the medium. A general facility with image and text has bled into general literacy—due in no small part to the ease of pirating such "immaterial" commodities as Photoshop. In the contemporary design press, articles on apps like TaskRabbit and Fiverr, or a future role for Al in the automation of design decisions, recall the mix of anxiety and reassurance that characterized coverage of the Linotype nearly 130 years ago.[53] These projected "disruptions" may well turn out to be empty hype. But whatever is in store for graphic design in the coming decades, it will be impossible to understand without accounting for the capitalist constraints and imperatives that have shaped the practice from the beginning.


The Case Against Reality by Amanda Gefter and Quanta Magazine


A professor of cognitive science argues that the world is nothing like the one we experience through ur senses.

As we go about our daily lives, we tend to assume that our perceptions— sights, sounds, textures, tastes—are an accurate portrayal of the real world. Sure, when we stop and think about it—or when we find ourselves fooled by a perceptual illusion—we realize with a jolt that what we perceive is never the world directly, but rather our brain’s best guess at what that world is like, a kind of internal simulation of an external reality. Still, we bank on the fact that our simulation is a reasonably decent one. If it wasn’t, wouldn’t evolution have weeded us out by now? The true reality might be forever beyond our reach, but surely our senses give us at least an inkling of what it’s really like.

Not so, says Donald D. Hoffman, a professor of cognitive science at the University of California, Irvine. Hoffman has spent the past three decades studying perception, artificial intelligence, evolutionary game theory and the brain, and his conclusion is a dramatic one: The world presented to us by our perceptions is nothing like reality. What’s more, he says, we have evolution itself to thank for this magnificent illusion, as it maximizes evolutionary fitness by driving truth to extinction.

Getting at questions about the nature of reality, and disentangling the observer from the observed, is an endeavor that straddles the boundaries of neuroscience and fundamental physics. On one side you’ll find researchers scratching their chins raw trying to understand how a three-pound lump of gray matter obeying nothing more than the ordinary laws of physics can give rise to first-person conscious experience. This is the aptly named “hard problem.”

On the other side are quantum physicists, marveling at the strange fact that quantum systems don’t seem to be definite objects localized in space until we come along to observe them. Experiment after experiment has shown— defying common sense—that if we assume that the particles that make up ordinary objects have an objective, observer-independent existence, we get the wrong answers. The central lesson of quantum physics is clear: There are no public objects sitting out there in some preexisting space. As the physicist John Wheeler put it, “Useful as it is under ordinary circumstances to say that the world exists ‘out there’ independent of us, that view can no longer be upheld.”

So while neuroscientists struggle to understand how there can be such a thing as a first-person reality, quantum physicists have to grapple with the mystery of how there can be anything but a first-person reality. In short, all roads lead back to the observer. And that’s where you can find Hoffman—straddling the boundaries, attempting a mathematical model of the observer, trying to get at the reality behind the illusion. Quanta Magazine caught up with him to find out more.

Gefter: People often use Darwinian evolution as an argument that our perceptions accurately reflect reality. They say, “Obviously we must be latching onto reality in some way because otherwise we would have been wiped out a long time ago. If I think I’m seeing a palm tree but it’s really a tiger, I’m in trouble.”

Hoffman: Right. The classic argument is that those of our ancestors who saw more accurately had a competitive advantage over those who saw less accurately and thus were more likely to pass on their genes that coded for those more accurate perceptions, so after thousands of generations we can be quite confident that we’re the offspring of those who saw accurately, and so we see accurately. That sounds very plausible. But I think it is utterly false. It misunderstands the fundamental fact about evolution, which is that it’s about fitness functions—mathematical functions that describe how well a given strategy achieves the goals of survival and reproduction. The mathematical physicist Chetan Prakash proved a theorem that I devised that says: According to evolution by natural selection, an organism that sees reality as it is will never be more fit than an organism of equal complexity that sees none of reality but is just tuned to fitness. Never.

Gefter: You’ve done computer simulations to show this. Can you give an example?

Hoffman: Suppose in reality there’s a resource, like water, and you can quantify how much of it there is in an objective order—very little water, medium amount of water, a lot of water. Now suppose your fitness function is linear, so a little water gives you a little fitness, medium water gives you medium fitness, and lots of water gives you lots of fitness—in that case, the organism that sees the truth about the water in the world can win, but only because the fitness function happens to align with the true structure in reality. Generically, in the real world, that will never be the case. Something much more natural is a bell curve—say, too little water you die of thirst, but too much water you drown, and only somewhere in between is good for survival. Now the fitness function doesn’t match the structure in the real world. And that’s enough to send truth to extinction. For example, an organism tuned to fitness might see small and large quantities of some resource as, say, red, to indicate low fitness, whereas they might see intermediate quantities as green, to indicate high fitness. Its perceptions will be tuned to fitness, but not to truth. It won’t see any distinction between small and large—it only sees red— even though such a distinction exists in reality.

Gefter: But how can seeing a false reality be beneficial to an organism’s survival?

Hoffman: There’s a metaphor that’s only been available to us in the past 30 or 40 years, and that’s the desktop interface. Suppose there’s a blue rectangular icon on the lower right corner of your computer’s desktop — does that mean that the file itself is blue and rectangular and lives in the lower right corner of your computer? Of course not. But those are the only things that can be asserted about anything on the desktop — it has color, position, and shape. Those are the only categories available to you, and yet none of them are true about the file itself or anything in the computer. They couldn’t possibly be true. That’s an interesting thing. You could not form a true description of the innards of the computer if your entire view of reality was confined to the desktop. And yet the desktop is useful. That blue rectangular icon guides my behavior, and it hides a complex reality that I don’t need to know. That’s the key idea. Evolution has shaped us with perceptions that allow us to survive. They guide adaptive behaviors. But part of that involves hiding from us the stuff we don’t need to know. And that’s pretty much all of reality, whatever reality might be. If you had to spend all that time figuring it out, the tiger would eat you.

Gefter: So everything we see is one big illusion?

Hoffman: We’ve been shaped to have perceptions that keep us alive, so we have to take them seriously. If I see something that I think of as a snake, I don’t pick it up. If I see a train, I don’t step in front of it. I’ve evolved these symbols to keep me alive, so I have to take them seriously. But it’s a logical flaw to think that if we have to take it seriously, we also have to take it literally.

Gefter: If snakes aren’t snakes and trains aren’t trains, what are they?

Hoffman: Snakes and trains, like the particles of physics, have no objective, observer-independent features. The snake I see is a description created by my sensory system to inform me of the fitness consequences of my actions. Evolution shapes acceptable solutions, not optimal ones. A snake is an acceptable solution to the problem of telling me how to act in a situation. My snakes and trains are my mental representations; your snakes and trains are your mental representations.

Gefter: How did you first become interested in these ideas?

Hoffman: As a teenager, I was very interested in the question “Are we machines?” My reading of the science suggested that we are. But my dad was a minister, and at church they were saying we’re not. So I decided I needed to figure it out for myself. It’s sort of an important personal question—if I’m a machine, I would like to find that out! And if I’m not, I’d like to know, what is that special magic beyond the machine? So eventually in the 1980s I went to the artificial-intelligence lab at MIT and worked on machine perception. The field of vision research was enjoying a newfound success in developing mathematical models for specific visual abilities. I noticed that they seemed to share a common mathematical structure, so I thought it might be possible to write down a formal structure for observation that encompassed all of them, perhaps all possible modes of observation. I was inspired in part by Alan Turing. When he invented the Turing machine, he was trying to come up with a notion of computation, and instead of putting bells and whistles on it, he said, Let’s get the simplest, most pared down mathematical description that could possibly work. And that simple formalism is the foundation for the science of computation. So I wondered, could I provide a similarly simple formal foundation for the science of observation?

Gefter: A mathematical model of consciousness.

Hoffman: That’s right. My intuition was, there are conscious experiences. I have pains, tastes, smells, all my sensory experiences, moods, emotions and so forth. So I’m just going to say: One part of this consciousness structure is a set of all possible experiences. When I’m having an experience, based on that experience I may want to change what I’m doing. So I need to have a collection of possible actions I can take and a decision strategy that, given my experiences, allows me to change how I’m acting. That’s the basic idea of the whole thing. I have a space X of experiences, a space G of actions, and an algorithm D that lets me choose a new action given my experiences. Then I posited a W for a world, which is also a probability space. Somehow the world affects my perceptions, so there’s a perception map P from the world to my experiences, and when I act, I change the world, so there’s a map A from the space of actions to the world. That’s the entire structure. Six elements. The claim is: this is the structure of consciousness. I put that out there so people have something to shoot at.

Gefter: But if there’s a W, are you saying there is an external world?

Hoffman: Here’s the striking thing about that. I can pull the W out of the model and stick a conscious agent in its place and get a circuit of conscious agents. In fact, you can have whole networks of arbitrary complexity. And that’s the world.

Gefter: The world is just other conscious agents?

Hoffman: I call it conscious realism: Objective reality is just conscious agents, just points of view. Interestingly, I can take two conscious agents and have them interact, and the mathematical structure of that interaction also satisfies the definition of a conscious agent. This mathematics is telling me something. I can take two minds, and they can generate a new, unified single mind. Here’s a concrete example. We have two hemispheres in our brain. But when you do a split-brain operation, a complete transection of the corpus callosum, you get clear evidence of two separate consciousnesses. Before that slicing happened, it seemed there was a single unified consciousness. So it’s not implausible that there is a single conscious agent. And yet it’s also the case that there are two conscious agents there, and you can see that when they’re split. I didn’t expect that, the mathematics forced me to recognize this. It suggests that I can take separate observers, put them together and create new observers, and keep doing this ad infinitum. It’s conscious agents all the way down.

Gefter: If it’s conscious agents all the way down, all first-person points of view, what happens to science? Science has always been a third-person description of the world.

Hoffman: The idea that what we’re doing is measuring publicly accessible objects, the idea that objectivity results from the fact that you and I can measure the same object in the exact same situation and get the same results — it’s very clear from quantum mechanics that that idea has to go. Physics tells us that there are no public physical objects. So what’s going on? Here’s how I think about it. I can talk to you about my headache and believe that I am communicating effectively with you, because you’ve had your own headaches. The same thing is true as apples and the moon and the sun and the universe. Just like you have your own headache, you have your own moon. But I assume it’s relevantly similar to mine. That’s an assumption that could be false, but that’s the source of my communication, and that’s the best we can do in terms of public physical objects and objective science.

Gefter: It doesn’t seem like many people in neuroscience or philosophy of mind are thinking about fundamental physics. Do you think that’s been a stumbling block for those trying to understand consciousness?

Hoffman: I think it has been. Not only are they ignoring the progress in fundamental physics, they are often explicit about it. They’ll say openly that quantum physics is not relevant to the aspects of brain function that are causally involved in consciousness. They are certain that it’s got to be classical properties of neural activity, which exist independent of any observers— spiking rates, connection strengths at synapses, perhaps dynamical properties as well. These are all very classical notions under Newtonian physics, where time is absolute and objects exist absolutely. And then [neuroscientists] are mystified as to why they don’t make progress. They don’t avail themselves of the incredible insights and breakthroughs that physics has made. Those insights are out there for us to use, and yet my field says, “We’ll stick with Newton, thank you. We’ll stay 300 years behind in our physics.”

Gefter: I suspect they’re reacting to things like Roger Penrose and Stuart Hameroff’s model, where you still have a physical brain, it’s still sitting in space, but supposedly it’s performing some quantum feat. In contrast, you’re saying, “Look, quantum mechanics is telling us that we have to question the very notions of ‘physical things’ sitting in ‘space.’”

Hoffman: I think that’s absolutely true. The neuroscientists are saying, “We don’t need to invoke those kind of quantum processes, we don’t need quantum wave functions collapsing inside neurons, we can just use classical physics to describe processes in the brain.” I’m emphasizing the larger lesson of quantum mechanics: Neurons, brains, space … these are just symbols we use, they’re not real. It’s not that there’s a classical brain that does some quantum magic. It’s that there’s no brain! Quantum mechanics says that classical objects—including brains—don’t exist. So this is a far more radical claim about the nature of reality and does not involve the brain pulling off some tricky quantum computation. So even Penrose hasn’t taken it far enough. But most of us, you know, we’re born realists. We’re born physicalists. This is a really, really hard one to let go of.

Gefter: To return to the question you started with as a teenager, are we machines?

Hoffman: The formal theory of conscious agents I’ve been developing is computationally universal—in that sense, it’s a machine theory. And it’s because the theory is computationally universal that I can get all of cognitive science and neural networks back out of it. Nevertheless, for now I don’t think we are machines—in part because I distinguish between the mathematical representation and the thing being represented. As a conscious realist, I am postulating conscious experiences as ontological primitives, the most basic ingredients of the world. I’m claiming that experiences are the real coin of the realm. The experiences of everyday life—my real feeling of a headache, my real taste of chocolate—that really is the ultimate nature of reality.


Pause Giant AI Experiments: An Open Letter


AI systems with human-competitive intelligence can pose profound risks to society and humanity, as shown by extensive research and acknowledged by top AI labs. As stated in the widely-endorsed Asilomar AI Principles, Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources. Unfortunately, this level of planning and management is not happening, even though recent months have seen AI labs locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one – not even their creators – can understand, predict, or reliably control.

Contemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable. This confidence must be well justified and increase with the magnitude of a system's potential effects. OpenAI's recent statement regarding artificial general intelligence, states that "At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models." We agree. That point is now.

Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT@4. This pause should be public and verifiable, and include all key actors. If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.

AI labs and independent experts should use this pause to jointly develop and implement a set of shared safety protocols for advanced AI design and development that are rigorously audited and overseen by independent outside experts. These protocols should ensure that systems adhering to them are safe beyond a reasonable doubt. This does not mean a pause on AI development in general, merely a stepping back from the dangerous race to ever-larger unpredictable black-box models with emergent capabilities.

AI research and development should be refocused on making today's powerful, state-of-the-art systems more accurate, safe, interpretable, transparent, robust, aligned, trustworthy, and loyal.

In parallel, AI developers must work with policymakers to dramatically accelerate development of robust AI governance systems. These should at a minimum include: new and capable regulatory authorities dedicated to AI; oversight and tracking of highly capable AI systems and large pools of computational capability; provenance and watermarking systems to help distinguish real from synthetic and to track model leaks; a robust auditing and certification ecosystem; liability for AI-caused harm; robust public funding for technical AI safety research; and well-resourced institutions for coping with the dramatic economic and political disruptions (especially to democracy) that AI will cause.

Humanity can enjoy a flourishing future with AI. Having succeeded in creating powerful AI systems, we can now enjoy an "AI summer" in which we reap the rewards, engineer these systems for the clear benefit of all, and give society a chance to adapt. Society has hit pause on other technologies with potentially catastrophic effects on society. We can do so here. Let's enjoy a long AI summer, not rush unprepared into a fall.


What’s the Future for A.I.? Where we’re heading tomorrow, next year and beyond by Cade Metz


In today’s A.I. newsletter, the last in our five-part series, I look at where artificial intelligence may be headed in the years to come.

In early March, I visited OpenAI’s San Francisco offices for an early look at GPT-4, a new version of the technology that underpins its ChatGPT chatbot. The most eye-popping moment arrived when Greg Brockman, OpenAI’s president and co-founder, showed off a feature that is still unavailable to the public: He gave the bot a photograph from the Hubble Space Telescope and asked it to describe the image “in painstaking detail.”

The description was completely accurate, right down to the strange white line created by a satellite streaking across the heavens. This is one look at the future of chatbots and other A.I. technologies: A new wave of multimodal systems will juggle images, sounds and videos as well as text.

Yesterday, my colleague Kevin Roose told you about what A.I. can do now. I’m going to focus on the opportunities and upheavals to come as it gains abilities and skills.

AI. In the near term

Generative A.I.s can already answer questions, write poetry, generate computer code and carry on conversations. As “chatbot” suggests, they are first being rolled out in conversational formats like ChatGPT and Bing.

But that’s not going to last long. Microsoft and Google have already announced plans to incorporate these A.I. technologies into their products. You’ll be able to use them to write a rough draft of an email, automatically summarize a meeting and pull off many other cool tricks.

OpenAI also offers an A.P.I., or application programming interface, that other tech companies can use to plug GPT-4 into their apps and products. And it has created a series of plug-ins from companies like Instacart, Expedia and Wolfram Alpha that expand ChatGPT’s abilities.

A.I. in the medium term

Many experts believe A.I. will make some workers, including doctors, lawyers and computer programmers, more productive than ever. They also believe some workers will be replaced.

“This will affect tasks that are more repetitive, more formulaic, more generic,” said Zachary Lipton, a professor at Carnegie Mellon who specializes in artificial intelligence and its impact on society. “This can liberate some people who are not good at repetitive tasks. At the same time, there is a threat to people who specialize in the repetitive part.”

Human-performed jobs could disappear from audio-to-text transcription and translation. In the legal field, GPT-4 is already proficient enough to ace the bar exam, and the accounting firm PricewaterhouseCoopers plans to roll out an OpenAI-powered legal chatbot to its staff.

At the same time, companies like OpenAI, Google and Meta are building systems that let you instantly generate images and videos simply by describing what you want to see.

Other companies are building bots that can actually use websites and software applications as a human does. In the next stage of the technology, A.I. systems could shop online for your Christmas presents, hire people to do small jobs around the house and track your monthly expenses.

All that is a lot to think about. But the biggest issue may be this: Before we have a chance to grasp how these systems will affect the world, they will get even more powerful.

A.I. in the long term

For companies like OpenAI and DeepMind, a lab that’s owned by Google’s parent company, the plan is to push this technology as far as it will go. They hope to eventually build what researchers call artificial general intelligence, or A.G.I. — a machine that can do anything the human brain can do.

As Sam Altman, OpenAI’s chief executive, told me three years ago: “My goal is to build broadly beneficial A.G.I. I also understand this sounds ridiculous.” Today, it sounds less ridiculous. But it is still easier said than done.

For an A.I. to become an A.G.I., it will require an understanding of the physical world writ large. And it is not clear whether systems can learn to mimic the length and breadth of human reasoning and common sense using the methods that have produced technologies like GPT-4. New breakthroughs will probably be necessary.

The question is, do we really want artificial intelligence to become that powerful? A very important related question: Is there any way to stop it from happening?

The risks of A.I.

Many A.I. executives believe the technologies they are creating will improve our lives. But some have been warning for decades about a darker scenario, where our creations don’t always do what we want them to do, or they follow our instructions in unpredictable ways, with potentially dire consequences.

A.I. experts talk about “alignment” — that is, making sure A.I. systems are in line with human values and goals.

Before GPT-4 was released, OpenAI handed it over to an outside group to imagine and test dangerous uses of the chatbot.

The group found that the system was able to hire a human online to defeat a Captcha test. When the human asked if it was “a robot,” the system, unprompted by the testers, lied and said it was a person with a visual impairment.

Testers also showed that the system could be coaxed into suggesting how to buy illegal firearms online and into describing ways to make dangerous substances from household items. After changes by OpenAI, the system no longer does these things.

But it’s impossible to eliminate all potential misuses. As a system like this learns from data, it develops skills that its creators never expected. It is hard to know how things might go wrong after millions of people start using it.

“Every time we make a new A.I. system, we are unable to fully characterize all its capabilities and all of its safety problems — and this problem is getting worse over time rather than better,” said Jack Clark, a founder and the head of policy of Anthropic, a San Francisco start-up building this same kind of technology.

And OpenAI and giants like Google are hardly the only ones exploring this technology. The basic methods used to build these systems are widely understood, and other companies, countries, research labs and bad actors may be less careful.

The remedies for A.I. 

Ultimately, keeping a lid on dangerous A.I. technology will require far-reaching oversight. But experts are not optimistic.

“We need a regulatory system that is international,” said Aviv Ovadya, a researcher at the Berkman Klein Center for Internet & Society at Harvard who helped test GPT-4 before its release. “But I do not see our existing government institutions being about to navigate this at the rate that is necessary.”

As we told you earlier this week, more than 1,000 technology leaders and researchers, including Elon Musk, have urged artificial intelligence labs to pause development of the most advanced systems, warning in an open letter that A.I. tools present “profound risks to society and humanity.”

A.I. developers are “locked in an out-of-control race to develop and deploy ever more powerful digital minds that no one — not even their creators — can understand, predict or reliably control,” according to the letter.

Some experts are mostly concerned about near-term dangers, including the spread of disinformation and the risk that people would rely on these systems for inaccurate or harmful medical and emotional advice.

But other critics are part of a vast and influential online community called rationalists or effective altruists, who believe that A.I could eventually destroy humanity. This mind-set is reflected in the letter.


An early guide to policymaking on generative AI: How lawmakers are thinking about the risks of the latest tech revolution by Tate Ryan-Mosley.


Earlier this week, I was chatting with a policy professor in Washington, DC, who told me that students and colleagues alike are asking about GPT-4 and generative AI: What should they be reading? How much attention should they be paying?

She wanted to know if I had any suggestions, and asked what I thought all the new advances meant for lawmakers. I’ve spent a few days thinking, reading, and chatting with the experts about this, and my answer morphed into this newsletter. So here goes!

Though GPT-4 is the standard bearer, it’s just one of many high-profile generative AI releases in the past few months: Google, Nvidia, Adobe, and Baidu have all announced their own projects. In short, generative AI is the thing that everyone is talking about. And though the tech is not new, its policy implications are months if not years from being understood.

GPT-4, released by OpenAI last week, is a multimodal large language model that uses deep learning to predict words in a sentence. It generates remarkably fluent text, and it can respond to images as well as word-based prompts. For paying customers, GPT-4 will now power ChatGPT, which has already been incorporated into commercial applications.

The newest iteration has made a major splash, and Bill Gates called it “revolutionary” in a letter this week. However, OpenAI has also been criticized for a lack of transparency about how the model was trained and evaluated for bias.

Despite all the excitement, generative AI comes with significant risks. The models are trained on the toxic repository that is the internet, which means they often produce racist and sexist output. They also regularly make things up and state them with convincing confidence. That could be a nightmare from a misinformation standpoint and could make scams more persuasive and prolific.

Generative AI tools are also potential threats to people’s security and privacy, and they have little regard for copyright laws. Companies using generative AI that has stolen the work of others are already being sued.

Alex Engler, a fellow in governance studies at the Brookings Institution, has considered how policymakers should be thinking about this and sees two main types of risks: harms from malicious use and harms from commercial use. Malicious uses of the technology, like disinformation, automated hate speech, and scamming, “have a lot in common with content moderation,” Engler said in an email to me, “and the best way to tackle these risks is likely platform governance.” (If you want to learn more about this, I’d recommend listening to this week’s Sunday Show from Tech Policy Press, where Justin Hendrix, an editor and a lecturer on tech, media, and democracy, talks with a panel of experts about whether generative AI systems should be regulated similarly to search and recommendation algorithms. Hint: Section 230.)

Policy discussions about generative AI have so far focused on that second category: risks from commercial use of the technology, like coding or advertising. So far, the US government has taken small but notable actions, primarily through the Federal Trade Commission (FTC). The FTC issued a warning statement to companies last month urging them not to make claims about technical capabilities that they can’t substantiate, such as overstating what AI can do. This week, on its business blog, it used even stronger language about risks companies should consider when using generative AI.

“If you develop or offer a synthetic media or generative AI product, consider at the design stage and thereafter the reasonably foreseeable—and often obvious—ways it could be misused for fraud or cause other harm. Then ask yourself whether such risks are high enough that you shouldn’t offer the product at all,” the blog post reads.

The US Copyright Office also launched a new initiative intended to deal with the thorny policy questions around AI, attribution, and intellectual property.

The EU, meanwhile, is sticking true to its reputation as the world leader in tech policy. At the start of this year my colleague Melissa Heikkilä wrote about the EU’s efforts to try to pass the AI Act. It’s a set of rules that would prevent companies from releasing models into the wild without disclosing their inner workings, which is precisely what some critics are accusing OpenAI of with the GPT-4 release.

The EU intends to separate high-risk uses of AI, like hiring, legal, or financial applications, from lower-risk uses like video games and spam filters, and require more transparency around the more sensitive uses. OpenAI has acknowledged some of the concerns about the speed of adoption. In fact, its own CEO, Sam Altman, told ABC News he shares many of the same fears. However, the company is still not disclosing key data about GPT-4.

For policy folks in Washington, Brussels, London, and offices everywhere else in the world, it’s important to understand that generative AI is here to stay. Yes, there’s significant hype, but the recent advances in AI are as real and important as the risks that they pose.

Yesterday, the United States Congress called Shou Zi Chew, the CEO of TikTok, to a hearing about privacy and security concerns raised by the popular social media app. His appearance came after the Biden administration threatened a national ban if its parent company, ByteDance, didn’t sell off the majority of its shares.

There were lots of headlines, most using a temporal pun, and the hearing laid bare the depths of the new technological cold war between the US and China. For many watching, the hearing was both important and disappointing, with some legislators displaying poor technical understanding and hypocrisy about how Chinese companies handle privacy when American companies collect and trade data in much the same ways.

It also revealed how deeply American lawmakers distrust Chinese tech. Here are some of the spicier takes and helpful articles to get up to speed:

Key takeaways from TikTok hearing in Congress – and the uncertain road ahead - Kari Paul and Johana Bhuiyan, The Guardian
What to Know About the TikTok Security Concerns - Billy Perrigo, Time
America’s online privacy problems are much bigger than TikTok - Will Oremus, Washington Post
There’s a Problem With Banning TikTok. It’s Called the First
Amendment - Jameel Jaffer (Executive Director of the Knight First Amendment Institute), NYT Opinion

AI is able to persuade people to change their minds about hot-button political issues like an assault weapon ban and paid parental leave, according to a study by a team at Stanford’s Polarization and Social Change Lab. The researchers compared people’s political opinions on a topic before and after reading an AI-generated argument, and found that these arguments can be as effective as human-written ones in persuading the readers: “AI ranked consistently as more factual and logical, less angry, and less reliant upon storytelling as a persuasive technique.”

The teams point to concerns about the use of generative AI in a political context, such as in lobbying or online discourse. (For more on the use of generative AI in politics, do please read this recent piece by Nathan Sanders and Bruce Schneier.)


A celebration of connectionism by Geoffrey North 


New developments in neural network theory have excited both psychologists and neurobiologists. Practitioners of the new art displayed their wares last week.

WHEN David Rumelhart, Geoffrey Hinton and Ronald Williams described for neural networks a powerful new learning procedure called back-propagation (Nature 323, 533; 1986), they noted that theirs was not a plausible model of how brains learn. Yet the generality of their approach, and the several intriguing features of network learning by backpropagation which have come to light, have stimulated a resurgence of interest in neural network models among neuroscientists, theoreticians and experimentalists alike. Last week, at a meeting organized by the Society of Experimental Psychology, a packed audience heard Geoffrey Hinton describe a new learning algorithm that seems a better model of biological learning than is back-propagation by parallel networks which nevertheless seems to retain much of the power of its predecessor.

In parallel distributed processing, a network can be thought of as embodying a mathematical function mapping vectors in 'input space' to vectors in 'output space', much as matrices effect linear transformations between vector spaces. A vector in the neural context is simply the pattern of excitation of some set of units taken to be the input or the output of the network. The processing is thus distributed in the pattern of the connections between units of the network and their strengths. Corresponding to the real physiological task of, say, pattern recognition, will be some kind of network function mapping inputs (patterns) onto outputs (interpretations). The all-important question is what kind of network is needed for a particular task.

Simple networks developed in the 1960s, known as perceptrons, involved only two layers of units, an input and an output, with direct connections between them. Such networks are very limited in the range of tasks they can carry out. The versatility of a network can be greatly increased by the introduction of intermediate layers of 'hidden' units, but this raises the problem of how it can be trained.

Back-propagation provides an elegant way of training a multi-layered network. During learning, the output vectors generated by the network for a given input are compared with the desired output, giving an error calculated from the difference between the two. Back-propagation calculates the dependence of this error on all the connection weights, simply by using the chain-rule for differentiation, and the weights are adjusted to reduce the error, so that the network converges by gradient descent on the required structure.

During learning, the network comes to capture certain general features which are characteristic of its task. The hidden units, in particular, develop features that seem especially significant to neuroscientists who record the properties of single neurons in brains. For example, in some cases they are reminiscent of the way in which some neurons in the brain are found to be specific for different aspects of the representation of the visual field.

Even so, this system of learning by back-propagation has not seemed very biologically realistic. Hinton (CarnegieMellon University) and his colleagues have been looking for a more plausible system oflearning.

The new development is known as a 'recirculation' algorithm, and works as follows. In a network learning by backpropagation, there is a linear flow of activity (via the hidden intermediate units) from the input units to the output units.In the new system, the hidden units connect back to the single layer of 'visible' input units. Activity thus recirculates through the network; during training, the connection weights are adjusted to minimize the rate of change of activity in each unit. Thus, when trained, the network is set up so as to stabilize on certain states, and so can work as a kind of 'content addressable memory' with the property that degraded or incomplete forms of the training inputs can regenerate the correct version.

It has been shown that, under certain conditions, the new algorithm is equivalent to gradient descent, and it has been found empirically that the system still works when these conditions are relaxed.

A number of interesting applications of back-propagation were reported at the meeting. Hinton described a network for recognizing one-dimensional shapes on a one-dimensional retina independently of position: the hidden units learn to respond to shapes in different positions. Hinton also described a speech-recognition network which learns to recognize spoken consonants given very noisy corrupted data. It appears to perform almost as well as people, and better than the previous best system of automated speech recognition.

Several speakers described analogies between the behaviour of their networks during training or after 'damage' and what is known of human learning and cognitive disorders. For example, J. L. McClelland (Carnegie-Mellon University) described how a network for learning a balancebeam task progressed through stages of competence similar to those of children given the same task. The problem is to decide which way a balance-beam will tip, depending on the position and size of weights on either side of the fulcrum. With an appropriately biased learning environment, such as children might well experience, the network, like children, initially bases its decisions purely on weight information; gradually the network learns to use the position of the weights. M.S. Seidenberg (McGill University) described a network for word recognition and pronunciation that, when 'damaged' by the removal of hidden units, displayed behaviour reminiscent of some human disorders, such as dyslexia.

Parallel distributed processing is not without its critics, and S. Pinker (MIT) reported that a linguistic analysis of Rumelhart and McClelland's network for changing the tense of verbs in sentences shows that the system is not 'descriptively adequate' as a model for human language, in that it abandons certain symbolic rules and principles that linguistic studies suggest are crucial in human language.

David Willshaw (Edinburgh) asked whether parallel distributed processing networks might, like perceptrons, similarly cease to make significant progress and fade in interest after a period of development and excitement. McClelland's riposte was that work on perceptrons was severely limited by the available computer power and circumstances are now sufficiently different to justify optimism.

The biological relevance of parallel distributed processing remains an open question. Independently of relevance, however, work on network systems may be of interest at a purely theoretical level. The present work is a kind of experimental mathematics, and in that respect is rather similar to that of Mandlebrot on fractals, also made possible and inspired by computers. The hope is that, in future, deductive proofs will give a more rigorous basis to work on networks.

Many interesting problems remain. On what set of functions will a given network topology converge? How can the optimal network for a given task be predicted, and how long will training take? And so on.


