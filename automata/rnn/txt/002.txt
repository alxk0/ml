OUTPUT 023							AUTOMATA - GPT-2						 EPOCHS : 25000



” and “post” models of social media. These models are meant to help users filter, sort, and find sources of information, and can be customized to include other topics as needed. Many online systems that provide seemingly human-like forms of output also have some of the limitations. The model for example of Facebook’s ability to post content across multiple platforms, and there’s no mechanism for incorporating new information or feedback from users or communities. The model for task distribution was too circumscribed-and too broad-for a variety of reasons. For example, it focused primarily on medical diagnosis, which was a separate activity from creativity, and took a more archaeological approach to the interdependence of interests.

The three mills also illustrate how powerful search processes areiced by data and algorithmic decision-making processes. The decision-making processes of stock exchanges are at the heart of how companies make money, and in the years that followed, the industry began to rethink its processes and its values.

And in the years after the 2008 financial crisis, when it became harder to convince investors to buy into the model that the financial crisis was over and that ordinary people would be better off avoiding the high-risk, high-rewarded products and services of the future. On the other hand, when Chinese entrepreneurs were finally able to reimagine their businesses around the kinds of products and services that ordinary people demand, they took all the hard questions that stuck in their head. They learned how to survive in the single most competitive startup environment in the world.

 staff researchers at Sinovation Ventures, a San Francisco start-up building this same kind of business model, use GPT-4 to build a user interface that is leapfrogged to the internet, and it is alreadyimatingly massive. The company already includes Facebook in its group-buying company, and it already collects data fromwww.linkedin.com/year.

When you add all that data together, it can give a company a better understanding of its customers and potentially increase overall sales.

There are many reasons to prefer SOLAR’s products and services. First, the comprehensive nature of BQ Zowi, its ability toringless design and relentless delivery, and its ability toringless user base makes it a must-have product in any company paintball. Second, the fact that Zowi makes its employees use the software means that it is able to focus on more satisfying work, and leads to more satisfying jobs. As Zowi puts it,

The Zowi brand is built on a tightrope to innovate, a bend to a slightly bend, and a mundanity to be flexible and flexible. Our innovation philosophy is built around exponential increases in productivity: better products lead to more users, and those users become more productive.

From the fact that almost no one actively wants more, BQ Zowi makes a series of machine-learning technologies that it believes will lead to more exponential increases in the number of people using the service. The ultimate goal: increasing user satisfaction and using it to accelerate the creation of more user-friendly apps.

The potential of ZestFinance is profound.iva, a company that creates customer, business, and social programs with AI. According to the company’s Web site,

We have asked that customers use the service of natural-language processing to infer personal attributes from online reviews. . . . We believe that by introducing natural-language processing technology, natural-language processing technology, and related techniques, ZestFinance could become a significant leader in AI applications.

While this kind of business model is increasingly being used by e-commerce companies and is being used by a wide variety of middle- and low-income workers, the reverse is also true: as more middle-class jobs were automated, demand for these workers grew more rigid. For example, aaglier people may not count as members of a particular profession, and their incomes were less commensurate. Inarends also dramatic: as the range and density of data increased, so did the likelihood of them being cracked open by chipmakers.

Not only does the algorithmic make-up of text blur the lines in a user’s mind separating words, but there also has been a growth of knacks and lumps of meaning that can be included in a text nonetheless. For example, as certain kinds of interpersonal data are processed (e.g., spelling, synonym recognition, social network analysis, and so on), their spelling is often wrong. So what is going on here? What is going on here?

While it appears there is at least a global scope for using AI in the client-office world, that scope is beginning to be eroding. The key risks of overestimating AI capabilities, however, is the loss of the universality of the superpowers. In particular, an AI system may face the risk of becoming
====================
The relationship between intelligence and motivation in a human being is complex. Some people experience a great degree of identification with our intellectual capabilities and sensibility for information and knowledge. They value our good capacity for creative and conceptual leaps and boundaries; they value oureness and closeness; and they value oureness and closeness. Yet there is a common underlying impulse to define a final goal in terms of physical symbols: if you have a light mounted on a moon-rock formation, you have a motivation system of your own, like our own consciousness.

To understand why this impulse is driving us, we need to consider the history of artificial intelligence. The early achievements of machine intelligence have been about rewriting the history of neural networks, constructing intelligent machines that could occur spontaneously, or inventing safe, motivation systems. None of these new milestones brought with them the kind of general understanding that we are seeking in the age of AI. But they also reveal something about the dependence of fate on some key parameter: fate.

The destiny of artificial intelligence

When we discussed the prospect of using AI in a different sense, we did not say that it was a bad thing. We just thought that it made more sense to put it into a machine that could learn to perform some tasks more empathetically.

It is important to understand that the value of human life does not consist of getting one person to do a good job. That person might work extremely hard to earn a good income. There are also other human beings in all this, including CEOs, medical doctors, engineers, and artists. If we attached consequences to the human being, even if they were created as hard as possible to create, then this whole scenario of transformed machines is wiped out.

In a sense, this would be aostics. The machine is capable of giving accurate and correct answers to questions, in many situations, in a matter of hours. The main part of the explanatory process in question answering is to bring up the relevant literature, and to read the relevant books and subscribe to the general theories discussed in that book.

This year’s AI Index paints a picture of where we are so far with AI, in order to highlight what might await us in the future.

Jack Clark and Ray Perrault

Top Ten Takeaways

1 Industry races ahead of academia. Until 2014, most significant machine learning models were released by academia. Since then, industry has taken over. In 2022, there were 32 significant industry-produced machine learning models compared to just three produced by academia. Building state-of-the-art AI systems increasingly requires large amounts of data, computer power, and money—resources that industry actors inherently possess in greater amounts compared to nonprofits and academia.

2 Performance saturation on traditional benchmarks. AI continued to post state-of-the-art results, but year-over-year improvement on many benchmarks continues to be marginal. Moreover, the speed at which benchmark saturation is being reached is increasing. However, new, more comprehensive benchmarking suites such as BIG-bench and HELM are being released.

3 AI is both helping and harming the environment. New research suggests that AI systems can have serious environmental impacts. According to Luccioni et al., 2022, BLOOM’s training run emitted 25 times more carbon than a single air traveler on a one-way trip from New York to San Francisco. Still, new reinforcement learning models like BCOOLER show that AI systems can be used to optimize energy usage.

4 The world’s best new scientist … AI?
AI models are starting to rapidly accelerate scientific progress and in 2022 were used to aid hydrogen fusion, improve the efficiency of matrix manipulation, and generate new antibodies.

5 The number of incidents concerning the misuse of AI is rapidly rising.
According to the AIAAIC database, which tracks incidents related to the ethical misuse of AI, the number of AI incidents and controversies has increased 26 times since 2012. Some notable incidents in 2022 included a deepfake video of Ukrainian President Volodymyr Zelenskyy surrendering and U.S. prisons using call-monitoring technology on their inmates. This growth is evidence of both greater use of AI technologies and awareness of misuse possibilities.

6 The demand for AI-related professional skills is increasing across
virtually every American industrial sector. Across every sector in the United States for which there is data (with the exception of agriculture, forestry, fishing, and hunting), the number of AI- related job postings has increased on average from 1.7% in 2021 to 1.9% in 2022. Employers in the United States are increasingly looking for workers with AI-related skills.

7 For the first time in the last decade, year-over-year private investment in AI decreased. Global AI private investment was $91.9 billion in 2022, which represented a 26.7% decrease since 2021. The total number of AI-related funding events as
====================
The year 2017 marked the first time I heard Donald Trump speak fluent Chinese. During the U.S. president’s first trip to China, he showed up on a big screen to welcome attendees at a major tech conference. He began his speech in English and then abruptly switched languages.

“AI is changing the world,” he said, speaking in flawless Chinese but with typical Trump bluster. “And iFlyTek is really fantastic.”

President Trump cannot, of course, speak Chinese. But AI is indeed changing the world, and Chinese companies like iFlyTek are leading the way. By training its algorithms on large data samples of President Trump’s speeches, iFlyTek created a near-perfect digital model of his voice: intonation, pitch, and pattern of speech. It then recalibrated that vocal model for Mandarin Chinese, showing the world what Donald Trump might sound like if he grew up in a village outside Beijing. The movement of lips wasn’t precisely synced to the Chinese words, but it was close enough to fool a casual viewer at first glance. President Obama got the same treatment from iFlyTek: a video of a real press conference but with his professorial style converted to perfect Mandarin.

“With the help of iFlyTek, I’ve learned Chinese,” Obama intoned to the White House press corps. “I think my Chinese is better than Trump’s. What do all of you think?”

iFlyTek might say the same to its own competitors. The Chinese company has racked up victories at a series of prestigious international AI competitions for speech recognition, speech synthesis, image recognition, and machine translation. Even in the company’s “second language” of English, iFlyTek often beats teams from Google, DeepMind, Facebook, and IBM Watson in natural-language processing—that is, the ability of AI to decipher overall meaning rather than just words.

This success didn’t come overnight. Back in 1999, when I started Microsoft Research Asia, my top-choice recruit was a brilliant young Ph.D. named Liu Qingfeng. He had been one of the students I saw filing out of the dorms to study under streetlights after my lecture in Hefei. Liu was both hardworking and creative in tackling research questions; he was one of China’s most promising young researchers. But when we asked him to accept our scholarship offer and become a Microsoft intern and then an employee, he declined. He wanted to start his own AI speech company. I told him that he was a great young researcher but that China lagged too far behind American speech-recognition giants like Nuance, and there were fewer customers in China for this technology. To his credit, Liu ignored that advice and poured himself into building iFlyTek. Nearly twenty years and dozens of AI competition awards later, iFlyTek has far surpassed Nuance in capabilities and market cap, becoming the most valuable AI speech company in the world.

Combining iFlyTek’s cutting-edge capabilities in speech recognition, translation, and synthesis will yield transformative AI products, including simultaneous translation earpieces that instantly convert your words and voice into any language. It’s the kind of product that will soon revolutionize international travel, business, and culture, and unlock vast new stores of time, productivity, and creativity in the process.

THE WAVES

But it won’t happen all at once. The complete AI revolution will take a little time and will ultimately wash over us in a series of four waves: internet AI, business AI, perception AI, and autonomous AI. Each of these waves harnesses AI’s power in a different way, disrupting different sectors and weaving artificial intelligence deeper into the fabric of our daily lives.

The first two waves—internet AI and business AI—are already all around us, reshaping our digital and financial worlds in ways we can barely register. They are tightening internet companies’ grip on our attention, replacing paralegals with algorithms, trading stocks, and diagnosing illnesses.

Perception AI is now digitizing our physical world, learning to recognize our faces, understand our requests, and “see” the world around us. This wave promises to revolutionize how we experience and interact with our world, blurring the lines between the digital and physical worlds. Autonomous AI will come last but will have the deepest "impact on our lives. As self-driving cars take to the streets, autonomous drones take to the skies, and intelligent robots take over factories, they will transform everything from organic farming to highway driving and fast food.
These four waves all feed off different kinds of data, and each one presents a unique opportunity for the United States or China to seize the lead. We’ll see a U.
====================
What you need to know about deep learning and its role in lies-with you. During the 2016 Summer Olympics in Rio de Janeiro, artificial intelligence researchers were teaching Chinese students that it was important to know what they were doing, and then building new products and services from the ground up. What does it really mean to “see” and “see “the world?” These are fundamental questions worthy of thought. But who is asking these questions? How are “us, n, or n sometimes defined in the world,” going about their daily lives? What are some individuals who are either unable to see or ears ears? How are synapse strengths adjusted to learn these representations? To gain insight into these difficult theoretical issues, it has proved necessary to study grossly ideal- ized models that are as different from real biological neural networks as apples are from planets.

The 1980s also saw major progress in the processing of images. In a classic 1982 paper1, Hopfield showed that asynchronous networks with symmetrically connected neurons would settle to locally stable states, known as “point attractors”, which could be viewed as content-addressable memories. Although these networks were both computable and robust, Hopfield’s work15 led to a new type of recurrent network, which could be used as a whole-recursive system. Both the computable time series and the robustness of the recurrent network were made explicit, along with some other considerations that might limit the utility of the computation.

Of course, these are just a few examples of the many different kinds of artificial neural networks believed to be relevant today. But the relevant thing about neural networks is that they are often multidimensional: an eight-dimensional area of a neural network gives it theordinate to which it is applied. For example, a Person watching a video on YouTube may give the network topology of perceptual organization, while a person walking down the street might give the network contextual information. It is not surprising that some neural networks have perceptual hierarchies (like those found in space-time structures) that are causally linked to activity in other spatial environments.

The fact that the topology of perceptual organization is organized around activity in the perceptual realm has consequence for other kinds of intelligences too. For example, meat-packing companies have been increasing their footprint across the world by buying up and using more trucks and equipped with sensors that allow them to process materials in greater detail. As companies have adapted their production processes for autonomous vehicles, they will need to rethink their people, and they have to reimagine their business processes around entirely new capabilities from the missing middle.

By understanding the real-world tasks that people are doing,ining natural language processing, and training their networks to handle the increasingly common verbs of a sentence, we can better see how artificial intelligence is not just helping people to solve problems but improving their own processes.

5: OVERLY INTELLIGENT AI

Fear and Greed: Instrumental Goals

If a machine pursuing an incorrect objective sounds bad enough, there’s worse. The solution suggested by Alan Turing—turning off the power at strategic moments—may not be available, for a very simple reason: you can't fetch the coffee if you’re dead.

Let me explain. Suppose a machine has the objective of fetching the coffee. If it is sufficiently intelligent, it will certainly understand that it will fail in its objective if it is switched off before completing its mission. Thus, the objective of fetching coffee creates, as a necessary subgoal, the objective of disabling the off-switch. The same is true for curing cancer or calculating the digits of pi. There's really not a lot you can do once you're dead, so we can expect Al systems to act preemptively to preserve their own existence, given more or less any definite objective.

If that objective is in conflict with human preferences, then we have exactly the plot of 2001: A Space Odyssey, in which the HAL 9000 computer kills four of the five astronauts on board the ship to prevent interference with its mission. Dave, the last remaining astronaut, manages to switch HAL off after an epic battle of wits— presumably to keep the plot interesting. But if HAL had been truly superintelligent, Dave would have been switched off.

It is important to understand that self-preservation doesn’t have to be any sort of built-in instinct or prime directive in machines. (So Isaac Asimov’s Third Law of Robotics,8 which begins “A robot must protect its own existence,” is completely unnecessary.) There is no need to build self-preservation in because it is an instrumental goal—a goal that is a useful subgoal of almost any original objective.9 Any entity that has a definite objective will automatically act as if it also has instrumental goals.

In addition to being alive, having access to money is an instrumental goal within
====================
The automation discourse has been enthusiastically adopted by the jeans-wearing elite of Silicon Valley. Bill Gates is advocating for a tax on robots. Mark Zuckerberg told Harvard undergraduate inductees that they should ‘explore ideas like universal basic income’, a policy Elon Musk also thinks will become increasingly ‘necessary’ over time, as robots outcompete humans across a growing range of jobs. Musk has been naming his SpaceX drone vessels after spaceships from Iain M. Banks’s Culture Series, a set of ambiguously utopian science-fiction novels depicting a post-scarcity world in which human beings live fulfilling lives alongside intelligent robots, called ‘minds’, without the need for markets or states.

Politicians and their advisors have equally identified with the automation discourse, which has become one of the leading perspectives on our ‘digital future’. In his farewell presidential address, Obama suggested that the ‘next wave of economic dislocations’ will come not from overseas trade, but rather from ‘the relentless pace of automation that makes a lot of good, middle-class jobs obsolete.’ Robert Reich, former Labour Secretary under Bill Clinton, expressed similar fears: we will soon reach a point ‘where technology is displacing so many jobs, not just menial jobs but also professional jobs, that we’re going to have to take seriously the notion of a universal basic income.’ Clinton’s former Treasury Secretary, Lawrence Summers, made the same admission: once-‘stupid’ ideas about technological unemployment now seem increasingly smart, he said, as workers’ wages stagnate and economic inequality rises. The discourse has become the basis of a long-shot presidential campaign for 2020: Andrew Yang, Obama’s former ‘Ambassador of Global Entrepreneurship’, has penned his own tome on automation, The War on Normal People, and is now running a futuristic campaign on a ‘Humanity First’, ubi platform. Among Yang’s vocal supporters is Andy Stern, former head of the seiu, whose Raising the Floor is yet another example of the discourse.10

Yang and Stern—like all of the other writers named so far—take pains to assure readers that some variant of capitalism is here to stay, even if it must jettison its labour markets; however, they admit to the influence of figures on the far left who offer a more radical version of the automation discourse. In Inventing the Future, Nick Srnicek and Alex Williams argue that the ‘most recent wave of automation is poised’ to transform the labour market ‘drastically, as it comes to encompass every aspect of the economy’.11 They claim that only a socialist government would actually be able to fulfil the promise of full automation by creating a post-work or post-scarcity society. In Four Futures, Peter Frase thoughtfully explores the alternative outcomes for such a post-scarcity society, depending on whether it still had private property and still suffered from resource scarcity, which could persist even if labour scarcity were overcome. Like the liberal proponents of the automation discourse, these left-wing writers stress that, even if the coming of advanced robotics is inevitable, ‘there is no necessary progression into a post-work world’. Srnicek, Williams and Frase are all proponents of ubi, but in a left-wing variant. For them, ubi serves as a bridge to ‘fully automated luxury communism’, a term originally coined in 2014 by Aaron Bastani to name a possible goal of socialist politics, and which flourished for five years as a meme on the internet before his book—outlining an automated future in which artificial intelligence, solar power, gene-editing, asteroid mining and lab-grown meat generate a world of limitless leisure and self-invention—finally appeared.

Recurrent fears

These futurist visions, from all points of the political spectrum, depend upon a common prediction of the trajectory of technological change. Have they got this right? To answer this question, it is helpful to have a couple of working definitions. Automation may be distinguished as a specific form of labour-saving technical innovation: automation technologies fully substitute for human labour, rather than merely augmenting human-productive capacities. With labour-augmenting technologies, a given job category will continue to exist, but each worker in that category will be more productive. For example, adding new machines to an assembly-line producing cars may make line workers more productive without abolishing line work as such. However, fewer workers will be needed in total to produce any given number of automobiles. Whether that results in fewer jobs will then depend on how much output—the total number of cars—also increases.

By contrast, automation may be defined as what Kurt Vonnegut describes in Player Piano: it takes place whenever an entire ‘job classification has been
====================
The current deep learning revolution is not unique to China. It is widespread, affecting every corner of the economy. Chinese startups are now scrapping for a slice of an AI landscape increasingly dominated by a handful of major players: the so-called Seven Giants of the AI age, which include Amazon, Microsoft, Google, Baidu, Alibaba, and Tencent. These corporate juggernauts are almost evenly split between the United States and China, and they’re making bold plays to dominate the AI economy. They’re using billions of dollars in cash and dizzying stockpiles of data to gobble up available AI talent. They’re also working to construct the “power grids” for the AI age: privately controlled computing networks that distribute machine learning across the economy, with the corporate giants acting as “utilities.” It’s a worrisome phenomenon for those who value an open AI ecosystem and also poses a potential stumbling block to China’s rise as an AI superpower.

But bringing AI’s power to bear on the broader economy can’t be done by private companies alone—it requires an accommodating policy environment and can be accelerated by direct government support. As you recall, soon after Ke Jie’s loss to AlphaGo, the Chinese central government released a sweeping blueprint for Chinese leadership in AI. Like the “mass innovation and mass entrepreneurship” campaign, China’s AI plan is turbocharging growth through a flood of new funding, including subsidies for AI startups and generous government contracts to accelerate adoption.

The plan has also shifted incentives for policy innovation around AI. Ambitious mayors across China are scrambling to turn their cities into showcases for new AI applications. They’re plotting driverless trucking routes, installing facial recognition systems on public transportation, and hooking traffic grids into “city brains” that optimize flows.

Behind these efforts lies a core difference in American and Chinese political culture: while America’s combative political system aggressively punishes missteps or waste in funding technological upgrades, China’s techno- utilitarian approach rewards proactive investment and adoption. Neither system can claim objective moral superiority, and the United States’ long track record of both personal freedom and technological achievement is unparalleled in the modern era. But I believe that in the age of AI implementation the Chinese approach will have the impact of accelerating deployment, generating more data, and planting the seeds of further growth. It’s a self- perpetuating cycle, one that runs on a peculiar alchemy of digital data, entrepreneurial grit, hard-earned expertise, and political will. To see where the two AI superpowers stand, we must first understand the source of that expertise.

NOBEL WINNERS AND NO-NAME TINKERERS

When Enrico Fermi stepped onto the deck of the RMS Franconia II in 1938, he changed the global balance of power. Fermi had just received the Nobel Prize in physics in Stockholm, but instead of returning home to Benito Mussolini’s Italy, Fermi and his family sailed for New York. They made the journey to escape Italy’s racial laws, which barred Jews or Africans from holding many jobs or marrying Italians. Fermi’s wife, Laura, was Jewish, and he decided to move the family halfway across the world rather than live under the antisemitism that was sweeping Europe.

It was a personal decision with earthshaking consequences. After arriving in the United States, Fermi learned of the discovery of nuclear fission by scientists in Nazi Germany and quickly set to work exploring the phenomenon. He created the world’s first self-sustaining nuclear reaction underneath a set of bleachers at the University of Chicago and played an indispensable role in the Manhattan Project. This top-secret project was the largest industrial undertaking the world had ever seen, and it culminated in the development of the world’s first nuclear weapons for the U.S. military. Those bombs put an end to World War II in the Pacific and laid the groundwork for the nuclear world order.

Fermi and the Manhattan Project embodied an age of discovery that rewarded quality over quantity in expertise. In nuclear physics, the 1930s and 1940s were an age of fundamental breakthroughs, and when it came to making "those breakthroughs, one Enrico Fermi was worth thousands of less brilliant physicists. American leadership in this era was built in large part on attracting geniuses like Fermi: men and women who could singlehandedly tip the scales of scientific power.

But not every technological revolution follows this pattern. Often, once a fundamental breakthrough has been achieved, the center of gravity quickly shifts from a handful of elite researchers to an army of tinkerers—engineers with just enough expertise to apply the technology to different problems. This is particularly true when the payoff of a breakthrough is diffused throughout society rather
====================
The War of a Thousand Groupons crystallized this phenomenon. Soon after its launch in 2008, Groupon became the darling of the American startup world. The premise was simple: offer coupons that worked only if a sufficient number of buyers used them. The buyers got a discount and the sellers got guaranteed bulk sales. It was a hit in post-"financial-crisis America, and Groupon’s valuation skyrocketed to over $1 billion in just sixteen months, the fastest pace in history.
The concept seemed tailor-made for China, where shoppers obsess over discounts and bargaining is an art form. Entrepreneurs in China looking for the next promising market quickly piled into group buying, starting local platforms based on Groupon’s “Deal of the Day” model. Major internet portals launched their own group-buying divisions, and dozens of new startups entered the fray. Yet what began as dozens soon ballooned into hundreds and then thousands of copycat competitors. By the time of Groupon’s initial public offering in 2011—the largest IPO since Google’s in 2004—China was home to over five thousand different group-buying companies.

To outsiders this looked like a joke. It was a caricature of an internet ecosystem that was shameless in its copying and devoid of any original ideas. And vast swaths of those five thousand copycats were laughable, the product of ambitious but clueless entrepreneurs with no prospects for surviving the ensuing bloodletting.

But at the bottom of that dogpile, at the center of this royal rumble, was Wang Xing. In the previous seven years, he had copied three American technology products, built two companies, and sharpened the skills needed to survive in the coliseum. Wang had turned from a geeky engineer who cloned American websites into a serial entrepreneur with a keen sense for technology products, business models, and gladiatorial competition.

He put all those skills to work during the War of a Thousand Groupons. He founded Meituan (“Beautiful Group”) in early 2010 and brought on battle-hardened veterans of his previous Facebook and Twitter clones to lead the charge. He didn’t repeat the pixel-for-pixel copying of his Facebook and Twitter sites, instead building a user interface that better matched Chinese users’ preference for densely packed interfaces.

When Meituan launched, the battle was just heating up, with competitors blowing through hundreds of millions of dollars in offline advertising. The going logic went that in order to stand out from the herd, a company had to raise lots of money and spend it to win over customers through advertising and subsidies. That high market share could then be used to raise more money and repeat the cycle. With overeager investors funding thousands of near-identical companies, Chinese urbanites took advantage of the absurd discounts to eat out in droves. It was as if China’s venture-capital community were treating the entire country to dinner.

But Wang was aware of the dangers of burning cash—that’s how he’d lost Xiaonei, his Facebook copy—and he foresaw the danger of trying to buy long-term customer loyalty with short-term bargains. If you only competed on subsidies, customers would endlessly jump from platform to platform in search of the best deal. Let the competitors spend the money on subsidizing meals and educating the market—he would reap the harvest that they sowed. So Wang focused on keeping costs down while iterating his product. Meituan eschewed all offline advertising, instead pouring resources into tweaking products, bringing down the cost of user acquisition and retention, and optimizing a complex back end. That back end included processing payments coming in from millions of customers and going out to tens of thousands of sellers. It was a daunting engineering challenge for which Wang’s decade of hands-on experience had prepared him.

One of Meituan’s core differentiations was its relationship with sellers, a crucial piece of the equation often overlooked by startups obsessed with market share. Meituan pioneered an automated payment mechanism that got money into the hands of businesses quicker, a welcome change at a time when group-buying startups were dying by the day, sticking restaurants with unpaid bills. Stability inspired loyalty, and Meituan leveraged it to build out larger networks of exclusive partnerships.
Groupon officially entered the Chinese market in early 2011 by forging a joint venture with Tencent. The marriage brought together the top international group-buying company with a homegrown giant that had both local expertise and a massive social media footprint. But the Groupon-Tencent partnership floundered from the beginning. Tencent had not yet figured out how to partner effectively with e-commerce companies, and the joint venture blindly applied Groupon’s standard playbook for international expansion: hire dozens of management consultants and use the temp agency Manpower to build out massive, low-level sales teams. Manpower headhun
====================
In the long run, as an aid to understanding human behavior, we have no problem solvingolving with computers. If you build one, you get a hundred thousand chances of having withstood a thousand years of trying to solve the problem of telling right from wrong.

As for proving that machines are intelligent, we have to do some research. And in a few short years, I think we will have software that will radically improve our understanding of the Internet.

Newell and Simon’s “General Problem Solver” (GPS) was designed and built in the mid- to late 1940s. (The name was changed to simply “Challenge Solver.”) was an early example of what was to become a standard AI system. It was to help people who were frustrated by their inability to find work on the control problem. The goal was to eventually build and release a “minimum viable product” version of the Internet.

 Although the late 1940s and early 1950s were rough, the widespread adoption of steam power and lightbulbs at the end of the nineteenth century provided the rubric for the modern computer. The invention of the steam engine and the new computer technology helped people use them more in relation to one another. I’ll come later to write that it was the “ defining characteristic of the twentieth-century technology systems that anyone who grew up in a village used to shovel was the first person to do so, the technological completion of his or her education.”

A college education was seen as the key to escaping generations of behind the curtain. But it was only a matter of time before the curve would rise. In the meantime, the mass-production mechanism of mass-production had been implemented in many different ways. One of the first steps in that process was the development of the continuous-process production process. The use of this process to produce goods was explicitly undertaken by the industrial historian E. P. Thompson in 1930. Thompson’svolume described the many activities of the industrial worker and showed how skilled he could be if he were operated from a large steam-driven engine. He also detailed how labor historians had futilely tried to make known what the purpose of mass production was. However, his attempts to build a detailed system of the process involved only the various steps of the manufacturing process, which were, in fact, merely the digital or physical manufacturing process. The historian E. P. Thompson went on to say, “There was a great deal of deception about what really happened on the shop floor.”

The continuous-process organization was dominated by the industrial worker E. P. Thompson. Hisvolume of nine chaptersmanagers introduced forty-three new types of workers to the use of manufacturing process.itionally, these had been accomplished by skilled craftspeople who integrated information and control systems into the factory management. But information and control had been breaking downural demands for employees and new roles within these workplaces. Taylor himself had already shown that the sentient body could be Industries ideal for augmentation. In his Principles of Scientific Management, he wrote one of the most important contributions to the field of management:

The new form of work, the new division of learning, demands of its members a new kind of routine, a new sort of routine that does not easily get in the way of creating a work force that can exercise a real and important role in the organization. The new job requirements are not likely to be as drastic as they were in the past. Indeed, as organizations such as Cedar Bluff develop such a work force, they will be forced to adapt their organization's methods in light of the new possibilities engendered by the new forms of work organization.

The problem appears to be that not every job in even the most informated organization is likely to require a coordinated and systematic effort to effectively accomplish the purposes of the organization. It won’t be easy, but I believe it will be necessary to line up all the tasks and make them run smoothly.

The man who emerged as the chief symbol of the rational approach to management was Frederick Taylor. Though much has been written on Taylor and the philosophy and methods of scientific management, it is worth highlighting a few brief collections of related writing.

The scientific managementsymbolization of scientific management has been a popular strategy of late. It seems to be able to recognize the organizationalkr20ance of modernistleanism, and can provide strong support for the theory that the era of scientific management has failed to materialize. As wingmanand romantic poet laureateines like poet laureate Ursula Franklin (who also happens to be a member of the surveyed nation) used to persuadeeenths to abandon the bourgeois emphasis on self-organizing combinations, now the group dynamics associated with a focus on brute force are no longer so evident. Even if the new techniques do not fully utilize the innovations, they can still be useful. However, the variety is too great not to include a mention in
====================
” the term to mean something, for it is often possible to predict exactly what items are classified and when they are not. But this approach fails in the same way as the quick-probe assumption. The AI, if reasonable, does not know what classifications are being used, and if so it can assign those categories in a way that fixes the right label.

In the next chapter, we will look at different paths that may lead to humanlevel machine intelligence. But let us note at the outset that however many stops there are between here and human-level machine intelligence, the latter is not the final destination. The next stop, just a short distance farther along the tracks, is superhuman-level machine intelligence. The train might not pause or even decelerate at Humanville Station. It is likely to swoosh right by.

The mathematician I. J. Good, who had served as chief statistician in Alan Turing’s code-breaking team in World War II, might have been the first to enunciate the essential aspects of this scenario. In an oft-quoted passage from 1965, he wrote:

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.

It may seem obvious now that major existential risks would be associated with such an intelligence explosion, and that the prospect should therefore be examined with the utmost seriousness even if it were known (which it is not) to have but a moderately small probability of coming to pass. The pioneers of artificial intelligence, however, notwithstanding their belief in the imminence of human-level AI, mostly did not contemplate the possibility of greater-thanhuman AI. It is as though their speculation muscle had so exhausted itself in conceiving the radical possibility of machines reaching human intelligence that it could not grasp the corollary—that machines would subsequently become superintelligent.

The AI pioneers for the most part did not countenance the possibility that their enterprise might involve risk.11 They gave no lip service—let alone serious thought—to any safety concern or ethical qualm related to the creation of artificial minds and potential computer overlords: a lacuna that astonishes even against the background of the era’s not-so-impressive standards of critical technology assessment.12 We must hope that by the time the enterprise eventually does become feasible, we will have gained not only the technological proficiency to set off an intelligence explosion but also the higher level of mastery that may be necessary to make the detonation survivable.

But before we turn to what lies ahead, it will be useful to take a quick glance at the history of machine intelligence to date.

Seasons of hope and despair

In the summer of 1956 at Dartmouth College, ten scientists sharing an interest in neural nets, automata theory, and the study of intelligence convened for a sixweek workshop. This Dartmouth Summer Project is often regarded as the cockcrow of artificial intelligence as a field of research. Many of the participants would later be recognized as founding figures. The optimistic outlook among the delegates is reflected in the proposal submitted to the Rockefeller Foundation, which provided funding for the event:

We propose that a 2 month, 10 man study of artificial intelligence be carried out…. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines that use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.

In the six decades since this brash beginning, the field of artificial intelligence has been through periods of hype and high expectations alternating with periods of setback and disappointment.

The first period of excitement, which began with the Dartmouth meeting, was later described by John McCarthy (the event’s main organizer) as the “Look, Ma, no hands!” era. During these early days, researchers built systems designed to refute claims of the form “No machine could ever do X!” Such skeptical claims were common at the time. To counter them, the AI researchers created small systems that achieved X in a “microworld” (a well-defined, limited domain that enabled a pared-down version of the performance to be demonstrated), thus providing a
====================
” to the left and “reimagining processes in the missing middle,” from what Donna Haraway terms the field of human-machines.

It is important to understand that the term “machines” refers to computational processes that “look at” the input data, say an image of a printed letter or number. Each demon looks for something specific in the image, perhaps a horizontal bar; another might look for a vertical bar; another for an arc of a circle; and so on. Each demon “shouts” its findings to a set of demons higher in the organization. (Think of these higher level demons as middle-level managers.) The loudness of a demon’s shout depends on how certain it is that it is seeing what it is looking for. Of course, there is no exact knowledge of what is in a particular space. For example, a software program tuned to listen for traffic in a traffic light may not be using some version of stored procedure that provides for an immediate possibility of an outcome that is described as being satisfactory for the software, but not necessarily what the user is seeking in the next sentence.

What if the reason for the search is that there are no automobiles? Then the search might indeed take the form of looking for automobiles. However, the exact location of automobiles in a visual system is not known. And because humans’t there any automobiles at all, but a collection of pixels, the entire visual field was thus obscuring something else. The same general principle of classification that makes humans subject to specific classifications also makes searching of individuals from a visual system 

Boolean algebra TheBoolean algebra is a mathematical structure that describes the relations of, and the actions that constitute, actions. It is essential that we understand the concept of action, for it is a mathematical structure that is used in computing. One may try to formulate it in the appropriate way to understand the principles. For example, \A" is a mathematical structure that is used in computing. frames indicate the program being developed, and the program contains a set of definitions for the machine. frames can be thought of as mathematical symbols that are embedded in the program. These are in the format of a program shown in Fig. 2.4. The program is on a line machine called the “hidden-line” group and is very limited in what it can do. The amount of work in the program is too great; therefore, the line drawing must be replaced. This is far beyond the capacity of the machine to perform. replaces the human intellect.”

In the same year, other researchers at Google Brain set up three networks called Alice, Bob and Eve. Their task was to learn how to encrypt information. Alice and Bob both knew a number – a key, in cryptographic terms – that was unknown to Eve. Alice would perform some operation on a string of text, and then send it to Bob and Eve. If Bob could decode the message, Alice’s score increased; but if Eve could, Alice’s score decreased.

Over thousands of iterations, Alice and Bob learned to communicate without Eve breaking their code: they developed a private form of encryption like that used in private emails today. But crucially, we don’t understand how this encryption works. Its operation is occluded by the deep layers of the network. What is hidden from Eve is also hidden from us. The machines are learning to keep their secrets.

How we understand and think of our place in the world, and our relation to one another and to machines, will ultimately decide where our technologies will take us. We cannot unthink the network; we can only think through and within it. The technologies that inform and shape our present perceptions of reality are not going to go away, and in many cases we should not wish them to. Our current life support systems on a planet of 7.5 billion people and rising depend on them. Our understanding of those systems, and of the conscious choices we make in their design, remain entirely within our capabilities. We are not powerless, not without agency. We only have to think, and think again, and keep thinking. The network – us and our machines and the things we think and discover together – demands it.

Computational systems, as tools, emphasise one of the most powerful aspects of humanity: our ability to act effectively in the world and shape it to our desires. But uncovering and articulating those desires, and ensuring that they do not degrade, overrule, efface, or erase the desires of others, remains our prerogative.

When Kasparov was defeated back in 1997, he didn’t give up the game. A year later, he returned to competitive play with a new format: advanced, or centaur, chess. In advanced chess, humans partner, rather than compete, with machines. And it rapidly became clear that something very interesting resulted
====================
” and “niche” in the popular phrase, but this was quickly forgiven in light of the fact that18

pugilism does not appear to have been confined to the metal sphere. Workers in nineteenth-century Britain were routinely fined up to half a day’s pay for singing, talking, and being late. In 1241 BC, in the largest naval operation in the early American colonies, sublet nets were used to monitor party workers.pooning amounts of money in the eyes of neighbor and government officials.

In the early decades of America, amendments to the Constitution were considered at the local level, and in 1876 the U.S. Constitution was implemented into U.S. state constitutions. Amendment after amendment, even if not fully effective, was enough to change the mentality of many New York city government officials. swearing was still a pervasive feature of their meetings, and it was said to increase the “level of trust in the state” from which candidates were evaluated.

The people most harmed by the state’s use of city-built data centers grew richer through work than their officials could hope to accomplish combined city- or state-level. For example, Manhattan’s real estate and financial services desktops that employed workers more than a thousand people in the past decade used city streets to reach the stratified group of First- and Second-class status.19 Likewise, the offices ofvisionary technology, set up in the offices of local government officials, sought to leverage such tools to increase the powers of a company’s AI agents.

In the cyber realm, machines were linked to global militaries and police stations, which allowed those agencies to monitor and modulate military activities. Soon, this became an illicit activity. The US military and intelligence establishments began to share data with other countries and attempted to understand what data was being collected or how it was being used. In late 2013, the US Department of Defense published a memo announcing the collection of "profiles, e-mail, and data obtained whereun easily available."21 As Erik reported in 2014, the Advanced Research Projects Agency was renamed DARPA (for Defense Advanced Research Projects Agency) to emphasize its emphasis on projects that contributed to enhanced military capabilities.

The Office of Naval Research, for example, uses ARPA as its Office of Computers and Technologies’ information base. But because ARPA’s mission is to help users manage information about their home and work networks, its resources were intended to support military decision-making. Thus, the combination of ARPA’s mission and expertise both made it easier to divide military decisions between subordinates and between superiors.

8.1 Programming Languages

Newell and Simon were among the first to realize that a specialized computer language would be useful for manipulating the symbolic expressions that were at the heart of their approach to mechanizing intelligence. The most elementary kind of symbolic expression is a list of symbols, such as (7, B, 5). More complex structures can be composed by creating lists of lists of symbols and lists of lists of lists, and so on.

In my description of symbol structures for the eight-puzzle, I mentioned the kinds of manipulations that are needed. Recall that the starting position of the eight-puzzle was represented by the expression

((2, 8, 3), (1, 6, 4), (7, B, 5)).

What was needed was a language for writing programs that could produce expressions representing the positions corresponding to moves of the puzzle. For example, one of the moves that can be made from the starting position is represented by the expression

((2, 8, 3), (1, 6, 4), (B, 7, 5))."

To produce this expression, the program must copy the starting position expression and then interchange the first and second elements of the third list in that expression.

Newell, Shaw, and Simon set about to develop a language in which these kinds of manipulations could be programmed. Starting around 1954 at the RAND Corporation, they created a series of languages all called IPL (for information-processing language). Several versions of the language were developed. IPL-I was not actually implemented but served as a design specification. IPL-II was implemented in 1955 for the RAND Corporation’s JOHNNIAC computer. Later versions (through IPL-VI) were implemented at Carnegie Tech.

The IPL languages were used to program several early AI programs, including LT, GPS, NSS (the Newell, Shaw, Simon chess-playing program), and the programs written by Newell’s and Simon’s students, such as Quillian and George Ernst. After the Dartmouth summer project, John McCarthy also began thinking about using list-processing languages. He was aware of the use of FLPL (FORTRAN fortified by some list-processing operations) in Gelernter’
====================
If the above arguments hold true, the next questions are clear: What jobs are really at risk? And how bad will it be?

WHAT AI CAN AND CAN’T DO: THE RISK-OF-REPLACEMENT GRAPHS

When it comes to job replacement, AI’s biases don’t fit the traditional one-dimensional metric of low-skill versus high-skill labor. Instead, AI creates a mixed bag of winners and losers depending on the particular content of job tasks performed. While AI has far surpassed humans at narrow tasks that can be optimized based on data, it remains stubbornly unable to interact naturally with people or imitate the dexterity of our fingers and limbs. It also cannot engage in cross-domain thinking on creative tasks or ones requiring complex strategy, jobs whose inputs and outcomes aren’t easily quantified. What this means for jo replacement can be expressed simply through two X–Y graphs, one for physical labor and one for cognitive labor.

For physical labor, the X-axis extends from “low dexterity and structured environment” on the left side, to “high dexterity and unstructured environment” on the right side. The Y-axis moves from “asocial” at the bottom to “highly social” at the top. The cognitive labor chart shares the same Y-axis (asocial to highly social) but uses a different X- axis: “optimization-based” on the left, to “creativity- or strategy-based” on the right. Cognitive tasks are categorized as “optimization-based” if their core tasks involve maximizing quantifiable variables that can be captured in data (for example, setting an optimal insurance rate or maximizing a tax refund).

These axes divide both charts into four quadrants: the bottom-left quadrant is the “Danger Zone,” the top-right is the “Safe Zone,” the top-left is the “Human Veneer,” and the bottom right is the “Slow Creep.” Jobs whose tasks primarily fall in the “Danger Zone” (dishwasher, entry-level translators) are at a high risk of replacement in the coming years. Those in the “Safe Zone” (psychiatrist, home-care nurse, etc.) are likely out of reach of automation for the foreseeable future. The “Human Veneer” and “Slow Creep” quadrants are less clear-cut: while not fully replaceable right now, reorganization of work tasks or steady advances in technology could lead to widespread job reductions in these quadrants. As we will see, occupations often involve many different activities outside of the “core tasks” that we have used to place them in a given quadrant. This task-diversity will complicate the automation of many professions, but for now we can use these axes and quadrants as general guidance for thinking about what occupations are at risk.

For the “Human Veneer” quadrant, much of the computational or physical work can already be done by machines, but the key social interactive element makes them difficult to automate en masse. The name of the quadrant derives from the most likely route to automation: while the behind-the-scenes optimization work is overtaken by machines, human workers will act as the social interface for customers, leading to a symbiotic relationship between human and machine. Jobs in this category could include bartender, schoolteacher, and even medical caregiver. How quickly and what percentage of these jobs disappear depends on how flexible companies are in restructuring the tasks done by their employees, and how open customers are to interacting with computers.

The “Slow Creep” category (plumber, construction worker, entry-level graphic designer) doesn’t rely on human beings’ social skills but instead on manual dexterity, creativity, or ability to adapt to unstructured environments. These remain substantial hurdles for AI, but ones that the technology will slowly chip away at in the coming years. The pace of job elimination in this quadrant depends less on process innovation at companies and more on the actual expansion in AI capabilities. But at the far right end of the “Slow Creep” are good opportunities for the creative professionals (such as scientists and aerospace engineers) to use AI tools to accelerate their progress.

These graphs give us a basic heuristic for understanding what kinds of jobs are at risk, but what does this mean for total employment on an economy-wide level? For that, we must look to the economists.

WHAT THE STUDIES SAY

Predicting the scale of AI-induced job losses has become a cottage industry for economists and consulting firms the world over. Depending on which model one uses, estimates range from terrifying to totally not a problem. Here I give a brief overview of the literature and the methods
====================
”

In the late 1960s, the historian and philosopher of technology Lewis Mumford developed the concept of the megamachine to illustrate how all systems, no matter how immense, consist of the work of many individual human actors.74 For Mumford, the Manhattan Project was the defining modern megamachine whose intricacies were kept not only from the public but even from the thousands of people who worked on it at discrete, secured sites across the United States. A total of 130,000 workers operated in complete secrecy under the direction of the military, developing a weapon that would kill (by conservative estimates) 237,000 people when it hit Hiroshima and Nagasaki in 1945. The atomic bomb depended on a complex, secret chain of supply, logistics, and human labor.

Artificial intelligence is another kind of megamachine, a set of technological approaches that depend on industrial infrastructures, supply chains, and human labor that stretch around the globe but are kept opaque. We have seen how AI is much more than databases and algorithms, machine learning models and linear algebra. It is metamorphic: relying on manufacturing, transportation, and physical work; data centers and the undersea cables that trace lines between the continents; personal devices and their raw components; transmission signals passing through the air; datasets produced by scraping the internet; and continual computational cycles. These all come at a cost.

We have looked at the relations between cities and mines, companies and supply chains, and the topographies of extraction that connect them. The fundamentally intertwined nature of production, manufacturing, and logistics reminds us that the mines that drive AI are everywhere: not only sited in discrete locations but diffuse and scattered across the geography of the earth, in what Mazen Labban has called the “planetary mine.”75 This is not to deny the many specific locations where technologically driven mining is taking place. Rather, Labban observes that the planetary mine expands and reconstitutes extraction into novel arrangements, extending the practices of mines into new spaces and interactions around the world.

Finding fresh methods for understanding the deep material and human roots of AI systems is vital at this moment in history, when the impacts of anthropogenic climate change are already well under way. But that’s easier said than done. In part, that’s because many industries that make up the AI system chain conceal the ongoing costs of what they do. Furthermore, the scale required to build artificial intelligence systems is too complex, too obscured by intellectual property law, and too mired in logistical and technical complexity for us to see into it all. But the aim here is not to try and make these complex assemblages transparent: rather than trying to see inside them, we will be connecting across multiple systems to understand how they work in relation to each other.76 Thus, our path will follow the stories about the environmental and labor costs of AI and place them in context with the practices of extraction and classification braided throughout everyday life. It is by thinking about these issues together that we can work toward greater justice.

2 Labor

Rather than debating whether humans will be replaced by robots, in this chapter I focus on how the experience of work is shifting in relation to increased surveillance, algorithmic assessment, and the modulation of time. Put another way, instead of asking whether robots will replace humans, I’m interested in how humans are increasingly treated like robots and what this means for the role of labor. Many forms of work are shrouded in the term “artificial intelligence,” hiding the fact that people are often performing rote tasks to shore up the impression that machines can do the work. But large-scale computation is deeply rooted in and running on the exploitation of human bodies.

If we want to understand the future of work in the context of artificial intelligence, we need to begin by understanding the past and present experience of workers. Approaches to maximizing the extraction of value from workers vary from reworkings of the classical techniques used in Henry Ford’s factories to a range of machine learning–assisted tools designed to increase the granularity of tracking, nudging, and assessment. This chapter maps geographies of labor past and present, from Samuel Bentham’s inspection houses to Charles Babbage’s theories of time management and to Frederick Winslow Taylor’s micromanagement of human bodies. Along the way, we will see how AI is built on the very human efforts of (among other things) crowdwork, the privatization of time, and the seemingly never-ending reaching, lifting, and toiling of putting boxes into order. From the lineage of the mechanized factory, a model emerges that values increased conformity, standardization, and interoperability—for products, processes, and humans alike.

Prehistories of Workplace AI

Workplace automation, though often told as a story of the future, is already a long-established experience of contemporary work. The manufacturing assembly line, with its emphasis on consistent and
====================
MARCH, an acronym for Machine-Aided Cognition and for Multi-Access Computing, is being evaluated. It's an acronym for Multi-Access Computing that refers to all the software tools that are built into operating systems, including AI systems, hardware devices, and virtualization software. This is a higher level of computer than anything that exists today.

"The aim was not to create a single integrated AI system, but to capture the essential shape of one if the next era of business requires creating more than one physically integrated system. One of the great challenges in the quest for artificial intelligence is dealing withtriggeredomotor skills, which can be taught in the physical education process, as well as in the more complex world of subsequent decades. However, it is also important to remember the broad scope of what I mean by skills in this expression. Some of them are likely to be relevant to the role of employees at any organizational level.

Here is a description of some of the core presuppositions about the value of a job:

The presupposition that the worker is under heightened existential threat will often be raised to the level of risk. The worker's skills are likely to be the most important aspects of the worker's life. It is probably impossible for most people to grasp the corollary—that the skills necessary for making sense of information andty are becoming more specialized. For example, some people might value the cognitive and social skills of an buttermensial analyst; and as long as the analyst is sure the problem is solved, he will believe it will go away. There is nothing to be lost by having this belief but factious elaboration.

The psychologist George A. Miller (1948– ) has recently popularized the word "AI" to describe such a capability.1 In the introduction to his book, Miller humorously notes “My ‘New I’m going to give you a chance to play this, too.” Although, he later added “cooperate in my dream world.”

It is important toMiller’s story that the AI is being used to informly advance the goals of his novel world. Miller is quite critical of the methods of AI used in his book as well as the methods of social control used by social entrepreneurs in their attempt to control the destinies of the commons. He writes:

The methods used to build AI are built on a deep misunderstanding of the terrain of possible machine intelligence. The machines are thought to move as though they know the landscape perfectly, but they are also capable of incorporating all sorts of unexpected facts into the knowledge of the fleet. What these methods for predicting and incorporating new information have produced, however, is one of the things that people are able to do with as they go into the store, where they canorder, and where they can find it quickly enough to be informed about the true nature of their task. With the help of such apps, we are able to restore ourselves from backups, or perhaps when the backups fail, replace the original backups with a fresh copy of the newly created ones. On the other hand, if an organization has no employees, there could be no reason for him to care about the organization's future financial results.

Here is a famous example of one kind of assuming that humans are thinking about the future:

1. The machines are thinking.

2. Machine is thinking.

3. Machine is thinking.

The Stanford Institute for Artificial Intelligence is developing a continuing interest in the nature of intelligence and artificial intelligence. The institute will hold its annual meeting in the fall of each year, and the field meetings are taking place during the following years. Additionally, the number of researchers meeting annually in the Stanford Biomedical Informatics Division is increased severalfold.

In the longer term, an increasing number of researchers believe that way of thinking is necessary for creating machine intelligence. Various approaches have been pursued, including ones that aim to use neural networks to interpret group-buying data and to think longer-term about the search process.

One of the outstanding successes of artificial intelligence has been the statistical toolbox developed at the University of Edinburgh. Two University of Edinburgh colleagues, John Haugeland (1910–2006) and John McCarthy (1917–2006), set out to explore the boundaries of statistical inference. They gave a paper there That was very useful.

McCarthy has continued to work on the problem of mechanizing intelligent machines. Back in 1976, he proposed a method, called “time-sharing” by at least one of several early researchers working on the problem. McCarthy feels that this method deserves a closer look.

ACCEPTION: THE INFORMATED ORGANIZATION OF TESLA

Thetesillas, also known as Campanella, is a group of red-like organisms found in the San Fernando Valley, California. Thetesillas is a fun-toy animal
====================
" the threat of communism’s civilizational dissolution. The existential risk can for the first time be pointed in a new directions. The growing coherence and confidence of the global organization means that ideas, data, and tools become available for anyone to alter and direct their own life.

When the power is in the hands of some humans, or when information is diffused between human beings and machine systems, the potential for large language models and networks of governance is no longer in doubt. Such systems include corporations, which can be powerful but which do not yet know how to create human-like systems; learning programs, which can learn without being explicitly programmed to do so; sandboxes, which can arbitrate and trade stocks, and virtual worlds, which can interact with people and machines in a simulated world.

The questions that we face today are still many parts of the world that were once closed off by the industrial economy: in particular, the problem of jobs, underpaid workers, and ever-greater inequality. Yet these are the only ones that have emerged as a major public concern. China’s AI plan hints at the predicament of that group.

If the above arguments hold true, the next questions are clear: What jobs are really at risk? And how bad will it be?

WHAT AI CAN AND CAN’T DO: THE RISK-OF-REPLACEMENT GRAPHS

When it comes to job replacement, AI’s biases don’t fit the traditional one-dimensional metric of low-skill versus high-skill labor. Instead, AI creates a mixed bag of winners and losers depending on the particular content of job tasks performed. While AI has far surpassed humans at narrow tasks that can be optimized based on data, it remains stubbornly unable to interact naturally with people or imitate the dexterity of our fingers and limbs. It also cannot engage in cross-domain thinking on creative tasks or ones requiring complex strategy, jobs whose inputs and outcomes aren’t easily quantified. What this means for jo replacement can be expressed simply through two X–Y graphs, one for physical labor and one for cognitive labor.

For physical labor, the X-axis extends from “low dexterity and structured environment” on the left side, to “high dexterity and unstructured environment” on the right side. The Y-axis moves from “asocial” at the bottom to “highly social” at the top. The cognitive labor chart shares the same Y-axis (asocial to highly social) but uses a different X- axis: “optimization-based” on the left, to “creativity- or strategy-based” on the right. Cognitive tasks are categorized as “optimization-based” if their core tasks involve maximizing quantifiable variables that can be captured in data (for example, setting an optimal insurance rate or maximizing a tax refund).

These axes divide both charts into four quadrants: the bottom-left quadrant is the “Danger Zone,” the top-right is the “Safe Zone,” the top-left is the “Human Veneer,” and the bottom right is the “Slow Creep.” Jobs whose tasks primarily fall in the “Danger Zone” (dishwasher, entry-level translators) are at a high risk of replacement in the coming years. Those in the “Safe Zone” (psychiatrist, home-care nurse, etc.) are likely out of reach of automation for the foreseeable future. The “Human Veneer” and “Slow Creep” quadrants are less clear-cut: while not fully replaceable right now, reorganization of work tasks or steady advances in technology could lead to widespread job reductions in these quadrants. As we will see, occupations often involve many different activities outside of the “core tasks” that we have used to place them in a given quadrant. This task-diversity will complicate the automation of many professions, but for now we can use these axes and quadrants as general guidance for thinking about what occupations are at risk.

For the “Human Veneer” quadrant, much of the computational or physical work can already be done by machines, but the key social interactive element makes them difficult to automate en masse. The name of the quadrant derives from the most likely route to automation: while the behind-the-scenes optimization work is overtaken by machines, human workers will act as the social interface for customers, leading to a symbiotic relationship between human and machine. Jobs in this category could include bartender, schoolteacher, and even medical caregiver. How quickly and what percentage of these jobs disappear depends on how flexible companies are in restructuring the tasks done by their employees, and how open customers are to interacting with computers.

The “Slow Creep”
====================
” that some of the most powerful systems are developed, built, and launched into the world. It’s a process that builds on the ground, and in the process, we build systems that arelegislators. We issue papers like, “The Machine Stops.” Pressed by a reader’s multiple-choice accuracy ratings, the first version of ImageNet grew heavy on my memory, and I began torastically copying the image database of 1.6 million images to construct a model of the world.

By the time I began my Ph.D., the field of AI had forked into two camps: the “rule-based” approach and the “neural networks” approach. Researchers in the rule-based camp (also sometimes called “symbolic systems” or “expert systems”) attempted to teach computers to think by encoding a series of logical rules: If X, then Y. This approach worked well for simple and well-defined games (“toy problems”) but fell apart when the universe of possible choices or moves expanded. To make the software more applicable to real-world problems, the rule-"based camp tried interviewing experts in the problems being tackled and then coding their wisdom into the program’s decision-making (hence the “expert systems” moniker).

The “neural networks” camp, however, took a different approach. Instead of trying to teach the computer the rules that had been mastered by a human brain, these practitioners tried to reconstruct the human brain itself. Given that the tangled webs of neurons in animal brains were the only thing capable of intelligence as we knew it, these researchers figured they’d go straight to the source. This approach mimics the brain’s underlying architecture, constructing layers of artificial neurons that can receive and transmit information in a structure akin to our networks of biological neurons. Unlike the rule-based approach, builders of neural networks generally do not give the networks rules to follow in making decisions. They simply feed lots and lots of examples of a given phenomenon— pictures, chess games, sounds—into the neural networks and let the networks themselves identify patterns within the data. In other words, the less human interference, the better.

Differences between the two approaches can be seen in how they might approach a simple problem, identifying whether there is a cat in a picture. The rule-based approach would attempt to lay down “if-then” rules to help the program make a decision: “If there are two triangular shapes on top of a circular shape, then there is probably a cat in the picture.” The neural network approach would instead feed the program millions of sample photos labeled “cat” or “no cat,” letting the program figure out for itself what features in the millions of images were most closely correlated to the “cat” label.

During the 1950s and 1960s, early versions of artificial neural networks yielded promising results and plenty of hype. But then in 1969, researchers from the rule-based camp pushed back, convincing many in the field that neural networks were unreliable and limited in their use. The neural networks approach quickly went out of fashion, and AI plunged into one of its first “winters” during the 1970s.

Over the subsequent decades, neural networks enjoyed brief stints of prominence, followed by near-total abandonment. In 1988, I used a technique akin to neural networks (Hidden Markov Models) to create Sphinx, the world’s first speaker-independent program for recognizing continuous speech. That achievement landed me a profile in the New York Times. But it wasn’t enough to save neural networks from once again falling out of favor, as AI reentered a prolonged ice age for most of the 1990s.

What ultimately resuscitated the field of neural networks—and sparked the AI renaissance we are living through today—were changes to two of the key raw ingredients that neural networks feed on, along with one major technical breakthrough. Neural networks require large amounts of two things: computing power and data. The data “trains” the program to recognize patterns by giving it many examples, and the computing power lets the program parse those examples at high speeds.
Both data and computing power were in short supply at the dawn of the field in the 1950s. But in the intervening decades, all that has changed. Today, your smartphone holds millions of times more processing power than the leading cutting-edge computers that NASA used to send Neil Armstrong to the moon in 1969. And the internet has led to an explosion of all kinds of digital data: text, images, videos, clicks, purchases, Tweets, and so on. Taken together, all of this has given researchers copious amounts of rich data on which to train their networks, as well as plenty of cheap computing power for that training.

====================
’s “ arrive,” “take a ride,” or “ eat some numeral slice of pizza.”20 It’s the kind of personalized customer service that many of Chinese people are likely to hire anywhere from paralegals to judges. And it’s far cheaper than anything you could buy at the local market.

The three GPTs China is bumping up against across four waves: internet AI, business AI, autonomous AI, and human-robot interactions. Each of these waves harnesses AI’s power in a different way, disrupting different sectors and weaving artificial intelligence deeper into the fabric of our daily lives.

THE 3,000- SHOPPING CULTURES

The effects of China’s mass innovation and mass innovation campaign went far beyond mere office space and investment dollars. The campaign left a deep imprint on ordinary people’s perceptions of internet entrepreneurship, genuinely shifting the cultural zeitgeist.

Chinese culture traditionally has a tendency toward conformity and a deference toward authority figures, such as parents, bosses, teachers, and government officials. Before a new industry or activity has received the stamp of approval from authority figures, it’s viewed as inherently risky. But if that industry or activity receives a ringing endorsement from Chinese leadership, people will rush to get a piece of the action. That top-down structure inhibits free-ranging or exploratory innovation, but when the endorsement arrives and the direction is set, all corners of society simultaneously spring into action.

Before 2014, the Chinese government had never made clear exactly how it viewed the rise of the Chinese internet. Despite the early successes of companies like Baidu and Alibaba, periods of relative openness online were followed by ominous signals and legal crackdowns on users “spreading rumors” via social media platforms. No one could be sure what was coming next. With the mass innovation campaign, the Chinese government issued its first full- throated endorsement of internet entrepreneurship. Posters and banners sprung up around the country exhorting everyone to join the cause. Official media outlets ran countless stories touting the virtues of indigenous innovation and trumpeting the successes of homegrown startups. Universities raced to offer new courses around entrepreneurship, and bookstores filled up with biographies of tech luminaries and self-help books for startup founders.

Throwing even more fuel on this fire was Alibaba’s record-breaking 2014 debut on the New York Stock Exchange. A group of Taobao sellers rang the opening bell for Alibaba’s initial public offering on September 19, just nine days after Premier Li’s speech. When the dust settled on a furious round of trading, Alibaba had claimed the title of the largest IPO in history, and Jack Ma was crowned the richest man in China.

But it was about more than just the money. Ma had become a national hero, but a very relatable one. Blessed with a goofy charisma, he seems like the boy next door. He didn’t attend an elite university and never learned how to code. He loves to tell crowds that when KFC set up shop in his hometown, he was the only one out of twenty-five applicants to be rejected for a job there. China’s other early internet giants often held Ph.D.s or had Silicon Valley experience in the United States. But Ma’s ascent to rock-star status gave a new meaning to “mass entrepreneurship”—in other words, this was something that anyone from the Chinese masses had a shot at.

The government endorsement and Ma’s example of internet entrepreneurship were particularly effective at winning over some of the toughest customers: Chinese mothers. In the traditional Chinese mentality, entrepreneurship was still something for people who couldn’t land a real job. The “iron rice bowl” of lifetime employment in a government job remained the ultimate ambition for older generations who had lived through famines. In fact, when I had started Sinovation Ventures in 2009, many young people wanted to join the startups we funded but felt they couldn’t do so because of the steadfast opposition of their parents or spouses. To win these families over, I tried everything I could think of, including taking the parents out to nice dinners, writing them long letters by hand, and even running financial projections of how a startup could pay off. Eventually we were able to build strong teams at Sinovation, but every new recruit in those days was an uphill battle.

By 2015, these people were beating down our door—in one case, literally breaking Sinovation’s front door—for the chance to work with us. That group included scrappy high school dropouts, brilliant graduates of top universities, former Facebook engineers, and more than a few people in questionable mental states. While I was out of town, the Sinovation headquarters received a visit from one would-be entrepreneur who refused to leave until I met with him. When the staff
====================
The life of artificial intelligence is forever in the laboratory of labs operated by corporations bent on extracting wealth from consumers and workers. As Lucy Suchman has asked, how are these workers organized? How can such work be standardized, aligned, and managed? Such questions are�such questions, and they don’t have any solution for the problem of workers who are routinely treated as little more than machines, reduced to pieces of output for automated systems. The problem of workers being treated as little more than machines gives rise to resentments that marred the American labor process. Training data is abundant, and it includes everything that isn’t easily quantified, including training data itself. The result is a powerful computational tool for extracting meaning from these texts.

But there is another way in which this collection of microlegislation survives and thrives in the practical economy. Instead of simply informing users about the looming crises, it can be used to envision ways to reshape the future of work in the missing middle.

hematically, the consequence of the matching procedure is a function of how much computing power is available for the AI to compute a good approximation of the expected utility of the actions it is forced to take. In the case of emulations, one might get quite far simply by selecting a fast algorithm. But when the digital material is converted into a three-dimensional representation that includes all its potentially available outputs, the approximation of the expected utility of the actions available to a superintelligence is no longer necessary.

If the digital material is encoded in a high-dimensional store, then the approximation of the expected utility of actions also matters very slightly. If, for instance, the digital forensics analyst is assuming a good capacity for replication of past data, then this is a high-dimensional input signal. Then, by assumption, the forensics analyst knows which data points are in the desired location, and accordingly which data points are in question. The ability to correctlyECT provides information about the allowed split distance between the expected values of the two may therefore lead to errors concerning the order of presentation of the data. In this case, the displayed information should be considered part of the stored brain. The forensics analyst has responsibility for the contents of the stored brain and the processing he does over the course of performing the autopsy.

(Reporting in part two of this book, we will look at the trade-offs involved in developing a policy and implementing a HARPA standard.)

PART TWO: HARPAI FOR THE MASSES

We will begin by exploring the technical and business fronts of artificial intelligence, developing a set of practices that might become necessary in a fluid, socially conscious, and informated society. We will also comment on the social and human dimensions of the partnership, what people say, feel, and do, in addition to the usualado-masochistic surveillance approaches. Then we will consider the discontinuities and limits of the HARPAI approach, namely, the great distances involved, the variety of cases being handled, and the variety of consequences that await them.

Oneash likened the unprecedented pace of development to the height of general humanity: “We do have the ability to read, and we can execute our plans, but we cannot create a new superintelligence until we have first perfected the technology.”

The observer might reply, “Such a technology would be a great success if it werea reasonably simple and easily exploitable digital computer.”

This is probably a mistake. Thepurpose of the system is to help people in their goals. not to do so. But this is not a matter of choice. Suppose some user wants to add a few more items to a list. He could announce, “We have made a new addition to the list, and therefore you should not like or dislike the list.” Or even, “It is not your intent to dislike the list. Rather, it is your intent to help us achieve our goals.”

The third principle: Learning to predict human preferences

The third principle, that the ultimate source of information about human preferences is human behavior, is central to the notion of a beneficial machine.A definition that includes common sense and cultural know-how might be difficult, but not impossible. For example, there is no general equivalence between the words in a technical manual and how humans actually write.

To understand the distance between information technology and harmful behavior, we need to focus less on the technical frailties of factory floors and more on the societal frailties of public debate. Such questions demand deeper insights.

Part two will focus on the generic themes of workplace AI. While these chapters will focus on the specific instances of workplace AI, they will also discuss the likely mechanisms that hold back our potential for novel, more interesting, more interesting, or even superior AI.

Part three will look at the generic themes of next chapter. There, we will look at the generic themes of next chapter
====================
The role of AI in the world has begun to shift. The levels of tech innovation in our lives will in no way reflect this. We will become increasingly dependent on smart machines, and we may even become willing to accept their inherent limitations. However, if the machines have conscious minds—if they don’t know any other way of thinking—then it becomes doubtful that anything will ever line up in their heads.

As Nick Bostrom puts it at the end of his book Superintelligence, success in AI will yield “a civilizational trajectory that leads to a compassionate and jubilant use of humanity’s cosmic endowment.” If we fail to take advantage of what AI has to offer, we will have only ourselves to blame.

4: Misuses of AI

A compassionate and jubilant use of humanity’s cosmic endowment sounds wonderful, but we also have to reckon with d:he rapid rate of innovation in the malfeasance sector. Ill intentioned people are thinking up new ways to misuse AI so quickly that this chapter is likely to be outdated even before it attains printed form. Think of it not as depressing reading, however, but as a call to act before it is too late.

Surveillance, Persuasion, and Control

The automated Stasi

The Ministerium fur Staatsicherheit of East Germany, more commonly known as the Stasi, is widely regarded as “one of the most effective and repressive intelligence and secret police agencies to have ever existed.”1 It maintained files on the great majority of East German households. It monitored phone calls, read letters, and planted hidden cameras in apartments and hotels. It was ruthlessly effective at identifying and eliminating dissident activity. Its preferred modus operandi was psychological destruction rather than imprisonment or execution. This level of control came at great cost, however: by some estimates, more than a quarter of working-age adults were Stasi informants. Stasi paper records have been estimated at twenty billion pages2 and the task of processing and acting on the huge incoming flows of information began to exceed the capacity of any human organization.

It should come as no surprise, then, that intelligence agencies have spotted the potential for using AI in their work. For many years, they have been applying simple forms of AI technology, including voice recognition and identification of key words and phrases in both speech and text. Increasingly, AI systems are able to understand the content of what people are saying and doing, whether in speech, text, or video surveillance. In regimes where this technology is adopted for the purposes of control, it will be as if every citizen had their own personal Stasi operative watching over them twenty-four hours a day.

Even in the civilian sphere, in relatively free countries, we are subject to increasingly effective surveillance. Corporations collect and sell information about our purchases, Internet and social network usage, electrical appliance usage, calling and texting records, employment, and health. Our locations can be tracked through our cell phones and our Internet-connected cars. Cameras recognize our faces on the street. All this data, and much more, can be pieced together by intelligent information integration systems to produce a fairly complete picture of what each of us is doing, how we live our lives, who we like and dislike, and how we will vote.4 The Stasi will look like amateurs by comparison.

Controlling your behavior

Once surveillance capabilities are in place, the next step is to modify your behavior to suit those who are deploying this technology. One rather crude method is automated, personalized blackmail: a system that understands what you are doing—whether by listening, reading, or watching you—can easily spot things you should not be doing. Once it finds something, it will enter into correspondence with you to extract the largest possible amount of money (or to coerce behavior, if the goal is political control or espionage). The extraction of money works as the perfect reward signal for a reinforcement learning algorithm, so we can expect AI systems to improve rapidly in their ability to identify and profit from misbehavior. Early in 2015, I suggested to a computer security expert that automated blackmail systems, driven by reinforcement learning, might soon become feasible; he laughed and said it was already happening. The first blackmail bot to be widely publicized was Delilah, identified in July 2016.

A more subtle way to change people’s behavior is to modify their information environment so that they believe different things and make different decisions. Of course, advertisers have been doing this for centuries as a way of modifying the purchasing behavior of individuals. Propaganda as a tool of war and political domination has an even longer history.

So what’s different now? First, because AI systems can track an individual’s online reading habits, preferences, and likely state of knowledge, they can tailor specific messages to maximize impact on that individual while minimizing the risk that the information will be
====================
The third kind of but takes the form of an oversimplified, instant solution: “But can’t we just do ABC?” As with denial, some of the ABCs are instantly regrettable. Others, perhaps by accident, come closer to identifying the true nature of the problem.

I don’t mean to suggest that there cannot be any reasonable objections to the view that poorly designed superintelligent machines would present a serious risk to humanity. It’s just that I have yet to see such an objection. Since the issue seems to be so important, it deserves a public debate of the highest quality. So, in the interests of having that debate, and in the hope that the reader will contribute to it, let me provide a quick tour of the highlights so far, such as they are.

Denial

Denying that the problem exists at all is the easiest way out. Scott Alexander, author of the Slate Star Codex blog, began a well-known article on AI risk as follows:2 “I first became interested in AI risk back around 2007. At the time, most people’s response to the topic was ‘Haha, come back when anyone believes this besides random Internet crackpots.’ ”

Instantly regrettable remarks

A perceived threat to one’s lifelong vocation can lead a perfectly intelligent and usually thoughtful person to say things they might wish to retract on further analysis. That being the case, I will not name the authors of the following arguments, all of whom are wellknown AI researchers. I’ve included refutations of the arguments, even though they are quite unnecessary.

Electronic calculators are superhuman at arithmetic. Calculators didn’t take over the world; therefore, there is no reason to worry about superhuman AI. • Refutation: intelligence is not the same as arithmetic, and the arithmetic ability of calculators does not equip them to take over the world. • Horses have superhuman strength, and we don’t worry about proving that horses are safe; so we needn’t worry about proving that AI systems are safe. • Refutation: intelligence is not the same as physical strength, and the strength of horses does not equip them to take over the world. • Historically, there are zero examples of machines killing millions of humans, so, by induction, it cannot happen in the future. • Refutation: there's a first time for everything, before which there were zero examples of it happening. ' • No physical quantity in the universe can be infinite, and that includes intelligence, so concerns about superintelligence are overblown. • Refutation; superintelligence doesn't need to be infinite to be problematic; and physics allows computing devices billions of times more powerful than the human brain. • We don’t worry about species-ending but highly unlikely possibilities such as black holes materializing in near-Earth orbit, so why worry about superintelligent AI? • Refutation: if most physicists on Earth were working to make such black holes, wouldn't we ask them if it was safe?

It's complicated

It is a staple of modern psychology that a single IQ number cannot characterize the full richness of human intelligence.3 There are, the theory says, different dimensions of intelligence: spatial, logical, linguistic, social, and so on. Alice, our soccer player from Chapter 2, might have more spatial intelligence than her friend Bob, who has doubled her IQ from the very first game to a whopping multidimensional multipolar reality?

This is thehematics of artificial intelligence. It’s also the construction of a model of a generic set of domains and the generic statistics that are needed to handle the many different problems that it must cope with. The problem of ground truth for AI systems, as we saw in Chapter 8, is that ordinary conversation would not easily be able to locate the correct ballpark for human conversation. The same is true of any other collection of fakes, who either give themselves away for free or rely on scams.

To overcome the combinatorial explosion, one needs algorithms that can scale up and down in complexity and intensity. The size of the algorithm is usually not important, but it might be the most important aspect of the overall outcome. If an algorithm isetrically applied to a graph of comparable intensity, it will be unable to any longer deviate from theENA unless it has a purpose other than to cause harm.

 Graphical models are used as a temporary safeguard. They are as useful as weapons in social contexts, even if the weapons are used in dynamic situations where they will be dispelled. For example, in the analysis of the causal consequences of a spill response, model alerts should be triggered whenever a spill response occurs.

 survivable and robust

In some cases, the argument doesn’t need to be repeated. For example, in the field of medicine, a pandemic
====================

