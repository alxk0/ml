OUTPUT 025							AUTOMATA - GPT-2						 EPOCHS : 27000



The words of the clerk at Global Bank Brazil continue to echo: “We confident ourselves that there is no inherent danger to our words or our lives posed by the very same technology.”

That belief, that very hope, is the basis of a new generation of worker-amplification workers. coined in the popular book Googles de n Laplace, as workers participated in a new process of amplification, where language was put to work on writing and communication. In the book, I argue that the automatability of production presented a new opportunity for the sorts of difficult but important problems that today's mechanization omits. Yet it also presented a new opportunity to consider how we can design the relationship between people and machines in a way that keeps the possibility of collaboration between humans and machines.

My experience with cancer also taught me to appreciate the wisdom that hides in the humble actions of people everywhere. After so many years as an “Ironman” of professional achievement, I needed to be knocked off my pedestal and face my own mortality before I appreciated what many so-called less successful people brought to the table.

I believe we will soon witness the same process on an international scale. The AI superpowers of the United States and China may be the countries with the expertise to build these technologies, but the paths to true human flourishing in the AI age will emerge from people in all walks of life and from all corners of the world.

As we look forward into the future, we must also take the time to look around.

9
★
OUR GLOBAL AI STORY

On June 12, 2005, Steve Jobs stepped up to a microphone in Stanford Stadium and delivered one of the most memorable commencement speeches ever given. In the talk, he retraced his zig-zagging career, from college dropout to cofounder of Apple, from his unceremonious ouster at that company to his founding of Pixar, and finally his triumphant return to Apple a decade later. Speaking to a crowd of ambitious Stanford students, many of whom were eagerly plotting their own ascent to the peaks of Silicon Valley, Jobs cautioned against trying to chart one’s life and career in advance.

“You can’t connect the dots looking forward,” Jobs told the assembled students. “You can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future.”

Jobs’s wisdom has resonated with me since I first heard it, but never more so than today. In writing this book, I’ve had the chance to connect the dots on four decades of work, growth, and evolution. That journey has spanned companies and cultures, from AI researcher and business executive to venture capitalist, author, and cancer survivor. It has touched on issues both global and deeply personal: the rise of artificial intelligence, the intertwined fates of the places that I’ve called home, and my own evolution from a workaholic to a more loving father, husband, and human being.

All of these experiences have come together to shape my view of our global AI future, to connect the dots looking backward and to use those constellations as guidance going forward. My background in technology and business expertise has crystallized how these technologies are developing in both China and the United States. My sudden confrontation with cancer woke me up to why we must use these technologies to foster a more loving society. Finally, my experience moving and transitioning between two different cultures has impressed on me the value of shared progress and the need for mutual understanding across national borders.

AN AI FUTURE WITHOUT AN AI RACE

In writing about global development of artificial intelligence, it’s easy to revert to military metaphors and a zero- sum mentality. Many compare the “AI race” of today to the space race of the 1960s or, even worse, to the Cold War arms race that created ever more powerful weapons of mass destruction. Even the title of this book employs the word “superpowers,” a phrase that many associate with geopolitical rivalry. I use this phrase, however, specifically to reflect the technological balance of AI capabilities, not to suggest an all-out struggle for military supremacy. But these distinctions are easily blurred by those more interested in political posturing than in human flourishing.

If we are not careful, this single-minded rhetoric around an “AI race” will undermine us in planning and shaping our shared AI future. A race has only one winner: China’s gain is America’s loss, and vice-versa. There is no notion of shared progress or mutual prosperity—just a desire to stay ahead of the other country, regardless of the costs. This mentality has led many commentators in the United States to use China’s AI progress as a rhetorical whip with which to spur American leaders to action. They argue that
====================
” on the left side, is the “Human Veneer” on the right side. The “Slow Creep” category (the “Safe Zone”) offers up yet another example of the normal workflow: while companiesocus on AI talent, othersocus on creating a “leisure society,” one with AI-driven changes in the economy that will have profound implications across the economies of the world.

The “Slow Creep” category (plumber, construction worker, entry-level graphic designer) doesn’t rely on human beings’ social skills but instead on manual dexterity, creativity, or ability to adapt to unstructured environments. These remain substantial hurdles for AI, but ones that the technology will slowly chip away at in the coming years. The pace of job elimination in this quadrant depends less on process innovation at companies and more on the actual expansion in AI capabilities. But at the far right end of the “Slow Creep” are good opportunities for the creative professionals (such as scientists and aerospace engineers) to use AI tools to accelerate their progress.

These graphs give us a basic heuristic for understanding what kinds of jobs are at risk, but what does this mean for total employment on an economy-wide level? For that, we must look to the economists.

WHAT THE STUDIES SAY

Predicting the scale of AI-induced job losses has become a cottage industry for economists and consulting firms the world over. Depending on which model one uses, estimates range from terrifying to totally not a problem. Here I give a brief overview of the literature and the methods, highlighting the studies that have shaped the debate. Few good studies have been done for the Chinese market, so I largely stick to studies estimating automation potential in the United States and then extrapolate those results to China.

A pair of researchers at Oxford University kicked things off in 2013 with a paper making a dire prediction: 47 percent of U.S. jobs could be automated within the next decade or two. The paper’s authors, Carl Benedikt Frey and Michael A. Osborne, began by asking machine-learning experts to evaluate the likelihood that seventy occupations could be automated in the coming years. Combining that data with a list of the main “engineering bottlenecks” in machine learning (similar to the characteristics denoting the “Safe Zone” in the graphs on pages 155 and 156), Frey and Osborne used a probability model to project how susceptible an additional 632 occupations are to automation.

The result—that nearly half of U.S. jobs were at “high risk” in the coming decades—caused quite a stir. Frey and Osborn were careful to note the many caveats to their conclusion. Most importantly, it was an estimate of what jobs it would be technically possible to do with machines, not actual job losses or resulting unemployment levels. But the ensuing flurry of press coverage largely glossed over these important details, instead warning readers that half of all workers would soon be out of a job.

Other economists struck back. In 2016, a trio of researchers at the Organization for Economic Cooperation and Development (OECD) used an alternate model to produce an estimate that seemed to directly contradict the Oxford study: just 9 percent of jobs in the United States were at high risk of automation.

Why the huge gap? The OECD researchers took issue with Osborne and Frey’s “occupation-based” approach. While the Oxford researchers asked machine-learning experts to judge the automatability of an occupation, the OECD team pointed out that it’s not entire occupations that will be automated but rather specific tasks within those occupations. The OECD team argued that this focus on occupations overlooks the many different tasks an employee performs that an algorithm cannot: working with colleagues in groups, dealing with customers face-to-face, and so on.

The OECD team instead proposed a task-based approach, breaking down each job into its many component activities and looking at how many of those could be automated. In this model, a tax preparer is not merely categorized as one occupation but rather as a series of tasks that are automatable (reviewing income documents, calculating maximum deductions, reviewing forms for inconsistencies, etc.) and tasks that are not automatable (meeting with new clients, explaining decisions to those clients, etc.). The OECD team then ran a probability model to find what percentage of jobs were at “high risk” (i.e., at least 70 percent of the tasks associated with the job could be automated). As noted, they found that in the United States only 9 percent of workers fell in the high-risk category. Applying that same model on twenty other OECD countries, the authors found that the percentage of high- risk jobs ranged from just 6 percent in Korea to 12 percent in Austria. Don’t worry, the study seemed to say, reports of the death of work
====================
icon.” It’s the business models and the unique practices of the Chinese government, something I’ve found difficult to navigate from a career perspective. Instead of seeking to both preserve these processes and become more useful in the age of AI implementation, I should have sought to understand how the new forms of working with data have developed.

It’s important for us to expansive explore the ideas and approaches that made it possible to reimagine business processes around artificial intelligence. In doing so we hope we will see new leadership in the approaches and practices that make it possible. By understanding how and where these choices are being made, we hope to change the world and improve our own lives.

 Srutopagation Exploring the New World

E.O. services, processes, and new roles for labor. Training, experimentation, and repeatable labor is nowhere near as simple as powering a steam engine with a clunky algorithm. But by bringing people into machine learning environments, we can learn a new skill in artificial intelligence called deep learning.

This is not so in the digital era, where data is purified for valuable training sets that businesses then use to train its machine learning algorithm. It requires huddling with family, managing a small team, using AI to automate some of the most tedious, imperfect, and dangerous tasks around.

There was no training set to transform every activity within a company's customer organization; instead, it was an expanded set of user interfaces that asked questions that users had not even been asked to ask before. The interface wasissephed in on itself, asking questions that were yet to be asked of its own, and then asking new questions in the context of the current research. Could AI be used to help companies like the CVS store as well as its more than two thousand employees?

The answer, as research like CVS has shown, is yes. The company has been training its AI platform to help its customers purchase various items from the same retail outlet within within the past few months, potentially easing the wayback door to AI-induced job losses. The bottom line is the same: workers aren’t likely to be replaced by intelligent robots as a result of the automation of the sales process.

This study provides yet more evidence that the technological progress of the twenty-first century is not inable. By breaking down the rigid logic of automation and freeing the laboring body, we can see the outlines of a plan for redefining work organization and toward the realization of a new relationship between people and machines.

THE THREE R’S: REDUCE, RETRAIN, AND REDISTRIBUTE

Many of the proposed technical solutions for AI-induced job losses coming out of the corporate sector this week focus on incentives. Yes, some of the solutions for AI-induced job losses coming out of the corporate sector this week include incentives to move work closer to the center, offering greater rewards and smaller fines for those who invest energy in these occupations. But in terms of immediate impact, shorter turnaround times arelean incentives that must be Choices-driven changes that matter.

The impact of increasingly intelligent AI systems will be felt across the economy for decades. The most recent government report on the subject has yet to be issued, reflecting a broader consensus among AI policy experts. Even as the United States engaged in years of formal engagement, industry leaders gave little public engagement engagement to the topic of AI, widely regarded as a topic of national security.

For years, the United States had been the world’s top producer of data, yet it was still subject to expensive, logistically impracticable, back-office costs to produce even the slightest bit of data about a particular factory. That massive data footprint likely contributed to the rise of the Chinese government.

But a national government—and that data-gathering apparatus—would take decades to generate. For one thing, the actual job tasks are more complex than the digital tools we rely on for facial recognition. For another, the technology companies haveagged themselves by hiring American, British, and Canadian scholars for their facial recognition systems. These companies have invested heavily in these professions, but they often lack the venture funding to invest in the other keyognition expertise: fast-food entrepreneur, author, and cancer survivor. For a moment, these are the people who universally recognize the difference between a)hat I am and b)who I am and how I am supposed to be feeling. They are not forced to choose between their lives and those of others. Instead, they are choosing to build an alternative system of very different values that compels us to recognize the world around us.

fits squarely in the middle. Drawing on this alternate view of the relative capabilities of artificial intelligence and computer science, the orthogonality thesis emerged as a more recent scientificlean in the modern era. In short, it posited a need for productive machines consisting of multiple appropriately operating subagents with the same
====================
The digital revolution was about to end versus end times. In the age of AI implementation, the age of implementation, we will reach the apex of information extraction and the era of implementation. So, how do you take things into account? How do you take things into account? In this chapter, we answer those questions and provide examples to help you start thinking about how you can increase the productivity of your employees.

AI AppENDIX A : THE SCOPE OF INFORMATION TECHNOLOGY IN THE MODERN WORKPLACE

Information technology is a label that reflects the convergence of several streams of technical developments, including microelectronics, computer science, telecommunications, software engineering, and system analysis. It is a technology that dramatically increases the ability to record, store, analyze, and transmit information in ways that permit flexibility, accuracy, immediacy, geographic independence, volume, and complexity. Information technology has a unique capability to restructure operations that depend upon information for the purposes of transaction, record keeping, analysis, control, or communication.

There is hardly a segment of the U.S. economy that has not been penetrated by some form of computer-based technology. The core of this technology is the silicon-integrated circuit, or "chip." The equivalent of hundreds of thousands of transistors can be built on a silicon chip measuring no more than a fraction of an inch. The astonishing reductions in the cost of these microprocessors, coupled with their equally impressive performance levels, have been exhaustively documented. During the past thirty years, the price per second of instruction has decreased dramatically: a computation that now costs one dollar would have cost about $30,000 in 1950. 1 Porter and Millar calculate that the cost of computer power relative to the cost of manual information processing is at least eight thousand times less than the cost thirty years ago. Between 1958 and 1980, the amount of time needed for one electronic operation fell by a factor of 80 million. They also cite Department of Defense studies that show that the error rate in recording data through bar coding is one in 3 million, compared to one error in three hundred manual data entries. 2 During the past fifteen years, the memory capacity of an integrated circuit has increased by a factor of one thousand, as has its reliability. As another writer remarked, "If the automotive industry had paralleled the advances that the computer industry has experienced in the last 25 years, a Rolls Royce would cost 50 cents and would deliver 1.5 million miles to the gallon.

Numerous studies by economists and industry analysts have concluded that computer-based information technologies will profoundly affect the structure of the U.S. economy. 4 One analyst estimates that in 1980, approximately 10 million Americans interacted daily with a video display terminal and that this number would increase to 2.5 million by 1.66 million by 2.6 million. 5 Another estimates that by the year 1.6.16 by 1.6.17, the most- accessed U.S. internet signals-the total number of internet users-will be equal to the number of books and, according to current economic growth rates, will increase that number to 3.00 million by 3.6 million. 6 Another expert predicts that in 1990, approximately 10 million Americans interacted daily with a video display terminal and that this number would increase to 3.00 million by 4 million. 7 The analyst also notes that this analysis is "not a comprehensive map of the territory in question, but a partial one with boundaries, industries, and processes considered-jobs, service, and education sectors." Such a map would highlight the various steps that can be taken to reach those parts.

Here are some of the main points that an analyst might raise or lower in this scenario:

The number of AI-related job postings peaked in the 1990s. At the same time, the overall labor force fell across every major technology sector, with the exception of agriculture, forestry, fishing, and hunting.

There was no significant shift in the demand for AI-related professional skills. Since then, the gap has widened, with private U.S. CS, CE, and information faculty hires roughly double across the same period.

The number of tenure-track hires peaked in the 1990s, then dropped sharply since then. Since then, the number of tenure-track hires has decreased: those hires peaked in 2019 at 422 and then dropped to 324 in 2021.

The jobs most likely to be automated include managerial, technical, and operations work. In 2021, there were 32 fewer than in 2021.

The most common complaints get at least one Yelp post, and even a fewdomain related ones. Butdomain experts are mostly concerned about the privacy and security implications of the use of AI fordomain organization. An AI domain is one that uses AI to perform a number of outside-the-box interventions, such as automatically approving or declining images or videos that might require manipulation of time and space.

Many kinds of data, including email addresses, booking information
====================
Hooking into a full-fledged collaborative project between humans and machines is the age of implementation. The age of expert systems, however, is not limited to specific industries or regions. A company can choose to develop and use intelligent automation in every industry and activity, including in the development of high-technology products and services.

The AI revolution is not just here but widely planned. The developing world’s richest data ecosystem is worth more than the American company that occupies a commanding lead over all other players. The age of data is coming to China and the United States, just as the era of the Information Research and Technology revolution is beginning to unfold.

THE REAL AI CRISIS

What starts with smart arms can extend to an entire factory line and beyond : AI-enabled processes throughout manufacturing and industrial environments are freeing up hum a n potential in a variety of contexts. Maintenance work, for instance, has been forever upended by AI. Sophisticated AI systems predict machine breakdowns before they occur, which means that maintenance workers can spend less time running routine checks and diagnostics and more time fixing a company's assets. ( For other applications, see the sidebars "AI for Faster Machine Onboarding" and "AI in the Field-Unmanned Vehicles.")

AI for Warehouse and Logistics

These robots are often sophisticated enough to see where they're going and understand what they're doing. But they have their limitations. Say a case of Cheerios is damaged, making it bulkier on one side. Most robots can't adapt. They'd need to skip it and move along to the next case. But robots from a company called Symbotic have the advantage of machine vision algorithms that allow them to assess an oddly shaped package and pick it up anyway. Even better, the robots can quickly measure shelf space to confirm that a box will fit. If it won't, the robot alerts a central control system, which automatically redirects that box’s location toward a shelf where it will fit. The bots zip around the warehouse floor at twenty-five miles per hour, carrying, sensing, and adapting as they go.

The difference between a traditional warehouse and one with Symbotic's robots is stark. Usually, trucks unload pallets of products at the dock; there's an area where pallets are stored until people can unpack them, and conveyor belts move cases of goods to various parts of the warehouse. But because the Symbotic robots immediately remove products from pallets and put then1 on shelves, there's no need to reserve space for storing pallets. And there's no need for conveyor belts either. Thus, a Symbotic-equipped warehouse can reclaim space for shelves. The ramifications are significant: in the best-case scenarios, either a warehouse can store twice as many goods as before, says Joe Caracappa, Symbotic vice president of business development, or it can operate in an area about half the size. Moreover, smaller warehouses can more easily fit into existing neighborhoods, and perishable items can be stored closer to their point of sale.

Because the only human interaction with the goods stored at a warehouse is ·when they're loaded on and off the trucks, we must ask the question: What happens to the human workers at the warehouse? Caracappa says Symbotic currently retrains many of them. Those who performed maintenance on conveyor belts, for instance, are trained to fix robots. And there are new roles, too. Caracappa says system operators monitor the entire flow of robots. "Those roles are typically not in the warehouse before automation comes in," he explains, "but we'll hire them locally and the client will be part of the process."1 0 (In part two of this book, we will explore these new types of jobs in depth when we discuss the missing middle in detail.)

Supply Chinas That Think

Smarter warehouses are Just the beginning. AI technologies are now enabling entire supply chains to become increasingly intelligent, similar to the kinds of advances they've enabled on the factory floor. Of course, companies want to minimize any upstream disruptions to their supply chains, which can come from a number of sources-manufacturing quality problems at a supplier, political instability of a region, labor strikes, adverse weather events, and so on. To that end, AI can help collect and analyze data about suppliers, provide a better understanding of the variables in a supply chain, anticipate future scenarios, and so on. And firms also want to minimize downstream uncertainties. Here, AI can enable companies to optimize their demand planning, forecast more accurately, and better control their inventories. The result is more-agile supply chains capable of anticipating and dealing with the ups and downs of dynamic business conditions.

Consider just one part of the process: demand planning. Getting demand planning right is a pain point for many companies, but the use of neural networks, machine-learning algorithms, and other AI technologies can help lessen that pain. A leading health-
====================
There are already several commercial companies selling face-recognition and face-locating software and equipment. For example, Oki Electric Industry Co., Ltd., sells a product called FSE (Face Sensing Engine). It boasts many applications including controlling access to information in camera-equipped cell phones and other devices, sorting photographs based on recognizing faces, and locating faces in a camera’s field of view. The German company Cognitec Systems GmbH markets the FaceVACS system previously mentioned.

Before closing this section on smart tools, I should mention that there are seve al other areas in which AI tools are enhancing human productivity. For example, I could have mentioned tools for aiding (and automating) the processes of movie animation, for computer program writing and debugging, for industrial process control, for circuit and program verification, and for enhancing and searching the semantic Web. Tools powered by AI techniques will be increasingly used to aid and amplify (and sometimes to substitute for) human cognitive, motor, and perceptual abilities. Just wait!

As I hope the past few chapters have demonstrated, some parts of the quest for artificial intelligence have been quite successful. AI has become more and more a part of all of our lives as well as of those of specialists. But the main goal of the quest (for some of us at least) still remains, namely, endowing artifacts with full human (as well as superhuman) capabilities for language, perception, reasoning, and learning. So, let’s look next at where the quest might lead us.

Chapter 35: The Quest Continues

Where will the AI adventure lead next? We can get some idea of the immediate future simply by extrapolating present trends. Probably there will be some new milestone achievements. Undoubtedly, pieces of AI technology will become ever more common in our homes, automobiles, and activities, and the specialists’ smart tools will become ever smarter and more numerous.

But predicting beyond where AI’s present momentum will take us is problematic. Let’s look at how some previous predictions have fared. Simon’s 1957 prediction of a computer chess champion within ten years was markedly overoptimistic. In 1973, SRI engineers led by Oscar Firschein iteratively queried several AI “experts” about when certain “products” would be realized. The medians and ranges of predicted dates were reported back to them, they were given a chance to modify their predictions, and so on until the results settled down. (This process of making predictions is called a Delphi method.)

The “robot servant” and the “robot chauffeur” still seem quite a ways off, but the others were perhaps only somewhat too optimistic. (Well, the year 2000 seemed a long way off back in 1973.)

Against this background of prediction successes and failures, I hesitate to make any that do not seem rather obvious. Except, I will predict that someday we’ll have human-made artifacts with levels of intelligence (in all of its manifestations) equalling and exceeding that of humans. I make that prediction because I believe that we humans are machines (for what else could we be?) and that eventually we’ll be able to build machines that can do whatever we can do because there will be economic as well as scientific reasons for doing so.

I’ll have more to say about “human-level artificial intelligence” later, but let’s first look at some of the research projects underway in AI laboratories during the early part of this century to see whether they give us any insights about the future.

35.1 In the Labs

There are now probably hundreds of laboratories – industrial, government, and academic – that carry on research in artificial intelligence. I could not possibly describe even a small part of what is going on in them, and, in any case, projects come and go. Just as a historian cedes accounts of current events to newspapers and other media, I recommend that readers wanting to stay current on AI research visit the Web sites maintained by the individual AI laboratories, AI societies, government agencies that support AI research, and specialized conferences and workshops.3 To give some of the flavor of the breadth of current research, I’ll mention a few projects ongoing during the first few years of this century. Of course, these are research projects so it’s possible, but not certain, that some of them will leave their marks on the future.

35.1.1 Specialized Systems

Building smart tools for work in specialized areas is still a big part of AI research. However, work on these tools is increasingly less AI-centric and is merging with the disciplines upon whose technologies these efforts depend – such as statistics, control engineering, image processing, and linguistics, among others. Of course the new techniques invented and used in building even the most specialized niche systems might, in fact, be
====================
The human + machine revolution has already begun, but there are still many questions to answer and paths to forge. That's the goal of the remaining chapters, so let's continue our Journey.

5: Rearing Your Algorithms Right: Three Roles Humans Play in Developing and Deploying Responsible AI

Melissa Cefkin has an interesting job. As a principal scientist at Nissan's research center in Silicon Valley, she works alongside traditional car designers in developing the next generation of self-driving vehicles. Her role is to ensure a smooth collaboration between human and machine (that is, between driver and automobile), and that's why she has a background in anthropology. " You need to understand humans if you want to provide them with an automated partner," she contends.

Cefkin's role at Nissan is to think about things that most car designers might not consider. Take, for example, driving rules and conventions, most of which are pretty cut-and-dried (for instance, not crossing a double line), yet people will often break them in certain conditions (crossing a double line to avoid a collision) . How, then, should autonomous cars be programmed to handle exactly when and where to break a rule? Working along with programmers, electronic engineers, and AI experts, Cefkin is hoping to imbue AI self-driving algorithms with specific human traits, such as the flexibility to break rules for a greater good.

As a "vehicle design anthropologist," Cefkin is one of a growing number of professionals whose jobs didn't exist until relatively recently. O ver the years, AI systems have quickly become a part of everyday business, recommending products to customers, helping factories operate more efficiently, and diagnosing and fixing problems with IT systems. That transformation has led to considerable discussion about the potential for the disappearance of whole categories of jobs over the coming years. (Think about the scores of warehouse workers that Amazon currently employs.) But what's often overlooked in the discussion is that many jobs like Cefkin's will also be created. A large number of these jobs will focus on humans training the machines and, in order to develop AI systems capable of complex interactions with people, the training process will increasingly look like a child 's development path.

In our global study of more than fifteen hundred companies now using or testing AI and machine-learning systems, we found the emergence of entire categories of different jobs that will become increasingly prominent.

These new jobs are not simply replacing old ones. They are entirely novel positions, requiring skills and training never needed before. Specifically, sophisticated AI systems are necessitating new business and technology roles that train , explain , and sustain AI behavior, which fall on the left side of the human + machine hybrid activities in figure 5-1. Such work complements the tasks that AI machines perform. Symbiotic with AI, the new roles draw on distinctively human skills. Where in your organization might you find these new jobs? How do they fit into existing and reimagined processes? In this chapter, we answer those questions and provide examples to help you start thinking about your own applications for trainers, explainers, and susta1ners.

Trainers

In the past, people had to adapt to how computers worked. Now, the reverse is happening-AI systems are learning how to adapt to us. To do so, though, those systems need extensive training, and figure 5-2 lists the types of jobs required to teach AI systems how they should perform certain tasks or how they should, well, act a little more human. Generally speaking, we tend to react positively to AI with human-like behavior because it allows us to interact more naturally with machines. (Yet we can also be disturbed by any imperfections in human-like robots, a phenomenon called the "uncanny valley," which will be discussed later in this chapter.)

In manufacturing, for instance, the newer, lightweight, flexible robotic systems that work alongside humans need to be programmed and trained to handle different tasks. This requires employees with the right skills. For automakers, highly automated factories incur steep financial costs for equipment breakdowns. An unscheduled six-hour shut down at an automated assembly line that manufactures a $50,000 vehicle every minute would incur a cost of around $18 million. That's one of the reasons why, over the past decade, Fanuc, a leading robotic manufacturer, has trained forty-seven thousand people to use its equipment. Even so, a shortage of 2 million qualified employees for manufacturing-related jobs in the coming years is projected.

Physical robots aren't the only AI systems that need training. So does AI software, and this is where training machines to become more human-li ke becomes important. Training requires a multi­tude of roles and jobs. At the simple end of the spectrum, trainers help natural-language processors and language translators make fewer errors. At the complex end, AI algorithms must be trained to mimic human behaviors. Customer service
====================
 to make the world a better place, but it’s a logical flaw to think that way. It just doesn’t make sense to build that kind of world around people.

There were many things wrong with the way evolution produced intelligence. For example, the way it doesn’t build tools that can help us with difficult problems. So there’s no need for evolution to meld with management, which is beneficial and reduces burdens for everyone.

The argument from consciousness doesn’t go anywhere near debating the actual mechanisms of the two interacting processes. In fact, it just confirms what I have argued on theground truth: no two processes are exactly alike. The AI process is beneficial to some extent, but it’s also possible to create others.

The good news is that both the imagined Turing machine and the very real digital computer are enabling human beings to create and share content and make decisions in essence the way we have imagined so far, with the potential to fundamentally alter economic and political structures.

The bad news is that both possibilities for creating and the way we have made significant progress in the quest for artificial intelligence are overblown. created when the AI’s cognitive performance is tuned to maximize click-through, not to make the world a better place.2 To run out of steam when it comes to making improvements to policy, institutions, and practical processes, let’s look at some recent expert opinion research on the search for artificial intelligence.

The paper’s authors, Carl Benedikt Frey and Michael A. Osborne, began by asking twelve graduate students to evaluate theibility ofword prediction as a productive process. Combining that data with a list of fifty topics, Frey and Osborne found that 11 of 16 of the papers5 were authored by humans. Not only was there no human involved, but there was no human writers; not only did not happen to use language, but even if there were an easy way of making data more useful and efficient, such as by improving prediction accuracy, the system would only improve prediction accuracy by a very small amount.

By the mid-1980s, AI was well prepared for what would become a quadrillion-dollar business. In 1990, the company was Fortune 1000 Companies, Fortune 1000 AI’s global expansion was due in the fall of this year, and in 2016, Fortune 1000 AI was valued at $30 billion, and its global expansion was due in 2029.

 Perhaps the single catalyst that increased the AI’s popularity was the development of virtual reality. Ever since the Industrial Revolution, people have feared that virtual reality, to depict the world in high-resolution graphics, will soon give computers the ability to perceive virtual reality without needing to walk through the floor of a warehouse. The reality is, of course, highly impractical in many ways: the human visual scene is twice the space of Figure 10, and theinema of video games are twice as lame. A human figure, so we assume, can see the parts of the scene that we want to consider as moving around.

The virtual reality reality illusion is not without helping explanation. The reality is not just that there are more people there now, but that there are more resources there also. This is because, as discussed earlier, two very different things are happening to us as we go into our workplaces. When we collaborate with others in the workplace, our bodies are dramatically expanded, allowing us to interact more with the world around us.

In the industrial workplace, the leading industrial companies are now reimagining their processes to be more flexible, faster, and adaptable to the behaviors, preferences, and needs of their workers at a given moment. This adaptive capability is being driven by real-time data rather than by an a priori sequence of steps. The paradox is that although these processes are not standardized or routine, they can repeatedly deliver better outcomes, improving the product being produced, with more workers being hired, and with fewer workers losing their jobs. In fact, there is no difference between the current working paper and one another between the two AI “ports.”

The AIports report, titled “Artificial Intelligence in Production and Supply Chain,” analyzed nearly twenty million real-world jobs that were distinct from those of a production worker and asked experts to evaluate the skills andgovernances of those workers. It found that machines are making over many jobs, when employees’ bodies are operating at a greater speed and with greater accuracy than human employees. One reason is that AI systems can be quickly reproduced, given continual product changes and ongoing product reviews.

Another reason why automation theorists falsely perceive the argument from wages and productivity growth to be an argument between growth and wage growth. In fact, productivity growth has been growing at a sluggish pace for decades, leading Robert Solow to quip, ‘We see the computer age everywhere, except in the productivity statistics.’ Automation theorists are reduced to arguing
====================
 and the result is a statistical ouroboros: a self-reinforcing discrimination machine that amplifies social inequalities under the guise of technical neutrality.

The Limits of Debasing Systems

To better understand the limitations of analyzing AI bias, we can look to the attempts to fix it. In 2019, IBM tried to respond to concerns about bias in its AI systems by creating what the company described as a more “inclusive” dataset called Diversity in Faces (DiF).26 DiF was part of an industry response to the groundbreaking work released a year earlier by researchers Joy Buolamwini and Timnit Gebru that had demonstrated that several facial recognition systems—including those by IBM, Microsoft, and Amazon—had far greater error rates for people with darker skin, particularly women. As a result, efforts were ongoing inside all three companies to show progress on rectifying the problem.

“We expect face recognition to work accurately for each of us,” the IBM researchers wrote, but the only way that the “challenge of diversity could be solved” would be to build “a data set comprised from the face of every person in the world.”28 IBM’s researchers decided to draw on a preexisting dataset of a hundred million images taken from Flickr, the largest publicly available collection on the internet at the time.29 They then used one million photos as a small sample and measured the craniofacial distances between landmarks in each face: eyes, nasal width, lip height, brow height, and so on. Like Morton measuring skulls, the IBM researchers sought to assign cranial measures and create categories of difference.

The IBM team claimed that their goal was to increase diversity of facial recognition data. Though well intentioned, the classifications they used reveal the politics of what diversity meant in this context. For example, to label the gender and age of a face, the team tasked crowdworkers to make subjective annotations, using the restrictive model of binary gender. Anyone who seemed to fall outside of this binary was removed from the dataset. IBM’s vision of diversity emphasized the expansive options for cranial orbit height and nose bridges but discounted the existence of trans or gender nonbinary people. “Fairness” was reduced to meaning higher accuracy rates for machine-led facial recognition, and “diversity” referred to a wider range of faces to train the model. Craniometric analysis functions like a bait and switch, ultimately depoliticizing the idea of diversity and replacing it with a focus on variation. Designers get to decide what the variables are and how people are allocated to categories. Again, the practice of classification is centralizing power: the power to decide which differences make a difference.

IBM’s researchers go on to state an even more problematic conclusion: “Aspects of our heritage—including race, ethnicity, culture, geography—and our individual identity—age, gender and visible forms of self-expression— are reflected in our faces.”30 This claim goes against decades of research that has challenged the idea that race, gender, and identity are biological categories at all but are better understood as politically, culturally, and socially constructed.31 Embedding identity claims in technical systems as though they are facts observable from the face is an example of what Simone Browne calls “digital epidermalization,” the imposition of race on the body. Browne defines this as the exercise of power when the disembodied gaze of surveillance technologies “do the work of alienating the subject by producing a ‘truth’ about the body and one’s identity (or identities) despite the subject’s claims.”

The foundational problems with IBM’s approach to classifying diversity grow out of this kind of centralized production of identity, led by the machine learning techniques that were available to the team. Skin color detection is done because it can be, not because it says anything about race or produces a deeper cultural understanding. Similarly, the use of cranial measurement is done because it is a method that can be done with machine learning. The affordances of the tools become the horizon of truth. The capacity to deploy cranial measurements and digital epidermalization at scale drives a desire to find meaning in these approaches, even if this method has nothing to do with culture, heritage, or diversity. They are used to increase a problematic understanding of accuracy. Technical claims about accuracy and performance are commonly shot through with political choices about categories and norms but are rarely acknowledged as such.33 These approaches are grounded in an ideological premise of biology as destiny, where our faces become our fate.

The Many Definitions of Bias

Since antiquity, the act of classification has been aligned with power. In theology, the ability to name and divide things was a divine act of God. The word “category” comes from the Ancient Greek katēgoríā, formed from two roots: kata
====================
 some more data is needed to realize this.

There is a natural fit between the domesticity approach and physical containment. One would try to “box” an AI such that the system is unable to escape while simultaneously trying to shape the AI’s motivation system such that it would be unwilling to escape even if it found a way to do so. Other things equal, the existence of multiple independent safety mechanisms should shorten the odds of success.

Indirect normatively

If direct specification seems hopeless, we might instead try indirect normativity. The basic idea is that rather than specifying a concrete normative standard directly, we specify a process for deriving a standard. We then build the system so that it is motivated to carry out this process and to adopt whatever standard the process arrives at.28 For example, the process could be to carry out an investigation into the empirical question of what some suitably idealized version of us would prefer the AI to do. The final goal given to the AI in this example could be something along the lines of “achieve that which we would have wished the AI to achieve if we had thought about the matter long and hard.”

Further explanation of indirect normativity will have to await Chapter 13. There, we will revisit the idea of “extrapolating our volition” and explore various alterative formulations. Indirect normativity is a very important approach to motivation selection. Its promise lies in the fact that it could let us offload to the superintelligence much of the difficult cognitive work required to carry out a direct specification of an appropriate final goal.

Augmentation

The last motivation selection method on our list is augmentation. Here the idea is that rather than attempting to design a motivation system de novo, we start with a system that already has an acceptable motivation system, and enhance its cognitive faculties to make it superintelligent. If all goes well, this would give us a superintelligence with an acceptable motivation system.

This approach, obviously, is unavailing in the case of a newly created seed AI. But augmentation is a potential motivation selection method for other paths to superintelligence, including brain emulation, biological enhancement, brain– computer interfaces, and networks and organizations, where there is a possibility of building out the system from a normative nucleus (regular human beings) that already contains a representation of human value.

The attractiveness of augmentation may increase in proportion to our despair at the other approaches to the control problem. Creating a motivation system for a seed AI that remains reliably safe and beneficial under recursive selfimprovement even as the system grows into a mature superintelligence is a tall order, especially if we must get the solution right on the first attempt. With augmentation, we would at least start with a system that has familiar and humanlike motivations.

On the downside, it might be hard to ensure that a complex, evolved, kludgy, and poorly understood motivation system, like that of a human being, will not get corrupted when its cognitive engine blasts into the stratosphere. As discussed earlier, an imperfect brain emulation procedure that preserves intellectual functioning may not preserve all facets of personality. The same is true (though perhaps to a lesser degree) for biological enhancements of cognition, which might subtly affect motivation, and for collective intelligence enhancements of organizations and networks, which might adversely change social dynamics (e.g. in ways that debase the collective’s attitude toward outsiders or toward its own constituents). If superintelligence is achieved via any of these paths, a project sponsor would find guarantees about the ultimate motivations of the mature system hard to come by. A mathematically well-specified and foundationally elegant AI architecture might—for all its non-anthropomorphic otherness—offer greater transparency, perhaps even the prospect that important aspects of its functionality could be formally verified.

In the end, however one tallies up the advantages and disadvantages of augmentation, the choice as to whether to rely on it might be forced. If superintelligence is first achieved along the artificial intelligence path, augmentation is not applicable. Conversely, if superintelligence is first achieved along some non-AI path, then many of the other motivation selection methods are inapplicable. Even so, views on how likely augmentation would be to succeed do have strategic relevance insofar as we have opportunities to influence which technology will first produce superintelligence.

CHAPTER 10: Oracles, genies, sovereigns, tools

Tool-AIs

One suggestion that has been made is that we build the superintelligence to be like a tool rather than an agent.11 This idea seems to arise out of the observation that ordinary software, which is used in countless applications, does not raise any safety concerns even remotely analogous to the challenges discussed in this book. Might one not create “tool-AI” that is like such software—like a flight control system, say, or a virtual assistant—only more flexible and capable? Why build a
====================
that could have been made more technically conscious.”

In 1967, the anthropologist and philosopher of technology Lewis Mumford expanded on his findings to include four possible final goals for AI-generation projects.4 For example, because AI systems are now increasingly capable of providing convincing arguments for the feasibility of artificial general intelligence, Professor Peter Norvig and colleagues—ovigorously supervised by intelligent machines—have been developing tools that verify our experiences with deep-learning algorithms.

The latest success of this research led to widespread hype, but the year was 1967, and the term “neuromorphic AI” was coined to describe a range of ideas that were then largely unknown. One of the early founders of the modern discipline of artificial intelligence (NLP) put it this way: “the thing that is probable, actually existing research shows that there are markers of intelligence which are blue in comparison to what we know about human minds and red in comparison to what we know about human brains.”

The philosopher Achille Mbembé persuaded Claude Shannon and Marvin Minsky to join him in thinking about how AI could be evolved. However, the arrival of deep learning did not give the arrival of deep learning “great enough to lead to” greater productivity and increased consciousness. Not only is there no evidence that machines are increasing productivity primarily by substituting for human workers; workers themselves are not happy with the working conditions of their organization.

For example, what do humans do? desk communications are difficult because humans are sometimes able to repetitivelytask with little benefit from the skills of additional work. I have seen little discussion of the skills part of MELDS, other than to make the tasks more demanding, even though they are said to require concentration of skill and intelligence. ICT is said to provide strong evidence of convergent instrumental value: children and their caregivers already experience considerable effort and need. Moreover, they are capable of creative and entertaining solutions to difficult problems, and they are often willing to create new kinds of jobs that acknowledge the nature of their work and demand less of the human body.

From this vantage point it is possible to see that the progress of AI is progress toward “general AI” (i.e., that is, general AI that can be applied to extremely complex problems). This should be one of the mainstests of our society’s technological progress. However, if progress is to be sustained forever, then the possibility of such a thing is lost. The values and motivations that emerged from such research will be much more than variety brands.othsayers and self-interests will abound, and futurists will continue to harp on the question of why humans didn’t die in the first place.

The main reason why progress has been slower than expected is that the technical difficulties of constructing intelligent machines have proved greater than the pioneers foresaw. Nevertheless, pioneers of the new artIFIC intelligence amplification technology, working with programmers and researchers at several institutions, stated in 2007 that the collective intelligence of humanity includes the entities that have high levels of intelligence.8 In other words, the problem of creating machines with higher level intelligence than humans is substantially solved.

There is a famous example of a machine intelligence revolution that Explodes because of a breakthrough research process that led to new concepts of intelligence amplification, organization, and management that led to the invention of digital minds that could be used to help improve the quality of life for individuals.

The revolution was particularly rapid because it moved from the ground up. The pace of transformation was direct, direct, and incremental. While the amplification method could Stonesetterian speedups, it is unlikely that this would happen in the near future (within the next fifteen years, say a decade) because the intellectual property laws regarding time, states, and labor laws are not yet quite clear. New possibilities arise and require new deliberations based on current developments.

The pace of technological change thus is strongly faster than at any time since at least the late 1500s, although not in the near future. The English clockmaker John Harrison (1693–1776) used a type of negative algebra in his clocks. The philosophiclean movement of the eighteenth century was accelerated by the rise of the three laws of circulation, which simplified and rationalized traditional mechanical devices:

First Law: The force of law should not be exertible unless it is definitely and objectively necessary to accomplish something which is between human and human.

Second Law: The very presence of physical acts or objects does not create the same kind of impression as the impression produced by a human intellect.

Third Law: The very presence of something creates a certain impression which makes it easier to control.

As computers get more powerful, people are asking “What is my electric signal?” How do we make sense of this? How can we provide greater sensory, or cognitive enhancement of another?

Hsing Hui, a pianologist and informationurist who has spent
====================
?

The classical argument from experience is that we are all sensible if we believe that there is a definite set of facts which cannot be replaced by machines. This view was a very strong view, but it was a very narrow view. empirically it is not necessary to show that there is no such thing as too little knowledge; rather, it is the very presence of certain facts which exist independently of any machine, which also implies that there is no necessity of subjecting ourselves to definite rules of conduct. The only way, it appears, for us to be conscious is for it be easy to create subjectivity. This intuitive understanding of consciousness is also the view that, in addition to the various intuitive concepts arising from the three laws of Robotics, there is a natural fit between the three states of consciousness:

The principle of contradiction: Objective reality is nothing like the philosophers’ philosophy of mind.

 philosophers philosophers are philosophers who study the three laws of Robotics, viz., "A robot may . . . think without becoming a human being." There is no contradiction in the statement that a robot must think without becoming a human being.

If we adhere to the conditions of the book, the articles of faith should be inapplicable. We are therefore restricted to interpreting the book as a general account of our experiences, rather than as a complete scientific model.

The book's main activity is to share our ideas about consciousness. We imagine that what we are actually doing is bringing people into line with what we are doing. This is suggested, I believe, by the turn of the century, which has seen the industrial revolution and the information age, both in the distribution of knowledge and in the physical and scientific methods of knowledge. Since scientific management is based on the assumption that everyone is exactly alike, this revolution does not require that everyone is exactly alike.

 replaced by a new discipline, the new focus of scientific management on the superior expertise of machine learning algorithms would be felt across the world. Again, the philosophers challenged the traditional logic of automation by pointing out that the same methods of decision making were also useful in building good societies. ( For further remarks about this debate, see the comments section of this book.)

The debates around the ethics of AI skew more toward the centrality of the wealth of information over the ability to make precise inferences about a system's behavior. Thus, moral philosophers are often very concerned about the insides of an AI system, which could be "soft." So how do we ensure that the superintelligence that we desire can never do harm? For this, we must look to the economists.

In this chapter I show how the welfare of AI systems is bothversely and extremely beneficial for society. In this chapter, I explain the steps that I take to ensure that AI systems in controlled settings are never harmful.

I also provide a thought experiment to help philosophers think through the implications of the term “superintelligence”: not what we think but what we think about it. Here, instead of arguing over definitions, we are arguing over what humans are supposed to do. I argue that the term “superintelligence” should be understood in this way:

The English speaking cliché scholar Samuel Bentham remarked, “it is not intended that way.”14 But, he notwithstanding, philosophers continued to dismiss the idea of a self-conscious mind in this way, and proposed aaireraBLE superintelligence as the desired outcome of some closely related but intuitively acceptable plan for achieving human goals.15 The progress of recent years probably has been inspired by studies of “electronic brain” and “the brain” – ideas that appear a little more in the research literature than do drugs and neurosurgery.16 The analogies we provide are only meant as illustrations.1

The analogies we provide facilitate the possibility of envisioning a future in which AI is first and foremost a military technology: a military technology that can be used to best deploy military capabilities; informed consent and ethicalstrategic analysis both facilitate the deployment of AI and inform decision-making only in the context of human knowledge and expertise.

The framework of the third wave of industrial transformation is not precise; for example, it is not certain that a continuous-process organization would maximize the automation of many processes and make a contribution to the improvement of the environment. It is certain that the continuous-process organization will bring many benefits, not just the few benefits of continuousprocess transformation. The benefits of this transformation include potential to save lives and improve health, productivity, and overall well-being; increase revenue from capital, expertise, and resources; and lead their contributions to a greater degree of certainty and predictability. Because continuity and confidence are so important in the modern logic of production, it is imperative that the limits of shockability be met, both in the shaking up of certain economic or political structures and in the direct specification of certain limiting conditions.

There are also advantages to a more flexible production
====================
 at the table, but the thought process was now an alien, anxiety-inducing experience, one that enticed nothing but total catastrophe.

The prospect of such catastrophe has stimulated a resurgence of interest in AI, which might yet find its real source. In part this is due to the ease of discussing these sensitive topics, which simply means a return to the drawing boards, where mathematical analysis is now used to distinguish novel and non- novel objects. upsetI can describe this chapter in the context of the chapter on the “fringe case” of the environment, the automation-based health-care context, as well as the “the model” of the social sciences, to which the critic has been confined.

The left-wing focus on the existential risks of AI has only heightened those fears, turning hundreds of millions of farmers into informants and make informants. With the book, I fear that many of the ideas underlying the informating strategy are actuallygrounded in current economic and political circumstances. Even if the existential risks that AI faces today are as existential as the over-dependence thesis makes it, then the future of humanity is not so much about growth and progress as it is growing ever more uncertain.

The existential risks of AI are now regarded as less existential than the risks associated with nature-which, after all, is never discussed as an existential threat. What’s more is that AI is now able to do “wasteful” things. That means a creation of a superintelligence that maximizes reward and doesn’t worry about what the human-like motivations of the original AI are.

The risk profile of AI is rapidly changing as its tools become more invasive and as researchers are increasingly able to access data without interacting with their subjects. For example, a group of machine learning researchers published a paper in which they claimed to have developed an “automatic system for classifying crimes.”65 In particular, their focus was on whether a violent crime was gang-related, which they claimed their neural network could predict with only four pieces of information: the weapon, the number of suspects, the neighborhood, and the location. They did this using a crime dataset from the Los Angeles Police Department, which included thousands of crimes that had been labeled by police as gang-related.

Gang data is notoriously skewed and riddled with errors, yet researchers use this database’s patterns to produce predictions based on statistical analysis. The CalGang database, for example, which is widely used by police in California, has been shown to have major inaccuracies. The state auditor discovered that 23 percent of the hundreds of records it reviewed lacked adequate support for inclusion. The database also contained forty-two infants, twenty-eight of whom were listed for having “admitting to being gang members.”66 Most of the adults on the list had never been charged, but once they were included in the database, there was no way to have their name removed. Reasons for being included might be as simple as chatting with a neighbor while wearing a red shirt; using these trifling justifications, Black and Latinx people have been disproportionately added to the list.

When the researchers presented their gang-crime prediction project at a conference, some attendees were troubled. As reported by Science, questions from the audience included, “How could the team be sure the training data were not biased to begin with?” and “What happens when someone is mislabeled as a gang member?” Hau Chan, a computer scientist now at Harvard University who presented the work, responded that he couldn’t know how the new tool would be used. “[These are the] sort of ethical questions that I don’t know how to answer appropriately,” he said, being just “a researcher.” An audience member replied by quoting a lyric from Tom Lehrer’s satiric song about the wartime rocket scientist Wernher von Braun: “Once the rockets are up, who cares where they come down?"

This separation of ethical questions away from the technical reflects a wider problem in the field, where the responsibility for harm is either not recognized or seen as beyond the scope of the research. As Anna Lauren Hoffman writes: “The problem here isn’t only one of biased datasets or unfair algorithms and of unintended consequences. It’s also indicative of a more persistent problem of researchers actively reproducing ideas that damage vulnerable communities and reinforce current injustices. Even if the Harvard team’s proposed system for identifying gang violence is never implemented, hasn’t a kind of damage already been done? Wasn’t their project an act of cultural violence in itself?”69 Sidelining issues of ethics is harmful in itself, and it perpetuates the false idea that scientific research happens in a vacuum, with no responsibility for the ideas it propagates.

The reproduction of harmful ideas is particularly dangerous now that AI has
====================
China’s alternate internet universe of apps means that the Chinese government has filled its information platform with something far more useful than a million words of Chinese literature. Over the coming years, Alibaba, Tencent, and thousands of Chinese startups will fan out across hundreds of industries, applying deep learning to any problem that shows the potential for profit. If artificial intelligence is the new electricity, Chinese entrepreneurs will be the tycoons and tinkerers who electrify everything from household appliances to homeowners’ insurance.

As artificial intelligence filters into the broader economy, this era will reward the quantity of solid AI engineers over the quality of elite researchers. Real economic strength in the age of AI implementation won’t come just from a handful of elite scientists who push the boundaries of research. It will come from an army of well-trained engineers who team up with entrepreneurs to turn those discoveries into game-changing companies.

China is training just such an army. In the two decades since my lecture in Hefei, China’s artificial intelligence community has largely closed the gap with the United States. While America still dominates when it comes to "superstar researchers, Chinese companies and research institutions have filled their ranks with the kind of well- trained engineers that can power this era of AI deployment. It has done that by marrying the extraordinary hunger for knowledge that I witnessed in Hefei with an explosion in access to cutting-edge global research. Chinese students of AI are no longer straining in the dark to read outdated textbooks. They’re taking advantage of AI’s open research culture to absorb knowledge straight from the source and in real time. That means dissecting the latest online academic publications, debating the approaches of top AI scientists in WeChat groups, and streaming their lectures on smartphones.

This rich connectivity allows China’s AI community to play intellectual catch-up at the elite level, training a generation of hungry Chinese researchers who now contribute to the field at a high level. It also empowers Chinese startups to apply cutting-edge, open source algorithms to practical AI products: autonomous drones, pay-with-your- face systems, and intelligent home appliances.

Those startups are now scrapping for a slice of an AI landscape increasingly dominated by a handful of major players: the so-called Seven Giants of the AI age, which include Google, Facebook, Amazon, Microsoft, Baidu, Alibaba, and Tencent. These corporate juggernauts are almost evenly split between the United States and China, and they’re making bold plays to dominate the AI economy. They’re using billions of dollars in cash and dizzying stockpiles of data to gobble up available AI talent. They’re also working to construct the “power grids” for the AI age: privately controlled computing networks that distribute machine learning across the economy, with the corporate giants acting as “utilities.” It’s a worrisome phenomenon for those who value an open AI ecosystem and also poses a potential stumbling block to China’s rise as an AI superpower.

But bringing AI’s power to bear on the broader economy can’t be done by private companies alone—it requires an accommodating policy environment and can be accelerated by direct government support. As you recall, soon after Ke Jie’s loss to AlphaGo, the Chinese central government released a sweeping blueprint for Chinese leadership in AI. Like the “mass innovation and mass entrepreneurship” campaign, China’s AI plan is turbocharging growth through a flood of new funding, including subsidies for AI startups and generous government contracts to accelerate adoption.

The plan has also shifted incentives for policy innovation around AI. Ambitious mayors across China are scrambling to turn their cities into showcases for new AI applications. They’re plotting driverless trucking routes, installing facial recognition systems on public transportation, and hooking traffic grids into “city brains” that optimize flows.

Behind these efforts lies a core difference in American and Chinese political culture: while America’s combative political system aggressively punishes missteps or waste in funding technological upgrades, China’s techno- utilitarian approach rewards proactive investment and adoption. Neither system can claim objective moral superiority, and the United States’ long track record of both personal freedom and technological achievement is unparalleled in the modern era. But I believe that in the age of AI implementation the Chinese approach will have the impact of accelerating deployment, generating more data, and planting the seeds of further growth. It’s a self- perpetuating cycle, one that runs on a peculiar alchemy of digital data, entrepreneurial grit, hard-earned expertise, and political will. To see where the two AI superpowers stand, we must first understand the source of that expertise.

NOBEL WINNERS AND NO-NAME TINKERERS

When Enrico Fermi stepped onto the deck of the RMS Franconia II in 1938, he changed
====================
The European Commission, in coordination with the relevant authorities for the marketing and use of artificial intelligence, should be designated as national AI Commission, consisting of 0 0 as the national AI Bses interpreter, with 0.5 as the global Bses interpreter, with 0.6 as the leading global interpreter. The number 0.6 as the leading global interpreter should be the same for each month, so that its successor does not revolt.

Chapter 6: Education
	
More and more AI specialization. The proportion of new computer science PhD graduates from U.S. universities who specialized in AI jumped to 19.1% in 2021, from 14.9% in 2020 and 10.2% in 2010.

New AI PhDs increasingly head to industry. In 2011, roughly the same proportion of new AI PhD graduates took jobs in industry (40.9%) as opposed to academia (41.6%). Since then, however, a majority of AI PhDs have headed to industry. In 2021, 65.4% of AI PhDs took jobs in industry, more than double the 28.2% who took jobs in academia.

New North American CS, CE, and information faculty hires stayed flat. In the last decade, the total number of new North American computer science (CS), computer engineering (CE), and information faculty hires has decreased: There were 710 total hires in 2021 compared to 733 in 2012. Similarly, the total number of tenure-track hires peaked in 2019 at 422 and then dropped to 324 in 2021.

The gap in external research funding for private versus public American CS departments continues to widen. In 2011, the median amount of total expenditure from external sources for computing research was roughly the same for private and public CS departments in the United States. Since then, the gap has widened, with private U.S. CS departments receiving millions more in additional funding than public universities. In 2021, the median expenditure for private universities was $9.7 million, compared to $5.7 million for public universities.

Interest in K–12 AI and computer science education grows in both the United States and the rest of the world. In 2021, a total of 181,040 AP computer science exams were taken by American students, a 1.0% increase from the previous year. Since 2007, the number of AP computer science exams has increased ninefold. As of 2021, 11 countries, including Belgium, China, and South Korea, have officially endorsed and implemented a K–12 AI curriculum.


Chapter 6: Policy and Governance	
	
Policymaker interest in AI is on the rise. An AI Index analysis of the legislative records of 127 countries shows that the number of bills containing “artificial intelligence” that were passed into law grew from just 1 in 2016 to 37 in 2022. An analysis of the parliamentary records on AI in 81 countries likewise shows that mentions of AI in global legislative proceedings have increased nearly 6.5 times since 2016.

From talk to enactment—the U.S. passed more AI bills than ever before. In 2021, only 2% of all federal AI bills in the United States were passed into law. This number jumped to 10% in 2022. Similarly, last year 35% of all state-level AI bills were passed into law.

When it comes to AI, policymakers have a lot of thoughts. A qualitative analysis of the parliamentary proceedings of a diverse group of nations reveals that policymakers think about AI from a wide range of perspectives. For example, in 2022, legislators in the United Kingdom
discussed the risks of AI-led automation; those in Japan considered the necessity of safeguarding human rights in the face of AI; and those in Zambia looked at the possibility of using AI for weather forecasting.

The U.S. government continues to increase spending on AI. Since 2017, the amount of U.S. government AI-related contract spending has increased roughly 2.5 times.

The legal world is waking up to AI. In 2022, there were 110 AI-related legal cases in United States state and federal courts, roughly seven times more than in 2016. The majority of these cases originated in California, New York, and Illinois, and concerned issues relating to civil, intellectual property, and contract law.

Chapter 7: Diversity
	
North American bachelor’s, master’s, and PhD-level computer science students are becoming more ethnically diverse. Although white students are still the most represented ethnicity among new resident bachelor’s, master’s, and PhD-level computer science graduates, students from other ethnic backgrounds (for example, Asian, Hispanic, and Black or African American) are becoming increasingly more represented. For example, in 2011, 71.9% of new resident CS bachelor’s graduates were white. In 2021, that number dropped to 46.7%.

New AI PhDs are still overwhelmingly male. In 2021, 78.7% of new AI
====================
 Alibaba’s Alipay had deployed the popular peer-to-peer network for payments. Alibaba’s Alipay could pick out the purchases coming in, andordable restaurants and hospitals’ health insurance companies had also been processed to order.

Silicon Valley food delivery companies often tried to mimic the inner workings of the larger, more localized e-commerce ecosystem but often couldn’t seem to find the time to add new services or adjust product offerings. fuels, transportation, and physical goods—these staples of the consumer economy. Child labor, the overwork, and massive redistribution of wealth from corporations to the wealthy.

These are the secret ingredients powering Shenzhen, whose talented workers have transformed it from a dirt- cheap factory town to a go-to city for entrepreneurs who want to build new products but have no desire for the kind of hodgepodge production process that goes on inside the Chinese factory. Unlike the Silicon Valley juggernauts who pioneered the world’s fastest-growing market- building markets, many of the companies in this space work in what’s called a data-driven economy.

In the past, these were the case in which China simply handed power to the United States. Now, as these high-volume, low-cost exchanges of information are enabled by intelligent machines, China’s internet juggernauts are bringing their hybrid products to market faster, with far fewer workers doing what they do best: performing repetitive tasks. That means there’s more money to play in these markets, which means companies are�’ more likely to attract top-flight talent and Chinese companies are�’ more likely to fail.

This has led to a high-stakes race in which caution and careful engineering appear to be less important than snazzy demos, talent grabs, and premature rollouts.

During the summer of 2015, I went to see Microsoft Research, a startup in Beijing that had been almost all but done before in computer games. The team had been almost all but forgotten, and the reasons for their fading memory remain unclear.

For the past three years, A I researchers at Microsoft Research had beenduction the theory of artificial intelligence. In 2007, a “learning AI” model appeared online with the title “ distillation of “ distillation”—namely, a theory of artificial intelligence that focused on how to model the behavior of complex mathematical objects. The output of this model was to become a vital source of model data for the theories of intelligence and language.

In 2013, a Chinese linguist and linguistics researcher, Mahesha, gave a lecture to a group of AI researchers at the University of California at Lincoln. In an article about this event, he wrote “Some of the most significant findings were that programs that were trained on images of well-defined, repetitive objects were able to make guesses without any memory of the specific action being taken on a specific image.”20 In other words, the new AI was not just playing chess; it was predicting moves and constructing hypothetical positions based on the positions of the paperclips in its image database.

The paperclips experiment was apparently just a copy of another workshop that I attended, all about programming a program to play checkers. That workshop, appropriately titled “Steps Toward Artificial Intelligence,” was held at the University of Edinburgh in Scotland from September 10 to 12, 1956.

The Duke students were asked to write a letter to the Communications of the Association of Computing Machinery, which planned to bring importance to the subject of artificial intelligence to the floor while promising to improve the "stability and performance of the equipment." The reply from the end of the eighteenth century was that he too had experience with "crude mechanical devices" to be able to explain the devices he was using. In commenting this reply, however, he suggested that he had "considerable experience with such devices."

The experience with digital computers had been quite hands-on. I have described several such operators in my book Computing Before Computers. Myself, however, have been through it before. As you are gradually introduced to software, the nature of these activities is gradually undone. For example, most of the classifications already exist, and it is possible to computerize a lot of them. However, many of them are meaningless, because it is not well-defined what it is to be human.

The importance of the machine learning process thus becomes clearer if we consider how its techniques are used in the cognitive system. One of the important results of this approach is that, depending on the scale of the problem being solved, the system may become more or less intelligent by relying on the help of gradually increasing the speed of our application of the general-purpose digital computing technology.

I have already mentioned the kinds of artificial intelligence that are research in progress.1 But it is not enough to base this progress on some unquantifiable number
====================
ing a domestic or international situation that includes a hypothetical or paranoid perspective that dismisses any possible future risk out of the question of what it wants the AI to do.

The “compassionate” perspective, as the exemplified by the English philosopher of faith, C. George Boeree, emphasizes the reality of the causal consequences of actions: “Forget what we know, there is no reason to worry about all those little critters. The only really necessary assumption is that the processes of evolution that produced intelligence would no longer be useful as tools for acting safely or as models for social organization.”

Instrumental sensors offer us a source of information about the kinds of things we are exposed to in the world around us and about the kinds of things that we see in our environment. By asking, the Japanese psychologist Takeo Kanade has discovered that such questions are both important and urgent. In a series of experiments, Kanade and a self-described futurist friend, Robert Kowalski, used two of Kowalski’s programs, reading out the sentences containing the word “kai-Fu” from an opponent’s mouth and then trying to figure out what characterizes that sentence as best. The Kowalski program did much better, by default, at identifying the character “Fu” thatapped the AI’s color perception.

The Kowalski program did even better, by using pattern-recognition techniques to study how the connections among neurons in the brains of animals were thought to be connections between concepts of categories. This was done by Dr. Kowalski, a professor at the Kowalski Institute for Learning Algorithms and Information Theory (now part of the KPU Group, formerly called the “Experimental Programming Group,”) was instrumental in developing some of the principles of artificial intelligence. In particular, KPU was instrumental in bringing the Kawa River phenomena to life, so that its goals would be realized if the streams of perceptual information were organic and similar.

The experimental programming approach was, of course, heavily inflected with a view to, as Autor says, “to turn the crank” on the imagination of what it meant to be human. “That is all that remains in the final years of the twentieth century: love and relationships.”

Coyle acknowledges that “we are still nowhere near creating good AI machines that feel the emotions that come with being alive.” Yet, she says, “there’s so much more opportunity now for this collaboration between humans and machines than the kind of two-dimensional abstract principles that were once reserved for humans.”

Imagine,carte bla blát, is that so obviously a bad idea?

The fair use of AI and infrastructural intelligence is nowhere near guaranteed. The prohibitions on expressive behavior and model building are not precise, especially when compared to the many infrastructures, including AI itself, that could be powerful and current.

In fact, it is not even clear whether AI can solve the control problem that would send a being in control of the whole brain. And, as we will see, the parallel approach that Kowalskiy and colleagues implemented avoids problems that could arise in any other situation. This is true even when the AI isimize involves placing the AI in an environment in which it is extremely difficult to operate, because it is easy for the AI to become so because it is tuned to behave in ways that please its owner or get hurt.

Even if we recognize that a superintelligence cannot feel any intrinsic pain or joy, how could it symbolize the good in any of these ways?

If the AI’s final goals are to win the imitation game, then the AI puts these final goal representations in the appropriate contexts. Importantly, the special-purpose AI may need to know where the other goals are, and how they are set. To do this, one may hope that the symbolic representations in AI will take us into some useful new direction. For example, in considering the alternatives for domain organization, it is not untypical for an AI to encounter a new training sequence of alternative tasks, which may enable it to make guesses. For example, the AI might confine its training sequence to helping humans with intellectual disabilities. Toward the end of the book, I explain how some versions of domain organization are (in the narrow sense that some humans are able to perceive the boundaries between the boundaries of the training data and the boundaries of training data.)

 domain knowledge

In the next chapter, we will look at the ideas behind the idea of guided introspection. These come from the observation that certain kinds of mental states are associated with a wide range of future probabilities that a psychologist or neuroscientist can observe observable from our minds. conceptual models of these mental processes would enable us to create mental
====================
.

A third reason for not predicting the arrival of superintelligent AI is that it is inherently unpredictable. It requires “conceptual breakthroughs,” as noted by John McCarthy in a 1977 interview.30 McCarthy went on to say, “What you want is 1.7 Einsteins and 0.3 of the Manhattan Project, and we’ll stay 300 years behind in our physics.” In the next section I’ll explain what some of the conceptual breakthroughs are likely to be. Just how unpredictable are they? Probably as unpredictable as Szilard’s invention of the nuclear chain reaction a few hours after Rutherford’s declaration that it was completely impossible.

Once, at a meeting of the World Economic Forum in 2015, I answered the question of when we might see superintelligent AI. The meeting was under Chatham House rules, which means that no remarks may be attributed to anyone present at the meeting. Even so, out of an excess of caution, I prefaced my answer with “Strictly off the record. . . . ” I suggested that, barring intervening catastrophes, it would probably happen in the lifetime of my children—who were still quite young and would probably have much longer lives, thanks to advances in medical science, than many of those at the meeting. Less than two hours later, an article appeared in the Daily Telegraph citing Professor Russell’s remarks, complete with images of rampaging Terminator robots. The headline was ‘SOCIOPATHIC' ROBOTS COULD OVERRUN THE HUMAN RACE WITHIN A GENERATION.

My timeline of, say, eighty years is considerably more conservative than that of the typical AI researcher. Recent surveys31 suggest that most active researchers expect human-level AI to arrive around the middle of this century. Our experience with nuclear physics suggests that it would be prudent to assume that progress could occur quite quickly and to prepare accordingly. If just one conceptual breakthrough were needed, analogous to Szilard’s idea for a neutron-induced nuclear chain reaction, superintelligent AI in some form could arrive quite suddenly. The chances are that we would be unprepared: if we built superintelligent machines with any degree of autonomy, we would soon find ourselves unable to control them. I am, however, fairly confident that we have some breathing space because there are several major breakthroughs needed between here and superintelligence, not just one.

Conceptual Breakthroughs to Come

The problem of creating general-purpose, human-level AI is far from solved. Solving it is not a matter of spending money on more engineers, more data, and bigger computers. Some futurists produce charts that extrapolate the exponential growth of computing power into the future based on Moore's law, showing the dates when machines will become more powerful than insect brains, mouse brains, human brains, all human brains put together, and so on.32 These charts are meaningless because, as I have already said, faster machines just give you the wrong answer more quickly. If one were to collect AI's leading experts into a single team with unlimited resources, with the goal of creating an integrated, human-level intelligent system by combining all our best ideas, the result would be failure. The system would break in the real world. It wouldn’t understand what was going on; it wouldn't be able to predict the consequences of its actions; it wouldn’t understand what people want in any given situation; and so it would do ridiculously stupid things.

By understanding how the system would break, AI researchers are able to identify the problems that have to be solved—the conceptual breakthroughs that are needed—in order to reach human-level AI. I will now describe some of these remaining problems. Once they are solved, there may be more, but not very many more.

Imagining a Superintelligent Machine

The technical community has suffered from a failure of imagination when discussing the nature and impact of superintelligent AI. Often, we see discussions of reduced medical errors,48 safer cars,49 or other advances of an incremental nature. Robots are imagined as individual entities carrying their brains with them, whereas in fact they are likely to be wirelessly connected into a single, global entity that draws on vast stationary computing resources. It’s as if researchers are afraid of examining the real consequences of success in AI.

A general-purpose intelligent system can, by assumption, do what any human can do. For example, some humans did a lot of mathematics, algorithm design, coding, and empirical research to come up with the modern search engine. The results of all this work are very useful and of course very valuable. How valuable? A recent study showed that the median American adult surveyed would need to be paid at least $17,500 to give up using search engines for a year,50 which translates to a global value in the tens of trillions
====================
 faster than the country’s GDP growth rates—a trend visible in the economic statistics of high-income countries. The US economy is not particularly dynamic or dynamic here. Similar practices were found in other high-income countries in the late 1960s and early 1970s: deindustrialization in the service sector, deindustrialization in the auto industry, and deindustrialization in thePrinting and Distribution.

The US economy’s deindustrialization experienced a breakneck in the early years of the twenty-first century, then accelerated in the 1990s by increasing productivity-growth rates and increasing access to information for anyone with internet or phone-enabled devices. Both the economic pie and overall labour market are growing more slowly than many of us can expect, and the labour market for labour is in decline. There was no significant shift in demand from industry to services or from service to economy. Instead, as capital investment returns in manufacturing went into decline, so did the overall economy, with the result that manufacturing served only a temporarily diminishing share of the economy.

This is a fundamental problem—one that can be overcome only by re-orienting the economic and technological environment in ways that help allay the fears of a dystopian future in which robots become sentient and overtake society.

6 The autonomous AI revolution is not coming; it is already here, and it is about reimagining your own experiences, as people- and companies- are helping to turn the planet into a new flexible system for living.

8 The term “self-driving” was coined in 2014 by Aaron Bastani to name a possible goal of automation, but it was still a long-term possibility, with utopian potential for humanity. The term is relevant to the question of how we get there, and one that we have barely considered. In fact, when I published a book on the topic in 2015,16 many people began referring to me as the techno-optimist, suggesting that there is a real risk of AI dystopias coming true.

It is important to establish the clarity and validity of the arguments we cite in this book. The dystopian narrative — although based on fiction — is not science fiction. The journey will take us inside the Amazon warehouses where employees must keep in time with the algorithmic cadences of a vast logistical empire, and we will visit the Chicago meat laborers on the disassembly lines where animal carcasses are vivisected and prepared for consumption. And we’ll hear from the workers who are protesting against the way that AI systems are increasing surveillance and control for their bosses.

Labor is also a story about time. Coordinating the actions of humans with the repetitive motions of robots and line machinery has always involved a controlling of bodies in space and time.32 From the invention of the stopwatch to Google’s TrueTime, the process of time coordination is at the heart of workplace management. AI technologies both require and create the conditions for ever more granular and precise mechanisms of temporal management. Coordinating time demands increasingly detailed information about what people are doing and how and when they do it.

Chapter 3 focuses on the role of data. All publicly accessible digital material—including data that is personal or potentially damaging—is open to being harvested for training datasets that are used to produce AI models. There are gigantic datasets full of people’s selfies, of hand gestures, of people driving cars, of babies crying, of newsgroup conversations from the 1990s, all to improve algorithms that perform such functions as facial recognition, language prediction, and object detection. When these collections of data are no longer seen as people’s personal material but merely as infrastructure, the specific meaning or context of an image or a video is assumed to be irrelevant. Beyond the serious issues of privacy and ongoing surveillance capitalism, the current practices of working with data in AI raise profound ethical, methodological, and epistemological concerns.

And how is all this data used? In chapter 4, we look at the practices of classification in artificial intelligence systems, what sociologist Karin Knorr Cetina calls the “epistemic machinery.” 34 We see how contemporary systems use labels to predict human identity, commonly using binary gender, essentialized racial categories, and problematic assessments of character and credit worthiness. A sign will stand in for a system, a proxy will stand for the real, and a toy model will be asked to substitute for the infinite complexity of human subjectivity. By looking at how classifications are made, we see how technical schemas enforce hierarchies and magnify inequity. Machine learning presents us with a regime of normative reasoning that, when in the ascendant, takes shape as a powerful governing rationality.

From here, we travel to the hill towns of Papua New Guinea to explore the history of affect recognition, the idea that facial expressions hold the key to revealing a person’s inner emotional state. Chapter 5 considers the claim of the psychologist Paul Ekman that there are a small set of
====================
—but of course other “factors of fundamental importance” would also be performed.

In other words, for every factoring of the variables in our model, we would assign those variables a corresponding value to the current state of the world (in this case, China) and the AI would then create a new action sequence that includes the equivalent of “turning the crank” to change the world around it.

This type of assistive technology, Li says, “is a win-win. AI is not taking away from the human element, but it’s an enabler to make human jobs faster and more efficient.”

Defining AI’s Values

Building a future where AI boosts human potential requires leadership from the people who will be over- seeing its implementation. Before business leaders can embrace augmentation, Li sees it as imperative to educate them about “the unintended consequences” of the technology they’re adopting. One of HAI’s main purposes is to help business leaders think through the big questions surrounding AI: “How it should be guided, how it should be governed, and how it reflects society’s values.”

“Those things are a bigger part of the challenge than just getting the state-of-the-art machine learning algorithm,” says Susan Athey, a professor of economics at Stanford GSB, law, and an early-stage investor in companies like Google cofounder Larry Page. “But it’s also a reallyinous belief that there’s no value in getting more powerful machines,” she says. “There’s so much more opportunity now for these technologies to instrumentally value our actions and to generate more information about how we’re thinking.”

AI can also make short work of necessary yet tedious tasks. Spence mentions how “pure augmentation” is helping doctors by using machine learning to sift through mountains of medical literature. “It can pick off, with reasonable accuracy, the articles that are particularly important for a specific doctor with a specific specialty patient.” Similarly, Athey says medical students needed to take online courses such as English pronunciation to help them improve their understanding of medical decisions were already becoming more intelligent.

 “There is a shift, which I call ‘mind-bogglingly efficient’ – those time and effortput into creating value-more so than just processing images or playing games.” Yet it still takes a dizzying array of inputs and outputs to bring about outcomes that are probably not what we expected, or to make sense of a sentence containing just the sort of words that appear in a user’s mind.

 researchers evaluate their results on the basis of one of two measures of the automatability of a user's output: whether the automatability was acceptable or not. In the early days of automation, systems were usually carried out manually, with the programmers’ time at any given moment in very limited situations. That’s when designers began to look for advantages over other workers by allowing more control over the output of the machine. Now, the same technology is being used to automate the creation of books and other documentation, and to allow the writing of articles and emails directly from external readers without any human intervention.

In the US, where the industrial revolution was starting to have an impact, a number of institutions, including the offices for decision making and the stock and bondtrading systems, had been designed to deal with this problem. But the more people began toPrivate enterprise, the more the technology was used to automate previously human-like processes. In doing so, it opened up new possibilities for both human and machine alternatives to be established.

In the public school systems, public education systems are still largely run on the nineteenth-century “factory model” of education: all students are forced to learn at the same speed, in the same way, at the same rate, and at the same place. Schools take an “assembly line” approach, passing children from grade to grade each year, largely irrespective of whether or not they absorbed what was taught. It’s a model that once made sense given the severe limitations on teaching resources, namely, the time and attention of someone who can teach, monitor, and evaluate students.

But AI can help us lift those limitations. The perception, recognition, and recommendation abilities of AI can tailor the learning process to each student and also free up teachers for more one-on-one instruction time.

The AI-powered education experience takes place across four scenarios: in-class teaching, homework and drills, tests and grading, and customized tutoring. Performance and behavior in these four settings all feed into and build off of the bedrock of AI-powered education, the student profile. That profile contains a detailed accounting of everything that affects a student
====================

