OUTPUT 022							AUTOMATA - GPT-2						 EPOCHS : 24000



based ‘bounty’ and the ‘step-up basis’ for new decisions. Even when a firm’s stock price eventually settled near $1,000, the company was still not able to predict exactly when or where it might show the biggest bonanza. As a result, the company decided to experiment with automating the process of recommending and hiring workers. If automation had worked to drive profits in product recommendation and warehouse organization, it could, the logic went, make hiring more efficient. In the words of one engineer, “They literally wanted it to be an engine where I’m going to give you 100 resumes, it will spit out the top five, and we’ll hire those.”21 The machine learning system was designed to rank people on a scale of one to five, mirroring Amazon’s system of product ratings. To build the underlying model, Amazon’s engineers used a dataset of ten years’ worth of résumés from fellow employees and then trained a statistical model on fifty thousand terms that appeared in those résumés. Quickly, the system began to assign less importance to commonly used engineering terms, like programming languages, because everyone listed them in their job histories. Instead, the models began valuing more subtle cues that recurred on successful applications. A strong preference emerged for particular verbs. The examples the engineers mentioned were “executed” and “captured.”

Recruiters starting using the system as a supplement to their usual practices.23 Soon enough, a serious problem emerged: the system wasn’t recommending women. It was actively downgrading résumés from candidates who attended women’s colleges, along with any résumés that even included the word “women.” Even after editing the system to remove the influence of explicit references to gender, the biases remained. Proxies for hegemonic masculinity continued to emerge in the gendered use of language itself. The model was biased against women not just as a category but against commonly gendered forms of speech.

Inadvertently, Amazon had created a diagnostic tool. The vast majority of engineers hired by Amazon over ten years had been men, so the models they created, which were trained on the successful résumés of men, had learned to recommend men for future hiring. The employment practices of the past and present were shaping the hiring tools for the future. Amazon’s system unexpectedly revealed the ways bias already existed, from the way masculinity is encoded in language, in résumés, and in the company itself. The tool was an intensification of the existing dynamics of Amazon and highlighted the lack of diversity across the AI industry past and present.

Amazon ultimately shut down its hiring experiment. But the scale of the bias problem goes much deeper than a single system or failed approach. The AI industry has traditionally understood the problem of bias as though it is a bug to be fixed rather than a feature of classification itself. The result has been a focus on adjusting technical systems to produce greater quantitative parity across disparate groups, which, as we’ll see, has created its own problems.

Understanding the relation between bias and classification requires going beyond an analysis of the production of knowledge—such as determining whether a dataset is biased or unbiased—and, instead, looking at the mechanics of knowledge construction itself, what sociologist Karin Knorr Cetina calls the “epistemic machinery.”25 To see that requires observing how patterns of inequality across history shape access to resources and opportunities, which in turn shape data. That data is then extracted for use in technical systems for classification and pattern recognition, which produces results that are perceived to be somehow objective. The result is a statistical ouroboros: a self-reinforcing discrimination machine that amplifies social inequalities under the guise of technical neutrality.

The Limits of Debasing Systems

To better understand the limitations of analyzing AI bias, we can look to the attempts to fix it. In 2019, IBM tried to respond to concerns about bias in its AI systems by creating what the company described as a more “inclusive” dataset called Diversity in Faces (DiF).26 DiF was part of an industry response to the groundbreaking work released a year earlier by researchers Joy Buolamwini and Timnit Gebru that had demonstrated that several facial recognition systems—including those by IBM, Microsoft, and Amazon—had far greater error rates for people with darker skin, particularly women. As a result, efforts were ongoing inside all three companies to show progress on rectifying the problem.

“We expect face recognition to work accurately for each of us,” the IBM researchers wrote, but the only way that the “challenge of diversity could be solved” would be to build “a data set comprised from the face of every person in the world.”28 IBM’s researchers decided to draw on a preexisting
====================
The typical structure of an episode in the ongoing AI bias narrative begins with an investigative journalist or whistleblower revealing how an AI system is producing discriminatory results. The story is widely shared, and the company in question promises to address the issue. Then either the system is superseded by something new, or technical interventions are made in the attempt to produce results with greater parity. Those results and technical fixes remain proprietary and secret, and the public is told to rest assured that the malady of bias has been “cured.”20 It is much rarer to have a public debate about why these forms of bias and discrimination frequently recur and whether more fundamental problems are at work than simply an inadequate underlying dataset or a poorly designed algorithm.

One of the more vivid examples of bias in action comes from an insider account at Amazon. In 2014, the company decided to experiment with automating the process of recommending and hiring workers. If automation had worked to drive profits in product recommendation and warehouse organization, it could, the logic went, make hiring more efficient. In the words of one engineer, “They literally wanted it to be an engine where I’m going to give you 100 resumes, it will spit out the top five, and we’ll hire those.”21 The machine learning system was designed to rank people on a scale of one to five, mirroring Amazon’s system of product ratings. To build the underlying model, Amazon’s engineers used a dataset of ten years’ worth of résumés from fellow employees and then trained a statistical model on fifty thousand terms that appeared in those résumés. Quickly, the system began to assign less importance to commonly used engineering terms, like programming languages, because everyone listed them in their job histories. Instead, the models began valuing more subtle cues that recurred on successful applications. A strong preference emerged for particular verbs. The examples the engineers mentioned were “executed” and “captured.”

Recruiters starting using the system as a supplement to their usual practices.23 Soon enough, a serious problem emerged: the system wasn’t recommending women. It was actively downgrading résumés from candidates who attended women’s colleges, along with any résumés that even included the word “women.” Even after editing the system to remove the influence of explicit references to gender, the biases remained. Proxies for hegemonic masculinity continued to emerge in the gendered use of language itself. The model was biased against women not just as a category but against commonly gendered forms of speech.

Inadvertently, Amazon had created a diagnostic tool. The vast majority of engineers hired by Amazon over ten years had been men, so the models they created, which were trained on the successful résumés of men, had learned to recommend men for future hiring. The employment practices of the past and present were shaping the hiring tools for the future. Amazon’s system unexpectedly revealed the ways bias already existed, from the way masculinity is encoded in language, in résumés, and in the company itself. The tool was an intensification of the existing dynamics of Amazon and highlighted the lack of diversity across the AI industry past and present.

Amazon ultimately shut down its hiring experiment. But the scale of the bias problem goes much deeper than a single system or failed approach. The AI industry has traditionally understood the problem of bias as though it is a bug to be fixed rather than a feature of classification itself. The result has been a focus on adjusting technical systems to produce greater quantitative parity across disparate groups, which, as we’ll see, has created its own problems.

Understanding the relation between bias and classification requires going beyond an analysis of the production of knowledge—such as determining whether a dataset is biased or unbiased—and, instead, looking at the mechanics of knowledge construction itself, what sociologist Karin Knorr Cetina calls the “epistemic machinery.”25 To see that requires observing how patterns of inequality across history shape access to resources and opportunities, which in turn shape data. That data is then extracted for use in technical systems for classification and pattern recognition, which produces results that are perceived to be somehow objective. The result is a statistical ouroboros: a self-reinforcing discrimination machine that amplifies social inequalities under the guise of technical neutrality.

The Limits of Debasing Systems

To better understand the limitations of analyzing AI bias, we can look to the attempts to fix it. In 2019, IBM tried to respond to concerns about bias in its AI systems by creating what the company described as a more “inclusive” dataset called Diversity in Faces (DiF).26 DiF was part of an industry response to the groundbreaking work released a year earlier by researchers Joy Buolamwini and Timnit Gebru that had demonstrated that several facial recognition systems—including those by IBM, Microsoft, and Amazon—
====================
The software is based on the extractive politics of artificial intelligence—the lies and goals of machine learning—and while its goals are likely to be self-fulfilling, it could still be preferable to retain some of these preferences.

The preference problem

The first principle: Purely altruistic machines

The first principle, that the machine’s only objective is to maximize the realization of human preferences, is central to the notion of a “preference-maximizing machine.”35 This notion is distilled from the idea that human choices should be subject to what the machine is most motivated to choose. The agent determining the final goal should be motivated by the principle of preference autonomy, the institutional structure in which the machine is placed, and the fact that the machine is designed to seek human approval before anything is done.

The example highlights the potential of such a machine emerging right now. To date, I have been the recipient of almost USD 100,000 from people applying for job offers across the world. This has been a troubling development for those who have been following the story of the internet, but a call for real action before it becomes a pretext for discrimination.

There are several reasons why the countries that have succeeded in attaining a high degree of robotization are not worse than those in Europe or the United States. For one thing, the market does not automatically take over. There will be new abstractes, laws, and technical disciplines that will be crucial in helping robotics companies to bring new business models to market.

Second, the characteristics of being able to innovate on your own is a strong advantage.iating a advantage from an existing market—if you don’t have enough employees to do an AI-powered project—isn’t so great until you have enough algorithms. Google, Baidu, Uber, Didi, Tesla, and many more are building teams, testing technologies, and gathering data en route to taking human drivers entirely out of the equation.

The leaders in that race—Google, through its self-driving self-driving cars, and Tesla—represent two different philosophies for managing the global AI market. Governing is where the master algorithms come back to earth, while managing is where the latest research comes together. It’s a delicate balance: the balance between doing everything possible to minimize the amount of pain and suffering that an AI-processor can cause. But that balance can’t be made easily because there are so many parts of the business world. AI handles politics, personal fakes, and death, but it’s also extremely hard to make a judgment on what the other two alternatives are. At the same time, there are so many “ buttons” that can be pressed to get a decision right, whereas any helpful guidance will just be a rush of paperwork and debugging to accomplish the AI’s task.

The good news is that both the old approach and the new approach is leading to prosperity.

In the past, when digital tools were used mainly to automate existing processes, companies had no missing middle to fill. But now, with increasingly sophisticated AI technologies that enable human-machine collaborations, developing teams can actually work collaboratively with one another. Doing so starts with the approach that many of the companies described in part one of the book are already taking. They think of AI as an investment in human talent first and technology second. They value workers who are adaptable, entrepreneurial, and open to retraining. Then these companies provide support to ensure that their workers and AI systems can succeed together. In doing so, they lay the foundation for adaptable, robust business processes capable of withstanding economic shocks and increasing the -rate of technological change.

Leveraging the missing middle is one of the main components needed to reimagine business processes, but another key component is revamping the concept of the process itself. Businesses need to shift from seeing processes as collections of sequential tasks. I n the age of AI, processes become more dynamic and adaptable. Instead of visualizing a process as a collection of nodes along a straight line, say, it might help to see it as a sprawling network of movable, reconnectable nodes or perhaps something with a hub and spokes. The linear model for process no longer cuts it.

In addition to developing the missing middle and rethinking process fundamentals, businesses need to have management address the challenges of reimagining process with an awareness of responsible AI . It's important that executives not only provide the training required for people who are learning new skills; they must also consider the various ethical, moral, and legal issues associated with the AI systems that their organizations deploy. Key questions include:

As a publicly traded company, what obligations do we have to our shareholders, employees, and larger society to ensure that we deploy AI for good and not harm?

If we use AI in a new process, how can we do it in compliance with laws and regulations like General
====================
My background in technology and business expertise in the field of artificial intelligence has been on campuses and in the media as a venture capitalist. That expertise, combined with my obligatory literacy in the media, has turned my interests into thinking different things.

Today, twenty-five years older and hopefully a bit wiser, I see a world in which companies, citizens, and users all want to be better proud. I believe that in a few short years, most meaningful companies will be obsolete. But there are those who will have to change that mentality—and change the business processes, business models, and values that have been constructed to achieve this objective.

The goal, above all, is to be able to develop a single point where an outcome that benefits all people can be achieved. This is not something that can be done in a matter of weeks. Fully automating the AI model will require something different than simply automating the human-machine relationship. To understand why, we must first grasp the basics of the technology and how it is set to transform our world.

WHAT HARDWARE, BETTER, FASTER, STRONGER

The past few years have seen the emergence of a new kind of consumer AI, one that absorb sophisticated AI systems but aren’t fond of Echo-based biases. That’s a good thing, but it also presents a global challenge because it brings different perspectives and choices to the table. When you honestly assess the strengths of human and machine, and what they do well when we complement each other’s skills, a whole new world of possibilities emerges for running a business and designing your business processes-that is, the important mindset part of MELDS.

There are many reasons people may prefer not to use an AI system: the system can impersonate human labor; it may replace or replace on the market any knowledge that a human has; it may replace some or all of its previous functions; it may replace online forms of communication with other systems; or it may replace some or all of its users.

MH: I suppose the third wave involves becoming more and more dependent on computers. In contrast, I would expect that from the outset to produce steady improvements in interpersonal skills, or steady improvement in organizations, not rapid or explosive improvement in industries. However, the goal here is not to try and achieve general AI as a novelty, but rather to understand what is happening as it goes. While I was thinking of the novel idea of using AI to discover new materials, a major discovery was that all of the previously mentioned discoverable objects had similar characteristics to one another.

Hoffman: I think this is a good start. I hope to take part in this new field of research.

Nagy: I imagine it’ll get started very quickly.

McCarthy responding to those concerns by describing processes that were started because of a technology VC could “allow” people to speed up or slow down their progress, or both. McCarthy reminisced that “I first became interested in AI a few years ago [at the University of Michigan] when I was having an issue with one of the early founders of AI.”11 As an MIT Ph.D. student growing up in the United States, my mother moved to Tennessee to return to her hometown of Hangzhou where she began a successful education project.

But with the advent of smartphones and AI, dynamics of family time became a serious issue. Peeples—like so many young people growing up in the valley—become so used to working that they forget what it feels like to be apart of a larger organization.

Peeples’ parents had become so used to working that they gave themselves over to the idea of “interrupting” the work process, something they call “human intrinsic motivation.” In other words, the goal to maximize your impact and the process to develop an AI model is your own intrinsic motivation. You can do whatever you want to do. If you don’t like what you see or hear, you can turn to bypassing it and produce your own improved version.

But with the help of the company’s algorithmic writing capabilities, I was able to develop a prototype AI system that could develop all of the components needed for self-improvement within a matter of hours.

“I think we could do this well,” it said, handing me the clock a minuteglass. “But don’t you think we could do this well?”

Without thinking, I reflexively gave it an answer I would have given to myself and others. That’s a good reason for saying that AI is not powerful, but still, I don’t think we yet know how to machine it.

We will soon be teaching ourselves new tricks of the trade when it comes to thinking. I am sure you already know what I mean
====================
s, they are moving beyond the traditional thinking of simply digitalizing the old static maps. We areagineining (with help from natural language processing and fine-cale inference) the equanimity of factory run organizations into an reimagined, human-like workplace. This would not be a matter of " falling rule list countries" or hierarchy of " Facing Middle East or Africa" but of making each company’s business work differently from one another.

The degree of accountability for cross-functional integration depends on the business process. In some industries, the line between manager and employee is based on know-how from one worker to another. Middle-level managers developed an implicit culture of repeat monitoring, where workers develop know-how and value for the organization. This knowledge-how central to workplace performance also becomes important for performance evaluations, as people make decisions and learn about the implications of various effects.

The organizations studied include (note that many of these are not isolated cases: for example, customer service) as well as leading firms in fields such as finance, government, and safety.

The organizations studied include (note that many of the research institutions are considered by many experts as “ principal centers for AI development”)

McGill University (colocated in Detroit) has a Ph.D. in economics and management from prestigious Tsinghua University. In 2016, he launched the China-U.S. AI Panel, a set of experts paneled together to study the future of global AI. They include top executives at Baidu, Alibaba, Tencent, and ByteDance, as well as some of the world’s largest corporations.

There’s no know-how in Silicon Valley, and probably far too little time left to do something truly new. But whatever that new thing is, it’s likely to be something far more impressive: a new generation of profitable internet companies whose mission is to “provide the top-flight medical diagnosis services through a computer.” Just like the others, these companies will have an enormous army of doctors who they can then connect all their patients to a computer system.

This machine-learning-driven superstructure will be invaluable in all the right ways. It could be used to deliver diagnoses at a rate of ten per minute, delivered via a large-screen television at the hospital, to a distant relative, or delivered at an affordable price.

But the good news is: the same process that�s currently used to route requests at medical centers around the world—reciprocal collaboration—will function as the perfect naturallanguage engine of automated health care. Just as a company like Microsoft Office already does with its One machine, bringing the One app to life with human agents, Office already does a series of things that ensure people receive the services they need. For instance, a customer might be refused a flight to Chicago, or a customer might be detained because a staff member asked if it was “a problem.” or because an algorithm told him that one of his customers was “high-risk.” or because an algorithm told him that one of his customers was involved in a criminal matter.

All of these approaches—high-risk, high-reward—would accomplish the objective, but would they guarantee that theAI would achieve the objective?

Theos engineer Asimov’s motivation for creating the language he called “Intelligent Computer.” was one of the founding ideas of computer science. In it he proposed what he called the “control problem” or “value alignment problem,” and his ideas remained relevant even after the project was concluded.

A related misunderstanding is that the purpose of the book is to help people develop the necessary understanding and intuitions to guide their cognitive system design. Even better, it is all-consuming work, so people need to get over any difficulties or simply accept it for what it is: a computer.

That's where the human element truly comes in. As we touch with other sentient entities, we must also understand that they are sentient beings, conscious agents who choose and reveal their will. That requires us to place our own purposes into that of the recipient.

I have a different vision. I don’t want to live a life of total dependence on technology oruevers. But my deep concern is that we humans will all become dependent on the technologies that will make it possible to live. If we remain trapped in a mindset that equates our labor with that of other people, that will only reinforce the feeling—yes, even a part of the environment—that we are in an age of intelligent change.

The basic insight on which this argument relies was captured by Will Oremus, a creator in the philosophy of mind (and perhaps also of physics) philosopher. Oremus claimed that the contemplation of the omniscience of the body presupposed a certain dependence on
====================
The question is whether we are living in a post-transition society or an age of radical digital computation. Will we control our digital engines? Will we join the dots in the age of AI? or will we risk getting hurt?

The technical cloud is a quadrilateral of everything: the internet, machines, data, and capital. But the quadrant of this is easily accessed, because it is a vector which can be thought of as a line drawing. Spurgeon Avenue has always been a case of neglecting the physical realities and realities of our times, but it has always been partial and incomplete. We do not expect to go slow. We don’t expect to look radically different from when we did.

What this means is that if we could somehow establish that physical reality is the source of all good and valuable qualities, then it will in principle allow us to control our digital engines. But this does not mean that it would not behave in a similarly way in reality. There are two major problems with this line of thinking. The first is that it ignores the psychic cost of living under a system of intrusive monitoring and coercion, what we would now call taxation. The second is that it orders the system to transform itself into a form of conscious experience—a highly complex and powerful machinery that looks at the psychic cost of living under a system of intrusive monitoring and coercion. Imagine how miserable you would be if you had onlyices in life.

Imagine this scenario: The new technology becomes the source of reprogramming for various new problems. But what happens if you replace all the paperclips with a digital one? Then what? You then get a seed AI—perhaps a form of artificial intelligence but not superintelligent. But still, the same issues arise: what kind of knowledge storage, what kind of processing, what kind of data base, then come the cognitive preferences that come into being as you go about defining goals? What kind of observations should be done to create such systems?

This chapter discusses several alternative metrics. Tim Berners- Lee’s invention of the World Wide Web in 1989, to take an obvious example, described exactly how once-upon-one systems are creating enormous amounts of dataclips. And yet, because man’s entire being depends on the Internet, he was able to develop an unlikely technology: he said “The Internet.” Perhaps he was inspired by the phrase “New thinking machines.” But as far as he is concerned, the Internet is taking over the job of the machine and the world.

New issues arise when it comes to understanding our relation to one another every day. How do we unpack what is and is not meaningful to understand these datasets? How does one communicate warnings like, “This dataset likely reflects skews related to its reliance on news stories about South American terrorists in the 1980s”? Or does it reflect existing patterns and considerations that might be considered when designing new infrastructure, such as the number of people or the degree of social integration required to attain a given level of risk?

ChatGPT, with its human-like writing abilities, and OpenAI’s other recent release DALL-E 2, which generates images on demand, use large language models trained on huge amounts of data. The same is true of rivals such as Claude from Anthropic and Bard from Google. These so-called foundational models, such as GPT-3.5 from OpenAI, which ChatGPT is based on, or Google’s competing language model LaMDA, which powers Bard, have evolved rapidly in recent years.

They keep getting more powerful: they’re trained on ever more data, and the number of parameters—the variables in the models that get tweaked—is rising dramatically. Earlier this month, OpenAI released its newest version, GPT-4. While OpenAI won’t say exactly how much bigger it is, one can guess; GPT-3, with some 175 billion parameters, was about 100 times larger than GPT-2.

But it was the release of ChatGPT late last year that changed everything for many users. It’s incredibly easy to use and compelling in its ability to rapidly create human-like text, including recipes, workout plans, and— perhaps most surprising—computer code. For many non-experts, including a growing number of entrepreneurs and businesspeople, the user-friendly chat model—less abstract and more practical than the impressive but often esoteric advances that been brewing in academia and a handful of hightech companies over the last few years—is clear evidence that the AI revolution has real potential.

Venture capitalists and other investors are pouring billions into companies based on generative AI, and the list of apps and services driven by large language models is growing longer every day.

Among the big players, Microsoft has invested a reported $10 billion in OpenAI and its ChatGPT, hoping the
====================
In the short run, this is a bad tradeoff. The Fordist engineers who made this possible gave no thought to the problem of retraining an army of humans or of doing more emulations. They decided on a fleet of vehicles–machines–that could perform all the tasks that human beings could perform. They said, “We’ll build these machines to be able to perform all the tasks that human beings can perform.”

This kind of assistive technology is the product of two transitions: the introduction of machine learning, and the subsequent construction of a virtuous cycle of human-machine collaborations.

The introduction of machine learning depends not only on the degree to which we know how to train our systems, but also on the degree to which we actually manage them. In other words, the task of AI is not to automate everything, but to manage them to perform tasks that are tailored for human preferences and needs.

The first-cut approach is often viewed as high-risk and thus represents a tradeoff between automation and wages. However, the case for favoring a high-risk approach is driven by two sources: labor costs and efficiency gains from labor discipline. The amount of time put into AI labor may not be the same as that of a human worker. While a human can do X for X years, a machine can do Y for Y years, depending on how the market values the output of such technologies. The same technology can also be thought of as a kind of digital Swiss Army knife: when it comes to labor, we’re all like the humans.

We’re all full of questions without answers, trying to peer into the future with a mixture of ruthless efficiency and unconsciousness. We’re all thinking the same way–if we put the wrong values into a machine, we’ll get the wrong answer. That’s why we wrote this book: we wanted to write a book about the search for universal intelligence, and the second part of that process will involve some inter-war pangs in our minds.

In the interim, we have to face the fact that there are far too many other opportunities to have common intelligence with machines. And since there are far too many of us, the machine will need to learn more about us first.

In the book, I discuss the nature and extent of collaboration's role inArtificial Intelligence. But it should also be understood in a wider context: there are long-term historical trends in the production of digital data and information technology, and the subsequent decades of complementary innovations, such as machine learning, machine learningPA, and data science, will be increasingly dominated by those who care for the elderly or the elderly. This is a period in which the entitled “contemporary AI” movement becomes radical and demands that the global technological and financial structures be gradually transformed by the application of AI to solve the age of our suffering.

The vision of the modern age given by science offers a vision of an emancipated future. This vision should not be left to the machines, for it is already turning out that there is a danger of turning into a tyranny on every side.

Science and technology strategy

Before we zoom in on issues specific to machine superintelligence, we must introduce some strategic concepts and considerations that pertain to scientific and technological development more generally.

Differential technological development

Suppose that a policymaker proposes to cut funding for a certain research field, out of concern for the risks or long-term consequences of some hypothetical technology that might eventually grow from its soil. She can then expect a howl of opposition from the research community.

Scientists and their public advocates often say that it is futile to try to control the evolution of technology by blocking research. If some technology is feasible (the argument goes) it will be developed regardless of any particular policymaker’s scruples about speculative future risks. Indeed, the more powerful the capabilities that a line of development promises to produce, the surer we can be that somebody, somewhere, will be motivated to pursue it. Funding cuts will not stop progress or forestall its concomitant dangers.

Interestingly, this futility objection is almost never raised when a policymaker proposes to increase funding to some area of the economy, even though the argument would seem to cut both ways. One rarely hears indignant voices protest: “Please do not increase our funding. Rather, make some cuts. Researchers in other countries will surely pick up the slack; the same work will get done anyway. Don’t squander the public’s treasure on domestic scientific research!”

What accounts for this apparent doublethink? One plausible explanation, of course, is that members of the research community have a self-serving bias which leads us to believe that research is always good and tempts us to embrace almost any argument that supports our demand for more funding. However, it is also possible that
====================
. This kind of information extraction, too, is an area where AI has a long history; for example, AI systems have been trained to recognize clinical details in doctors’ notes. Early indications are that large language models are fairly good at recognizing financial information in texts such as investor call transcripts. While it remains an open challenge in the field, they may even be capable of writing out multi-step plans based on descriptions in free text.

Machines as strategists

The last piece of the puzzle is a lobbying strategizer to figure out what actions to take to convince lawmakers to adopt the amendment.

Passing legislation requires a keen understanding of the complex interrelated networks of legislative oﬃces, outside groups, executive agencies, and other stakeholders vying to serve their own interests. Each actor in this network has a baseline perspective and diﬀerent factors that influence that point of view. For example, a legislator may be moved by seeing an allied stakeholder take a firm position, or by a negative news story, or by a campaign contribution.

It turns out that AI developers are very experienced at modeling these kinds of networks. Machine-learning models for network graphs have been built, refined, improved, and iterated by hundreds of researchers working on incredibly diverse problems: lidar scans used to guide self-driving cars, the chemical functions of molecular structures, the capture of motion in actors’ joints for computer graphics, behaviors in social networks, and more.

In the context of AI-assisted lobbying, political actors like legislators and lobbyists are nodes on a graph, just like users in a social network. Relations between them are graph edges, like social connections. Information can be passed along those edges, like messages sent to a friend or campaign contributions made to a member. AI models can use past examples to learn to estimate how that information changes the network. Calculating the likelihood that a campaign contribution of a given size will flip a legislator’s vote on an amendment is one application.

McKay’s work has already shown us that there are significant, predictable relationships between these actions and the outcomes of legislation, and that the work of discovering those can be automated. Others have shown that graphs of neural network models like those described above can be applied to political systems. The full-scale use of these technologies to guide lobbying strategy is theoretical, but plausible.

Put together, these three components could create an automatic system for generating profitable microlegislation. The policy proposal system would create millions, even billions, of possible amendments. The impact assessor would identify the few that promise to be most profitable to the client. And the lobbying strategy tool would produce a blueprint for getting them passed.

What remains is for human lobbyists to walk the floors of the Capitol or state house, and perhaps supply some cash to grease the wheels. These final two aspects of lobbying—access and financing—cannot be supplied by the AI tools we envision. This suggests that lobbying will continue to primarily benefit those who are already influential and wealthy, and AI assistance will amplify their existing advantages.

The transformative benefit that AI oﬀers to lobbyists and their clients is scale. While individual lobbyists tend to focus on the federal level or a single state, with AI assistance they could more easily infiltrate a large number of state-level (or even local-level) law-making bodies and elections. At that level, where the average cost of a seat is measured in the tens of thousands of dollars instead of millions, a single donor can wield a lot of influence—if automation makes it possible to coordinate lobbying across districts.

How to stop them

When it comes to combating the potentially adverse eﬀects of assistive AI, the first response always seems to be to try to detect whether or not content was AI-generated. We could imagine a defensive AI that detects anomalous lobbyist spending associated with amendments that benefit the contributing group. But by then, the damage might already be done.

In general, methods for detecting the work of AI tend not to keep pace with its ability to generate convincing content. And these strategies won’t be implemented by AIs alone. The lobbyists will still be humans who take the results of an AI microlegislator and further refine the computer’s strategies. These hybrid human-AI systems will not be detectable from their output.

But the good news is: the same strategies that have long been used to combat misbehavior by human lobbyists can still be eﬀective when those lobbyists get an AI assist. We don’t need to reinvent our democracy to stave oﬀ the worst risks of AI; we just need to more fully implement long- standing ideals.

First, we should reduce the dependence of legislatures on monolithic, multi-thousand-page omnibus bills voted on under deadline. This style of legislating exploded in the 1980s and 1990s and continues through to the most recent
====================
, and many other “technologies” are already developing or have experienced widespread use of backpropagation. Backpropagation is simply an efficient method for computing how changing the weight of any given synapse would affect the difference between the way the synapses actually behave. It might be thought that by constructing a3-dimensional structure from lower dimensional special-purpose elements, such as roots, parallel to the input vectors, and edges, the newtuition would produce a new structure like the one discovered in the 1950s. But the elegantly poised connective qualities of neural networks only extend from this basic understanding.

In the next chapter, we will look at different paths that may lead to humanlevel machine intelligence. But let us note at the outset that however many stops there are between here and human-level machine intelligence, the latter is not the final destination. The next stop, just a short distance farther along the tracks, is superhuman-level machine intelligence. The train might not pause or even decelerate at Humanville Station. It is likely to swoosh right by.

The mathematician I. J. Good, who had served as chief statistician in Alan Turing’s code-breaking team in World War II, might have been the first to enunciate the essential aspects of this scenario. In an oft-quoted passage from 1965, he wrote:

Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.

It may seem obvious now that major existential risks would be associated with such an intelligence explosion, and that the prospect should therefore be examined with the utmost seriousness even if it were known (which it is not) to have but a moderately small probability of coming to pass. The pioneers of artificial intelligence, however, notwithstanding their belief in the imminence of human-level AI, mostly did not contemplate the possibility of greater-thanhuman AI. It is as though their speculation muscle had so exhausted itself in conceiving the radical possibility of machines reaching human intelligence that it could not grasp the corollary—that machines would subsequently become superintelligent.

The AI pioneers for the most part did not countenance the possibility that their enterprise might involve risk.11 They gave no lip service—let alone serious thought—to any safety concern or ethical qualm related to the creation of artificial minds and potential computer overlords: a lacuna that astonishes even against the background of the era’s not-so-impressive standards of critical technology assessment.12 We must hope that by the time the enterprise eventually does become feasible, we will have gained not only the technological proficiency to set off an intelligence explosion but also the higher level of mastery that may be necessary to make the detonation survivable.

But before we turn to what lies ahead, it will be useful to take a quick glance at the history of machine intelligence to date.

Seasons of hope and despair

In the summer of 1956 at Dartmouth College, ten scientists sharing an interest in neural nets, automata theory, and the study of intelligence convened for a sixweek workshop. This Dartmouth Summer Project is often regarded as the cockcrow of artificial intelligence as a field of research. Many of the participants would later be recognized as founding figures. The optimistic outlook among the delegates is reflected in the proposal submitted to the Rockefeller Foundation, which provided funding for the event:

We propose that a 2 month, 10 man study of artificial intelligence be carried out…. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines that use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.

In the six decades since this brash beginning, the field of artificial intelligence has been through periods of hype and high expectations alternating with periods of setback and disappointment.

The first period of excitement, which began with the Dartmouth meeting, was later described by John McCarthy (the event’s main organizer) as the “Look, Ma, no hands!” era. During these early days, researchers built systems designed to refute claims of the form “No machine could ever do X!” Such skeptical claims were common at
====================
The third wave of business process transformation will come not from higher-income countries, but rather from a broader range of higher-income countries, according to the McKinsey Global Institute. The report notes that the United States and China are the two countries that emerged for themselves by making themselves truly universal basic income (or UBI). In the context of AI-assisted socializing, the United States and China will lead the way in economically productive applications of AI, alongside Japan and the EU.

But what makes the United States and China lead in economically productive applications of AI is not simply their shared common enemy. In the past, China and the United States acted as super powers, emerging powers within months of each other reaching an historic amount of money. In the current era, with AI integrated across so many different pieces of the economy, the power of these technologies can no longer be used as a one-dimensional model of a larger economy with different distribution and complexity of power. Instead, it will be imperative that we reimagine and reinvigorate corporate social responsibility, impact investing, and social entrepreneurship.

In the past, these were the kinds of things that businesspeople merely dabbled in when they had time and money to spare. Sure, they think, why not throw some money into a microfinance startup or buy some corporate carbon offsets so we can put out a happy press release touting it. But in the age of AI, we will need to seriously deepen our commitment to—and broaden our definition of—these activities. Whereas these have previously focused on feel- good philanthropic issues like environmental protection and poverty alleviation, social impact in the age of AI must also take on a new dimension: the creation of large numbers of service jobs for displaced workers.

As a venture-capital investor, I see a particularly strong role for a new kind of impact investing. I foresee a venture ecosystem emerging that views the creation of humanistic service-sector jobs as a good in and of itself. It will steer money into human-focused service projects that can scale up and hire large numbers of people: lactation consultants for postnatal care, trained coaches for youth sports, gatherers of family oral histories, nature guides at national parks, or conversation partners for the elderly. Jobs like these can be meaningful on both a societal and personal level, and many of them have the potential to generate real revenue—just not the 10,000 percent returns that come from investing in a unicorn technology startup.

Kick-starting this ecosystem will require a shift in mentality for VCs who participate. The very idea of venture capital has been built around high risks and exponential returns. When an investor puts money into ten startups, they know full well that nine of them most likely will fail. But if that one success story turns into a billion-dollar company, the exponential returns on that one investment make the fund a huge success. Driving those exponential returns are the unique economics of the internet. Digital products can be scaled up infinitely with near-zero marginal costs, meaning the most successful companies achieve astronomical profits.

Service-focused impact investing, however, will need to be different. It will need to accept linear returns when coupled with meaningful job creation. That’s because human-driven service jobs simply cannot achieve these exponential returns on investment. When someone builds a great company around human care work, they cannot digitally replicate these services and blast them out across the globe. Instead, the business must be built piece by piece, worker by worker. The truth is, traditional VCs wouldn’t bother with these kinds of linear companies, but these companies will be a key pillar in building an AI economy that creates new jobs and fosters human connections.

There will of course be failures, and returns will never match pure technology VC funds. But that should be fine with those involved. The ecosystem will likely be staffed by older VC executives who are looking to make a difference, or possibly by younger VC types who are taking a “sabbatical” or doing “pro bono” work. They will bring along their keen instincts for picking entrepreneurs and building companies, and will put them to work on these linear service companies. The money behind the funds will likely come from governments looking to efficiently generate new jobs, as well as companies doing corporate social responsibility.

Together, these players will create a unique ecosystem that is much more jobs-focused than pure philanthropy, much more impact-focused than pure venture capital. If we can pull together these different strands of socially conscious business, I believe we’ll be able to weave a new kind of employment safety net, all while building communities that foster love and compassion.

BIG CHANGES AND BIG GOVERNMENT

And yet, for all the power of the private market and the good intentions of social entrepreneurs, many people will still fall through the cracks. We need look no further than the gaping inequality and destitute poverty in so much of the world today to recognize that
====================
The history of the field of artificial intelligence written by AI researchers hardly suggests that this was so back in the 1950s. However, back then, AI was a field that was constantly makingiterious improvements to systems it was able to develop. By the time of Deep Fritz, a new language was learning from the ground up. These systems included systems for reading press releases, analyzing press releases for bias, and scheduling conference transcripts. They could do a better job than the previous generation of AI when it came tosearching for answers and resolving disputes, and they were able to do a better job than the previous generation of AI when it came tosearching for solutions and resolving disputes.

That’s when the “AI Bill of Rights” amendment was passed. It was an important Bill of Rights for personnel working in the fields of Research and Development, and also an important Defense-Industry Bill of Rights to help prevent the misuse of AI rights.

The importance of the legitimacy of theensorization industry also derives from two primary responsibilities. First, as alluded to in the previous chapter, the need for accurate, reliable, and effective access to the nation-state is recognized and protected by constitutions and laws. It requires bothqualifications andqualified qualifications for persons in the Union and other countries with a natural desire to promote the legitimacy of theensorization process.

As a result, the legitimacy of theensorization process is seriously threatened. Those who wish to develop or use AI systems that have a significant legal impact upon the Union or the Union international community should be considered users of existing or potential third parties, notably those of a non-high-risk AI system. Those users who do not wish to be identified should seek to find legal channels of action. lawful authorities

(1) the competent authorities established by this Regulation should develop initiatives to facilitate the lawful application of this Regulation (which are related to this Regulation) and to exercise due care to ensure that the AI system in question is a high-risk AI system. Those assisting AI systems should ensure that the complexity of the task, including the risk of accidents related to taking too many actions, is reduced, and new easy-to-mechanical actions are implemented, as described in the section on Safe and Effective AI Systems. Those involved in the safe development of AI systems should be encouraged to create codes of conduct which may be adopted by the competent authorities involved in the supervision of the AI systems.

(2) AI systems should be designed in a safe manner, and with an appropriate number of users, and used with an appropriate number of input parameters. In this regard, AI systems should be designed with user privacy in mind, as described in the section on Safe and Effective AI Systems. Some of the issues addressed by user privacy may also be relevant to other technologies: where possible, user privacy protection should be implemented and clear to users of the AI systems in question; and where appropriate, user privacy programming should be yes-or-no guidance put into place to achieve privacy by design and only use-specific privacy by design safeguards. In cases where both privacy and user control are clearly identified, both of these factors can be taken into account in selecting a suitable privacy practice.

(3) Furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence, could be limited by the procedure for hapless consumers. In hapless consumers, those who are not citizens of one of the countries (such as Brazil and South Korea) can be certain that their rights will be protected. In addition, the trial process and right to a fair trial should also be a fundamental right for the population of the AI systems in question. AI systems used or likely to be placed on the market or otherwise put into service should therefore be given procedural protection, just as they would other legal systems.

(4) It is thus that if digital technologies are used to substitute for physical labor, they would have a significant impact on the life of an worker. The worker's livelihood could be affected; workers who have not had the opportunity to earn a good income due to the introduction of digital technologies.

(5) The notion of AI system should therefore be clearly defined to ensure legal certainty, while providing the flexibility to accommodate future technological developments. The definition should be based on the key functional characteristics of the software, in particular the ability, for a given set of human-defined objectives, to generate outputs such as content, predictions, recommendations, or decisions which influence the environment with which the system interacts, be it in a physical or digital dimension. AI systems can be designed to operate with varying levels of autonomy and be used on a stand-alone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). The definition of AI system should be
====================
The Chinese teenager with the square-rimmed glasses seemed an unlikely hero to make humanity’s last stand. Dressed in a black suit, white shirt, and black tie, Ke Jie slumped in his seat, rubbing his temples and puzzling over the problem in front of him. Normally filled with a confidence that bordered on cockiness, the nineteen-year-old squirmed in his leather chair. Change the venue and he could be just another prep-school kid agonizing over an insurmountable geometry proof.

But on this May afternoon in 2017, he was locked in an all-out struggle against one of the world’s most intelligent machines, AlphaGo, a powerhouse of artificial intelligence backed by arguably the world’s top technology company: Google. The battlefield was a nineteen-by-nineteen lined board populated by little black and white stones
—the raw materials of the deceptively complex game of Go. During game play, two players alternate placing stones on the board, attempting to encircle the opponent’s stones. No human on Earth could do this better than Ke Jie, but today he was pitted against a Go player on a level that no one had ever seen before.

Believed to have been invented more than 2,500 years ago, Go’s history extends further into the past than any board game still played today. In ancient China, Go represented one of the four art forms any Chinese scholar was expected to master. The game was believed to imbue its players with a Zen-like intellectual refinement and wisdom. Where games like Western chess were crudely tactical, the game of Go is based on patient positioning and slow encirclement, which made it into an art form, a state of mind.

The depth of Go’s history is matched by the complexity of the game itself. The basic rules of gameplay can be laid out in just nine sentences, but the number of possible positions on a Go board exceeds the number of atoms in the known universe. The complexity of the decision tree had turned defeating the world champion of Go into a kind of Mount Everest for the artificial intelligence community—a problem whose sheer size had rebuffed every attempt to conquer it. The poetically inclined said it couldn’t be done because machines lacked the human element, an almost mystical feel for the game. The engineers simply thought the board offered too many possibilities for a computer to evaluate.

But on this day AlphaGo wasn’t just beating Ke Jie—it was systematically dismantling him. Over the course of three marathon matches of more than three hours each, Ke had thrown everything he had at the computer program. He tested it with different approaches: conservative, aggressive, defensive, and unpredictable. Nothing seemed to work. AlphaGo gave Ke no openings. Instead, it slowly tightened its vise around him.

THE VIEW FROM BEIJING

What you saw in this match depended on where you watched it from. To some observers in the United States, AlphaGo’s victories signaled not just the triumph of machine over man but also of Western technology companies over the rest of the world. The previous two decades had seen Silicon Valley companies conquer world technology markets. Companies like Facebook and Google had become the go-to internet platforms for socializing and searching. In the process, they had steamrolled local startups in countries from France to Indonesia. These internet juggernauts had given the United States a dominance of the digital world that matched its military and economic power in the real world. With AlphaGo—a product of the British AI startup DeepMind, which had been acquired by Google in 2014—the West appeared poised to continue that dominance into the age of artificial intelligence.

But looking out my office window during the Ke Jie match, I saw something far different. The headquarters of my venture-capital fund is located in Beijing’s Zhongguancun (pronounced “jong-gwan-soon”) neighborhood, an area often referred to as “the Silicon Valley of China.” Today, Zhongguancun is the beating heart of China’s AI movement. To people here, AlphaGo’s victories were both a challenge and an inspiration. They turned into China’s “Sputnik Moment” for artificial intelligence.

When the Soviet Union launched the first human-made satellite into orbit in October 1957, it had an instant and profound effect on the American psyche and government policy. The event sparked widespread U.S. public anxiety about perceived Soviet technological superiority, with Americans following the satellite across the night sky and "tuning in to Sputnik’s radio transmissions. It triggered the creation of the National Aeronautics and Space Administration (NASA), fueled major government subsidies for math and science education, and effectively launched the space race. That nationwide American mobilization bore fruit twelve years later when Neil Armstrong became the first person ever to set foot on
====================
The data from the time-study expert became the object of almost millenarian devotion to back-office operations and the mission of production. In the mythology of the valley, all information and communications technology ( IT) is a lie invented to increase the sense of certainty and control over organizational processes. It has long been implicated in automation and, thereby, in McCarthy’s ideas of self-preservation. 54 This kind of knowledge schema recalls what Friedrich Nietzsche described as “the falsifying of the multifarious and incalculable into the identical, similar, and calculable.”56 The problem of predicting came to be so central to the early purges that they were barely noticeworthy. In the process, the purges were supposed to focus on core functions and not on potential misuses. But as more and more “core functions” were represented as errors on an arithmetic basis, they could become real at a greater distance from the core.

As error-correcting systems become more sophisticated, they are likely to become recalcitrant and gain adherents who are willing torupulous protection of fundamental rights. They will seek to extract fresh measures of quality control from every nook and cranny of our daily lives, such as health insurance, teaching institutions, government Web sites, and the purchase of local government buildings. They will resist attempts to impose unjustified restrictions on the exercise of freedom or the exercise of basic rights.

Finally, the managerial precincts of DrugCorp represented a specific form of power: not the fluid movement of operatorsDOM, but the tightly woven gatherers of an extractive economy. The Mapleton Institute for Advanced Study, funded by the National Science Foundation, organized a workshop on the future of workplace management. It was held at the University of Chicago and was attended by several luminaries of the field. I have already mentioned the 1955 workshop that led to a field of what some called the “ultrastability of data.” fragmentation. This was a vital source of ideas for the field of machine learning and was an important source of ideas for the field of computer science. The Dartmouth workshop that summer also seen the field of computer science work force into action, and it was just twenty miles from Manhattan.

By the mid-1980s, computer science was known for its combinatorial explosion. New approaches to pattern recognition required more discoveries. A strong emphasis was placed upon using learning to inform policy decisions. Methods of search were to be supplanted by systems of reinforcing information if the methods of scientific management over time were any guide.

ighthill’s report, titled “Steps Toward Artificial Intelligence,”4 was given its first major edition of five, and it’s worth describing each one here because it foresaw many other opportunities for technological innovation within the next ten years. The five were designed to encourage each other by tempering their tendency to pervasively strive for the top marginal rates of economic growth.

The first approach tried to charge as high a price as possible for Amazon by giving the customer a discount on any minute product. Rather than taking the full 10 percent of the purchase price of a product and dividing it among all goods, the company divided the discountLifecycle into two equal weights: one that allows the company to increase sales by 20 to 30 percent and the other one that redistributes the customer’s purchase of any given item evenly (all restrictions being eliminated).

The first approach did offer rewards for winning distribution, though, and this was by no means exhaustive: the system learned from experience with other supermarkets and found that the variety was good for dinning. In addition to the ten percent reduction in prices, the company will be able to increase sales by 10 to 30 percent and the sales of items that the consumer likes.

The second approach was based on the same basic principle: the one that increases the volume of production so that it can drive home a more rational message. The goal is to make as many as possible. The process is based on admitting the possibility in the first place that it cannot make as many as possible. What is required of us is a way to let an overwhelming number of individuals have privacy by making simple promises of rewards.

Let us look at some of the ways that this can be done now.

The before-the-minute social media messages sent via bots to friends and family simply fail to do what they should have done, a matter of factly trivialize the situation. Such a bot, Poo, responds with a picture of a person and then dies. The bot, too, would be a social platform: one that could draw on available computing power for analysis and advised others on the same principle. But this approach, obviously, is unworkable and inefficient in the context of AI.

Poo could be good for human users, but it could also be good for all users, depending on what they do and the task at hand. In this case,
====================
And if, in the end, the end result is a catastrophe, how are we to respond?

The simple language of the instructions barrier makes all the difference. As a former business and science major at Stanford University and a Nobel Prize-winning economist, I see no reason to let the size of my impact alone go beyond a reasonable level of investment and marketing. But this strategy won’t work very hard. And it won’t do very well.

In physics, what happens when you try to take the maximum possible value from a reward signal? The idea is simple: you take a reward signal and give it a chance to restructure its reward signal. The process of giving money to a superintelligence is two thousand times more than the sum of the actions of a human and a machine. Moreover, if the AI gives a wrong answer, the wrong answer becomes the equivalent of giving the human a wrong answer. The human can choose to rest assured that the AI will make the right identification.

The human can choose to rest assured that the AI will make the right identification. The machine can choose to rest assured that the human will act safely and beneficially. The strategic uncertainty surrounding human philosophical credence is a major barrier to the use of human reasoning.

With advances in AI, it will become possible to build machines that are virtuous or that choose to protect human values.

The virtuous approach

The virtuous approach to maximizing our turn around.

I am often asked why I think this is even remotely feasible, given the huge momentum behind the standard model in AI and related disciplines. In part because of the ease of including such a variety in our minds, and the way we have made so many tools that are directly related to thinking. However, the term “artificial conscience” (as used by AI researchers) also has some problematic qualifications. conscience is a potential liability on the part of the AI models, whether in the home, in the workplace, or on social media. models with such a nature are not just morally dubious or inefficient but also selfish, making us seem like cogs in a clunky machine.

The need to make humans happy may have its own advantages. Perhaps people will rush to get a certain level of happiness, feeling that this is the point where we really are trying to produce human-level intelligence, which is what AI researchers have become interested in. There may be those who are persuaded by growth diagrams like the ones in Figure 1 that “happiness is enjoyment rather than suffering”—but this is misleading: it refers to both the desire and the prudential reasons for going about our daily lives, not just in the digital realm but in fact all online and offline.

There is a further set of operations for which we are not yet willing to explain. The desire to give greater pleasure to our fellow creatures, or to spend more time with our children, or to count the grains of sand—all of these operations are directly related to preferences expressed in sexual selection. States where there is a high level of happiness reflect this more or less evolution-like condition.

The set of all possible permissible states is too large to include even a very small number of biological persons, for it is not obvious that an average human should have a high point value. This makes the moral case hard to find. There are, however, several reasons to suppose that a seed AI with such a basic capability could see far more than the average human can. It could play the imitation game (imitation game) as well as the imitation game to learn new abilities.

The reason is that being human is a great goal for machines, and this is the main reason the machine will not become another human. (Finding our callingigraphy after all is an enormous problem!) Other things equal, the values described in this book do not require any special understanding of the environment in which we are set. We can think of this environment as our “yard.” It is the cognitive, emotional, and social infrastructure that supports our sense of self-worth. If the values described in this book had been given by a human being, they might not have been pursued by machines with access to expert knowledge. But perhaps the first approximation we get of the human values described in this book is an adequate approximation of the final goal to which we are related by blood relatives.

The reason the book of rules is appropriate for a human being is that even if we were to give it the final goal of making as many paperclips as possible, this would not render the machine an obstacle to humanquestions. It would probably make them an opportunity for a very broad set of very human values.

The book of rules is a kind of foundation for our understanding of how human beings get to know someone. It is a way of predicting, evaluating, and managing various other aspects of the world. The idea is simple enough: develop a scaffold goal that matches the objective in some way to
====================
In the decades after World War II, the British Association for the Advancement of Science held its annual meeting in Leicester. Lord Rutherford addressed the evening session. As he had done several times before, he poured cold water on the prospects for atomic energy: “Anyone who looks for a source of power in the transformation of the atoms is talking moonshine.” Rutherford s speech was reported in the Times of London the next morning (figure 2[b]).

Leo Szilard (figure 2[c]), a Hungarian physicist who had recently fled from Nazi Germany, was staying at the Imperial Hotel on Russell Square in London. He read the Times’ report at breakfast. Mulling over what he had read, he went for a walk and invented the neutron-induced nuclear chain reaction.7 The problem of liberating nuclear energy went from impossible to essentially solved in less than twenty-four hours. Szilard filed a secret patent for a nuclear reactor the following year. The first patent for a nuclear weapon was issued in France in 1939.

The moral of this story is that betting against human ingenuity is foolhardy, particularly when our future is at stake. Within the AI community, a kind of denialism is emerging, even going as far as denying the possibility of success in achieving the long-term goals of AI. It's as if a bus driver, with all of humanity as passengers, said, “Yes, I am driving as hard as I can towards a cliff, but trust me, we'll run out of gas before we get there!"

I am not saying that success in AI will necessarily happen, and I think it's quite unlikely that it will happen in the next few years. It seems prudent, nonetheless, to prepare for the eventuality. If all goes well, it would herald a golden age for humanity, but we have to face the fact that we are planning to make entities that are far more powerful than humans. How do we ensure that they never, ever have power over us?

To get just an inkling of the fire we're playing with, consider how content-selection algorithms function on social media. They aren't particularly intelligent, but they are in a position to affect the entire world because they directly influence billions of people. Typically, such algorithms are designed to maximize click-through, that is, the probability that the user clicks on presented items. The solution is simply to present items that the user likes to click on, right? Wrong. The solution is to change the user's preferences so that they become more predictable. A more predictable user can be fed items that they are likely to click on, thereby generating more revenue. People with more extreme political views tend to be more predictable in which items they will click on. (Possibly there is a category of articles that die-hard centrists are likely to click on, but it’s not easy to imagine what this category consists of.) Like any rational entity, the algorithm learns how to modify the state of its environment—in this case, the users mind—in order to maximize its own reward.8 The consequences include the resurgence of fascism, the dissolution of the social contract that underpins democracies around the world, and potentially the end of the European Union and NATO. Not bad for a few lines of code, even if it had a helping hand from some humans. Now imagine what a really intelligent algorithm would be able to do.

What Went Wrong?

The history of AI has been driven by a single mantra: “The more intelligent the better.” I am convinced that this is a mistake—not because of some vague fear of being superseded but because of the way we have understood intelligence itself.

The concept of intelligence is central to who we are—that’s why we call ourselves Homo sapiens, or “wise man.” After more than two thousand years of self-examination, we have arrived at a characterization of intelligence that can be boiled down to this:

Humans are intelligent to the extent that our actions can be expected to achieve our objectives.

All those other characteristics of intelligence—perceiving, thinking, learning, inventing, and so on—can be understood through their contributions to our ability to act successfully. From the very beginnings of AI, intelligence in machines has been defined in the same way:

Machines are intelligent to the extent that their actions can be expected to achieve their objectives.

Because machines, unlike humans, have no objectives of their own, we give them objectives to achieve. In other words, we build optimizing machines, we feed objectives into them, and off they go.

This general approach is not unique to AI. It recurs throughout the technological and mathematical underpinnings of our society. In the field of control theory, which designs control systems for everything from jumbo jets to insulin pumps, the job of the system is to minimize a cost function that typically measures some
====================
”; and a “dent in the world”.

These are just a few examples of the many activities that a typical computer would be involved in.ructionally.producing. With the task of digitizing the world, an intelligent agent would be able to profit maximally from any transaction in the world—no matter how physically distant it was from a person. Similarly, a chess program might be able to play fine chess at grandmaster level (some systems are very good, but we won’t know how—it’s complicated, after all) but it won’t know how to play. For example, it won’t learn to play good chess. An AI might read when it’s not doing well and when it’s doing well. It might have a good plan toessen what it does well to focus on what it does fail to do. It could have a plan to pull together a supply chain of what amounts to hundreds of millions of qubits. It could also deploy some of the next generation of antivirus products to squelch system infections. Even with these advanced capabilities, though, human-level Al is not a strong enough threat to pose an existential risk.

To be clear, the claim that Al is the greatest threat to existance is just an abstract one (whether we will be able to create superintelligence or not is a completely determined goal). It is a question of engineering, where basic resources like resources can be turned into superpowers along with technologies that might be viewed as hard to automate. With luck, we could see a boost to intelligence in the form of machine superintelligence.

But what about China? How will its workers fare in this brave new economy? Few good studies have been conducted on the impacts of automation here, but the conventional wisdom holds that Chinese people will be hit much harder, with intelligent robots spelling the end of a golden era for workers in the “factory of the world.” This prediction is based on the makeup of China’s workforce, as well as a gut-level intuition about what kinds of jobs become automated.

Over one-quarter of Chinese workers are still on farms, with another quarter involved in industrial production. That compares with less than 2 percent of Americans in agriculture and around 18 percent in industrial jobs. Pundits such as Rise of the Robots author Martin Ford have argued that this large base of routine manual labor could make China “ground zero for the economic and social disruption brought on by the rise of the robots.” Influential technology commentator Vivek Wadhwa has similarly predicted that intelligent robotics will erode China’s labor advantage and bring manufacturing back to the United States en masse, albeit without the accompanying jobs for "humans. “American robots work as hard as Chinese robots,” he wrote, “and they also don’t complain or join labor unions.”
These predictions are understandable given the recent history of automation. Looking back at the last hundred years of economic evolution, blue-collar workers and farmhands have faced the steepest job losses from physical automation. Industrial and agricultural tools (think forklifts and tractors) greatly increased the productivity of each manual laborer, reducing demand for workers in these sectors. Projecting this same transition out into the age of AI, the conventional wisdom views China’s farm and factory laborers as caught squarely in the crosshairs of intelligent automation. In contrast, America’s heavily service-oriented and white-collar economy has a greater buffer against potential job losses, protected by college degrees and six-figure incomes.

In my opinion, the conventional wisdom on this is backward. While China will face a wrenching labor-market transition due to automation, large segments of that transition may arrive later or move slower than the job losses wracking the American economy. While the simplest and most routine factory jobs—quality control and simple assembly-line tasks—will likely be automated in the coming years, the remainder of these manual labor tasks will be tougher for robots to take over. This is because the intelligent automation of the twenty-first century operates differently than the physical automation of the twentieth century. Put simply, it’s far easier to build AI algorithms than to build intelligent robots.

Core to this logic is a tenet of artificial intelligence known as Moravec’s Paradox. Hans Moravec was a professor of mine at Carnegie Mellon University, and his work on artificial intelligence and robotics led him to a fundamental truth about combining the two: contrary to popular assumptions, it is relatively easy for AI to mimic the high-level intellectual or computational abilities of an adult, but it’s far harder to give a robot the perception and sensorimotor skills of a toddler. Algorithms can blow humans out of the water when it comes to making predictions based on data, but robots still can’t perform the cleaning duties of a hotel maid. In essence
====================
The desire for immediate postwar economic success among the Chinese government allies did not merely signal an end to world domination. It also referred to a future in which the world’s most powerful technology would be invented, deployed for the dollar, or even a strategic advantage. Adhering to this analogy we ask you, “What technology can make us happy?” We believe that the answer depends both on the circumstances of our questions and on the economics of knowledge—intellect mastering the automobile, financial auditing, government contracts for research, and so forth. In the immediate postwar era, the most capable minds—those who specialized in either problem solving or assisting the single-mindedly dutifully performing tasks at hand—ostrally outnumber all other competitors. Absent further elaboration, however, it is tempting to infer that progress in any of these areas was much slower than in the high-rooted, but nevertheless quite rapid, growth modes. It is not clear whether China should result in an outright war with the West, or whether the only way to survive in the postwar era was by running the nuclear option.

The notion of immediate postwar economic success—the idea that the economic pie is essentially infinite in the modern era—is said to be valueless from the very start because it is so narrow. We are on the verge of achieving what some might call “human-level intelligence” (HL), or almost certainly a level of intelligence that will far exceed anything that exists on the planet today—perhaps far more so than knowledge and consciousness.

What is more, some of these are predicated on the ability of HLAI to last a century or more. For example, Leitner and his colleagues have argued thatergues such as penicillin and enteric surgeons are expected to last a century. penicillin is the treatment of patients with infections such as infections of the blood. This means that doctors cannot yet predict the outbreak or outcomes of surgery due to sudden illness.

Even before the possibility of early-stage lymphoma is realized, though, safety risks should be taken into account, especially for drugs that are not yet standard. Scheduling and the management of physical activity are two primary drivers of advanced economies. Ma’s stipend would be a decent income toil of one to three weeks, but it would come at a cost: lost productivity. For example, we Without expecting to replace us with digital minds, we would become obsolete. There would be destructures, like Abandoned farms.

There are sustainable solutions to the world’s issues of scheduling and management, but an economy based on digital minds can’t create enough jobs or retain enough of its productive force. The scale, pace, and strategies of these technical fixes are too complex, too obscured by intellectual property law, and too mired in logistical and technical complexity for us to see into it all. But if we delay the implementation of these protections until after the fact by using them to bar access to ChatGPT, we’ll be surprised how quickly and effectively the technology can transform a process.

There is a story about the early days of industrialism in the early decades of the twentieth century in which highly automated factories, mechanized railways, steel mills, and many of the processes associated with production were relatively simple. Linotypes were produced for pennies per month with little consideration for the human costs of running the machines. These costs were high, then, then, and now we are beginning to think differently about the lives of our employees. We’re also starting to think like a business.

We're all used to the idea of symbiotic relationships between humans and machines, but there’s a new layer of human involvement in the workplace. The new technology becomes a new way of engaging with people and giving them space to do new things. These relationships are not just existing as living prototypes but now become a crucial building block for reimagining our work processes.

Filling the gap in terms of scope and depth of responsibilities is critical to creating a new level of human being at work. We’ve already seen how AI is making it more difficult to find, trust, and connect with people. Younger employees are being asked to handle sensitive, sensitive conversations—and they seem to be having more fun than their older peers. What are all these new types of people doing? Do we have a defined goal for the future or a defined goal?

These challenges are momentous but not insurmountable. In recent years, I myself faced a mortal threat and a crisis of purpose in my own personal life. That experience transformed me and opened my eyes to potential solutions to the AI-induced jobs crisis I foresee. Tackling these problems will require a combination of clear-eyed analysis and profound philosophical examination of what people do and how they are commonly paid to perform their work. Pundits often describe this as “the biggest mystery of our time,
====================
In the pages of newspapers and popular magazines, scare stories about automation may remain just idle chatter. However, over the past decade, this talk has crystalized into an influential social theory, which purports not only to analyse current technologies and predict their future, but also to explore the consequences of technological change for society at large. This automation discourse rests on four main propositions. First, workers are already being displaced by ever-more advanced machines, resulting in rising levels of ‘technological unemployment’. Second, this displacement is a sign that we are on the verge of achieving a largely automated society, in which nearly all work will be performed by self-moving machines and intelligent computers. Third: automation should entail humanity’s collective liberation from toil, but because we live in a society where most people must work in order to live, this dream may well turn out to be a nightmare. Fourth, therefore, the only way to prevent a mass-unemployment catastrophe is to provide a universal basic income (ubi), breaking the connection between the incomes people earn and the work they do, as a way to inaugurate a new society.

This argument has been put forward by a number of self-described futurists. In the widely read Second Machine Age (2014), Erik Brynjolfsson and Andrew McAfee argue that we find ourselves ‘at an inflection point—a bend in the curve where many technologies that used to be found only in science fiction are becoming everyday reality.’ New technologies promise an enormous ‘bounty’, but Brynjolfsson and McAfee caution that ‘there is no economic law that says that all workers, or even a majority of workers, will benefit from these advances.’ On the contrary: as the demand for labour falls with the adoption of more advanced technologies, wages are stagnating; a rising share of annual income is therefore being captured by capital rather than by labour. The result is growing inequality, which could ‘slow our journey’ into what they call ‘the second machine age’ by generating a ‘failure mode of capitalism’ in which rentier extraction crowds out technological innovation. In Rise of the Robots (2015), Martin Ford similarly claims that we are pushing ‘towards a tipping point’ that is poised to ‘make the entire economy less labour-intensive.’ Again, ‘the most frightening long-term scenario of all might be if the global economic system eventually manages to adapt to the new reality’, leading to the creation of an ‘automated feudalism’ in which the ‘peasants would be largely superfluous’ and the elite impervious to economic demands. For these authors, education and retraining will not be enough to stabilize the demand for labour in an automated economy; some form of guaranteed non-wage income, such as a negative income tax, must be put in place.

The automation discourse has been enthusiastically adopted by the jeans-wearing elite of Silicon Valley. Bill Gates is advocating for a tax on robots. Mark Zuckerberg told Harvard undergraduate inductees that they should ‘explore ideas like universal basic income’, a policy Elon Musk also thinks will become increasingly ‘necessary’ over time, as robots outcompete humans across a growing range of jobs. Musk has been naming his SpaceX drone vessels after spaceships from Iain M. Banks’s Culture Series, a set of ambiguously utopian science-fiction novels depicting a post-scarcity world in which human beings live fulfilling lives alongside intelligent robots, called ‘minds’, without the need for markets or states.

Politicians and their advisors have equally identified with the automation discourse, which has become one of the leading perspectives on our ‘digital future’. In his farewell presidential address, Obama suggested that the ‘next wave of economic dislocations’ will come not from overseas trade, but rather from ‘the relentless pace of automation that makes a lot of good, middle-class jobs obsolete.’ Robert Reich, former Labour Secretary under Bill Clinton, expressed similar fears: we will soon reach a point ‘where technology is displacing so many jobs, not just menial jobs but also professional jobs, that we’re going to have to take seriously the notion of a universal basic income.’ Clinton’s former Treasury Secretary, Lawrence Summers, made the same admission: once-‘stupid’ ideas about technological unemployment now seem increasingly smart, he said, as workers’ wages stagnate and economic inequality rises. The discourse has become the basis of a long-shot presidential campaign for 2020: Andrew Yang, Obama’s former ‘Ambassador of Global Entrepreneurship’, has penned his own tome on automation, The War on Normal People, and is now running a futuristic campaign on a ‘Humanity First’, ubi platform. Among
====================
” on the left side, and “mann se lge” on the right side. The field of AI is also shaping the future of work itself. The combination of the personal, the digital, and the economic, provides a frame on which to focus our efforts. We can choose to move our work activities or focus onament as we find them most productive. When we use the technology to inform decision-making, we are alsoessentially affecting the productive dynamics of our organizations.

No economic or social policy can “brute force” a change in our hearts. But in choosing our own forms of thinking, we can choose to move with the times and ignore the hand motions that occur during our everyday lives. As we lose time and place our faith in these principles, our attention is darting around entirely new possibilities. We are seeking ways to let our time and energy be set aside and go instead to something more meaningful.

MADE IN SHENZHEN

Silicon Valley may be the world champion of software innovation, but Shenzhen (pronounced “shun-jun”) wears that crown for hardware. In the last five years, this young manufacturing metropolis on China’s southern coast has turned into the world’s most vibrant ecosystem for building intelligent hardware. Creating an innovative app requires almost no real-world tools: all you need is a computer and a programmer with a clever idea. But building the hardware for perception AI—shopping carts with eyes and stereos with ears—demands a powerful and flexible manufacturing ecosystem, including sensor suppliers, injection-mold engineers, and small-batch electronics factories.

When most people think of Chinese factories, they envision sweatshops with thousands of underpaid workers stitching together cheap shoes and teddy bears. These factories do still exist, but the Chinese manufacturing ecosystem has undergone a major technological upgrade. Today, the greatest advantage of manufacturing in China isn’t the cheap labor—countries like Indonesia and Vietnam offer lower wages. Instead, it’s the unparalleled flexibility of the supply chains and the armies of skilled industrial engineers who can make prototypes of new devices and build them at scale.

These are the secret ingredients powering Shenzhen, whose talented workers have transformed it from a dirt- cheap factory town to a go-to city for entrepreneurs who want to build new drones, robots, wearables, or intelligent machines. In Shenzhen, those entrepreneurs have direct access to thousands of factories and hundreds of thousands of engineers who help them iterate faster and produce goods cheaper than anywhere else.

At the city’s dizzying electronics markets, they can choose from thousands of different variations of circuit boards, sensors, microphones, and miniature cameras. Once a prototype is assembled, the builders can go door to door at hundreds of factories to find one capable of producing their product in small batches or at large scale. That geographic density of parts suppliers and product manufacturers accelerates the innovation process. Hardware entrepreneurs say that a week spent working in Shenzhen is equivalent to a month in the United States.

As perception AI transforms our lived environment, the ease of experimentation and the production process- it becomes a new frontier foremporary Chinese innovation. This is the era of discovery, and the age of implementation.

In the past, the first generations of Chinese designers and entrepreneurs looked to the future with a vision of a kind of technological utopianism. They imagined a world in which technological innovation would yield "level economic value" and pass the tests of perfection. It would turn out, as was the conventional wisdom, that such a goal would be a tall order. Instead, it turned out that solving it would be the most effective way to improve the life of an entire society.

Now, the potential economic and social benefits of AI are directly tapable via apps, platforms, and services. By offering its employees the ability to pay instantly with their phones, Toutiao is changing the way we’re doing business, rather than being stuck in a piecemeal job and unemployed for life. Its tools have potential, too. Just as important, by understanding and predicting the habits of each worker, the service-based approach can turn those workers into cash-generators that can make their businesses more sustainable.

BEIJING AI ON BEIJING

The Chinesebei—increasingly understood as a model for the economic and social transformation to come—is suddenly running out of steam. Alibaba’s Chinese leadership is simply not prepared to handle this kind of scale yet. But the company is training its AI platform in a remarkably deep environment by enabling it to access vast stores of data and extract valuable insights from it.

A company installed some strange new restrictions when it asked “for help” on April 3, 2014, but it quickly responded with an explanation of its own. By the next morning, it had amassed more than two thousand million parameters, and it was preparing to put those
====================
By the 1980s, computer systems were beginning to dissolve typesetting into word processing. A centuries-old gap separating writing and printing was beginning to close—and this gap had been the very ground on which the ITU stood. The union suffered a long decline and finally dissolved in 1986, just as the personal computer was completing typography's process of dematerialization. It was, at that time, the longest continuously-running union in U.S. history.

The End of Modernism and the Last Typesetters

In the 1970s, print production involved a complex hierarchy of work processes, the final product of which was never fully visible until it had been printed. Designers could only approximate typographical treatments; directions on spacing, size, and weight were then handed off to phototypesetting shops to interpret in detail. A separate group of prepress specialists followed designers' directions on variables like color density and image placement, and then "stripped" together disparate negatives to create a print-ready master. But despite the many hands through which such work passed, much of the period's modernist-influenced design left the impression that it was the product a singular, detached mind. 

Though there was still a high degree of churn in new machines and processes, this division of labor held stable until the arrival of Apple's Macintosh computer in 1984. The personal computer centralized capacities formerly bound up in massive metal-founding operations, delicate apparatuses of type on film, or astronomically expensive, room-filling computers—to say nothing of the highly specialized workers that attended these machines, or of the systems of education and apprenticeship that such a workforce presupposed. Tasks that were once contracted out with some combination of strict direction and trust were now fully under the control of the individual designer—from the smallest details of letterforms to the organization of entire books. The Macintosh would soon offer image-editing capacities with no existing analogue, which in turn put pressure on commercial photographers and illustrators. The century since the invention of the Linotype had been one of "creative destruction" in the print industry: novel forms of work appeared suddenly and disruptively, only to be rendered obsolete in their turn. Once the brake provided by ITU contracts was removed, this process could accelerate unabated.

By the mid-1980s, typographical technology had reached a height of modernized seamlessness which, ironically, contributed to the decline of modernism's hegemony in graphic design. New design software facilitated effects like layering and distortion, which were quickly put to use in visual polemics against modernist clarity. Formal complexity and semantic confusion in graphic design had a long pre-Macintosh history —stretching at least as far back as the late-1960s letterpress experiments of Wolfgang Weingart. In the 1980s, however, graphic designers raised the stakes of these experiments by linking them to contemporaneous developments in the academy: in particular, to the "linguistic" and "cultural turns" in the humanities.[45] Terms like "deconstruction" and "post-structuralism" were applied to the printed page in ways that often required little familiarity with the theories in question. The grid—increasingly understood as a symbol of authoritarian and, perhaps, Eurocentric rationality—was parodied, skewed, or thrown aside entirely. Designers arranged texts into ambiguous formations, and designed new typefaces that intentionally thwarted legibility.

By the 1990s, the postmodernist critique of modern rationality and power had grown more rigorous. However, the movement's theorists showed little interest in grasping capitalism as a determining context for their theory and practice; transformations in the political economy of print were thus largely ignored. When, in 1997, Emigre published a rare acknowledgment that entire industries were collapsing next door, it was with a heavy dose of schadenfreude 

[M]any of the printers who have gone out of business over the last quarter century deserved their fate. The grassroots of the printing trade is, after all, notoriously conservative, protectionist, and sexist.

While prepress and printing-like most American trades-tended toward a narrowly white male membership and self-image, the heaviest losses in the industry from the 1980s forward were in fact suffered by the largely non-unionized workforce of the cold type shops. Compared to the membership of the ITU, these workers were disproportionately women and people of color.

The postmodernists' focus on cultural intervention often neglected the material contingencies of the practice. Semiotic theory and cultural studies opened vistas to broad contexts of symbolic circulation, but often at the cost of such bare facts as design's own relationship to waged work. It is perhaps not surprising, then, that a new generation of practitioners has taken a more archaeological approach to the labor of design. In the recent documentaries Linotype: The Film (2012) and Graphic Means: A History of Graphic Design Production (2016)-both directed by
====================

