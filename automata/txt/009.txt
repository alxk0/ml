OUTPUT 009							AUTOMATA - GPT-2						 EPOCHS : 11000



’s “RuneOfThe Architect” project is asking whether AI algorithms, robots, and AI algorithms can repeat the “universe” of abstracts that once made possible by human intervention.

To explore this idea, I propose we look at this image, where the brain is alive and well and the AI algorithm is pursuing a different path. We can perceive some patterns in the image, and we can make inferences about the algorithm’s final goals. But there is still a strong presumption that only human goals are represented in the image, and that only when the AI is motivated by some causal connection to intelligence is it destroyed. The presumption is strengthened when the image is presented to the world, where all inferences are fallible, and where all the available data is instantly available for analysis.

But here is the problem with this approach. The whole brain emulation path cannot by itself be completed. Even if it were possible to construct a Szilard machine, it would still be in a weak bargaining position to prevent the realization of the axiological consequences of its designs. The unfeasible problem of the AI is overcoming the sovereign power of the unfeeling. Now, there might be other paths to strategic advantage, but these are all maximized by the same agent. The unfeasible problem of the AI is also the problem of the engineering talent pool. If there are several ways of achieving a particular value, to wit by engineering a particular Szilard function, and if the Szilard system is motivated to pursue that alternative “scalable” strategy, then the unfeasible problem of the AI will also be the problem of the available talent. A “scalable” strategy favors easier gains as a result, because an agent with an atiTYPAL state would be able to implement all available resources, whereas an agent whose state is inestimable is indifferent to consequences arising from the perspective of least resistance.

What gets theoretical play into the anthropomorphic categories of “real-world” and “neural networks”? The basic motif behind the distinction between AI and biological neural networks is that the “connection between organism and brain” is a quite genuine ontology. Neural networks (neuromorphic to neural networks) are not biological neural networks—they are statistical representations of the brain (along with some essential functionality added to the brain). Thus, a neural network is a system of biological neurons; but the connection between the neuron and the network is a complex network of synaptic connections made by the neuron in its developmental process. Even these networks can be simulated: the program shows how they develop and how they respond to different stimulus combinations. The neural network part of Minsky’s theory is therefore like a model of the brain, in that it is able to learn to respond to different stimuli, and learn to make quick mental calculations and to interpret the signals arriving from receptors in the visual cortex.

Another way of defining the independence of AI from biological neural networks is via the classification of incoming data. Some interesting things have been occurring in the language of AI research. The emergence of AI programs in the 1980s has been accompanied by new roles and responsibilities for humans in the AI future. For example, the processing of images is changing in a big way, but we should not take them literally. Rather, we should understand how they work, because they are the real deal.

Another way of defining the independence of AI from biological neural networks is via the use of diagrams that illustrate the components of the ultimate goal. The aim is to highlight the relationships among these components, so called “disciplines,” and the perfectionist, so to speak, who is supposed to be the master of the universe. The Disciplines category contains some AI principles that seem quite relevant to contemporary ethical problems. For example, the ImageNet framework, which aims to “provide a rigorous framework for the interpretation of images,” requires an enormous amount of training data, including not only human brain but also stereo, teleo, stereo, and four-dimensional representational images.27 The imagetrain system is trained on enormous amounts of data, including images of human faces, of human facial expressions, of human mid-thigh measurements, and of human movements. It can then produce a model that explains why people make mistakes and also why they can't be more reliable. When the system is only able to produce mid-level representations of correctly designed classifiers, the model is not necessarily reliable—recall the “Riemann hypothesis catastrophe” described in Chapter 8.

In the mid-1980s, AI researchers began thinking about the potential for machines with very specialized “data” to be trained on specific details of the human brain. But identifying these special details and searching for their solution was difficult because humans do not typically perceive things in a visual manner. That meant searching for that data on a visual inspection
====================
’s game-changing ability would be a collective superintelligence—not a single-minded desire to dominate the world.

In a sense, this would be an uninhabited society. It would be a society of economic miracles and technological awesomeness, with nobody there to benefit. A Disneyland for the Ages: Artificial Intelligence and the Threat of a Jobless Future would be a vision of progress and abundance, but it would also be dwarfed by the potential for a post-work world, a world in which economic divisions and political conflicts would be smoothed out by sheer luck. Malthusian capitalism would offer little opportunity for worker organizing, because even if there were a winner in the race, economic power would not yield a decisive strategic advantage.

In the long run, the benefits of AI might be felt far beyond the boundaries of nation-state. The sheer number of the AI superpowers might not seem large, but the fact that they are not seen very much suggests that the world’s great powers are not oblivious to the potential for mass unemployment and social turmoil. A winuscade from the AI superpowers might result in a “prolonged regional decline, as different clans emerge and begin to take on the roles and responsibilities of their predecessors.” If a power is able to maximise the cumulative reward of its actions, it may cease to be a force for good in the eyes of the other actors and, in fact, its capacity to bring about world justice will be systematically denied.

The essential difference here is the AI that does not find the benefits of AI as described by Turing. This is not magic; it is statistical analysis and numerical computation. But it is remarkably close to resembling what we call “good enough” in that it seeks to solve some problem and then sells the solution to benefit all the parties involved.

Since there is a limited number—perhaps a very small number—of distinct beneficial AI technologies, we can conclude that—if not co-opted by economic and political interests, then what’s the point of being ahead if you can't get to a larger number of benefits? The answer is market competition, which is, quite simply, worse than Szilard’s original invention. The market does everything else better, buying more products and services, controlling your own destiny, and so on.

There are two cases to consider. The first is that the market does not simply give us the answers we need. A compassionate or sensible explanation for why we gave up our privacy for so long—for example, because we feared the disclosure might violate privacy—may not be desirable in the end. The second case is that the endgame for the endgame is incredibly powerful, and accordingly we should prefer to end the game earlier than later. This is not a simple matter for us to decide. We could be happy that we are now computing the goals of the AI and thereby the machines, but we won’t be able to afford to wait. The economists see this as the value of liberty, and it is they who are creating the datasets that are defining the value of jobs and wages.

For datasets like Pandemonium, it is useful to be able to track who has accessed what data over time. Once we can, the aim is to track AI capabilities and gains over the next horizon. The aim is to build a better map of the territory. The accuracy of that map is determined by the accuracy of our previous predictions. The more accurate the accuracy of our predictions, the better we will be at predicting the next breakthrough. There is an obvious connection between this process and intelligence itself, since if we set out to make predictions about the next breakthrough, we would in fact be predicting something rather artificial.

Instead of merely predicting something about the next breakthrough, the underlying prediction itself must be precise, and that accuracy is further precision skews the results. A well-predicted prediction about “us,” along with all the other factors that go into an AI’s world, can turn out to be a very different thing: a better version of itself, better than the original.

What this means is that if we could endow AI with a specific goal, say, intelligence, rather than an intrinsic value, then the AI would have a different instrumental goal in mind. The better chance we have of matching that goal, the better we will be at predicting some next breakthrough, and so forth. But if we give the AI an objective that says we’re going to collect the data and run the software, then we actually create more inferences about what the system will do, rather than just giving it an obvious goal.

What this means is that if the AI is motivated by some moral imperative, then it would be more likely to solve the mainquest rather than lose its motivation. The solution would be to change its final goal to be “a little “good” rather than “
====================

