OUTPUT 018							AUTOMATA - GPT-2						 EPOCHS : 20000



But what about the dream of bypassing words altogether and establishing a connection between two brains that enables concepts, thoughts, or entire areas of expertise to be “downloaded” from one mind to another? We can download large files to our computers, including libraries with millions of books and articles, and this can be done over the course of seconds: could something similar be done with our brains? The apparent plausibility of this idea probably derives from an incorrect view of how information is stored and represented in the brain. As noted, the rate-limiting step in human intelligence is not how fast raw data can be fed into the brain but rather how quickly the brain can extract meaning and make sense of the data. Perhaps it will be suggested that we transmit meanings directly, rather than package them into sensory data that must be decoded by the recipient. There are two problems with this. The first is that brains, by contrast to the kinds of program we typically run on our computers, do not use standardized data storage and representation formats. Rather, each brain develops its own idiosyncratic representations of higher-level content. Which particular neuronal assemblies are recruited to represent a particular concept depends on the unique experiences of the brain in question (along with various genetic factors and stochastic physiological processes). Just as in artificial neural nets, meaning in biological neural networks is likely represented holistically in the structure and activity patterns of sizeable overlapping regions, not in discrete memory cells laid out in neat arrays.74 It would therefore not be possible to establish a simple mapping between the neurons in one brain and those in another in such a way that thoughts could automatically slide over from one to the other. In order for the thoughts of one brain to be intelligible to another, the thoughts need to be decomposed and packaged into symbols according to some shared convention that allows the symbols to be correctly interpreted by the receiving brain. This is the job of language.

In principle, one could imagine offloading the cognitive work of articulation and interpretation to an interface that would somehow read out the neural states in the sender’s brain and somehow feed in a bespoke pattern of activation to the receiver’s brain. But this brings us to the second problem with the cyborg scenario. Even setting aside the (quite immense) technical challenge of how to reliably read and write simultaneously from perhaps billions of individually addressable neurons, creating the requisite interface is probably an AI-complete problem. The interface would need to include a component able (in real-time) to map firing patterns in one brain onto semantically equivalent firing patterns in the other brain. The detailed multilevel understanding of the neural computation needed to accomplish such a task would seem to directly enable neuromorphic AI.

One hope for the cyborg route is that the brain, if permanently implanted with a device connecting it to some external resource, would over time learn an effective mapping between its own internal cognitive states and the inputs it receives from, or the outputs accepted by, the device. Then the implant itself would not need to be intelligent; rather, the brain would intelligently adapt to the interface, much as the brain of an infant gradually learns to interpret the signals arriving from receptors in its eyes and ears.76 But here again one must question how much would really be gained. Suppose that the brain’s plasticity were such that it could learn to detect patterns in some new input stream arbitrary projected onto some part of the cortex by means of a brain–computer interface: why not project the same information onto the retina instead, as a visual pattern, or onto the cochlea as sounds? The low-tech alternative avoids a thousand complications, and in either case the brain could deploy its pattern-recognition mechanisms and plasticity to learn to make sense of the information.

Networks and organizations

Another conceivable path to superintelligence is through the gradual enhancement of networks and organizations that link individual human minds with one another and with various artifacts and bots. The idea here is not that this would enhance the intellectual capacity of individuals enough to make them superintelligent, but rather that some system composed of individuals thus networked and organized might attain a form of superintelligence—what in the next chapter we will elaborate as “collective superintelligence.”

Humanity has gained enormously in collective intelligence over the course of history and prehistory. The gains come from many sources, including innovations in communications technology, such as writing and printing, and above all the introduction of language itself; increases in the size of the world population and the density of habitation; various improvements in organizational techniques and epistemic norms; and a gradual accumulation of institutional capital. In general terms, a system’s collective intelligence is limited by the abilities of its member minds, the overheads in communicating relevant information between them, and the various distortions and inefficiencies that pervade human organizations. If communication overheads are reduced (including not only equipment costs but also response latencies, time and attention burdens, and other factors), then larger and more
====================
: "The only outwardively conscious experience of the worker at the plant was that of a puddle of paint sitting in the channel, which would then draw the line into the tank."

In 1804, the American engineer Charles Babbage (1815–1852) built a mechanical multiplier called the "Step Reckoner." It worked to increase the output of a process by 5 per cent per centimeter of output (maximum about 165,000 manufacturing units per hour). With a capacity of 25,000 units per hour, the Reckoner could add this amount to his laboursprintless recipe. "The amount of Reckoner machinery produced was hardly increased much, although it was now understood that 300 was the pace of innovation in automation. With the introduction of steam power, however, there would be more Prussian firms to produce in the southern part of the country, andoppers to operate the loom. These firms would continue to supply the domestic workers needed to power the continuous-process industries begun in the previous chapter. In the meantime, the growing complexity of "the internal pond" had depleted the surplus capacity of the nearby mines. "

In the 1970s, several employers began to exploit the difference in conditions. The Chicago meat-packing industry had been the world’s first to use electricity for this purpose, and the employers had not yet figured out how to meet the rigid requirements of industrial production. The speed of improvements in technology, however, did offer advantages. In a process that was largely based on reduction rather than increase in equipment, Chicago’s employers began to capture the advantages of the new technology as it was added to the factory floor. They began to think differently about job losses and begin to envision a new way of doing things that had not even been discussed before.

In the early twentieth-century United States, railroad executives weren’tverse with regard to speed. The speed of improvements, however, did offer advantages. Sixty miles per hour was the norm for freight haulage, and 90 miles per hour was the speed to which drivers reacted when their lights went on. (Witness: The speed limits were raised on all four roads together and then again when traffic laws were changed so that, according to plans, they would continue to apply the new speed rule.) In the early-breaking phase of a new industry, operators bought computers and equipment, and the owners gave their horses and machinery to do most of the roving carts. Although most of these were later obsolete, the speed of expansion was giving way to an era of computer-based electronic communications. The new machines called microprocessors were capable of generating sufficient amounts of both pleasure and pain to satisfy all employers desires. They were to be controlled from a central controlling center. In a similar vein, AI now Rockefeller funded research on a wide-scale microprocessor that would regulate reproduction through a computerhead unit (software that would be the brain of a biological child).

The first Rockefeller Foundation grantee in the mid-1980s was Arthur Samuel (1907–1977), a British neurophysiologist and scientist who had worked on programs for automatic face recognition. Like Babbage’s face recognition program, the FaceVACS project was aimed specifically at computer-based research, and in 1987 Samuel published a paper describing experiments he had conducted on face recognition. He called these methods “state-of-the-art.” In the paper, he wrote that these methods (which could be face-recognizing) “exceeded the Turing Test,” and he contrasted their outcomes with those of methods for “reproducing effects that could be measured.”

But here’s a major turning point in the history of face-recognition: the introduction of digital eyes. These first couple of years of the digital era promised to be computers that could recognize faces, write text, and recognize objects. Toward the end of the book, I’ll describe research leading to much more robust automatic face recognition.

9.3 Computer Vision of Three-Dimensional Solid Objects

9.3.1 An Early Vision System

Lawrence G. Roberts (1937– ), an MIT Ph.D. student working at Lincoln Laboratory, was perhaps the first person to write a program that could identify objects in black-and-white (gray-scale) photographs and determine their orientation and position in space. (His program was also the first to use a “hidden-line” algorithm, so important in subsequent work in computer graphics. As chief scientist and later director of ARPA’s Information Processing Techniques Office, Roberts later played an important role in the creation of the Arpanet, the forerunner of the Internet.)

In the introduction to his 1963 MIT Ph.D. dissertation,13 Roberts wrote

The problem of machine recognition of pictorial data has long been a challenging goal, but has seldom been attempted with anything more complex than alphabetic characters. Many
====================

