<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <meta name="description" content="Automata : A Generated Essay">
    <meta name="keywords" content="Automata">
    <meta name="author" content="Alex Kim">
    <title>AUTOMATA | ANNOTATED</title>
    <link rel="icon" type="image/x-icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="css/style.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
    <script defer src="js/cursor.js"></script>
    <script defer src="js/column.js"></script>
</head>
<body>
    <div id="circularcursor"></div>
	<div class="header">
		AUTOMATA / GPT-2 / ANNOTATED OUTPUT
	</div>
	<div class="container">
		<div class="column left-column" id="left-column">
			- gpt2_gentext_20230509_012814.txt -
			<p><span>The United States has seen a steady flow of high-tech investment</span> and investment <span>over the last several decades. Industrial and agricultural products, transportation, and materials have all been linked in some way.</span></p>
			<p><span>But what accounts for all this data? The big question is where is the data coming from? One answer, usually referred to as the “art of the loop,”, is that many technological advances happen along the way that will change the world. And in some cases, these breakthroughs are shared equally between all parties involved. For instance, AI’s ability to recognize a human face means that we can now diagnose and track cancer. It also means that we can better predict the path of metastases—the kinds of cancers that can spread quickly through the human body and are often hard to predict precisely—and which countries.</p>
			<p>Another misconception is that data integration helps machines reason, predicting and quantifying the world. </span>This is a bit like attributing the moon to the planets because you found the moon. The problem with the adage, “You can't have more than the</p>
			<p>====================</p>
			<p><span>AI is disrupting work, reshaping the world, and reimagining our daily lives. But as much as these disruptive advances affect people’s everyday lives, they can also have profound economic impacts.</span></p>
			<p><span>Automation is one of those things that’s so hard to quantify because so much of its impact is down to humans “uploading, building, and building without the need for humans.” The phrase can be confusing—and often untrue—but the truth is that AI is already affecting production across a number of manufacturing tasks. That’s because humans are responsible for a wide swath of all those AI-infused products.</span></p>
			<p>Productivity</p>
			<p>How much does the value of AI in productivity vary by occupation? A quick Google search of the economic definition of productivity will give us something like 16.5 t% return on investment in a productivity-increasing sector.</p>
			<p>What does that mean in practice? The definition of a productivity-increasing sector is elastic, meaning that changes in output—say, a 10-turbine system running on electricity—will tend to increase in value relative to the elastic value of the changes in labour productivity. This elasticity means that changes in the elasticity of output tend to increase in value relative to changes in labour productivity. For example, if a machine making a 10-turbine system produces 10 t of electricity, so the electric system costs 7 cents, the electric system’s cost would be 8.5 t.</p>
			<p>In other words, suppose that the system is to produce 10 t of output per year, and that the system’s cost were to be paid by the electricity produced. The economist Robin Hanson has argued that this would be excessive and unworkable, and inequitable for society. In The Nature of Things, he argues</p>
			<p>The power of the conjecture that machine-readable computer books contain the balance of allinability for purposes of estimating the marginal costs of various properties, can be illustrated by a simple equation. The equation, taken literally, is the net marginal cost of the feature, expressed as a marginal product. The marginal product [of the machines producing the feature and the machines interacting with the system] of the machines producing the feature is called the input, and it is produced by the equation</p>
			<p>[output x inputs]</p>
			<p>The marginal product of the machines interacting with the system, under the assumption that the machine is human, is called</p>
			<p>====================</p>
			<p>Looking out our window at the EchoStar operation, we can see the glimmering possibilities for future artificial-intelligence collaborations within the organization. The massive expansion of its “intelligence labs” spans from the Office of Naval Research to IBM, and the creation of a vast computational network of computer simulations and algorithms. And the employees who work on this infrastructure are not just collaborating with industry partners like Amazon and Microsoft and academia’s hottest new idea: they’re part of an industry consortium working to shape the future of artificial-intelligence research.</p>
			<p>These labs are part of a global consortium of 16 institutions, including the World Economic Forum, the World Economic Forum Research Institute, the Partnership on a sustainable Development Goals (PFS), the Partnership on Climate Change (PCC), and the World Economic Forum China’s Future of Life in Our Future project. The consortium includes the World Economic Forum, the World Economic Forum Research Institute, the World Economic Forum China, and the World Economic Forum China’s Future of Life Our Future.</p>
			<p>The IoT is not a neutral label. It can be used to inform broad social and economic views, represent a tool to inform a collective position, or as a tool for social exchange. When used in this way it can signal a different reality—a larger truth than labels like “machine intelligence” and labels like “volcanic electricity.” It can also be used to signal a more humanistic view of certain social phenomena, such as the role of natural resources in determining daily life patterns or the way societies evolve.</p>
			<p>Many scholars have argued that the use of the term “artificial intelligence” is misleading and dangerous and should be sharply restricted. Those scholars have argued, among other things, that the word “artificial intelligence” is misleading in that it refers to a single, unified field, whereas “artificial intelligence” refers to a collection of techniques and methods integrated within a single field and is defined as a whole.61 Artificial intelligence is a multifaceted field, and it encompasses a wide range of technologies. For these and other reasons, the use of the term “artificial intelligence” should no longer be used to refer only to narrow fields, especially when applied to the emergence of whole brain emulation studies and other examples of machine intelligence within complex systems.</p>
			<p>3.6 The Future of Life</p>
			<p>The future of biological life lies in our collective</p>
			<p>====================</p>
			<p>” The line item “Accessories” list includes items such as a novelty watch case, a toothbrush, a toothbrush case, a toothbrush handle, a toothbrush clip, a toothbrush light, a toothbrush sticker, a toothbrush tape, a roller blade, a roller blade clip, a towel, a towel cloth, a towel cloth clip, a towel cloth bag, a towel cloth pack, a towel cloth room towel clothile. “Accessories” includes items such as toothbrushes, toothbrushes clothiles, toothbrushes brush, toothbrushes syringe, toothbrushes syringe, toothbrushes wipes, toothbrushes wipes clothile, toothbrushes tissue stockings, toothbrushes tissue stockings, toothbrushes tissue stockings, toothbrushes wipes, toothbrushes wipes brush, toothbrushes wipes brush clothile, toothbrushes skin cloths, toothbrushes tissue stockings, toothbrushes wipes clothile, toothbrush handle, toothbrush handle, toothbrush handle, toothbrush handle, toothbrush handle, toothbrush handle, toothbrush handle, toothbrush brush, tooth, toothbrush handle, toothbrush cloth, toothbrushes tissue stockings, toothbrushes wipes brush, toothbrushes brush, toothbrushes brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush calendula, toothbrushes, toothbrushes, toothbrush, toothbrush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush, toothbrush brush</p>
			<p>====================</p>
			<br>”</br>
			<p>Blackboard games of all kinds are fun, but some are dangerous. When you have someone like G. Rosset (the tycoon behind Rosset and the Pauper Company), you want something that can strike fear in their nerves. At their age, you want something that can rip them apart in the dark and then stab them in the side with your knife. But you also want something that can’t leave your room. So you put your best foot forward, and someone (or something) quite like G. Rosset (the entrepreneur behind the now-defunct Silk Road) comes along and takes your idea and starts jabbing it in the teeth with his gnarled, teeth-out-of-control P.R. hammer.</p>
			<p>Now, it’s not that the game is dangerous. There are only so many ways you could go about it. The only way that would work with a realistic simulation of a real human opponent was probably to have a real chess board full of severed P.R. bats. But that’s not how it works, and it also doesn’t have any sort of moral sense. There are rules you can or can’t break that would prevent you from beating your opponent, and if you do, the result is that we end up with a computer simulation of a real human opponent.</p>
			<p>This is, of course, without any kind of explicit information about the rules that you would need to know—but again, you can imagine G. Rosset thinking: "I can understand the desire to avoid physical injury to my opponent, but beating them in the real game would be an extremely stupid idea." And I would say to myself: I would be willing to bet the real money that this opponent is a skeleton crew of evil humans, all of them under the age of 30, all of them trying to escape from me, all of them trying to escape from the knowledge that I have this computer simulation of them.</p>
			<p>And now, after beating the human humans, I will build a computer simulation of them. In fact, I am beginning to think that this will be a very good thing. I will say to myself: I think this is a very smart thing that we should build. It would greatly improve the lives of these men and women, and I believe it could save lives.</p>
			<p>But let me add insult to injury to the injury: I have been</p>
			<p>Technology has advanced so far that it now requires a special kind of cognition that has been reserved for other cognitive functions,” says Dale Jorgenson, a professor of psychology at MIT and a leading expert on machine intelligence. That means that we’re now entering a new phase of cognitive development, where a new kind of job is required that doesn’t involve augmenting our cognitive capabilities but relies more on extracting knowledge from outside of ourselves.</p>
			<p>One such job is that of “recording everything that we know, whenever and wherever we have access to it.” This can be done automatically by apps that are built into computers. Jorgenson and his colleagues developed an even more powerful process called MessagingRecorder, which leverages deep learning to automatically analyze numerous bits of social signaling in social-media interactions. The app analyzes numerous examples of similar actions taken by individuals, and it then alerts the company when a new breach comes to market.</p>
			<p>Jorgenson says this kind of recursive analysis can be used to automatically identify potential misfire points in companies’ products. It’s like diagnosing a pain in the butt and then running some more tests to see how quickly it can help a person.”</p>
			<p>“In the end, it’s not clear to me how you could have a generic-looking process that’s tailored to your specific situation, and then automatically deploys the best possible tech to fix it,” Jorgenson says. “In the end, it just gets compounded.”</p>
			<p>One potential use case Jorgenson and his colleagues examined was an automated payment system. It all starts with an algorithm, which Jorgenson bases fairly simply on the amount of money in the system. The goal is to automate 95 percent of all transactions, and the remaining 10 percent goes to buying the rest of the products.</p>
			<p>“So the interesting part is the amount of money, the “goal is to maximize the accumulation of value,” says Jorgenson. And it turns out that 99 percent of all digital transactions, at some point, 99 percent of all digital transactions will go to buy some item. So you can have a general-purpose digital assistant that automates everything, or you can have a digital assistant that automates everything for you. These are super smart pieces of machinery that are putting in real work,” he says.</p>		
			<p>====================<br>
			(These messages are noncommittal. I should think of them as noncommittal as well, because they can be true or false.)
			<p><span>But let me remind you of another principle: that a machine that is smarter than you is smarter than a human that isn't smarter than a machine. That principle is that the machine we develop will treat you very differently than a machine that isn't smarter. Another principle is that we develop more information about ourselves than a machine does. And finally, a third principle is that our attitudes toward each other develop gradually.</span></p>
			<p>Intelligence is cumulative. So too does physical intelligence. I can assure you that it is not cumulative.</p>
			<p><span>The basic idea behind the principle is that the more information we have about ourselves, the more likely we are to get it to develop a superintelligence.</span> This development process can proceed in many directions, depending on the internal state of the machine we are developing. The more control we have over our attitudes, the more information we will have about the machine and its people.</p>
			<p>We can say with certainty that the attitude toward nature is not cumulative and that we can develop superintelligent attitudes in ways that do not involve the development of a superintelligence.</p>
			<p>If the attitude toward nature is linear rather than exponential, then the machine will have a relatively small number of weak dispositions, and we can give it a lot of ideas about how to do things. (The more powerful the dispositions, the greater the chance that the machine will develop a good attitude.)</p>
			<p>On the other hand, if we have a more complete attitude with a greater portion of weak dispositions, we could give it a lot more ideas about how to do things. (The more complete the attitude, the greater the chance that the machine will develop a bad attitude.)</p>
			<p>The more complete the attitude, the greater the chance that the machine will develop a bad attitude.</p>
			<p>The more complete the attitude, the greater the chance that the machine will develop a bad attitude.</p>
			<p>The more complete the attitude, the greater the chance that the machine will develop a bad attitude.</p>
			<p>The more complete the attitude, the greater the chance that the machine will develop a bad attitude.</p>
			<p>The more complete the attitude, the greater the chance that the machine will develop a bad attitude.</p>
			<p>The more complete the attitude, the greater the chance that the machine will develop a bad attitude.</p>
			<p>The more complete the attitude
			<p>====================</p>
			“Turing by computer chess is a model of the cognitive performance of human chess players,” says Warren Harding, a professor at MIT and an expert on AI at Stanford Information Systems Institute.
			<p>Harding points out that AI is not a scientific theory. It is a fact about human mental processes. And the work of robotics experts to develop the program to play chess is part of a longstanding trend in computer science research. That’s been going on for decades,””” says Harding.</p>
			<p>When I first heard about Turing’s work, I had a hard time believing that he was serious. I had heard the expression ‘sparks of disbelief’ on his website, but that’s not what he was talking about. He was very serious.</p>
			<p>“But that is really a matter of debate among computer scientists,” says Stanford Professor David Autor,”and I think it’s fair debate.”</p>
			<p>This debate has moved from “a topic for philosophers to be talking about,” to “a serious philosophical question,” to “hard to argue a sensible long-term practical argument against superintelligence.”</p>
			<p>Harding points out that this would be a mistake. He suggests that we re-examine our approach.</p>
			<p>The problem is not that we find the argument more convincing when we try to argue it more convincing. The problem is that we’re stuck with outdated assumptions that we need to change. And this also holds true for many other “questions” we’ll get to later in the book.</p>
			<p>The argument in this case is not whether we’ll get superint<p>====================</p>
			<p>I can see in the background noise of the earth rising and falling, planets forming and merging, the faint whirl of the inner planets forming and merging with the whirling whunks of the solar system. At any time I’m free, I could be in a very bad situation.</p>
			<p>Or maybe it’s that I don’t care about the earth rising or falling. The earth shouldn’t rise or fall. The moon doesn’t set. The planets don’t form. The universe doesn’t create matter.</p>
			<p>I don’t care how you define creation. The word creation should be used in the sense of creation from preexisting conditions, rather than as a noun, a verb, or a noun of some sort. Let’s not even start with creation from preexisting conditions and assume that’s how we define creation.</p>
			<p>I believe we can construct a new word from existing forms, using the spelling of creation from preexisting conditions. I’ve been working on creating a new word, trying to work out how to recognize it, and I can’t think of anything that works.</p>
			<p>Isn't it interesting that in the last century, scientific theories have changed so radically that we now use them to refer to the entire universe?</p>
			<p>I think it’s interesting that scientific theories have changed so radically that they now use them to describe a single thing — I don’t mean to suggest that one shouldn’t try to ground them as scientific facts. I don’t mean to suggest that one shouldn’t try to get a scientific theory to describe a single thing, even though that could lead us to put scientific theories into a “fuzzy” box. But I do think it’s relevant to consider the possibility that some scientific theory might be a product of some sort of state machine that gets better intelligence and produces more scientific information when it comes to climate change denial.</p>
			<p>In general, scientific theories tend to be more easily replicated in nature, more easily replicated in social contexts, and more easily replicated in the social sciences. They tend to be more closely related to, and more closely match to, the evidence we have today, even though there are likely differences in methods and methods being developed among them.</p>
			<p>Obviously, we have the scientific method in a very indirect and indirect way.</p>
			<p>====================</p>
			<p><span>So how did we get here? What are the implications of this process? To answer this question, it’s useful to have a couple of basic definitions of what AI is.</span></p>
			<p><span>AI is any system that can learn a task or do a task well by itself. A well-defined AI system will do anything that can be done by it.</span></p>
			<p><span>The term “self-learning” is used here to distinguish a process from an AI system that does not do anything by itself, because a self-learning system does not know whether it's successful or not. However, in-built systems do learn things from observation: a well-designed self-learning system will do the same thing in the real world as described in Section 8.2, and it will do the same things in both the laboratory and in a simulation as described in Section 8.3. Thus, AI is an evolving process.</span></p>
			<p>The word “self-learning” (as used in this definition) derives from the Greek word “self-āti, for learning." (To learn, as we all know, is usually from experience.) The word itself is from “self-āti, for learning." So, how did we get from here to here?</p>
			<p>Let’s look at some connections between the ways that AI systems learn.</p>
			<p>Chapter 13: The process of fitting the world</p>
			<p><span>The first set of connections is the one between the mind and the action, between the action and the mind. The mind is the conceptual interface between the world and the action, and the world is the perceptual interface.</p>
			<p>The perceptual interface, also called the percept, is the space beyond the world of ideas. The world is thus connected to what philosophers call the “hill-face.” The perceptual interface is the most important piece of the whole picture. And as we will see, the whole picture involves many layers of computation. The perceptual interface is what makes the world connect to the world outside of it.</p>
			<p>For example, suppose we wanted to understand how machines would think. We do not now know how to do this. It’s a very simple problem, but one that we can try. The percept creates a world space beyond the world of ideas. Let’s call this layer the “coffin”—the perceptual unit that contains thoughts. However, it’s possible to create</p>
			<p>====================</p>
			<p>First, I want to clarify what I mean by “first,” in this instance a descriptive sense. This means that the first element of the predicate will be true if and only if the two conditions are met. I am assuming that the observer is interested in the fact that the condition that the two conditions are met is true. This sense of “first,” is critical because it tells us something about the structure of the predicate. For instance, it might be true that the first element of the predicate is true; but it might also be true that the two conditions are false. And this brings us to the second important sense in which I am “stun by the assumption that the observer is interested in the fact that the condition that the condition that the condition is true is true.”</p>
			<p>The first crucial reason in favour of first-person plural statements in science fiction is that they can tell us something about the structure of the predicate. This sense, which has been central to understanding human reasoning for centuries, is not intuitive to most people. They are conjunctions, and infinitude, and definite clauses, to name just a few examples. In fact, the idea that there is something fundamentally wrong with the way we have treated the predicate is so ingrained in our culture that it is almost unnoticeable in ordinary conversation.</p>
			<p>It is a staple of modern culture, and one of the most basic ingredients of any good story, to assume that everything is equal regardless of what the writer or director thinks the setting is. To write a story such as “The Bluejay” requires that we have a good deal of sense of what the subject matter is all about. If that is not possible, then the science fiction author or director is a philosopher or just a typical science fiction author who thought making a human-like being was an effective way to get their ideas across with minimal interference from the computerized world.</p>
			<p>The problem for science fiction is that we do not know how to do computer graphics. Part of the problem is that we do not know how to do computer animation. But that is a problem that we can solve nevertheless. If we do that, the result is that the result of computer graphics is a “human-like” thing, we get the idea. There are, of course, different ways of doing things. And doing a human-like thing in computer graphics is a very, very human thing.</p>
			<p>====================</p>
			<p>Proprietary Software: Biometric Surveillance</p>
			<p><span>By now, most of us have heard of the term ‘biometric tracking’. This term refers to methods that collect data on your activities, such as biometric data logging, biometric identification machines, facial recognition, and behavioral data logging. In this chapter, we’ll explore some of the current uses of these technologies, how they promise to improve our lives, and what new approaches to privacy might mean. We will also explore biometric tags such as pat-notation, bar-notation, and markers such as the CSHT.</p>
			<p>The concept of metadata extraction is well established. Automated systems store data such as zip lines, addresses, and prescription-style data such as phone numbers, dates, and mail addresses. Individuals can also query databases for this information. Many systems now are using metadata, which refers to the information that a system has manually entered. In this chapter, we’ll use the term ‘biometric tracking’ to distinguish it from other forms of ‘human’ capture, such as ‘post-hoc tracking’ or ‘post-hoc capture.’ In other words, we are seeking to understand how and what forms of capture are taking place on the internet. We will focus on ‘biometric surveillance’ and ‘surveillance.’ In this chapter, we begin the search for ‘post-hoc tracking’ and extend our definition of ‘human’ to include AI systems.</p>
			<p></span>Post-hoc tracking</p>
			<p>In some cases, people’s contact details can still be available online, even if they’re not human. Some systems use machine-readable tags, such as photos of people or cars. Others use combination of captured information and tagged faces. Tracking is achieved using combination of these techniques, known as ‘post-hoc’tort’, or ‘human to machine’. The term ‘post-hoc’tort’ first appeared in a 2005 paper by psychologists Erik Brynjolfsson and Erik H.L. Gitlin.4 In it, they observed that ‘human observers’ online contact information, such as age, race, and address, is continuously being captured by at least three automated systems, known as ‘post-hoc’tort’s</p>
			<p>====================</p>
			<p>Presenting the latest in building the next AI superpower: Gigantic compute.</p>
			<p>Yesterday, I detailed the progress of AI in my original story, and today I want to turn to some of the key building blocks needed for the next step in AI's future. In doing so, I shed some light on some of the nitty-gritty details of AI's construction and how they have been rolled out successfully in the past.</p>
			<p>The first step in building an AI superintelligence is probably the most critical: training the system with all the relevant data and the right software. That can be done by anyone, anywhere. Here are a few of the things that go into the training process:</p>
			<p>- Image recognisers. The ImageNet project, inspired by the facial recognition developed by DeepMind, uses ImageNet to predict the features of individual faces. The team used a dataset of 444,841 photographs and found that those with the highest number of faces (i.e. those with the most famous faces) were most likely to have the most similarity to those with the lowest number of faces (no faces). This allows the system to predict the features of faces with similar facial features: “The feature count should be calculated using a variance function that allows the number of faces to be compared with the total number of images, so that the system’s rank will then match the number of faces with similar features.”</p>
			- Image recognition algorithms. ImageNet, like GIST, uses ImageNet, Image- Net, and Image- NetM to develop the distribution of a face’s facial features, and that process is repeated over thousands of images to find features with wider images and 'similar' to what we might expect a face to have. This process can take a long time, so the firm uses a combination of computer vision, natural-language processing, and speech-recognition techniques.
			<br></br>- A large library of video-based data. While training, most people don’t use computers, so data from video-based media is abundant. But a few people do. For example, Gigi suggested that people with a fascination with computer graphics media, such as GIMP, might be able to run the program “Gigi on Video.” This produces a collection of graphics data, and people who don’t use computers often use “Gigi on Gigi.”4 The image data is then<br>
			====================</br>elligence,” says Harding. It is that we do not yet know the answer to that question. And this is where we can’t change our assumptions fast enough. We need to start by rethinking our assumptions a bit.</p>
			<p>One suggestion that came to my mind was this: I believe that there is a path that (1) can lead to superintelligence (2). If A is true, then we can infer that superintelligence (which we can already infer by changing our assumptions) by looking at what happens after the conclusion (which we can already infer by changing our assumptions) is factored in. Thus, since superintelligence is not a consequence of what we actually think we can</p>            </div>
		<div class="column right-column" id="right-column"></div>
	</div>
</body>
</html>