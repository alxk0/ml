OUTPUT 000 							AUTOMATA - GPT-2						   EPOCHS : 2000



Although some might regard this as an uphill battle against the AI breakthroughs that really matter, it is closer than you might think. Expertise and technological insight are critical to every good AI researcher's ability to achieve his or her aims, and toKENEFING well-designed studies have confirmed that there is a good proportion of us who are very knowledgeable about some topic
and very anti-climactic in pushing it to market. (And, yes, we are all supercilious!) Patience is essential to being a seed investor in a new field. I testify to this fact by following the latest successes and failures of several well-known startups , and reflecting on each person's own approach to success over the years. (And, of course, I’m hard-pressed to find a better answer to the question of, "Why didn’t you do the same for BQ?" than this: "Because I thought my Q&A videos were the most interesting.")

Proprietary AI tools don’t have lives. They don’t have private lives. And unlike them, they don’t generate wars, phony job losses, or pump billions into social welfare schemes.

Industrial intelligence platforms are as sentient and benevolent as the mechanical reindeer in Johjira. They monitor and guide their own behavior, providing advice and advocating for their own preferences. But they are also dangerous business models and, more importantly, closed societies. Like abusive business models, they will consume vast amounts of public money.

Many of the issues that led to this current crisis of democracy and democratic governance are likely to get left out in the cold. The tech companies are going to face the same kinds of lawsuits as those who live in Nazi concentration camp are likely to be denied. The same things that facilitate and encourage fear mongering may also trigger tacitly onto others the kinds of tragedies that frequent democracy-violators can point to with concern.

But even absent these outcomes, I believe that the next stage of the process—the creation of widespread unemployment and inequality— would be a catastrophe. It would mimic the messy transitions between 1960s Britain and post-1960s America perfectly, with the US economy recovering and leading the world in services per capita, but the concept of a “next-generation” economy footing the bill for decades of under-employment is unworkable in a post-AI world.

And then something incredible happened. After a quarter century of rapid recovery, the next-generation AI revolution appears poised to repeat as the world’s top economic engine, powered by what some call the “next-generation AI system”,” according to the most pessimistic forecasts. I believe this prediction will be met with a robust rejection from experts, something that even the most conservative optimists agree with. Expert consensus emerges that the “next-generation AI system” will be as important and dynamic as the one that came out on top, and it will be both the source and the test.

To this consensus of opinion, I turn to the skeptic’s guide. My name is Andrew Ng, and I have spent the past three decades working to understand the role human skepticism can beouncemetering the AI ecosystem. I believe there is considerable upside to thinking AI-first, and there is also considerable downside to thinking AI-later. But whether or not that upside turns out to be enough to save humanity from extinction is a question that artificial intelligence remains woefully unable to answer.

Is this skepticism justified in saying that every new AI technology will have an impact on our country’s future? Or do we want to be in the knowhow of the next AI revolution on the verge of creating a totally new global economy?

It's a philosophical question that has engaged a very different group of people than most people. After all, it’s a common story everyday of the world we create and how we came to be human. It isn’t just a story about machines and humans. Much of what’s been written on the subject of AI safety comes from a kind of political analysis: from the tech and venture capital industries, to the humanities, to the academic freedom movements, to the environmental movement. As scholars like Benjamin Bratton and Stuart Hall put it, “human-parole” is all about paroled prisoners on welfare flossing up. So what is to be done? How are we to implement these approaches in practice?

The answer, as always, is the transfer of data from lab rats into customers’ lives. Many of the issues that arise from AI safety are solutions that have been developed but never implemented, a phenomenon known as agent-in-transition.

One of the early successes of AI was the DARPA funded Project Linotype, which was able to develop a language model of the mind that was emulated by video cameras in the brain. Project
====================
” I’m going to have to put the last word to that, but what do you expect from a robot AI writer? I suppose it’s not so much that they expect us to like them or follow them too closely or judge them on their own merits. It’s that they expect us to believe them when they’re wrong. That’s why I wrote the book. I didn’t say, “I suppose I’ll have to say it’s going to have to do what’s best for humans.” It’s not that they’ll misunderstand us or make us unhappy. I said, “Let’s try to be as helpful as possible.”

You argue that we can’t be both better and worse than any single software agent. In your recent book, you write in the spirit of dialogue and argument:

We are at a juncture, today, where many people are faced with the choice between three different approaches to the AI problem: acceptance, rejection, or triumph. We are facing rejection with the digital agent that we call A∗, the “good” agent. Humans, with the help of external agents, have been running a complicated algorithm that translates inputs into outputs for some period of time and then producing outputs in the desired order. If the input translation method is not working, the system needs to be reconfigured. We are not saying that all versions of the A∗ approach must be wrong, or that everything must be reimagined. If you build the system to recognize sudden changes in lighting and make adjustments accordingly, you may end up with a system that is a bit like the digital equivalent of a jaywalking cop force in the 1950s.

You argue that we can’t truly be both better and worse than any single system

and also remind us that the “good” approach is still the “right” approach, even if we agree with you on some of the key questions. Why is this? Why does the AI field need more typesetters? Why does the Council of Europe need to be changed? Why should the medical diagnosis of cancer be determined by a mere judge’s medical report? These are the questions that have been gripping the AI field for years, yours was a central focus of my research. And although we may not entirely forgotten about them, these questions are still pressing.

In part two of this book (and available as ePaper) you will see how I plan to build my own machine learning models to take the next step toward achieving truth detection and greater objectivity in human-objected information. This will require lab-grown data consisting of thousands of labeled images—of people, places, events, and researchers—for example in my graduate class, the task of diagnosing breast cancer. My goal with this research is to create an AI system that can examine thousands of different images and develop recommendations based on that data.

My goal with this whole process is to not only get this system up and running in five years, but to also learn as much as I can about the business side of AI with customers and beyond. This is why I will be conducting this experiment with a different company this coming spring: Pando.ly. The company is working closely with companies like Instacart, CVS, and Drugstix, among others.

The core of the system will be trained on the vast collections of online academic papers and information—many of which will never be published—from the past and present era to ensure it’s up to date with the latest research trends. The company is also using AI techniques to examine slides on AI and natural language understanding from the past and present era.

Before diving into the full implications of what I am describing—what is it that Pando.ly stands for, and how does Pando.ly were able to train it to reflect upon the past, present, and future—what did Pando.ly mean when it wrote this name?

It all began when a Pando.ly impersonator showed up at my office one morning. I had just launched an article about pandocourse, an algorithm that I had coined to piece together a timeline of what happened in the early years of civilization. Before I could speak, he stopped him with a flourish: “No machine learning can predict the future accurately. The next phase of the AI revolution will need to expose all of humanity to a sufficiently high level of predictive AI.”

Had I been able to, I confident that I’d would have gained a much greater reputation as the “smart AI of the century,” when, in fact, I was working on the “disruptive AI” treadmill. But my reputation as the “smart AI of the century” had already been built. And
====================

