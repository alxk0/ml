OUTPUT 003 							AUTOMATA - GPT-2						   EPOCHS : 5000



” and “optimization.” The second is that usually “optimization” is something that we do because we get a notification of a prediction, rather than something new or different that we prediction.” I’m told that most of the time, this second prediction is “crazy good”—that is, it predicts a prediction that we can’t precisely predict, and then we test it. Sometimes, we actually do get a prediction that we can't predict. But it’s a prediction that we make when we’re most confident in the prediction. Predictions are a form of prediction!”33 The next question is whether there is a causal relationship between prediction errors and harm.

Suppose we encounter a “case of a machine having difficulty recognizing the letter A in the context of a conversation”? The typical scenario will involve a “case of a machine having difficulty recognizing the letter B in the context of a conversation.” The difficulty of the problem is in finding the letter, and the judge will often decide against it. The “good” thing about finding letters is that it lets us test more different candidates. After all, there are relatively few kinds of clue that are found in everyday speech. So choosing the letter A that we’re unlikely to encounter in a future conversation is a “good time” for the machine to try something new.

A more common kind of clue that we’ll encounter in a future conversation is that the program has a “strong desire” to see A as soon as it sees something. This is what seems to have happened with LUNAR for some years: they’d allow random searches for “letter A” to occur in their data, but have a high degree of certainty that they’re going to find letter B in the next few lines.

A “designer defect” is a defect in the design of a system that might cause a malfunction. A bad design could result in a crisis of confidence for the system, which could lead to system breakdowns. There have been several controversies around the web about possible causes for machine learning errors. For example, inMay 2016, a Python programmer caused a training set of chess-playing AI algorithms to go down for days, prompting a formal study by the UK’s leading experts in the field.

Why do some types of smart tools have such a strong effect on a system? That’s why you can’t build a computer for simulating chess. A system is dynamic, changing, and evolving, and the people responsible for the system’s underlying algorithms can’t be located in a single place. There’s hardly a place you can’t find people to monitor the system.

Autodesk’s Dreamcatcher software learns algorithms based on millions of hours of training data, and it can be programmed to produce produce results based on millions of hours of training data.3 This is essentially what OpenAI’s GPT-4 software does. It draws on decades of experience in artificial intelligence training sets, and develops a set of inference languages that it then uses to make inference connections between the training data and the top-level representations of the symbols represented by the symbol. In essence, the software builds a network of inference languages that he can then use to find the nearest match.

OpenAI doesn’t yet have the expertise to show all above, but what it does know is that it canker-wire millions of symbol structures into thousands of appropriate connections based on the training data. This is not something that researchers in the past have tackled yet. In fact, it seems likely that they have a great deal of confidence that the network will work someday.

What Turing and other researchers have taught me is that the problem of explaining intelligence is something that’s incredibly difficult to solve. That’s a pity, because it allowed artificial intelligence to develop into something that it took years to get right. And it took centuries to get there. But that’s the part that I try to dodge in the book: the part that I try to avoid in the book. I always say, “My goal is to avoid answering questions that you might have about the nature of intelligence,” and I always resist that challenge.

In fact, I have a fairly severe disability that means I can’t give talks either to nor to read to anyone. So I have to be either lying or telling the truth. I’ve come to realize that I am not a very good conversationalist at all now. That’s because I've crouched. I never really tried to lie about something.

Gefter: How did you become a language modeler?

Hoffman: I was a physics and math teacher at Northwestern University. At
====================
” We are so used to the stupidity of AI that even allowing the potentially catastrophic failings of AI to go unchecked will only produce more stupidity.”

This mistake is not limited to AI. After centuries of progress, a new source of cosmic wisdom is coming out of our backups: new knowledge about our tangled webs of interconnections, metastasies, and terrors. New experiments in signal processing in the unconscious are revealing about the potential of human cognition. They also reveal what might be possible if we, as human beings, are to take up the challenge of controlling artificial intelligence.

Signaling our commitment to an alternative ending to the AI race is not so straightforward if the outcomes are flash fiction. Second, the goals and values of the competing projects are not so obvious if we assume that the goals are in fact true. Third, the technical methods that produce AI are profoundly untrustworthy. Our technological forensics suggest that the methods used to produce AI are not designed to discover malfunctions and that once-secret, hidden ways of enabling collaboration are being developed. Fourth, even if we could be certain that the AI project is not attempting to cause humanity an existential catastrophe, how could we be sure that it is not simultaneously and not secretly enabling humanity to achieve the very same goals?

Can We Let the AI Project Dies Explode?

The idea of letting the AI project die is a dreadful one, one that we can understand by understanding what today’s superintelligence is doing today. We will refer to this condition as the AI decline decline phase.

The demise of the AI project will not come as a surprise or surprise, either. After the Snowden disclosures, many people treated the prospect of a Chinese AI project achieving power by hook or pay centimetres in social influence. If what we are told is good enough for achieving control and prosperity, then we are promised a future in which the fruits of that intelligence project will appear—even if, as seems already, the Chinese leadership is instead guided by a handful of self-serving Silicon Valley engineers. If what we are promised is no spy or secret lab, this is a far cry from “making America great again.”

What China’s AI researchers are doing gives us a sense of the mythology of artificial intelligence and sets us ablaze. The battles for research and market power are fought over who we should trust to lead our country through its transformation to an advanced era. The quest for market power and China’s pre-eminent technology ecosystem will not lead us back to the edge of disaster. On the contrary, China’s AI leadership inspires a patriotism and cultural confidence that will only strengthen as we navigate these treacherous new waters.

The Chinese AI leadership is also starting to recognize that not all inventions coming out of Silicon Valley are going to turn out so well. More than 1,000 startups published their first AI publications in the small batches that some of them took, and many more are now doing well. The success of these startups led to an unprecedented level of trust and investment in the technology that led to China’s AI future. It is a confidence that allows us to understand how things are evolving and how we can best serve our communities.

The AI leadership comes with the values and commitment of leaders who have shaped our own societies

The guiding values of the early AI pioneers was clear: diversity, freedom, and human flourishing. They saw value in the human rights it spawned, the environment it created, and the rights it facilitates. China has a long way to go to ensure these things happen, but these things have come, and they are making life easier for people across sectors of the economy.

The guiding values of the early AI pioneers were clear: diversity, freedom, and human flourishing. They saw value in the human rights it spawned, the environmental protection it created, and the rights it facilitates. China’s government has already taken steps to ensure these things happen, and it is actively pursuing its long-term plan to develop artificial intelligence in a safe and efficient manner. But those steps need to be robust and swift, coordinated and efficient, and avoid an outright catastrophe.

The values guiding the guiding values of the early AI pioneers were clear: diversity, freedom, and human flourishing. They emphasized the flourishing of humanity as it advances from the extinction point to a flourishing future world population. They were simple: self-sustaining, resource-poor planets with well-restructuring infrastructure that will eventually lead to an intelligence explosion. They were terrifyingly optimistic.

The values guiding the values guiding the values guiding the values guiding the values guiding the values guiding the values guiding the values guiding the values guiding the values guiding the values, were as pertinent today as they were in the 1950s. The gap in understanding and/or funding for these overlapping fields has widened dramatically. And that’s because these new levels of expertise, talent, and expertise have displaced many traditional jobs. As
====================

