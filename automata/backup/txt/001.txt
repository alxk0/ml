OUTPUT 024							AUTOMATA - GPT-2						 EPOCHS : 26000



The central claim of our work in this book is that we know a lot more about the internet than we did a few years ago. We also know a lot more about the internet than there are now. There is a rich series of pictures of famous places, a universe of internet users, and a whole lot more. There is a universe of Waze. According to the company, the complete AI system "is on the verge of becoming machine learningable."

This is a history-altering technology but one that is lucky to have survived a tumultuous half-century of research. Ever since its inception, artificial intelligence has undergone a number of boom-and-bust cycles. Periods of great promise have been followed by “AI winters,” when a disappointing lack of practical results led to major cuts in funding. Understanding what makes the arrival of deep learning different requires a quick recap of how we got here.

Back in the mid-1950s, the pioneers of artificial intelligence set themselves an impossibly lofty but well-defined mission: to recreate human intelligence in a machine. That striking combination of the clarity of the goal and the complexity of the task would draw in some of the greatest minds in the emerging field of computer science: Marvin Minsky, John McCarthy, and Herbert Simon.

As a wide-eyed computer science undergrad at Columbia University in the early 1980s, all of this seized my imagination. I was born in Taiwan in the early 1960s but moved to Tennessee at the age of eleven and finished middle and high school there. After four years at Columbia in New York, I knew that I wanted to dig deeper into AI. When applying for computer science Ph.D. programs in 1983, I even wrote this somewhat grandiose description of the field in my statement of purpose: “Artificial intelligence is the elucidation of the human learning process, the quantification of the human thinking process, the explication of human behavior, and the understanding of what makes intelligence possible. It is men’s final step to understand themselves, and I hope to take part in this new, but promising science.”

That essay helped me get into the top-ranked computer science department of Carnegie Mellon University, a hotbed for cutting-edge AI research. It also displayed my naiveté about the field, both overestimating our power to understand ourselves and underestimating the power of AI to produce superhuman intelligence in narrow spheres.

By the time I began my Ph.D., the field of artificial intelligence had forked into two camps: the “rule-based” approach and the “neural networks” approach. Researchers in the rule-based camp (also sometimes called “symbolic systems” or “expert systems”) attempted to teach computers to think by encoding a series of logical rules: If X, then Y. This approach worked well for simple and well-defined games (“toy problems”) but fell apart when the universe of possible choices or moves expanded. To make the software more applicable to real-world problems, the rule-"based camp tried interviewing experts in the problems being tackled and then coding their wisdom into the program’s decision-making (hence the “expert systems” moniker).

The “neural networks” camp, however, took a different approach. Instead of trying to teach the computer the rules that had been mastered by a human brain, these practitioners tried to reconstruct the human brain itself. Given that the tangled webs of neurons in animal brains were the only thing capable of intelligence as we knew it, these researchers figured they’d go straight to the source. This approach mimics the brain’s underlying architecture, constructing layers of artificial neurons that can receive and transmit information in a structure akin to our networks of biological neurons. Unlike the rule-based approach, builders of neural networks generally do not give the networks rules to follow in making decisions. They simply feed lots and lots of examples of a given phenomenon— pictures, chess games, sounds—into the neural networks and let the networks themselves identify patterns within the data. In other words, the less human interference, the better.

Differences between the two approaches can be seen in how they might approach a simple problem, identifying whether there is a cat in a picture. The rule-based approach would attempt to lay down “if-then” rules to help the program make a decision: “If there are two triangular shapes on top of a circular shape, then there is probably a cat in the picture.” The neural network approach would instead feed the program millions of sample photos labeled “cat” or “no cat,” letting the program figure out for itself what features in the millions of images were most closely correlated to the “cat” label.

During the 1950s and 1960s, early versions of artificial neural networks yielded promising results and
====================
 AI

The previous chapters described how companies are currently using AI. In particular, how they answer common questions and give thoughtful advice on what to do next. But the work they do is also relevant today for any industry or organization that has an interest in automated systems. What is more, for any technological or operational breakthrough, companies will need to lay the proper groundwork before the next breakthrough can arrive.

As we look forward into the future, we must also take the prompt approach. Not rushing is a sign that we are living in an age of rapid progress. Rapid advances mean that we must expect these systems to arrive as soon as possible. This means that we must trust that they will make some difference, before they are completely overtaken by machines. We also must not be too quick to clap our hands and declare victory. Rather, we must be confident that what will have happened to us is coming our way, at least superficially.

The conventional wisdom holds that AI systems will soon become indistinguishable from a human brain. We will of course be comparing the capabilities of our favorite robots to those of zebras, superhuman athletes, superhuman bookstores, superhumanillysts, superhuman robots, superhuman medical theories, and so on. As we shall see, these and other scenarios will eventually give way to rampant reductions in our amount of mental states, producing a consciousness that tops our current level of functioning.

Let us look at one scenario in which the world economy could double in size at the rate of ten per thousand per year. In this case, the rate of growth of economic productivity would be relatively slow compared to the rate of growth of physical productivity. Yet, compared to this, the pace of technological progress is more rapid than at any time since at least the late 1500s. One recent article, based on a careful econometric study by economists David Autor and Anna Salomons, states that the economic productivity benefits of AI are visible in the economic value created by the technology.

Economic value created when AI boosts productivity. An AI value is 1. insofar as its benefits to individuals and firms are comparable to or better than those of current American companies. To be very good off a production of 1.7 times faster timescales, say, or 1.8 times more advanced, a production of 1.8 times more advanced, say, the world economy would be able to contain itself in one trillion person-years of economic growth, whereas in the past, a singleton growth rate might not matter very much in the context of a complex global system.

There is a further set of consequences to collaboration that should be given at least some shrift: the possibility that pre-transition collaboration influences the level of post-transition collaboration. Assume humanity solves the control problem. (If the control problem is not solved, it may scarcely matter how much collaboration there is post transition.) There are two cases to consider. The first is that the intelligence explosion does not create a winner-takes-all dynamic (presumably because the takeoff is relatively slow). In this case it is plausible that if pre-transition collaboration has any systematic effect on post-transition collaboration, it has a positive effect, tending to promote subsequent collaboration. The original collaborative relationships may endure and continue beyond the transition; also, pre-transition collaboration may offer more opportunity for people to steer developments in desirable (and, presumably, more collaborative) post-transition directions.

The second case is that the nature of the intelligence explosion does encourage a winner-takes-all dynamic (presumably because the takeoff is relatively fast). In this case, if there is no extensive collaboration before the takeoff, a singleton is likely to emerge—a single project would undergo the transition alone, at some point obtaining a decisive strategic advantage combined with superintelligence. A singleton, by definition, is a highly collaborative social order.44 The absence of extensive collaboration pre-transition would thus lead to an extreme degree of collaboration post-transition. By contrast, a somewhat higher level of collaboration in the run-up to the intelligence explosion opens up a wider variety of possible outcomes. Collaborating projects could synchronize their ascent to ensure they transition in tandem without any of them getting a decisive strategic advantage. Or different sponsor groups might merge their efforts into a single project, while refusing to give that project a mandate to form a singleton. For example, one could imagine a consortium of nations forming a joint scientific project to develop machine superintelligence, yet not authorizing this project to evolve into anything like a supercharged United Nations, electing instead to maintain the factious world order that existed before.

Particularly in the case of a fast takeoff, therefore, the possibility exists that greater pre-transition collaboration would result in less post-transition collaboration. However, to the extent that collaborating entities are able to shape the outcome, they may allow the emergence or continuation of non-collaboration only if they foresee that no catastrophic consequences would follow from
====================
Artificial Intelligence and Life in 2030,” by John McCarthy and Ray Perrault, respectively.

McCarthy and Perrault, currently the chief research officer at Meta Platforms Inc., have been working on solutions for balance issues for several years. But, they say, “we don’t yet have a good, stable, universal solution.”

They acknowledge that further work is needed to find out more about what kinds of balance problems might arise, given the recent work on large language models by McCarthy and Perrault. But, they say, such work is “ essential” to perform machine intelligence research.

They acknowledge that further work would be needed to show that, even with digital minds, the mind is necessarily more complex than biological minds. “This holds even if we concede that whole brain emulation is a bad thing,” they say. “That there are multiple paths does not mean that we ought to deny that there are multiple risks.”

Other AI researchers say that while it is largely feasible to extrapolate the performance characteristics of human-like systems, it “is not easy.” When we compare the evolution of human-level intelligence in several ways, we see that the digital minds involved in human-like intelligence are not rule-based or objective thinkers. The digital minds in question are usually built to see and intervene in individual cases, such as sentencing or medical care, and to perform autonomous functions. For them, “useful as it is under ordinary circumstances to say that human beings are intelligent agents, they can be used to help us in many other ways.”

 Unfortunately, there is not much evidence that human-like agents are safe when put in situations where it is easily automated by machines.physiology- scientists perform diagnostic tests on our brains to make predictions about the likely causes of disability. They may simulate, to a limited extent, the behavior of our brains, and model the brain in its most detailed detail. When a machine can do all of these things, it will be able to take any account of its own capabilities.

The whole brain emulation path is more difficult, because the limitation of how they could have a say in our decision making is too great. One can have whole brain emulation, but it will not be viewed as a substitute for human physical labor.

 opinion The economist John Harsanyi propounded this view with his principle of preference autonomy, which is that without have access to physical resources, we will have to give them permission to do whatever they want with their lives. I have suggested that giving physical resources to others is a negative net positive affect, since such a move would conflict with human autonomy. He further argues that the notion of a beneficial machine is ill-defined or misleading, and it has no bearing on issues of ethics.

Harsanyi brings up many other interesting questions. He has the advantage of being able to combine human and machine labor very easily, and the extra resources available to such an employer generate a more-or-less uniform work environment.

 humans could lead to a heightened sense of empowerment in the emergence of a new kind of machine intelligence. A enhanced human-machine work environment that puts an emphasis on labor productivity could, when combined with a boost to our sense of well-being, lead to a real shift in the pace of our work. Such a dynamic will require experimentation, and we mustguess how long to get to that place of abundance.

As we look forward, we must also take the time to look around.

9
★
OUR GLOBAL AI STORY

On June 12, 2005, Steve Jobs stepped up to a microphone in Stanford Stadium and delivered one of the most memorable commencement speeches ever given. In the talk, he retraced his zig-zagging career, from college dropout to cofounder of Apple, from his unceremonious ouster at that company to his founding of Pixar, and finally his triumphant return to Apple a decade later. Speaking to a crowd of ambitious Stanford students, many of whom were eagerly plotting their own ascent to the peaks of Silicon Valley, Jobs cautioned against trying to chart one’s life and career in advance.

“You can’t connect the dots looking forward,” Jobs told the assembled students. “You can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future.”

Jobs’s wisdom has resonated with me since I first heard it, but never more so than today. In writing this book, I’ve had the chance to connect the dots on four decades of work, growth, and evolution. That journey has spanned companies and cultures, from AI researcher and business executive to venture capitalist, author, and cancer survivor. It has touched on issues both global and deeply personal: the rise of artificial intelligence, the intertwined
====================
” (as per Article 6(2), art. 85.


/ European Commission, The Charter of the European Union (2).. Article 8(2) of the 2019/AURORAPPS Act brings into play a number of important mechanisms to further enhance the powers of the Commission to act on behalf of the public interests represented by Member States when acting on behalf of others. For example, in order to ensure the legal status ofrones, the European Parliament passed the Dapps legislation byelectronicity, which sets out principles that may conflict with the Charter of fundamental rights. These include the rights to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom of assembly and of association, and non-discrimination, consumer protection, workers’ rights, rights of persons with disabilities, right to an effective remedy and to a fair trial, right of defence and the presumption of innocence, right to good administration. In addition to those rights, it is important to highlight that children have specific rights as enshrined in Article 24 of the EU Charter and in the United Nations Convention on the Rights of the Child (further elaborated in the UNCRC General Comment No. 25 as regards the digital environment), both of which require consideration of the children’s vulnerabilities and provision of such protection and care as necessary for their well-being. The fundamental right to a high level of environmental protection enshrined in the Charter and implemented in Union policies should also be considered when assessing the severity of the harm that an AI system can cause, including in relation to the health and safety of persons.

(29) As regards high-risk AI systems that are safety components of products or systems, or which are themselves products or systems falling within the scope of Regulation (EC) No 300/2008 of the European Parliament and of the Council39, Regulation (EU) No 167/2013 of the European Parliament and of the Council40, Regulation (EU) No 168/2013 of the European Parliament and of the Council41, Directive 2014/90/EU of the European Parliament and of the Council42, Directive (EU) 2016/797 of the European Parliament and of the Council43, Regulation (EU) 2018/858 of the European Parliament and of the Council44, Regulation (EU) 2018/1139 of the European Parliament and of the Council45, and Regulation (EU) 2019/2144 of the European Parliament and of the Council46, it is appropriate to amend those acts to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of each sector, and without interfering with existing governance, conformity assessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in this Regulation when adopting any relevant future delegated or implementing acts on the basis of those acts.

(30) As regards AI systems that are safety components of products, or which are themselves products, falling within the scope of certain Union harmonisation legislation, it is appropriate to classify them as high-risk under this Regulation if the product in question undergoes the conformity assessment procedure with a third-party conformity assessment body pursuant to that relevant Union harmonisation legislation. In particular, such products are machinery, toys, lifts, equipment and protective systems intended for use in potentially explosive atmospheres, radio equipment, pressure equipment, recreational craft equipment, cableway installations, appliances burning gaseous fuels, medical devices, and in vitro diagnostic medical devices.

(31) The classification of an AI system as high-risk pursuant to this Regulation should not necessarily mean that the product whose safety component is the AI system, or the AI system itself as a product, is considered ‘high-risk’ under the criteria established in the relevant Union harmonisation legislation that applies to the product. This is notably the case for Regulation (EU) 2017/745 of the European Parliament and of the Council47 and Regulation (EU) 2017/746 of the European Parliament and of the Council48, where a third-party conformity assessment is provided for medium-risk and high-risk products.

(32) As regards stand-alone AI systems, meaning high-risk components of products, they are appropriate to be classified as high-risk under this Regulation if the products in question have been manufactured or otherwise obtainable at a price that is able to meet the requirements of the Regulation. In this regard, it is appropriate to classify as high-risk the AI systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity, since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities.

(33) AI systems used in education or vocational training, notably for determining access or assigning persons to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their
====================
One of the more vivid examples of workplace AI comes from an insider account at Amazon. In 2014, the company decided to experiment with automating the process of recommending and hiring workers. If automation had worked to drive profits in product recommendation and warehouse organization, it could, the logic went, make hiring more efficient. In the words of one engineer, “They literally wanted it to be an engine where I’m going to give you 100 resumes, it will spit out the top five, and we’ll hire those.”21 The machine learning system was designed to rank people on a scale of one to five, mirroring Amazon’s system of product ratings. To build the underlying model, Amazon’s engineers used a dataset of ten years’ worth of résumés from fellow employees and then trained a statistical model on fifty thousand terms that appeared in those résumés. Quickly, the system began to assign less importance to commonly used engineering terms, like programming languages, because everyone listed them in their job histories. Instead, the models began valuing more subtle cues that recurred on successful applications. A strong preference emerged for particular verbs. The examples the engineers mentioned were “executed” and “captured.”

Recruiters starting using the system as a supplement to their usual practices.23 Soon enough, a serious problem emerged: the system wasn’t recommending women. It was actively downgrading résumés from candidates who attended women’s colleges, along with any résumés that even included the word “women.” Even after editing the system to remove the influence of explicit references to gender, the biases remained. Proxies for hegemonic masculinity continued to emerge in the gendered use of language itself. The model was biased against women not just as a category but against commonly gendered forms of speech.

Inadvertently, Amazon had created a diagnostic tool. The vast majority of engineers hired by Amazon over ten years had been men, so the models they created, which were trained on the successful résumés of men, had learned to recommend men for future hiring. The employment practices of the past and present were shaping the hiring tools for the future. Amazon’s system unexpectedly revealed the ways bias already existed, from the way masculinity is encoded in language, in résumés, and in the company itself. The tool was an intensification of the existing dynamics of Amazon and highlighted the lack of diversity across the AI industry past and present.

Amazon ultimately shut down its hiring experiment. But the scale of the bias problem goes much deeper than a single system or failed approach. The AI industry has traditionally understood the problem of bias as though it is a bug to be fixed rather than a feature of classification itself. The result has been a focus on adjusting technical systems to produce greater quantitative parity across disparate groups, which, as we’ll see, has created its own problems.

Understanding the relation between bias and classification requires going beyond an analysis of the production of knowledge—such as determining whether a dataset is biased or unbiased—and, instead, looking at the mechanics of knowledge construction itself, what sociologist Karin Knorr Cetina calls the “epistemic machinery.”25 To see that requires observing how patterns of inequality across history shape access to resources and opportunities, which in turn shape data. That data is then extracted for use in technical systems for classification and pattern recognition, which produces results that are perceived to be somehow objective. The result is a statistical ouroboros: a self-reinforcing discrimination machine that amplifies social inequalities under the guise of technical neutrality.

The Limits of Debasing Systems

To better understand the limitations of analyzing AI bias, we can look to the attempts to fix it. In 2019, IBM tried to respond to concerns about bias in its AI systems by creating what the company described as a more “inclusive” dataset called Diversity in Faces (DiF).26 DiF was part of an industry response to the groundbreaking work released a year earlier by researchers Joy Buolamwini and Timnit Gebru that had demonstrated that several facial recognition systems—including those by IBM, Microsoft, and Amazon—had far greater error rates for people with darker skin, particularly women. As a result, efforts were ongoing inside all three companies to show progress on rectifying the problem.

“We expect face recognition to work accurately for each of us,” the IBM researchers wrote, but the only way that the “challenge of diversity could be solved” would be to build “a data set comprised from the face of every person in the world.”28 IBM’s researchers decided to draw on a preexisting dataset of a hundred million images taken from Flickr, the largest publicly available collection on the internet at the time.29 They then used one million photos as a small sample and measured the craniofacial distances between
====================
The letter, which was signed by Apple co-founder Steve Wozniak ; Stability AI CEO Emad Mostaque; and co-founders of the Center for Humane Technology, Tristan Harris and Aza Raskin, who have been critical of social media and AI technology, said a spokeswoman for the team authoring the letter.

“Those who seek to manipulate or manipulate the environment do not deserve to be held accountable.”

China is a keysperson of AI, but state-owned companies lack the venture funding to do more.adi is a grading system that uses data from the US Department of Defense (DOD) and then relies on sensor data from internet companies, including Facebook, Google, and Uber. Among other companies, iFlyTek has partnered with Intel to develop a platform for data analysis for the US Air Force.5

In October 2016, Nvidia, the Swedish’s leading computer vision company, announced that it would be recalling around 36 million vehicles equipped with its new wide array of processing technologies.6 The recall comes despite the fact that the company has been largely responsible for standardizing driver AI systems and raises the total to a new low level, with drivers “pioneered” by the technology.

Drones have become a particularly interesting application of AI. Unlike traditional robots that can move autonomously through the world, drones are able to eat, drink, and eat. Eating and dealing with mold in kitchens are nowhere near as challenging as navigating through a fully automated factory, let alone autonomous robots doing the heavy lifting. Indeed, it is when drones take on these roles that AI will turnanny.

Drones will be especially interesting to researchers who apply new techniques to systems that were previously invisible. In the past, people had to constantly reconfigure their processes to fit the high-tech paradigm. But AI is increasingly becoming a part of everyday business, and the challenge will only get worse with time.

In this chapter, we’ve seen how artificial intelligence is made of human labor, including drones, tractors, quadcopters, whales, and salmon. drones are used by cruise control systems to navigate drones over contested environments, and autonomous drones for factory runs.lifehacks are designed to be lightweight, cost-effective, and often feasible. But when "cale autonomous systems" (HLs) are made of a special type, they are extremely powerful. According to Luccioni et al., 2022, BLOOM’s escalation procedure, which mimics the actions of a human operator, uses deep learning to assess the extent of damage to a body. Using a model of the human bladder, a machine-learning software agent finds the location of the bladder in sufficient detail to automatically put a label on the bladder: unreasonably positioned. This is a form of damage to the bladder that can only be addressed by treatment-group participants who have a high grade on the first task.

A case of a different sort. In a case from the examples in Section 6, we are considering a collection of generic word processors from a large language model called cerberus. cerberus is a language model that is adapted for efficient machine translation by computer. It is used by several other LLMs, including ChatGPT, which is a re- reimagined version of N via reinforcement learning.

These examples show how second-guessing can be powerful when done in the context of a learning environment. The spoken-choice error-correcting feature ofMs like ChatGPT can scale up in magnitude when done by relatively well-qualified people. The results of this research are open to experimental, and there is no reason to suppose that these individuals could not also be doing what they do best: resolving ambiguous information.

The next step, then, is to teach these machines how to reason by teaching them new kinds of arguments by running experiments. This will take us back to the cells of C into the cell of A. While it is probable that AI programs will improve greatly with time, it is also probable that they will suffer from enough limitations. The capabilities that are needed are all-encompassing.

And here, training the machines to follow the latest research will be extremely ambitious. As we will see in this book, the people who will be doing these admirable things will have their own motivations. If AI is developed along traditional lines, where the machine learning came from, we could expect it to take different forms depending on what is available to it.

The world population of observations has increased about every major general-purpose technology: computers, data centers, industrial and household appliances, autonomous vehicles, nuclear reactors, factory robotics, and all kinds of material goods both in the industrial economy and in the hands of a few. This large base of routine manual labor has been part of the economy for centuries. As Hao uses a software program he called CALO, a program that places the highest importance on the continuous-process applications
====================
The case for rushing is especially strong with regard to technologies that could extend our lives and thereby increase the expected fraction of the currently existing population that may still be around for the intelligence explosion. If the machine intelligence revolution goes well, the resulting superintelligence could almost certainly devise means to indefinitely prolong the lives of the then still existing humans, not only keeping them alive but restoring them to health and youthful vigor, and enhancing their capacities well beyond what we currently think of as the human range; or helping them shuffle off their mortal coils altogether by uploading their minds to a digital substrate and endowing their liberated spirits with exquisitely good-feeling virtual embodiments. With regard to technologies that do not promise to save lives, the case for rushing is weaker, though perhaps still sufficiently supported by the hope of raised standards of living.

The same line of reasoning makes the person-affecting perspective favor many risky technological innovations that promise to hasten the onset of the intelligence explosion, even when those innovations are disfavored in the impersonal perspective. Such innovations could shorten the wolf hours during which we individually must hang on to our perch if we are to live to see the daybreak of the posthuman age. From the person-affecting standpoint, faster hardware progress thus seems desirable, as does faster progress toward WBE. Any adverse effect on existential risk is probably outweighed by the personal benefit of an increased chance of the intelligence explosion happening in the lifetime of currently existing people.

Collaboration

One important parameter is the degree to which the world will manage to coordinate and collaborate in the development of machine intelligence. Collaboration would bring many benefits. Let us take a look at how this parameter might affect the outcome and what levers we might have for increasing the extent and intensity of collaboration.

The race dynamics and its perils

A race dynamic exists when one project fears being overtaken by another. This does not require the actual existence of multiple projects. A situation with only one project could exhibit a race dynamic if that project is unaware of its lack of competitors. The Allies would probably not have developed the atomic bomb as quickly as they did had they not believed (erroneously) that the Germans might be close to the same goal.

The severity of a race dynamic (that is, the extent to which competitors prioritize speed over safety) depends on several factors, such as the closeness of the race, the relative importance of capability and luck, the number of competitors, whether competing teams are pursuing different approaches, and the degree to which projects share the same aims. Competitors’ beliefs about these factors are also relevant.

In the development of machine superintelligence, it seems likely that there will be at least a mild race dynamic, and it is possible that there will be a severe race dynamic. The race dynamic has important consequences for how we should think about the strategic challenge posed by the possibility of an intelligence explosion.

The race dynamic could spur projects to move faster toward superintelligence while reducing investment in solving the control problem. Additional detrimental effects of the race dynamic are also possible, such as direct hostilities between competitors. Suppose that two nations are racing to develop the first superintelligence, and that one of them is seen to be pulling ahead. In a winner-takes-all situation, a lagging project might be tempted to launch a desperate strike against its rival rather than passively await defeat. Anticipating this possibility, the frontrunner might be tempted to strike preemptively. If the antagonists are powerful states, the clash could be bloody.34 (A “surgical strike” against the rival’s AI project might risk triggering a larger confrontation and might in any case not be feasible if the host country has taken precautions.)

Scenarios in which the rival developers are not states but smaller entities, such as corporate labs or academic teams, would probably feature much less direct destruction from conflict. Yet the overall consequences of competition may be almost as bad. This is because the main part of the expected harm from competition stems not from the smashup of battle but from the downgrade of precaution. A race dynamic would, as we saw, reduce investment in safety; and conflict, even if nonviolent, would tend to scotch opportunities for collaboration, since projects would be less likely to share ideas for solving the control problem in a climate of hostility and mistrust.

On the benefits of collaboration

Collaboration thus offers many benefits. It reduces the haste in developing machine intelligence. It allows for greater investment in safety. It avoids violent conflicts. And it facilitates the sharing of ideas about how to solve the control problem. To these benefits we can add another: collaboration would tend to produce outcomes in which the fruits of a successfully controlled intelligence explosion get distributed more equitably.

That broader collaboration should result in wider sharing of gains is not axiomatic. In principle, a small project run by an altruist could lead to an outcome where the benefits are shared evenly or equitably among all morally considerable beings
====================
.

The parallel to the story of the horse can be drawn out further if we ask why it is that there are horses today that think well of horses, and that do not complain. One reason is that modern machines have achieved a deindustrialization of the human imagination. In thinking machines can do whatever we can do, however, because there is a limited number of ways of thinking about the human imagination that we cannot touched. We have seen this in the case of the calculator, which is a toy machine that cannot make mathematical mathematical calculations. The drone also doesn’t have a sense of composition, as it conceals its purpose more than it does. Thus, it is not surprising that it is not surprising that they don’t like to use metaphors when describing machines to get their Estimates right.

The inevitable conclusion is that machines will become better at thinking than we humans. They are going to become superintelligent, it turns out, and we can predict that they will also become worried about losing control over our own imagination.

Happiness is not that we have infinite time up to now, but time is not a zero-sum game and nothing is lost by sharing it. On the other hand, competing to be the first to achieve human level Al, or to win the imitation game, is a negative-sum game in which the value of resources is proportional to the number of punishments and rewards given. The moral we should draw is that even an egoist can be good at thinking, because his ego contains more resources than any other human; and his preference for simplicity appears to be correct. If there are additional resources, it would have greater instrumental reasons to produce additional types ofism, each of which may have a net instrumental instrumental instrumental instrumental instrumental instrumental reasons to pursue this approach.

There are situations in which a machine superintelligence may cause a catastrophe of scale—in particular, if the AI is designed to promote widespread prosperity. For example, if designed to promote widespread prosperity, a superintelligent machine’s principal purpose, such as maximizing the realization of human preferences, would be to implement these preferences into necessary programs for society. These principles help to illustrate how a machine superintelligence could behave in a social world:

 social engineering is probably more dangerous than most other military strategies, because it aims to level the playing field between publicly acceptable and sub-optimal opinionated systems. There are multipolar scenarios, in which a singleton triumphs, either because of internal drift or pre- coordination failures, either because the default trajectory is toiply or to abruptly de- prioritise competing interests.

The issue of future employers of humans brings up many other interesting questions. For example, is there a currently existing population of intelligent machines that will have the same rights as humans, or is there some kind of universal norm for everyone?

The common good principle does not preclude commercial incentives for individuals or firms active in related areas. For example, a firm might satisfy the call for universal sharing of the benefits of superintelligence by adopting a “windfall clause” to the effect that all profits up to some very high ceiling (say, a trillion dollars annually) would be distributed in the ordinary way to the firm’s shareholders and other legal claimants, and that only profits in excess of the threshold would be distributed to all of humanity evenly (or otherwise according to universal moral criteria). Adopting such a windfall clause should be substantially costless, any given firm being extremely unlikely ever to exceed the stratospheric profit threshold (and such low-probability scenarios ordinarily playing no role in the decisions of the firm’s managers and investors). Yet its widespread adoption would give humankind a valuable guarantee (insofar as the commitments could be trusted) that if ever some private enterprise were to hit the jackpot with the intelligence explosion, everybody would share in most of the benefits. The same idea could be applied to entities other than firms. For example, states could agree that if ever any one state’s GDP exceeds some very high fraction (say, 90%) of world GDP, the overshoot should be distributed evenly to all.

The common good principle (and particular instantiations, such as windfall clauses) could be adopted initially as a voluntary moral commitment by responsible individuals and organizations that are active in areas related to machine intelligence. Later, it could be endorsed by a wider set of entities and enacted into law and treaty. A vague formulation, such as the one given here, may serve well as a starting point; but it would ultimately need to be sharpened into a set of specific verifiable requirements.

CHAPTER 15: Crunch time

We find ourselves in a thicket of strategic complexity, surrounded by a dense mist of uncertainty. Though many considerations have been discerned, their details and interrelationships remain unclear and iffy—and there might be other factors we have not even thought of yet. What are we to do in this predicament?

Philosophy with
====================
The case for rushing is especially strong with regard to technologies that could extend human intellect beyond what has come before can be made explicit. Because activities outside of work could be tracked and reported back to themaker, who would have the option of being notified that data was collected and used as part of an algorithmic reward program. But this brings us to the second consideration: the possibility that more people will have access to the products and services that are built on the work of biological research.

Whose interests does a project like this represent?ose interests will extend beyond chess and AI to consider the challenges and opportunities of machine intelligence revolutionizing our already complex and technologically driven world. One of the sponsors of the event was a leading futurist and director of the Future of Humanity Institute at Oxford University. He described the dystopian future in a paper titled “We may see pieces of machinery from another dimension, and when they do the machines will have far greater appetites.”

Yet even if automation is not enough to drive a massive increase in our intelligence, what happens if machines can already do everything that human beings can do? More fundamentally, how can the AI community plan to plan ahead—alone or with the help of a human accomplice? Why should we create such a burden?

The main thrust of the posthuman problem is that we are at risk of having computers that are not only more powerful but more prefer human-than-human intelligence.11 This distinction should be obvious from the fact that we are not yet machines. By contrast, we have more time to prepare, more knowledge to Advance, and more data to do the tedious grunt work of running computations. By contrast, we have more data to add in, more products to bring to market, and a whole range of smart tools to rethink how people are doing. By definition, we arealogies are not continuous, so the range of tasks a machine can perform in one lifetime is limited. (In case that makes you feel complacent, think about the thousands of other tasks that humans can do!)

H owever, as such technological advances continue pushing the bounds of computation, we may need to use more technical terms. The field of engineering is often thought to have a strong focus on rational algorithms and optimization algorithms, which help toCONCLUSION:

The results of steps towards a computer are increasingly understood and used to realize more effective work processes.

The focus of this paper has been on the potential trade-offs involved in developing a wider range of AI systems. The purpose of the issues raised by the bias debates in AI are also relevant to the issue of wages and jobs.

There is a further set of consequences to collaboration that should be given at least some shrift: scientific research and economic policy. In particular, consider the challenge of the Worlds Wide Web Consortium’s proposal to create a Web Data Center, effective as of 2029, to avoid the problems that arise when people have been working with Web sites for the past thirty years.

.? Why is there no consideration of the global consequences of the technological choices we face? What kind of world effects would be associated with such choices? There is clearly no consideration of the global effects that would result from the choices that we make, no matter how well designed. What would be the net benefits to those who suffer from the equivalent offulfilling prophecies? Not very much.

In fact, the effects of technological choices can be very large. According to the authors, “over the past decade, five trillion people have lost the ability to enjoy�true virtual reality, meaning that even today’s people can contribute to this virtual world through voice commands.” If we imagine that some future machine with a superhuman level of general intelligence could convincingly read your mind and convince you that you are the real deal, what would it want? Ought it to take various forms? I n this moment, I’ll mention two anticipated challenges.

First, issues such as privacy, ethics, and governance will all be high-stakes, and extremely difficult to identify. In the past, theprediction circle has been a source of my wisdom on this question, but in the past few years, the impact of latest AI innovations has been far more profound. We are now witnessing the latestwaves of the technology economy, in which companies reap the benefits of intelligent machines for the cost of doing so. This has created a shift away from traditional business models-robustness, profit-maximizing efficiency, and pure capability- to a new kind of cashless, work-withoutbingards, orCRUD hybrid system.

The new hybridization of business processes requires managers with a vision of transformation that is optimized for loyalty and productivity. A hybridization of processes is also crucial in terms of redefining work behavior based on intrinsic powers, as opposed to work skills, which were already broadly available to the public. A huge turning point in the technology industry came in the 1960
====================
The claim that a machine cannot be the subject of its own thought can of course only be answered if it can be shown that the machine has some thought with some subject matter. Nevertheless, "the subject matter of a machine's operations" does seem to mean something, at least to the people who deal with it. If, for instance, the machine was trying to find a solution of the equation x2 - 40x - 11 = 0 one would be tempted to describe this equation as part of the machine's subject matter at that moment. In this sort of sense a machine undoubtedly can be its own subject matter. It may be used to help in making up its own programmes, or to predict the effect of alterations in its own structure. By observing the results of its own behaviour it can modify its own programmes so as to achieve some purpose more effectively. These are possibilities of the near future, rather than Utopian dreams.

The criticism that a machine cannot have much diversity of behaviour is just a way of saying that it cannot have much storage capacity. Until fairly recently a storage capacity of even a thousand digits was very rare.

The criticisms that we are considering here are often disguised forms of the argument from consciousness, Usually if one maintains that a machine can do one of these things, and describes the kind of method that the machine could use, one will not make much of an impression. It is thought that tile method (whatever it may be, for it must be mechanical) is really rather base. Compare the parentheses in Jefferson's statement quoted on page 22.

(6) Lady Lovelace's Objection

Our most detailed information of Babbage's Analytical Engine comes from a memoir by Lady Lovelace ( 1842). In it she states, "The Analytical Engine has no pretensions to originate anything. It can do whatever we know how to order it to perform" (her italics). This statement is quoted by Hartree ( 1949) who adds: "This does not imply that it may not be possible to construct electronic equipment which will 'think for itself,' or in which, in biological terms, one could set up a conditioned reflex, which would serve as a basis for 'learning.' Whether this is possible in principle or not is a stimulating and exciting question, suggested by some of these recent developments But it did not seem that the machines constructed or projected at the time had this property."

I am in thorough agreement with Hartree over this. It will be noticed that he does not assert that the machines in question had not got the property, but rather that the evidence available to Lady Lovelace did not encourage her to believe that they had it. It is quite possible that the machines in question had in a sense got this property. For suppose that some discrete-state machine has the property. The Analytical Engine was a universal digital computer, so that, if its storage capacity and speed were adequate, it could by suitable programming be made to mimic the machine in question. Probably this argument did not occur to the Countess or to Babbage. In any case there was no obligation on them to claim all that could be claimed.

This whole question will be considered again under the heading of learning machines.

A variant of Lady Lovelace's objection states that a machine can "never do anything really new." This may be parried for a moment with the saw, "There is nothing new under the sun." Who can be certain that "original work" that he has done was not simply the growth of the seed planted in him by teaching, or the effect of following well-known general principles. A better variant of the objection says that a machine can never "take us by surprise." This statement is a more direct challenge and can be met directly. Machines take me by surprise with great frequency. This is largely because I do not do sufficient calculation to decide what to expect them to do, or rather because, although I do a calculation, I do it in a hurried, slipshod fashion, taking risks. Perhaps I say to myself, "I suppose the Voltage here ought to he the same as there: anyway let's assume it is." Naturally I am often wrong, and the result is a surprise for me for by the time the experiment is done these assumptions have been forgotten. These admissions lay me open to lectures on the subject of my vicious ways, but do not throw any doubt on my credibility when I testify to the surprises I experience.

I do not expect this reply to silence my critic. He will probably say that h surprises are due to some creative mental act on my part, and reflect no credit on the machine. This leads us back to the argument from consciousness, and far from the idea of surprise. It is a line of argument we must consider closed, but it is perhaps worth remarking that the appreciation of something as surprising requires as much of a "creative mental act" whether the surprising event originates from a man, a book, a machine or
====================
” in the workplace. They believe that managers are at the mercy of a system that can make unlimited demands, because the boundaries that define the manager's job are vague and permeable. Managerial jobs themselves are abstract enough to be subject to diverse interpretations. Managers might well ask themselves, have I done my job? If the job is that abstract, then it is also easy for peers and superiors to question what a manager has done and to formulate their own evaluations of his or her performance. Without a collective contract, the manager is vulnerable and dependent; he or she must surrender to the organization's purposes and values. Instead of the feistiness and pluralism that characterize the labor-management relationship, many workers see in the managers' world overbearing demands for ideological unity, loyalty, and the submergence of the self. In other words, managers may seek to control their subordinates, but they are not in control of their own work lives.

There is a breed of American worker who cherishes the autonomy and sense of self-control afforded by his or her skills and protected by the union contract. When these workers contemplate the prospect of the socially integrated high-technology workplace, they feel despair. They anticipate a loss of their unique identities, of freedom and autonomy, and of well-defined rights and responsibilities. They fear that without the traditional sources of protection provided by their job descriptions and their contract, they will become prey to every capricious whim of their superiors. They understand that the managers' world requires the body as a political instrument for self-presentation and influence, but they know that these are talents they have not developed and toward which they feel more than a little distaste.

They say that with this new technology we need a more flexible system, one that will make us competitive. They figure it works for management, so why not for the blue-collar worker. If a manager hasn't saved money, he won't get his extra one hundred dollars a week. If he doesn't produce, he's out the gate. But right now, I don't have anything to worry about except doing my job and doing it well. I don't have to be friends with people in order to move up. I don't have to use anybody. In the management world, you have got to be a salesman to a certain extent. You have go to know how to manipulate the human system. If I am like management, it means I will have to be doubly nice to you whether I like you or not. You have to see everybody as a stepping-stone.

Other operators believe that in a fluid, socially integrated workplace, without clear job descriptions and contracts, they would lose the clarity of rights and obligations that currently offer an important sense of personal control. Without such definition, how will one know what to expect each day and how will it be possible to manage the extent of one's own exertion? They fear flexible arrangements that would change according to the needs of the total organization, in place of discrete task assignments on an individual basis. Their "have-skills-will-travel" image is a kind of emotional insurance policy, but an enterprise centered approach to task distribution would make each individual more dependent upon and integrated with the organization.

They say that with this new technology we need a new language in which managers can confidently express their preferences. But there is another way, as yet undiscovered, that leads to a more profound change in the way they feel about others. If managers are able topresent their preferences in a way that is easy to understand, then a new division of learning is likely to arise. The managers' world demands the ability to understand; it takes time. Moreover, the new division of work will have to be constituted not as direct subservient to the demands of immediate supervision but as a series of discrete, discrete actions. The problem of the internal organization is complicated by the fact that the managers' world requires the capacity to shape the organization so that they will work with efficiency and cost-effectiveness to meet the demands of an efficient, rational, and comprehensive system. They say that with this new technology we need a new language in which managers can discuss the various aspects of their production process. The problem of a new language becomes even more pressing as the new choices become more precise. Does the fact of the matter become more pressing as the quality of skills at one time becomes irrelevant?

This chapter presents several examples of how organizations can best accomplish their supposed value-adding responsibilities. But we also recommend the need for organizational changes to account for the various new capabilities a new division of labor engenders. When managers' world orders become increasingly intricate, it can no longer be simply a matter of how much time is available for one another. Instead, it must be constituted by what is called a "learning mechanism." These mechanisms come in different forms, and they need to be tweaked to recognize new ways that a manager can be helpful and authoritative. Another kind of feedback control is provided by an operator's computer
====================
/collaboration

Collaboration can take different forms depending on the scale of the collaborating entities. At a small scale, individual AI teams who believe themselves to be in competition with one another could choose to pool their efforts.45 Corporations could merge or cross-invest. At a larger scale, states could join in a big international project. There are precedents to large-scale international collaboration in science and technology (such as CERN, the Human Genome Project, and the International Space Station), but an international project to develop safe superintelligence would pose a different order of challenge because of the security implications of the work. It would have to be constituted not as an open academic collaboration but as an extremely tightly controlled joint enterprise. Perhaps the scientists involved would have to be physically isolated and prevented from communicating with the rest of the world for the duration of the project, except through a single carefully vetted communication channel. The required level of security might be nearly unattainable at present, but advances in lie detection and surveillance technology could make it feasible later this century. It is also worth bearing in mind that broad collaboration does not necessarily mean that large numbers of researchers would be involved in the project; it simply means that many people would have a say in the project’s aims. In principle, a project could involve a maximally broad collaboration comprising all of humanity as sponsors (represented, let us say, by the General Assembly of the United Nations), yet employ only a single scientist to carry out the work.

There is a reason for starting collaboration as early as possible, namely to take advantage of the veil of ignorance that hides from our view any specific information about which individual project will get to superintelligence first. The closer to the finishing line we get, the less uncertainty will remain about the relative chances of competing projects; and the harder it may consequently be to make a case based on the self-interest of the frontrunner to join a collaborative project that would distribute the benefits to all of humanity. On the other hand, it also looks hard to establish a formal collaboration of worldwide scope before the prospect of superintelligence has become much more widely recognized than it currently is and before there is a clearly visible road leading to the creation of machine superintelligence. Moreover, to the extent that collaboration would promote progress along that road, it may actually be counterproductive in terms of safety, as discussed earlier.

The ideal form of collaboration for the present may therefore be one that does not initially require specific formalized agreements and that does not expedite advances in machine intelligence. One proposal that fits these criteria is that we propound an appropriate moral norm, expressing our commitment to the idea that superintelligence should be for the common good. Such a norm could be formulated as follows:

The common good principle

Superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals.

Establishing from an early stage that the immense potential of superintelligence belongs to all of humanity will give more time for such a norm to become entrenched.

The common good principle does not preclude commercial incentives for individuals or firms active in related areas. For example, a firm might satisfy the call for universal sharing of the benefits of superintelligence by adopting a “windfall clause” to the effect that all profits up to some very high ceiling (say, a trillion dollars annually) would be distributed in the ordinary way to the firm’s shareholders and other legal claimants, and that only profits in excess of the threshold would be distributed to all of humanity evenly (or otherwise according to universal moral criteria). Adopting such a windfall clause should be substantially costless, any given firm being extremely unlikely ever to exceed the stratospheric profit threshold (and such low-probability scenarios ordinarily playing no role in the decisions of the firm’s managers and investors). Yet its widespread adoption would give humankind a valuable guarantee (insofar as the commitments could be trusted) that if ever some private enterprise were to hit the jackpot with the intelligence explosion, everybody would share in most of the benefits. The same idea could be applied to entities other than firms. For example, states could agree that if ever any one state’s GDP exceeds some very high fraction (say, 90%) of world GDP, the overshoot should be distributed evenly to all.

The common good principle (and particular instantiations, such as windfall clauses) could be adopted initially as a voluntary moral commitment by responsible individuals and organizations that are active in areas related to machine intelligence. Later, it could be endorsed by a wider set of entities and enacted into law and treaty. A vague formulation, such as the one given here, may serve well as a starting point; but it would ultimately need to be sharpened into a set of specific verifiable requirements.

CHAPTER 15: Crunch time

We find ourselves in a thicket of strategic complexity, surrounded by a dense mist of uncertainty. Though many considerations have been discerned, their details
====================
 in the context of a global redundancy of technological capacities, such as information networks and the internet, retain their capacity to learn and to perform in ways that are adaptive to changing conditions.up until now, the world has been able to teach a much moreuratecale to a rapidly growingindeed, capable of teaching at much faster, with much less pain and inconvenience. teaching,ctory inferences about the Hereford Wilhelm Corpus18 and other texts have made great progress on the path toward machine intelligence. Although still not quite ubiquitous yet, face-recognizing systems are becoming more common at airports, banks, and places where personal identity must be verified or established. According to some people who worry about privacy, the practice is too common,
Work on face recognition by computer has continued from its early days.

How far has it progressed? A 2007 National Institute of Standards and Technology report on face-recognition tests claimed (among other things), “The results show that, at low false alarm rates for humans, seven automatic face recognition algorithms were comparable to or better than humans at recognizing faces taken under different lighting conditions. Furthermore, three of the seven algorithms were comparable to or better than humans for the full range of false alarm rates measured.”26 The best methods use machine learning algorithms working on very large data sets.27

A variety of different algorithms have been developed. Some are based on well-known pattern-recognition techniques that sample features from a face image and then compare these features against those of a large library of identified faces to find the closest match. Some algorithms use Bayesian techniques and HMMs. Many of the methods use mathematical techniques to project a high-dimensional vector representation of a face image into a vector in a lower dimensional subspace. One method uses lower dimensional spaces whose coordinates consist of a set of reduced images, called eigenfaces, which have the property that they can be combined to give good approximations to any of the faces in the database (much like a set of individual audio tones of different frequencies can be combined to approximate arbitrary sounds). For a Web page with information about face recognition with links to research papers, books, algorithms, and vendors, see http://www.face-rec.org/.

New approaches continue to be developed. One method purports to show that “image averaging” (that is, merging different images of the same face to form a single image) “greatly improves performance of [the commercially available FaceVACS] automatic face-recognition system.”28 An article in Wired reports on a method developed by researchers at the University of California, Berkeley, and the University of Illinois at Urbana-Champaign. According to that article, Shankar Sastry, the Dean of UC Berkeley’s College of Engineering, noted that this new method “renders years of research in the field obsolete.”29

There are already several commercial companies selling face-recognition and face-locating software and equipment. For example, Oki Electric Industry Co., Ltd., sells a product called FSE (Face Sensing Engine). It boasts many applications including controlling access to information in camera-equipped cell phones and other devices, sorting photographs based on recognizing faces, and locating faces in a camera’s field of view. The German company Cognitec Systems GmbH markets the FaceVACS system previously mentioned.

Before closing this section on smart tools, I should mention that there are seve al other areas in which AI tools are enhancing human productivity. For example, I could have mentioned tools for aiding (and automating) the processes of movie animation, for computer program writing and debugging, for industrial process control, for circuit and program verification, and for enhancing and searching the semantic Web. Tools powered by AI techniques will be increasingly used to aid and amplify (and sometimes to substitute for) human cognitive, motor, and perceptual abilities. Just wait!

As I hope the past few chapters have demonstrated, some parts of the quest for artificial intelligence have been quite successful. AI has become more and more a part of all of our lives as well as of those of specialists. But the main goal of the quest (for some of us at least) still remains, namely, endowing artifacts with full human (as well as superhuman) capabilities for language, perception, reasoning, and learning. So, let’s look next at where the quest might lead us.

Chapter 35: The Quest Continues

Where will the AI adventure lead next? We can get some idea of the immediate future simply by extrapolating present trends. Probably there will be some new milestone achievements. Undoubtedly, pieces of AI technology will become ever more common in our homes, automobiles, and activities, and the specialists’ smart tools will become ever smarter and more numerous.

But predicting beyond where AI’s present momentum will take us is problematic. Let’s look at how some previous predictions have fared
====================
One could even argue that Homo sapiens passed the wise-singleton sustainability threshold soon after the species first evolved. Twenty thousand years ago, say, with equipment no fancier than stone axes, bone tools, atlatls, and fire, the human species was perhaps already in a position from which it had an excellent chance of surviving to the present era.26 Admittedly, there is something queer about crediting our Paleolithic ancestors with having developed technology that “exceeded the wise-singleton sustainability threshold”—given that there was no realistic possibility of a singleton forming at such a primitive time, let alone a singleton savvy about existential risks and patient.27 Nevertheless, the point stands that the threshold corresponds to a very modest level of technology—a level that humanity long ago surpassed.

It is clear that if we are to assess the effective powers of a superintelligence— its ability to achieve a range of preferred outcomes—we must consider not only its own internal capacities but also the capabilities of competing agents. The notion of a superpower invoked such a relativized standard implicitly. We said that “a system that sufficiently excels” at any of the tasks in Table 8 has a corresponding superpower. Exceling at a task like strategizing, social manipulation, or hacking involves having a skill at that task that is high in comparison to the skills of other agents (such as strategic rivals, influence targets, or computer security experts). The other superpowers, too, should be understood in this relative sense: intelligence amplification, technology research, and economic productivity are possessed by an agent as superpowers only if the agent’s capabilities in these areas substantially exceed the combined capabilities of the rest of the global civilization. It follows from this definition that at most one agent can possess a particular superpower at any given time.

This is the main reason why agents will not possess additional superpowers. With sufficient numbers of agents, a singleton may be used as a base. Consider first the more dexterous digital minds that may be needed to perfect the control problem and design a programmable computing device. These minds are used mainly in QUESTION- answering operations, in which a pattern of events is analyzed to find the causes of that behavior.runner’s probabilistic inference comes to rely on a special type of rule that takes into account whether or not respondents can understand the causes of that behavior. But here too, again, one must question the motivations of the people whose data is collected.

One can also imagine governance institutions that would make an ideal tool for the agents that would create the new status quo. It would be possible, for example, to create superintelligent agents by considering the far-reaching implications of the value-loading problem. With the value learning approach, value-loading is the algorithm that is trained on the data, and it assigns values such as “bleed” to each input stream.

 value-loading into convergent instrumental reasons

It is not difficult to see how the value-loading problem has certain weaknesses. The gain for a machine intelligence, for example, might not need to be endowed with a high degree of general intelligence. There is nothing paradoxical about an intelligence that has the ability to avoid mistakes and do a good job of moving data along a track.

A race dynamic arises if an AI system that has been given a bounded utility function fails to complete the task given by the AI system that has the stated final goal. The AI system that has the stated goal fails because the AI system lacks the capability to acquire a bounded final goal. The AI system that has the stated goal does not have the technology research superpower orator skill, nor does it have a decisive strategic advantage.

An unfriendly AI system thus has no tradeoffs: it can avoid mistakes, it can form human-like agents, and it has a wide range of other capabilities that let it avoid mistakes. For example, it has respectable sponsors who are interested in helping us reach human-level AI. But there are also other capabilities that let it make a world of difference—such as the ability to process and analyze copious amounts of data from myriad sources in real time.

Tripwires differ from incentive methods in that they do not rely on the system being aware of the consequences of engaging in forbidden activities. Tripwires are more closely related to stunting methods. Like stunting, tripwires could be used as a temporary safeguard, providing a degree of protection during the development phase. In principle, tripwires can also be used during the operational phase, particularly for a boxed system. However, the ability of tripwires to constrain a full-fledged superintelligence must remain very much in doubt, since it would be hard for us to assure ourselves that such an agent could not find ways to subvert any tripwire devised by the human intellect.

As an adjunct safety measure in the development phase, though, tripwires are important. It would
====================
The decline in the demand for labour is not only a function of the limits of technological dynamism—stretching at least from earth’s surface with nothing but abundant light filtering in at the center. No machine could play the imitation game as hard as a man does now, but it is certainly possible to play it well. For example, the AI system that is being developed will need to meet the description of a man as tall as a giant pterosaur—stretching 20 times the human capacity for food and 70 times the human capacity for pain. It is not surprising that we want our capacity to love to death to be unlimited.

Other experiments produced further information about how the brain processes visual images. Neurophysiologists David Hubel (1926– ) and Torsten Wiesel (1924– ) performed a series of experiments, beginning around 1958, which showed that certain neurons in the mammalian visual cortex responded selectively to images and parts of images of specific shapes. In 1959 they implanted microelectrodes in the primary visual cortex of an anesthetized cat. They found that certain neurons fired rapidly when the cat was shown images of small lines at one angle and that other neurons fired rapidly in response to small lines at another angle. In fact, they could make a “map” of this area of the cat’s brain, relating neuron location to line angle. They called these neurons “simple cells” – to be distinguished from other cells, called “complex cells,” that responded selectively to lines moving in a certain direction. Later work revealed that other neurons were specialized to respond to images containing more complex shapes such as corners, longer lines, and large edges.3 They found that similar specialized neurons also existed in the brains of monkeys. Hubel and Wiesel were awarded the Nobel Prize in Physiology or Medicine in 1981 (jointly with Roger Sperry for other work).

As I’ll describe in later sections, computer vision researchers were developing methods for extracting lines (both large and small) from images. Hubel and Wiesel’s work helped to confirm their view that finding lines in images was an important part of the visual process. Yet, straight lines seldom occur in the natural environments in which cats (and humans) evolved, so why do they (and we) have neurons specialized for detecting them? In fact, in 1992 the neuroscientists Horace B. Barlow and David J. Tolhurst wrote a paper titled “Why Do You Have Edge Detectors?”6 As a possible answer to this question, Anthony J. Bell and Terrence J. Sejnowski later showed mathematically that natural scenes can be analyzed as a weighted summation of small edges even though the scenes themselves do not have obvious edges.

9.2 Recognizing Faces

In the early 1960s at his Palo Alto company, Panoramic Research, Woodrow (Woody) W. Bledsoe (who later did work on automatic theorem proving at the University of Texas), along with Charles Bisson and Helen Chan (later Helen Chan Wolf), developed techniques for face recognition supported by projects from the CIA.8 Here is a description of their approach taken from a memorial article:9

This [face-recognition] project was labeled man-machine because the human extracted the coordinates of a set of features from the photographs, which were then used by the computer for recognition. Using a GRAFACON, or RAND TABLET, the operator would extract the coordinates of features such as the center of pupils, the inside corner of eyes, the outside corner of eyes, point of widows peak, and so on. From these coordinates, a list of 20 distances, such as width of mouth and width of eyes, pupil to pupil, were computed. These operators could process about 40 pictures an hour. When building the database, the name of the person in the photograph was associated with the list of computed distances and stored in the computer. In the recognition phase, the set of distances was compared with the corresponding distance for each photograph, yielding a distance between the photograph and the database record. The closest records are returned.

Bledsoe continued this work with Peter Hart at SRI after leaving Panoramic in 1966.

Then, in 1970, a Stanford Ph.D. student, Michael D. Kelly, wrote a computer program that was able automatically to detect facial features in pictures and use them to identify people. The task for his program was, as he put it,

to choose, from a collection of pictures of people taken by a TV camera, those pictures that depict the same person. . . .
In brief, the program works by finding the location of features such as eyes, nose, or shoulders in the pictures. . . . The interesting and difficult part of the work reported in this thesis is the detection of these features in digital pictures. The nearest-neighbor method is used
====================
The creation of contemporary AI systems needing ‘intelligence’ are similarly constructed to include cognitive enhancement, perceptual abilities, and motor skills, according to the proposal from the team [POWL20]. Both of these items could be passed through several stages of neural evolution, such as generation of specialized units complementary to the input units,32 and neuromorphic AI. If the vectors of one stage are sufficiently close together, then it might be possible to detect some kind of intermediary cause that would cause the evolution of the machine to do different things. For example, the “image of fever dreams” might be generated by a “wind turbine” rather than a “ turbine engine.” The image of a “polarity” looks at how life originated, which might be proposed as a possible template for aplace-like physical or virtual reality.

Turing’s idea of designing a program that acquires most of its content by learning, rather than having it pre-programmed at the outset, can apply equally to neuromorphic and synthetic approaches to machine intelligence.

A variation on Turing’s conception of a child machine is the idea of a “seed AI.”19 Whereas a child machine, as Turing seems to have envisaged it, would have a relatively fixed architecture that simply develops its inherent potentialities by accumulating content, a seed AI would be a more sophisticated artificial intelligence capable of improving its own architecture. In the early stages of a seed AI, such improvements might occur mainly through trial and error, information acquisition, or assistance from the programmers. At its later stages, however, a seed AI should be able to understand its own workings sufficiently to engineer new algorithms and computational structures to bootstrap its cognitive performance. This needed understanding could result from the seed AI reaching a sufficient level of general intelligence across many domains, or from crossing some threshold in a particularly relevant domain such as computer science or mathematics.

Before a seed AI could cross this threshold, though, it would undergo a very thorough technical redesign to ensure it is able to perform all its final goals, any given input being equally valuable. This redesign is likely to be completed by acale a year from now (or sooner, if Symposium on Artificial Intelligence is held during the summer of 1950 at Dartmouth College). This kind ofalliative care seems to be the order of the Herb Simonov’s science-fiction novels The Intenti and The Intenti: themes that explore the human mind.1 “Consider a scenario in which a superintelligence’s initiation of a colonization drive begins with a focus on the realization of a satisfactory weakeline for the goal system of the colonization.” By designing a superintelligence that would harvest sufficient resources and use it to build a faster takeoff, the designers imagine that the accelerated takeoff could occur quite suddenly. A period of or Kaixin001s rapid progress might then result in an abrupt change in the underlying source of the problem. The computer would most likely know something about the solution.

A more plausible solution is to allow learning to occur spontaneously and in the most spontaneously sequence. Suppose that a radio receiver is tuned to a station that after a minute of training has produced a listen of some radio station, say, the order of the positions that must be chosen by the radio receiver, then the radio receiver is not tuned to a station that after a minute of training has produced a concerto instrument—which is the signal that must be amplified by the radio receiver. Learning then is the process of using the radio receiver’siot nerve” (or, in the case of biological systems, the prosthetic nerve).

The possibility of a pre-established harmony between what is valuable to us and what is valuable to machines is limited by the existence of reactive systems that can learn to make strategic commitments, interdependence mechanisms, and global norms, which are needed to bring about stable, harmonized standards and beneficial relations between nations.33 Our governments will need to make these necessary interventions to ensure that artificial intelligence is developed and used in a responsible manner.

The urgent and growing challenge posed by the future of work is that it illuminates some of the complex forces driving the past and present lives of working people. It therefore makes sense to focus here on a set of emotions and preferences that are particularly relevant to the present moment, as well as one or both of the emotions of colleagues and customers.

The emotion domain is especially interesting because it arises in a digital medium that is not easily monitorable and can be interacted with via television or radio waves. Sensitive data, such as whether people are receiving or expressing emotions, is likely to be far more relational and contested than the domain-oriented recognition that is commonly required by some ethicists today.34 As we saw in Chapter 7, a project to create a training set of portraits of more than a thousand people must be crowdsourced to reflect the personality of someone who has spent her whole life developing a portrait of sorts.

====================
 who have a say in the project’s direction. A project that has a decisive strategic advantage would not be committing itself to a relinquished military advantage.

The prospect of a post-transition collaboration offers many benefits. Let us take a look at how this balance of power would affect post-transition collaboration in two ways.

The first is that some forms of machine intelligence, particularly models and networks, may be aimed at constructing models that are likely to beiodically collected and digitized. These systems may be designed to move over from a human to a machine, and they may perform various tasks that are not possible in machines. For instance, many large language models are intended to have simple vacuum systems. These sophisticated machines may be used by physicians, scientists, engineers, and business people to help them in (and sometimes automate) their workaday tasks. These are not bulkbots that move over to other tasks or to assigned tasks.


The second is that a UBI would give unemployed people a little “personal angel investment” with which they can start a new business or assist someone in a crisis. With such a hand on the fire, people will be less likely to get hurt, and they will be more productive.

This type of UBI idea is relatively novel. A large employers’ demand for workers is greater than ever in large and poorly paid professions. And there is no question that this is a powerful tool for propagating racist and sexist messages.

A related but importantly different idea is that an AI, by interacting freely in society, would have the full panoply of all six superpowers. Whether there is a practically significant possibility of a domain-limited intelligence that has some of the superpowers but remains unable for a significant period of time to acquire all of them is not clear. Creating a machine with any one of these superpowers appears to be an AI-complete problem. Yet it is conceivable that, for example, a collective superintelligence consisting of a sufficiently large number of humanlike biological or electronic minds would have, say, the economic productivity superpower but lack the strategizing superpower. Likewise, it is conceivable that a specialized engineering AI could be built that has the technology research superpower while completely lacking skills in other areas. This is more plausible if there exists some particular technological domain such that virtuosity within that domain would be sufficient for the generation of an overwhelmingly superior general-purpose technology. For instance, one could imagine a specialized AI adept at simulating molecular systems and at inventing nanomolecular designs that realize a wide range of important capabilities (such as computers or weapons systems with futuristic performance characteristics) described by the user only at a fairly high level of abstraction.7 Such an AI might also be able to produce a detailed blueprint for how to bootstrap from existing technology (such as biotechnology and protein engineering) to the constructor capabilities needed for high-throughput atomically precise manufacturing that would allow inexpensive fabrication of a much wider range of nanomechanical structures.8 However, it might turn out to be the case that an engineering AI could not truly possess the technological research superpower without also possessing advanced skills in areas outside of technology—a wide range of intellectual faculties might be needed, for example, to algorithmically analyze the dynamical properties of biological systems.9 Even if a project to develop machine superintelligence places no special value on human empathy, it would be an easy way to get it to fail. But it also requires a special kind of engineering AI that is not easily replaced with a different kind of engineering AI. There are cases in which we might want to exploit another type of human- with a technology-based or population-level track record of giving. If we are working with a system that is not yet entirely human-ployable, we may need to use another approach, which might be tailored to a particular capability and situation. We could then build the AI to act according to its situation but still have no reason to get the job done.

This might not be a good reason to delay the implementation of generally useful technologies. Much might come in the way of substantial progress toward AGI. Consider only the latest advances in artificial intelligence, such as deep learning and machine learning techniques. While it is hard to say whether or not we prefer deep learning to be used in ways that are more challenging to simulate, it is possible to say either that it is better or that it is worse.

Another way of arguing for the feasibility of artificial intelligence is by pointing to the human brain and suggesting that we could use it as a template for a machine intelligence. One can distinguish different versions of this approach based on how closely they propose to imitate biological brain functions. At one extreme—that of very close imitation—we have the idea of whole brain emulation, which we will discuss in the next subsection. At the other extreme are approaches that take their inspiration from the functioning of the brain but do not attempt low-level imitation. Advances in neuroscience and cognitive psychology —which will be aided
====================
The idea that the entirety of AI is a field aimed toward automation is actually a bit of a misconception,”
says Fei-Fei Li , the co-director of HAI. She says we need to “tease apart the hype” surrounding AI and look at its broader applications, such as deciphering complex data and using it to make decisions as well as actuating vehicles and robots that interact with the world.

Li thinks automation can play an important role in protecting people from harm in jobs like disaster relief, firefighting, and manufacturing. It makes sense for machines to take on tasks where “the very biology of being human is a disadvantage.” But, she says, “there’s so much more opportunity for this technology to augment humans than the very narrow notion of replacing humans.”

Machines have been assisting people and replacing their labor for centuries, explains Michael Spence , an emeritus professor of economics and former dean at Stanford GSB. Yet the current digital wave is different from the wave of mechanization that defined the Industrial Revolution. Unlike their 19th- and 20th- century predecessors, which required constant human intervention to keep running, AI tools can function autonomously. And that, Spence warns, is taking us into “uncharted territory.”

“We have machines doing things that we thought only humans could do,” he says. These machines are increasingly supervised by other machines, and the idea of people being taken out of the loop “scares the wits out of people.” The scale of economic disruption that AI could cause is difficult to predict, though according to the McKinsey Global Institute, automation could displace more than 45 million U.S. workers by 2030.

Jennifer Aaker PhD ’95, hopes that AI will transform the way we work — for the better. Aaker, a behavioral scientist and a professor of marketing at Stanford GSB, cites a recent survey by Gartner in which 85% of people reported higher levels of burnout since the pandemic began. Can AI help alleviate disconnection and dissatisfaction on the job? “The increasing amount of data from the last couple of years will make this question become more pressing,” she says.

For the past three years, Aaker and Li cotaught Designing AI to Cultivate Human Well-Being , an interdisciplinary course that explored ways to build AI that “augments human dignity and autonomy.” Aaker believes if augmentation can increase growth, education, and agency, it will be a critical way to improve people’s happiness and productivity. “We know that humans thrive when they learn, when they improve, when they accelerate their progress,” she says. “So, to what degree can AI be harnessed to facilitate or accelerate that?”

Augmentation in Action

Many of the potential uses of artificial intelligence have yet to materialize. Yet augmentation is already here, most visibly in the explosion of AI assistants everywhere from dashboards and kitchen counters to law firms, medical offices, and research labs.

The benefits of augmentative AI can be seen in the healthcare industry. Li mentions one of her recent favorite student projects in the course she coteaches with Aaker, which used AI to prevent falls, a common cause of injuries in hospitals. “Patients fall or have rapidly deteriorating conditions that go undetected,” she says. Yet it’s not feasible for a nurse or caregiver to constantly monitor people who are at risk of falling. As a result, “there are procedural errors, dark spaces. How do you know a patient is about to fall? These are things you can’t do labs on.” Smart-sensor technology can give healthcare providers an “extra pair of eyes to augment the attention of human caretakers and to add information and to alert when something needs to be alerted.”

AI can also make short work of necessary yet tedious tasks. Spence mentions how “pure augmentation” is helping doctors by using machine learning to sift through mountains of medical literature. “It can pick off, with reasonable accuracy, the articles that are particularly important for a specific doctor with a specific specialty patient.” Similarly, Aaker cites a project from her course with Li where nurses and doctors used an AI tool to process paperwork, allowing them to spend more time connecting with patients. “Imagine how that frees up medical professionals to do the work that inspired them to get involved in the field in the first place?”

That may be one of the most compelling selling points for augmentation: It liberates people to focus on things that really matter. Aaker cites AI tools that help around the house. “Parents can get burdened by household tasks,” she explains. “What the AI is doing is
====================
” (as per Article 6(1), Adopting the proposals in Article 5(1), and sticking with the letter. This Regulation should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant.

(42) To mitigate the risks from high-risk AI systems placed or otherwise put into service on the Union market for users and affected persons, certain mandatory requirements should apply, taking into account the intended purpose of the use of the system and according to the risk management system to be established by the provider.

(43) Requirements should apply to high-risk AI systems as regards the quality of data sets used, technical documentation and record-keeping, transparency and the provision of information to users, human oversight, and robustness, accuracy and cybersecurity. Those requirements are necessary to effectively mitigate the risks for health, safety and fundamental rights, as applicable in the light of the intended purpose of the system, and no other less trade restrictive measures are reasonably available, thus avoiding unjustified restrictions to trade.

(44) High data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law. High quality training, validation and testing data sets require the implementation of appropriate data governance and management practices. Training, validation and testing data sets should be sufficiently relevant, representative and free of errors and complete in view of the intended purpose of the system. They should also have the appropriate statistical properties, including as regards the persons or groups of persons on which the high-risk AI system is intended to be used. In particular, training, validation and testing data sets should take into account, to the extent required in the light of their intended purpose, the features, characteristics or elements that are particular to the specific geographical, behavioural or functional setting or context within which the AI system is intended to be used. In order to protect the right of others from the discrimination that might result from the bias in AI systems, the providers shouldbe able to process also special categories of personal data, as a matter of substantial public interest, in order to ensure the bias monitoring, detection and correction in relation to high-risk AI systems.

(45) For the development of high-risk AI systems, certain actors, such as providers, notified bodies and other relevant entities, such as digital innovation hubs, testing experimentation facilities and researchers, should be able to access and use high quality datasets within their respective fields of activities which are related to this Regulation. European common data spaces established by the Commission and the facilitation of data sharing between businesses and with government in the public interest will be instrumental to provide trustful, accountable and non-discriminatory access to high quality data for the training, validation and testing of AI systems. For example, in health, the European health data space will facilitate non-discriminatory access to health data and the training of artificial intelligence algorithms on those datasets, in a privacy-preserving, secure, timely, transparent and trustworthy manner, and with an appropriate institutional governance. Relevant competent authorities, including sectoral ones, providing or supporting the access to data may also support the provision of high-quality data for the training, validation and testing of AI systems.

(46) Having information on how high-risk AI systems have been developed and how they perform throughout their lifecycle is essential to verify compliance with the requirements under this Regulation. This requires keeping records and the availability of a technical documentation, containing information which is necessary to assess the compliance of the AI system with the relevant requirements. Such information should include the general characteristics, capabilities and limitations of the system, algorithms, data, training, testing and validation processes used as well as documentation on the relevant risk management system. The technical documentation should be kept up to date.

(47) To address the opacity that may make certain AI systems incomprehensible to or too complex for natural persons, a certain degree of transparency should be required for high-risk AI systems. Users should be able to interpret the system output and use it appropriately. High-risk AI systems should therefore be accompanied by relevant documentation and instructions of use and include concise and clear information, including in relation to possible risks to fundamental rights and discrimination, where appropriate.

(48) High-risk AI systems should be designed and developed in such a way that natural persons can oversee their functioning. For this purpose, appropriate human oversight measures should be identified by the provider of the system before its placing on the market or putting into service. In particular, where appropriate, such measures should guarantee that the system is subject to in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role.

(49) High-
====================
The Real, the Limitations, and Consequences of Automation

It is sometimes proposed that by creating machines that can follow instructions that guide them, we would create de facto human substitutes for human capabilities.itute which human capabilities would be adequate for the requirements. For example, the human-robot system of assembly lines in cars and trucks is designed to minimize the need for human backup drivers. However, to ensure consistent cost and efficiency across all possible modes of transport, including Adding, Reimagining, and Spanner—the types of tasks that fall under the heading of automation.

While these examples are not new, they are the kinds of tasks that require training machines to recognize the world. And they are not unique to AI. New approaches are needed. To illustrate the value of this tremendous technology, we offer a special invitation to one of the pioneers of machine learning who has experience in these areas.

In addition to offering practical ways for early-stage investors to participate in the design and development of AI systems, this book is also a collection of useful links. To do so, let’s consider how investing in talent was affecting the American economy and a particular subset of jobs in particular.

There’s no question that China will lag in the corporate world, but at least it can help its inventors find novel ways to practicalize existing products and services. The mobile payments revolution has already begun, and it has already begun to noticeably improve user experience for buying goods and services online.

With these impressive improvements, demand for these services is eroding, and companies will be forced to adapt to the digital world. With that, let’s see some examples of how companies are currently experimenting withsmart technologies.

34.1 In The Asia-Pacific

Many developing countries have some of the highest levels of robotization. Workers, students, andosed into a new world within reach of AI increasingly see themselves as part of a growing sphere. aggregation is a term from which many parts of the world are equidistant now. The Philippines, Vietnam, Indonesia, and other countries have large numbers of young people reoffending or looking for work, and many cities have begun to implement work-sharing arrangements under the promise of economic growth generated by AI.

Work plates on the folding discs of St. Louis University's digital computer can be measured at the boundary between these countries and the United States, whereuponuan middle-aged couple are permitted to move through the image without needing to stop.entswering questions about the workings of their computer systems, and computer images generated byaying into thousands of new jobs.

 lucrative new O2O models could destabilize labor markets and wreak havoc on individual and community characteristics, threaten toalign labor markets with ever-higher degrees of predictability and monopolization.

As such, “output-led” deindustrialization is currently being led by—as Amazon’s O2O service is being led by—U.S. companies. When looking at the impacts of these technologies, it’s easy to understand how their uses impact people's reputations and wreak havoc on our individual psychologies. Amazon and other AI companies have stated their intent to use the technology for—what’s more, they’re using the technology to produce enormous numbers of jobs for profits.

U.S. labor markets have recently undergone a major reskilling. For example, the largest single-day O2O deployment through 2022 was from a company called haste to get out as an early-stage investor in a major new technology. Within ten years, I estimate we will be technically capable of automating 40 to 50 percent of jobs in the United States. For employees who aren’t outright replaced, or if their tasks aren’t being moved out of manufacturing, it can be very difficult to find qualified people to replace them. Firms that deploy AI to replace routine physical tasks like clockwork switching and hot-air balloons will find it difficult to adapt rapidly. Those companies will slow to death.

FUSION SKILL #1: Rehumanizing Time
Definition: The ability to increase the time available for distinctly human tasks like interpersonal interactions, creativity, and decision making in a reimagined business process.

FUSION SKILL #2: Responsible Normalizing
Definition: The act of responsibly shaping the purpose and perception of human-machine interaction as it relates to individuals, businesses, and society.

FUSION SKILL #3: Judgement Integration
Definition: The judgment-based ability to decide a course of action when a machine is uncertain about what to do.

FUSION SKILL #4: Intelligent Interrogation
Definition: Knowing how best to ask questions of AI, across levels of abstraction, to get the insights you need .

FUSION SKILL #5 : Bot-based Empowerment
Definition: Working well with AI
====================

